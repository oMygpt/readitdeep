{
  "0a6aae26-e1da-42f9-80f6-19a4250dbeaa": {
    "id": "0a6aae26-e1da-42f9-80f6-19a4250dbeaa",
    "filename": "高等教育的数智变革：基本逻辑、趋势特点及实践应对_李雪.pdf",
    "file_path": "./uploads/papers/0a6aae26-e1da-42f9-80f6-19a4250dbeaa.pdf",
    "status": "completed",
    "title": "高等教育的数智变革:",
    "category": null,
    "markdown_content": "# 高等教育的数智变革:\n\n# 基本逻辑、趋势特点及实践应对\n\n李雪 李永强\n\n以ChatGPT、DeepSeek等为代表的智能科学与技术的加速迭代升级及其在人类生产生活中的广泛应用正在不断改变着人类的物质生产方式、知识创新逻辑、人际交往模式和组织行为方式，其对高等教育的影响、渗透和改变远远超出了一般意义上科学技术的教育领域应用层面，引发了高等教育人才培养目标、知识创新生态、课程教学范式和大学治理体系等全方位深层次变革。高等教育正呈现出人才培养目标从专业知识教育转向未来素养培养、知识创新生态从大学组织内部转向多元主体协同、课程教学范式从传统教学转向智慧教育、教育治理体系从技术赋能转向系统重构等许多新的趋势特点。在国家高等教育体系中占据重要地位的高水平研究型大学应深刻把握这些新的趋势特点，全面推进发展范式、创新生态、育人体系、治理体系和评价体系的系统性改革，为我国建成世界高等教育中心、更好地服务教育强国建设发挥好示范引领作用。\n\n关键词：高等教育；数智变革；基本逻辑；趋势特点；高水平研究型大学；应对策略\n\n中图分类号：F0-4；G642 文献标识码：A 文章编号：1003—5656(2025)07—0076—11\n\nDOI:10.16158/j.cnki.51-1312/f.2025.07.007\n\n# 一、引言与文献综述\n\n从历史来看，教育既是推动科学技术革命的重要力量，也是被科学技术不断重塑的重要对象。学校的诞生、文字的出现、印刷术的发明以及电子计算机等现代科学技术在教育领域中广泛运用被视为导致人类四次教育革命的标志性事件。科技革命对教育的影响是深层次、全方位甚至颠覆性的，特别是现代社会，从计算机问世、互联网普及到人工智能的超常崛起，科学技术的迅速发展及其在教育领域的广泛运用推动了教育从思想、内容、方式到体制的重大变革，从而重塑了教育在不同时代的独特形态。\n\n“当前，新一轮科技革命和产业变革蓄势待发，人工智能技术加速迭代，正迎来爆发式发展，深刻改变人类生产生活方式、知识供给模式和科研创新范式，进而重塑思维方式与观念，教育已经进入改变底层逻辑、重塑教育生态，资源共创分享、消弭数字鸿沟，素质能力重构、促进全面发展，全球开放合作、推动文明互鉴的智能时代。”[1]显然，以ChatGPT、DeepSeek等为代表的生成式人工智能大模型在高等教育领域的应用推广引起了广泛关注和讨论，这些讨论主要集中在三个方向：一是研究生成式人工智能在高等教育领域的应用场景、风险挑战与治理对策，[2-6]整体上表现出乐观的欢迎和展望，也分析了包括伦理风险在内的可能局限和隐患。[7-8]有研究者用“ChatGPT究竟是‘阿拉丁神灯’还是‘潘多拉魔盒’”[9]形象概括了“ChatGPT教育应用的潜能与风险”①，ChatGPT既具有赋能教学创新的潜能，也存在诸如学业诚信、过度依赖、伦理问题等风险，这种相对客观的分析在一定程度上既保持了理性的乐观，也警醒过度沉迷的危害。二是聚焦教学内容、教学方式、教师素质、教育治理等高等教育的某个方面、要素或环节来分析人工智能对高等教育的影响，[10-11]总体上朝着更加细化和具体的方向深入。三是深入以ChatGPT\n\n等为代表的生成式人工智能大模型的基本原理、设计理念、技术架构及未来发展去分析把握其对高等教育的深刻影响，[12-13]得出一些更具有科学和技术基础的结论，这类研究对研究者的人工智能素养具有较高的要求。\n\n已有研究开拓了人们对数智革命影响高等教育变革趋势特点的深入认识，其中一些观点、洞见以及相应的实践探索为本文的分析研究提供了很好的借鉴和参照。但我们同时注意到，当前研究视角更侧重于科学技术在教育领域的应用层面，即从教育技术学的视角来审视智能科学与技术对高等教育的影响，缺乏相对宏观的分析视角和系统的分析，尤其是对数智革命如何影响高等教育变革的底层或基本逻辑缺乏深入的分析，没有很好做到“跳出教育看教育”。高等教育作为社会大系统的一个子系统，与整个社会大系统及其他子系统存在着内在的本质联系，与人类的物质生产方式、知识创新逻辑、人际交往模式、组织行为方式等具有深度同构关系。因而，分析数智革命对高等教育的影响不能仅仅局限在教育系统内部，必须深入把握以人工智能为代表的数智革命如何影响高等教育变革的基本逻辑，分析其趋势特点并作出更有前瞻性和针对性的实践应对，而这正是本文关注和分析的重点所在。\n\n# 二、高等教育数智变革的基本逻辑\n\n“人工智能是新一轮科技革命和产业变革的重要驱动力量，加快发展新一代人工智能是事关我国能否抓住新一轮科技革命和产业变革机遇的战略问题。”[14]数智革命加速创新并快速融入经济社会发展各领域全过程，重塑千行万业的速度之快、辐射范围之广、影响程度之深前所未有。中国、美国、英国、德国、日本等世界主要强国都把加快人工智能的发展作为推动第四次科技革命和产业变革的突破口，相继出台了体现国家意志的重大战略规划和行动，抢占新一轮科技革命和产业变革的制高点。数智革命正在改变人类生活的方方面面，重塑人类社会的整体面貌，进而要求高等教育必须及时作出相应的回应和变革。\n\n# （一）数智革命改变了人类的物质生产方式\n\n“生产力质态的每一次演进和发展，都是以重大科技创新为主导，形成新的生产方式，引发生产力产生质变，推动新的产业变革和社会变革。”[15]2023年6—7月，全球著名咨询机构麦肯锡相继发布《生成式AI的经济潜力：下一个生产力前沿》《2023技术趋势展望》两份研究报告，[16-17]前者通过对47个国家及地区的850种职业（覆盖全球  $80\\%$  以上劳动人口）的深入研究探讨了人工智能发展对全球经济的潜在影响，预测2030年至2060年之间，将有  $50\\%$  的职业逐渐被人工智能所取代，使当前  $60\\% -70\\%$  的工作实现自动化；后者评估认为生成式人工智能(Generative AI)“已经成为一个响亮的入口，并已经显示出变革性商业影响的潜力……在应用性人工智能(Applied AI)和机器学习工业化(Industrializing machine learning)等现有技术的基础上，生成式人工智能(Generative AI)在大多数行业都具有很高的潜力和适用性”。2024年11月，国际权威分析机构沙利文(Frost & Sullivan)发布《2024年中国行业大模型市场报告》，[18]全面分析了包括工业大模型在内的多个领域中国行业大模型发展态势，百度文心、商汤日日新·商量、腾讯混元以及华为盘古等大规模预训练模型在各行业中广泛应用展现出强大的语言理解和生成能力以及跨领域的泛化能力，已经广泛渗透到金融、教育、医疗、电商、传媒、法律等领域，被用于智能客服、智能写作、自动摘要、文本生成、知识问答、个性化推荐等多个应用场景，有效提升行业服务效率和服务质量。\n\n当前，随着自动化系统和机器人在制造业、物流和服务领域的广泛应用，自动化、数字化、智能化已成为智能时代的重要趋势，这不仅大幅提升了生产效率，同时也显著降低了人力成本。从更广的范围来看，传统产业正在为适应科技变革趋势而大规模采用智能化技术，越来越多的传统工作岗位正被自动\n\n化控制系统所替代，同时一系列新产业、新形态、新模式加速涌现，新兴产业和未来产业正在成为发达国家争先抢夺的未来制高点，国家之间在科技领域的激烈竞争正好反映出全球科技生态与未来发展方向的新动向。这一系列变化预示着人类的物质生产方式正在被以人工智能为核心的数智革命所改变、重塑或催生，从而导致了社会生产力形态的革命性变化，催生新质生产力，并引发生产力全球布局、国家经济结构、世界商业模式、社会组织形态等深层次甚至是颠覆性的变革。高等学校作为为人类经济社会发展提供思想、知识、技术和人才的最重要机构，其学科专业的设立演进、教学内容的更新迭代以及课程体系的持续变革，无不映射着高等教育对科技革命及由其推动的产业变革的积极回应。因而，高等教育必须积极适应和引领这种生产力形态的变革趋势，在教育理念、发展战略、培养目标、学科专业布局等方面进行系统性的变革，从而形成适应数智革命的新的发展范式。\n\n# （二）数智革命重塑了人类的知识生产逻辑\n\n马克思认为，“符合社会全部需要的生产”应当包括“劳动力的生产”“物质的生产”和“精神的生产”。[19]26-53科学技术不只是物质生产力函数的重要要素，也是人类知识生产力函数的重要因素，是引发知识生产模式变革的决定性力量。随着数智技术的发展，人们越来越深刻地感知到，知识生产、创造与分享的模式和生态正在发生重大变化。首位华人图灵奖获得者、著名的计算机科学家姚期智说，“从20年前的人脸识别，到后来的下围棋，这些单一的，人类觉得困难的工作，人工智能可以做到超过人类。近年来，伴随大模型的出现，人工智能在语言能力上表现出非凡的智能，我们现在都感觉到，最好的大模型确实比普通人，甚至是受过高等教育的人显得更加聪明。”[20]OpenAI公司相继发布的ChatGPT、Sora和o4，基于对人类认知过程尤其是人脑机制的有限认知，设计了模拟神经网络的transformer架构，并通过参数扩张实现了对认知过程的更精准模拟，其超常学习能力所形成的更为广博的知识积累、能力整合等能更好地突破人脑的局限性，在诸多领域已经表现得比人类更加“智能”，在特定的功能上远超人类大脑。2025年初，国产开源大模型DeepSeek凭借其强大的语言处理、知识推理与专业文本生成能力迅速在国内掀起了一股人工智能热潮，其以自主训练、中文强化、国产算力适配为特点引发了国内众多高校纷纷跟进部署并进行应用开发。\n\n人类传统的知识生产在很大程度上是科学共同体依托学科进行的。人工智能能够跨越学科的界限，整合不同领域的知识和方法，实现对具体问题的多角度、多层次、多维度思考，这突破了传统的学科专业细分领域更加窄化和具体的限制，跨越了传统知识创新的学科界限的束缚，在一定程度上颠覆了人类传统的知识生产逻辑。可以预见的是，随着人工智能的不断迭代升级，其卓越的自然语言理解和生成能力使其可以处理多学科领域的知识，跨越传统学科边界并将不同领域的知识融合在一起，还可以根据用户需求和问题生成高度个性化的知识单元，使知识可以更细化，更有针对性，而不受传统学科框架的限制。近期，DeepMind发布了《A new golden age of discovery-Seizing the AI for Science opportunity》报告，从知识、数据、模型、实验、解决方案五个维度揭示了AI for Science的最新应用前景和结果。[21]2024年，“AI教父”Geoffrey Hinton博士与John Hopfield教授因“他们在机器学习领域的开创性贡献”获得了诺贝尔物理学奖，“AlphaFold之父”Demis Hassabis博士和John Jumper博士以及华盛顿大学David Baker却因为“其借助AI工具分别因蛋白质结构预测和计算蛋白设计的贡献”而获得了诺贝尔化学奖。因而，人工智能对人类知识的生产方式、工具、效率、结果及其呈现方式等都产生了深刻的影响，必然重塑高等教育的知识生产模式和创新生态，并推动高等教育内容生成方式、结果、效率的转变。\n\n# （三）数智革命重构了人类的人际交往模式\n\n人与自然、人与社会、人与自我是人必须处理的三种关系。人的社会性是人的本质属性，这一属性是在人的社会交往中形成并展现出来的，而人与人的对话是社会交往的重要形式和途径，人作为“对话\n\n中的人”是人的交往活动与属性的重要体现。习近平总书记指出，以信息技术、人工智能为代表的新兴科技快速发展，大大拓展了时间、空间和人们认知范围，人类正在进入一个“人机物”三元融合的万物智能互联时代。[22]数字技术发展出人的数字交往这种新的交往方式，而数字技术演进到人工智能大模型后，人与智能机器的对话则具有了人与人对话的功能，以大模型为对象的人机对话将深刻改变人的交往方式，这对人的社会性的形成和发展具有十分重要的意义，或使“人机对话中的人”成为“对话中的人”的新常态。[23]人工智能改变了传统的人际交往模式，特别是第三个智能变量“机”的加入，打破了人际交往的时空限制、虚实界限和结构方式。人工智能已经可以创造虚拟的环境，让人们有更丰富的社交体验。OpenAI“文生视频”大模型Sora展现了令人惊叹的视频生成效果，甚至在部分样片中还展现了对“物理规律”超强的学习能力。可以预料的是，随着人工智能技术的不断进步，在未来人类生活的各个领域必然是一个人与机器人相处的世界。\n\n大学的教育教学范式一定程度上是人际交往模式在高等教育领域的表现，教师的教与学生的学都受到社会人际交互模式的影响。过去许多年来，虽然人工智能常常有各种创新，但“人机交互”都不足以认为可以挑战人类的现场互动，广泛存在的主要互动模式还是师生二元之间的，直到能让机器“听懂人话”的自然语言处理技术被应用于生成式人工智能才从根本上改变人们对“人机交互”的理解，并逐步消解教育的互动性原则。[24]同时，信息技术的快速发展，已使教育的内涵不再仅仅局限于学校之中，移动学习、泛在学习等新型教育模式使得学习的控制权逐渐从教师、管理者转移到了学习者手中，从而动摇了诞生于大工业时代，以标准化、教导主义和教师控制来批量培养人才的现行教育体系。[25][126-141]新一代人工智能技术对教育系统的干预导致了人机协同教育模式的出现，这一变革引发了学习者的学习方式、认知方式和互动模式的转变，并有望塑造以学生为中心的学习生态。[26]显然，在智能科学与技术的加持下，人类的人际交互模式正在不断被重构，高等教育中传统的面对面教学和纸质教材逐渐被在线学习平台、虚拟教师和个性化学习应用所替代，这种改变推动了教学范式朝着更加灵活、多样化、个性化的方向发展，尤其是使得大规模的因材施教成为新的可能。\n\n# （四）数智革命改变了人类的组织行为方式\n\n人类社会每进入一个新的阶段，都会产生一种新的合作方式、一种新的组织模式，从而催生新的组织形态。从历史来看，从手工时代的家庭作坊到工业革命时期的工厂体系，从现代社会的企业组织到后工业时代的平台组织，每一次技术的飞跃都导致组织结构及其运行方式的重大调整，从单一形态到多元形态，社会组织形态的演变展现了其适应性和灵活性，不断满足社会经济发展的新需求。从这个变迁的历程中可以窥见，科技革命不只是社会生产效率提升的驱动力量，也是社会组织模式变革的驱动力量，更深刻地影响着社会组织的形态和运作方式。数智技术以其智能化、去中心化、自适应性等特征深刻影响组织成员的可塑性、组织结构的灵活性和组织控制的科学性，改变了人类的社会组织形态和行为方式。其智能化特性赋予了其处理大量信息、制定复杂决策并模仿人类智能的能力，这种智能化技术可用来自动执行和优化各种任务，进而提升工作效率与质量。其去中心化特征导致决策与控制不再依赖于单一的权威，这深深地影响了组织内权力的分配格局及组织结构的变化。其自适应性使其能够学习和适应新的环境与需求，这使得组织内的成员不得不为满足快速变化的新的工作要求而适应并利用这些新技术。因而，随着数智技术加速融入组织体系变革，必将导致组织结构调整、流程再造和文化重塑，结构将变得更加扁平化，管理流程更强调快速响应外部变化和内部高度协同，组织文化更强调组织、员工与服务对象之间的价值共创。\n\n大学本质上是由教师、学生、管理者构成的教育共同体。现代大学已经发展成为一个结构复杂、功能多样、属员众多的组织。从组织行为学的视角来看，一个组织在不同成长阶段的组织结构、领导方式、\n\n管理体制、员工心态都有其特点，组织变革伴随组织成长的各个阶段，不同成长阶段要求不同的组织模式与之相适应。由于大学教育是由国家、家庭、教师、学生、用人单位等多元主体共同完成的，不同的群体都对大学有着不同的期待，因而大学具有典型的利益相关者组织的属性，大学治理就是要解决能在冲突和多元利益状况下管理其一般事务的组织性框架及体制机制建设的问题。如何确保大学内部各个机构、系统之间功能定位明确、职能职责清晰、运转科学高效，大学的治理体系和治理能力尤为重要。由数智革命驱动的社会组织形态的变革必然对大学治理产生深远的影响，使得多元、多层级的教育利益相关者与多功能、多样态的智能机器合作，综合利用行政管理和技术支撑等治理手段，可以推动实现人机交互、优势互补、高效合作的现代化教育治理。[27]\n\n# 三、高等教育数智变革的趋势特点\n\n数智革命正在迅速改变人类的物质生产方式、知识生产逻辑、人际交往模式和组织行为方式，这些深层次的改变都会直接映射到高等教育领域，引发高等教育人才培养目标、知识创新生态、课程教学范式、内部治理体系等的系统性深层次变革，从而使得智能时代的高等教育呈现出新的趋势特点。\n\n# （一）人才培养目标从专业知识教育转向未来素养培养\n\n培养什么人、怎样培养人、为谁培养人是教育的根本问题。随着人工智能不断取得重要突破，高等教育应该培养什么样的人以适应科技革命和产业变革的需求是高等教育要思考解决的根本问题。有人认为应包括人工智能思维、创造创新能力、沟通能力、团队协作能力、提出问题的能力和在人工智能协助下的学习能力；[4]有人认为应更加注重凸显学生的个性化优势的品质，加强学生创新品质、情感品质、道德感、价值观，以及人工智能相关素质能力的培养，向复合型人才培养模式转型；[28]有人则强调了人工智能时代批判性思考和解决问题的重要性，认为学校教育的重心应从知识、技能和职业准备转向人工智能时代的适应性学习。[6]显然，大家越来越深刻地体会到，现代大学作为工业化时代的产物，一直致力于规模化地培养符合社会生产需求的人才，然而，其教育内容已和人工智能时代日益凸显的创新人才挖掘和培养需求产生矛盾，[29]传统的人才培养目标及与之相适应的培养模式已经难以适应一个高度智能化的未来社会对受教育者的需求。\n\n要适应、把握和引领智能时代人类物质生产方式、知识创新逻辑等变化所导致对未来社会人才素养需求的改变，高等教育就必须重塑自己的人才培养目标，致力于培养适应一个更加智能化时代所需的人的前瞻能力和未来素养，这至少包括：一是“显而易见”的智能素养，包括数据思维、智能技术、人机协同能力等；二是深度学习能力。深度学习能力是智能生命快速迭代的根本所在，这是以ChatGPT为代表的生成式人工智能给予我们的最大启示；三是对未来的良好适应和创造能力。数智技术加速融入人类生产生活各领域，也迅速放大了更大范围和更深层次的易变性、不确定性、复杂性和模糊性，这对学生的适应能力、批判性思维和创新能力的需求更甚以往。因而，除了让学生具备传统的知识、能力、素养外，最核心的是学生的智能素养、深度学习能力以及对未来的适应能力和创造能力，与之相应的人才培养标准、模式、机制和评价也要随之发生改变，高等教育应据此重构自身人才培养体系。\n\n# （二）知识创新生态从大学组织内部转向多元主体协同\n\n大学的核心功能是知识的传承、创造与传播，并以其生产创造的知识培养和造就社会所需的各类人才，因此大学的知识生产本质上是知识和知识生产者的生产与再生产的过程。就新知识的生产而言，现代社会还没有创造出任何可以与大学相提并论的机构。[30]2-89传统大学知识生产主要依靠学者的知识积累及创新转化，而生成式人工智能跳过了学科、专业、领域等的限制，直接切入知识本身，形成更加细化的知识单元，并导致这些曾经归属不同学科的知识单元形成新的知识体系，促进了跨学科和超学科\n\n的知识整合，更好地满足了人类认识复杂性问题的需要，导致知识创新的主体、科学研究的范式、以学科为基础的传统组织模式等都发生了改变，也会导致知识创新模式的重构以及新的知识体系的涌现，对人类知识的生产方式、效率、结果及其呈现方式等都产生了深刻的影响。\n\n高等教育必须前瞻数智革命对大学知识创新逻辑的影响，不断突破传统的知识生产模式及相应的专业教育局限。一是科研范式将发生转变，AI for Science的重要作用将得到充分的彰显，基于复杂神经网络的深度学习能力极大提高了知识生产效率和更新速度，自然科学领域实验验证和社会科学领域定量研究的内容和形式都在被改变，作为创新主体的教师的知识结构优化尤其是数智素养的提升将成为影响大学创新能力提升的关键。中山大学医学院、阿里云与悉尼大学的研究团队利用其开发的深度学习算法LucaProt发现了超过16万种RNA病毒，包括7万种首次发现的新病毒，揭示了大量前所未知的病毒“暗物质”[31]“人工智能的算法模型能够挖掘出我们之前忽略或根本不知道的病毒，这种能力在疾病防控和新病原的快速识别中尤为重要。特别是在疫情暴发时，人工智能的速度和精度可以帮助科学家更快地锁定潜在病原体。”[32]如果没有人工智能，这项工作是不可能想象的。二是知识生产组织模式将催生新的创新联合体，多学科、跨学科和超学科成为科学革命最重要的途径，知识生产必然在更大范围和更深层次上超越传统的学术共同体的范畴，一些新形态的创新联合体将成为推动知识创新的重要组织形式。三是基础设施建设将实现新的升级，传统实验室、教室等教学科研设施将不得不为适应数智革命而改变，以数据、算力、智能互联设施、大模型等为主体的新型公共基础设施建设将不得不加快，学科、专业、课程、教材等数智化转型将步入快车道。四是人类知识版图将得到极大拓展，人工智能自身的领域及其在众多已知和未知领域的广泛应用必将催生许多新的知识领域，从而重塑人类的知识版图。因而，高等教育必须从学科专业设置、科学研究范式转变、创新平台团队的组建等方面入手构建新的创新生态，进而推动教育内容生成方式、结果、效率的转变。\n\n# （三）课程教学范式从传统教学转向智慧教育\n\n大学存在的理由就是促进社会交往，“我们可以展望未来10到20年，大学的一部分功能是促进社交，另一部分则是帮助学生更全面地了解社会。”[33]实际上，教育千百年来常被认为是一项互动性的艺术，传统上教育领域中的互动特指人与人的互动，而“互动”对促进学生知识建构、能力提升、人格养成等方面有至关重要的意义。古希腊的学园时期，受制于信息传播方式、手段的影响，口耳相传是当时主要的教学范式；随着科技的进步，尤其是电子信息技术的进步，声光电热等各种现代科技的发展导致人际交互的场地局限、方式局限等不断被突破，大学开放与开放大学成为教育领域重要的形式，以慕课、可汗学院等为代表的高等教育领域的新型教学范式得到了发展。当前，以AI为牵引的“大数据+大算法+大模型”，让教育教学从大规模、标准化转向个性化、智能化，实现了规模化教学和个性化学习的有机统一。\n\n人工智能技术正在改变传统的工业时代的教与学的方式，智能时代教育新生态的重构已经开始，高等教育必须适应人工智能发展所导致的人际交互模式的改变对高等教育教学范式的影响。一是交互场域从现实空间走向虚实融合，泛在化、个性化自适应学习越来越受关注，智能教学软件、在线资源、虚拟现实技术等在教学中广泛应用，不仅仅能够极大拓展学生的学习空间，未来学习中心、未来课堂等越来越赋予学生学习更大的自主性，尤为重要的是能够满足学生的个性化差异的需求，设计更科学、更富吸引力的教育环境，使得大规模的因材施教成为可能。二是人际交互结构从二元转向三元，“机”不仅是中介变量，而且是自变量甚至因变量，人机协同已延伸至跨界融合，教学的组织形态由过去“师一生”二元模式转变为“师一生一机”多向交互的开放学习生态，“师一机”协同的复合教育者与“生一机”协同的复合学习者，[34]可延伸师生的感知与认知，增强师生互动与体验，以升维方式实现信息传播方式的转变\n\n与内容创造的共享，促使教育范式向“学为中心”的技能本位迈进，并为广泛的人机互动和人际互联创建创新生态。[26]三是人际交互方式从口耳相传转向声光电图文并茂，技术始终在拓展人类表达与理解的边界，声光电与图文、视频、虚拟现实（VR）、增强现实（AR）等技术融合，形成多模态交互，让人际交互的实时性、互动性、沉浸感空前增强，这种由智能技术驱动的交互媒介的迭代升级不仅改变了信息传递的形态，更重塑了师生认知模式、社会关系与文化生态。因而，适应社会交往方式的改变对教育课程教学范式的影响，必须加强智慧教育基本科学问题、关键核心技术、重要应用示范等方面研究，探究新的人类学习机制，[2]打造泛在化学习的数字化校园，创新具有交互式的学习场域、创新多元化互动教学模式，设计更科学、更富吸引力的教育环境，让学生的学和教师的教更富有效率，以塑造更符合学生和教师需求的教育体验，实现教学范式的重大改革。\n\n# （四）教育治理体系从技术赋能转向系统重构\n\n数字化技术使得信息的获取和传播变得更加快捷和广泛，改变了人们的日常生活方式。运用人工智能等现代科学与技术进行教育治理创新，是智能技术赋能教育发展的重要一环。在智能技术的不断渗透下，现代大学不得不考虑科技对高等教育管理范式的影响。高等教育必须积极适应人工智能快速发展导致的组织行为模式的变革趋势，特别是适应信息技术进步与知识创造分享融合发展的教育形态、学习生态和创新生态，加快建设精准全面、集成共享的教师和学生数据库，建立以数据治理为核心、数智技术为驱动的决策支持系统和集中调度平台，围绕网络安全、数据安全、内部控制等管理服务现代化要求，整体推进教育管理与业务流程再造，从而提升教育治理的效能和水平。当然，虽然人工智能可以处理大多数日常任务，但复杂的决策和创新仍需要人类的智慧，在大学治理的各个环节的选择与决策中，人类的创造力仍然是无法替代的关键。\n\n高等教育必须积极适应人工智能快速发展导致的组织行为模式的变革趋势，围绕信息系统、数据安全、内部控制等管理服务现代化要求，整体推进教育管理与业务流程再造，从而提升治理效能和水平。一是治理理念上，要坚持有利于实现大学的本质功能，最能够激发大学的教育家精神和科学家创造力的根本导向，真正把以人为本、学术为魂贯穿大学治理的各个方面和环节。二是治理主体上，应恰当处理不同权利主体之间的利益与风险分配问题，包括教师、学生和管理者在内的大学组织成员的治理理念、素养构成、行为方式等将成为影响大学治理效能的重要因素。三是治理结构上，扁平化、模块化和科学化将突破现有的科层管理结构，大学组织内部及其与社会之间的部门联动、数据互通、资源协同等必将导致条块分割的治理结构被改变。四是治理方式上，由数据驱动的大学治理必将进一步提升决策的科学化、管理的精细化、服务的精准化。现实中，每一所大学都有自己的文化传统、行为习惯、管理文化、资源配置方式，大学的治理理念、治理结构、治理方式等必须不断突破传统的惯性运作，才能构建与高等教育数智变革趋势相适应的治理体系。\n\n# 四、高等教育数智变革的实践应对\n\n“每一次科技革命和产业变革都给教育带来跨越式发展”。①历史表明，世界教育中心、世界科学中心、世界人才中心本质上具有深度的同构逻辑和共生关系。我国是一个高等教育大国，高水平研究型大学在国家高等教育体系和创新体系中具有特殊而重要的地位，必须深刻把握高等教育数智变革的基本逻辑和趋势特点，加快推进发展范式、创新生态、育人体系、治理体系、评价体系的整体性重塑，着力构建符合自身发展目标定位、优势特色和阶段特点的高质量发展体系，在高等教育数智变革中发挥好示\n\n范引领作用。①\n\n# （一）推进教育发展范式系统性变革\n\n高等教育的数智变革不是高等教育传统发展路径的“局部改良”，而是一场深刻的“范式革命”。高水平研究型大学须前瞻把握数智革命影响高等教育变革的底层逻辑，一体推进自身办学理念、发展方式、重大战略的系统性改革，探索一种集思想、理念、标准、评价等为一体的发展新范式。办学理念方面，要牢牢把握教育的政治属性、人民属性、战略属性，以科技发展、国家战略需求为牵引，把为学生提供超越传统知识教育、能力培养和素养养成之上的未来教育作为重要理念，构建智慧教育生态系统，着力培养学生的家国情怀、创新能力、国际视野和未来素养，为每一个学生应对不确定的未来而做好准备。发展方式方面，突出高质量发展主题和内涵式发展主线，充分发挥数智技术在实施大规模因材施教、提升知识生产效率、推动产教深度融合、促进国际创新资源互通方面的重要作用，强化技术驱动、智慧赋能、跨界融合，以数据生态延伸创新生态，探索面向智能时代的高等教育发展新路径。重大战略方面，将以人工智能为代表的现代科技作为战略性变量融入知识生产、人才培养、科技创新、产教融合等创新生态，带动育人方式、办学模式、管理体制、保障机制全方位系统性变革，在回应科技革命和产业变革中加快推进高等教育战略转型。如西南财经大学以建设“新财经”探索高等财经教育发展的新范式，其基本方向就是积极适应和引领现代经济、科技和教育深刻变革，坚持以服务中国式现代化为根本使命、以培养财经拔尖人才为根本任务、以推动建构中国自主知识体系和支撑国家高水平科技自立自强为核心目标、以跨学科和促融通为基本路径、以全面数智赋能为关键动力，着力推进大学功能、学科要素和组织成员的系统性、深层次变革，加快建设财经特色鲜明的世界一流大学。\n\n# （二）推进大学创新生态系统性重构\n\n数智革命正在重构科学研究的底层逻辑和大学的创新生态体系。高水平研究型大学应一体推进学科建设、科研创新和人才发展系统性改革，实现学科布局优化、攻关方向聚焦、人才队伍适配深度耦合，提升创新体系的整体效能，为人类社会发展和文明进步贡献中国智慧。大力优化学科专业布局，着力健全科技创新发展、国家重大战略和区域产业布局需求牵引的学科专业设置调整机制，加快推进传统学科专业数智化转型，布局建设集成电路、人工智能、量子计算、网络空间安全、数据科学等一批能更好支撑发展新质生产力的新兴交叉学科专业，打造优势学科专业集群。推动科学研究范式转换，发挥AI处理海量科研数据（如清华3000+临床病例库）、辅助实验设计、协助数据分析（如DeepMind的AlphaScience项目）、发现传统方法难以捕捉的规律等方面的优势，提升教师的数智素养和创新效率，推动科研范式从假设驱动向数据驱动、从单一学科向交叉融合、从线性研究向非线性创新转变。创新科研组织模式，更加突出与产业发展、社会需求和科技前沿紧密衔接，打破学科边界、学术阻隔、学院壁垒和认知偏见，推动教育链、人才链、产业链、创新链深度融合，建设跨学科、超学科融合的新型创新联合体，提升科学研究组织化水平和原始创新能级。加快新型基础设施建设，建设数据中心、算力中心、智能感知、学科大模型等新型基础设施。如武汉理工大学紧扣云、网、数、端四大要素系统推进算力、网络、数据、平台四大基础要素建设；中国地质大学“元古大模型”利用多模态大模型技术除开展常规的地学图谱形成、地学知识问答外，还具备岩石、化石的智能鉴定、化石物种的智能鉴定、化石图像的智能解译、古生物化石复原、地质文献数据抽取等特色功能。\n\n# （三）推进人才培养体系系统性重塑\n\n教育的目标不是装备学生应对已知的世界，而是赋予他们重塑世界的能力。高水平研究型大学应\n\n着眼数智时代对拔尖创新人才道德伦理、知识结构、能力素养的新需求，一体推进人才培养标准、模式和机制系统性改革，完善以育人育才为中心的教育格局，重塑人才培养的新体系。优化人才培养标准，突出学生数智素养、创新潜能、人机协同能力培养，全面加强专业、课程、教材等标准体系建设，分类构建各学科专业知识图谱、能力图谱、素质图谱。如北京理工大学“知识图谱驱动的智慧教学系统”构建覆盖全校70+本科专业的知识图谱，实现跨学科知识贯通，形成可追溯的知识网络。创新人才培养模式，以跨学科和促融通为主要路径，深化学科、专业、课程交叉融合，转变课程教学范式，实施基于AI的精准教学。如北京大学的“北大问学”平台、中国人民大学“人大未来课堂AI智能助手”、清华大学构建“数据驱动-认知增强-人机协同”的新型的数智教育平台、教学模式等正在实现学生从“标准化生产”到“个性化培育”的转变，尊重和满足课堂中不同层次群体的多样化学习需求和自主选择的权利。健全人才培养机制，进一步完善拔尖人才的发现和培养机制，持续深化产教融合、科教融汇，与行业企业联合打造数字化的课程教材体系，以项目和任务设定牵引课堂，让学生在项目化的场景中学习用知识解决复杂问题，提升拔尖创新人才自主培养能力。如武汉大学通过构建“知识一能力一素质”三位一体的培养体系，让学生在掌握专业知识基础上具备引领未来的关键能力。\n\n# （四）推进内部治理体系系统性优化\n\n大学治理主要解决能在多元利益状况下管理其一般事务的组织性框架及体制机制的问题，尤其是通过内部制度安排，确定包括组织结构的分层、内部权力体系的构成等内部组织结构和运行机制。高水平研究型大学应一体推进治理理念、结构、方式系统性改革，构建体现大学本质规律、符合时代要求、适应中国国情的现代大学制度。在治理理念方面，坚持把促进师生的全面发展摆在首位，充分利用数智技术全面感知、高速传播、跨域交互的优势，推动大学治理从单一管理走向多元共治，增强治理的民主性和科学性。在治理结构方面，利用数智技术优化治理结构，降低信息横向和纵向通达的时间，打通部门之间的行政壁垒，着力解决部门职责交叉边界不清的问题；纵向缩减管理层级降低管理重心，赋予二级单位更大的资源分配权、指挥决策权和管理自主权，增强二级单位推动发展的自主性和创造性。如西北工业大学、南京航空航天大学等正在以大部制、扁平化改革推动大学治理结构优化，进一步核减管理人员的规模。在治理方式上，强化数据驱动的治理效能提升，重塑管理服务流程，建设AI应用生态，进一步提升决策的科学化、管理的精细化、服务的精准化。如西南财经大学与同方知网合作建设学校事业发展数据中台，以学科发展监测、评价等为应用场景实现内部治理信息系统互联互通；同济大学“基于ChatTJ的智慧招生管理平台”实现了大模型技术应用与招生服务的垂直融合。\n\n# （五）推进教育评价体系系统性改革\n\n教育评价事关教育发展方向，有什么样的评价指挥棒，就有什么样的办学导向。研究型大学推动构建自主评价体系须紧紧围绕教育强国建设战略目标和高等教育高质量发展战略主线，在教育评价体系的价值导向、指标设计、评价方式和结果应用上都要前瞻由数智革命驱动的人才培养目标、知识创新生态、课程教学范式、内部治理体系等高等教育的系统性深层次变革趋势，从而构建多元主体参与、符合我国实际、具有世界水平的教育评价体系。价值导向上，深入研究教育强国的具体内涵及其在大学评价中的具体体现，把引领加快建设中国特色世界一流大学和优势学科，更好培养国家急需创新人才、建构中国自主知识体系、支撑国家科技自立自强作为教育评价改革的根本导向，彰显中国教育发展的时代特征和教育评价改革的基本方向。指标设计上，突出扎根中国大地与借鉴世界经验相结合，创造性地吸取国外高等教育评价体系的合理元素，更加注重学科专业与生产力形态变革的适配度、原始创新贡献、学生数智素养培养、课程教学范式转变、内部治理效能等表征高等教育数智变革的新成效。评价方式上，优化基于指标内涵的评价方式，根据指标类型选择评价方式，积极探索多元主体评价、定量与定性\n\n相结合的融合评价，突出评价开放性、自主性、发展性，更加注重大数据、区块链、人工智能等新技术的应用，创新评价的工具、模型、方式，提升评价的科学性和有效性。结果应用上，强化评价结果的诊断功能和促进发展的功效，更加注重教育评价体系在推动大学办学模式、创新生态、育人体系创新中的引领作用，更好地服务国家的世界重要人才中心和创新高地建设。\n\n# 五、结 语\n\n教育部部长怀进鹏在世界教育大会上强调，发展数字教育，推动教育数字化转型，是大势所趋、发展所需、改革所向。随着智能科学与技术尤其是人工智能逐步从“狭义人工智能”（ANI）、“通用人工智能”（AGI）向“超级人工智能”（ASI）发展进阶，可以预料的是，社会各领域观念变革、结构重构、流程再造的广度、深度和速度必将加速演进，数智革命必将会持续渗透和扩大到高等教育的各个领域和层面，成为影响高等教育时代变革的关键动能，从而不断重塑智能时代高等教育的新形态。\n\n# 参考文献：\n\n[1]中国学位与研究生教育学会.怀进鹏在2025世界数字教育大会作主旨演讲[EB/OL].(2025-05-15)[2025-06-06].https://mp.weixin.qq.com/s/re-chwa47TZHqeqlajG3g.  \n[2]杨宗凯等. ChatGPT/生成式人工智能对教育的影响探析及应对策略[J]. 华东师范大学学报(教育科学版), 2023, 41(7): 26-35.  \n[3]徐和祥，申利侠.“智能+教育”：应用场景、风险挑战与治理对策[J].复旦教育论坛，2023,21(2):24-30.  \n[4]蒋里.AI驱动教育改革:ChatGPT/GPT的影响及展望[J].华东师范大学学报(教育科学版),2023,41(7):143-150.  \n[5]陈静远,胡丽雅,吴飞. ChatGPT/生成式人工智能促进以知识点为核心的教学模式变革研究[J].华东师范大学学报(教育科学版),2023,41(7):177-186.  \n[6]焦建利. ChatGPT: 学校教育的朋友还是敌人? [J]. 现代教育技术, 2023, 33(4):5-15.  \n[7]冯永刚,屈玲. ChatGPT运用于教育的伦理风险及其防控[J]. 内蒙古社会科学,2023,44(4):34-42.  \n[8]冯雨奂. ChatGPT 在教育领域的应用价值、潜在伦理风险与治理路径[J]. 思想理论教育, 2023(4):26-32.  \n[9]王佑镁，王旦，梁炜怡，等.“阿拉丁神灯”还是“潘多拉魔盒”：ChatGPT教育应用的潜能与风险[J].现代远程教育研究，2023,35(2):48-56.  \n[10]张务农，汤洁.知与非知——再论人工智能应用对教学主体的影响[J].电化教育研究,2023,44(3):36-43.  \n[11]张缨斌.感知情境与人在回路的智能教育——《人工智能与教学的未来：见解与提议》要点与反思[J].开放教育研究,2023,29(4):11-20.  \n[12]克劳斯·迈因策尔,贾积有,张誉月. ChatGPT和人工智能:从基本原理到教育应用[J].北京大学教育评论,2023,21(1):35-48.  \n[13]李会春. ChatGPT的智慧生成特征及对高等教育的挑战[J]. 江苏高教, 2023(8):1-12.  \n[14]学习强国. 习近平主持十九届中共中央政治局第九次集体学习[EB/OL].(2018-10-31)[2025-06-06].https://www.xuexi.cn/429f88258b2897476c9730c4d81161ee/e43e220633a65f9b6d8b53712cba9caa.html.  \n[15]李晓红.科技创新是发展新质生产力的核心要素[J].中国信息化,2024(7):5-7.  \n[16]MCKINSEY, COMPANY. The economic potential of generative AI: the next productivity frontier[EB/OL]. McKinsey & Company, (2023-06-14) [2025-05-06]. https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier.  \n[17]MCKINSEY, COMPANY. Technology trends outlook 2023[R/OL]. [2025-05-06]. https://www.mckinsey.com/~media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/mckinsey%20technology%20trends%20outlook%202023/mckinsey-technology-trends-outlook-2023-v5.pdf.  \n[18]FROST,SULLIVAN CHINA.2024年中国行业大模型市场报告[R/OL].FROST & SULLIVAN CHINA,2024[2025-05-06].https://www.frostchina.com/content/insight/detail/67346fe60bc989123b87e56d.\n\n[19]马克思.政治经济学批判大纲(1857—1858年手稿)导言[M//马克思恩格斯全集:第30卷.北京:人民出版社, 1995.  \n[20]新浪网. 院士洞见 | 首位华人图灵奖获得者、中科院院士姚期智《人工智能的科学视角》演讲全文[EB/OL]. (2024-10-04)[2025-05-06]. https://k.sina.com.cn/article_6375433705_17c0165e901901cy4q.html.  \n[21]中国科学院网信工作网. DeepMind 发布报告 AI for Science 黄金时代已来[EB/OL].(2024-12-19)[2025-05-06]. https://ecas.cas.cn/xxkw/kbcd/201115_146563/ml/xxhcxyyyal/202412/t20241219_5042988.html.  \n[22]习近平.论科技自立自强[M].北京：中央文献出版社，2023.  \n[23]肖峰.大模型时代的数字交往：“对话中的人”及其新形态[J].人民论坛·学术前沿,2024(19):65-72.  \n[24]李会春. ChatGPT技术热潮下教育变革的挑战和对策[J]. 复旦教育论坛, 2023, 21(2):13-23.  \n[25]阿兰·柯林斯,理查德·哈尔弗森.技术时代重新思考教育:数字革命与美国的学校教育[M].上海:华东师范大学出版社,2013.  \n[26]祝智庭,戴岭,赵晓伟. “近未来”人机协同教育发展新思路[J]. 开放教育研究,2023,29(5):4-13.  \n[27]陈星,吴叶林.人机协同教育治理的障碍与突破[J].现代远程教育研究,2022,34(1):40-47.  \n[28]朱永新,杨帆. ChatGPT/生成式人工智能与教育创新:机遇、挑战以及未来[J].华东师范大学学报(教育科学版), 2023,41(7):1-14.  \n[29]顾小清,胡艺龄,郝祥军. AGI临近了吗:ChatGPT热潮之下再看人工智能与未来教育发展[J].华东师范大学学报(教育科学版),2023,41(7):117-130.  \n[30]海尔格·诺沃特尼，等.反思科学：不确定性时代的知识与公众[M].上海：上海交通大学出版社，2011.  \n[31]HOU X, et al. Using artificial intelligence to document the hidden RNA viosphere[J].Cell, 2024,187(23):6929-6942.  \n[32]中山大学医学院.科研动态|我院施葵教授团队借助人工智能技术开展病毒学研究大幅拓宽RNA病毒库[EB/OL].(2024-10-11)[2025-05-06].https://szmed.sysu.edu.cn/zh-hans/article/2951.  \n[33]科学网.诺奖得主乔治·斯穆特:未来的大学教育中,知识传授将不再重要[EB/OL].(2024-11-01)[2025-05-06].https://news.sciencenet.cn/htmlnews/2024/11/533132.shtml.  \n[34]沈书生,祝智庭. ChatGPT类产品:内在机制及其对学习评价的影响[J]. 中国远程教育,2023(4):8-15.\n\n# Digital-Intelligent Transformation in Higher Education:\n\n# Fundamental Logic, Trends and Characteristics, and Practical Responses\n\n# Li Xue, Li Yongqiang\n\nAbstract: The accelerated iteration and upgrading of intelligent science and technology, represented by ChatGPT and DeepSeek, and their widespread application in human production and life are constantly changing the way humans produce material goods, the logic of knowledge innovation, interpersonal interaction patterns, and organizational behavior. Its impact, penetration, and transformation of higher education extend far beyond the application of science and technology in education in the general sense, triggering comprehensive and profound changes in the objectives of talent cultivation, the knowledge innovation ecosystem, course teaching paradigms, and university governance systems in higher education. Higher education is showing many new trends and characteristics, such as a shift in talent cultivation goals from professional knowledge education to future literacy cultivation, a shift in the knowledge innovation ecosystem from within university organizations to multi-subject collaboration, a shift in course teaching paradigms from traditional teaching to smart education, and a shift in education governance systems from technology empowerment to systemic reconstruction. High-level research universities, which occupy an important position in the national higher education system, should thoroughly grasp these new trends and characteristics, comprehensively promote systematic reforms in development models, innovation ecosystems, education systems, governance systems, and evaluation systems, and play an exemplary and leading role in building a world-class higher education center and better serving the construction of an education powerhouse.\n\nKeywords: Higher Education; Digital-Intelligent Transformation; Fundamental Logic; Trends and Characteristics; High-level Research Universities; Response Strategies\n\n(收稿日期:2025—05—28 责任编辑:赵爱清)",
    "translated_content": null,
    "created_at": "2025-12-15 11:36:36.339515",
    "updated_at": "2025-12-15 11:36:53.450787",
    "analysis": {
      "paper_id": "0a6aae26-e1da-42f9-80f6-19a4250dbeaa",
      "status": "completed",
      "started_at": "2025-12-16T07:32:57.503411",
      "completed_at": "2025-12-16T07:33:09.326356",
      "summary": "本文探讨以ChatGPT、DeepSeek等智能技术为代表的人工智能对高等教育带来的系统性变革。研究旨在分析数智变革影响高等教育的基本逻辑、发展趋势及应对策略。\n\n论文从物质生产方式、知识创新逻辑、人际交往模式和组织行为方式四个维度，剖析了数智技术对高等教育产生的深层次影响。研究发现，高等教育正呈现四大趋势转变：人才培养目标从专业知识教育转向未来素养培养；知识创新生态从大学内部转向多元主体协同；课程教学范式从传统教学转向智慧教育；教育治理体系从技术赋能转向系统重构。\n\n研究指出，高水平研究型大学应把握这些趋势特点，在发展范式、创新生态、育人体系等方面进行系统性改革，通过推进数智技术与教育体系的深度融合，为我国建设教育强国发挥示范引领作用。文章强调，高等教育必须超越单纯的技术应用层面，从更宏观的视角应对数智时代的教育变革挑战。",
      "methods": [
        {
          "name": "文献综述法",
          "description": "通过系统梳理和分析已有研究成果，为研究提供理论基础和研究方向。该方法有助于识别研究空白并确定本文的研究重点。",
          "location": {
            "start_line": 23,
            "end_line": 25,
            "text_snippet": "已有研究开拓了人们对数智革命影响高等教育变革趋势特点的深入认识，其中一些观点、洞见以及相应的实践探索为本文的分析研究提供了很好的借鉴和参照。"
          }
        },
        {
          "name": "逻辑分析法",
          "description": "从宏观视角分析数智革命影响高等教育的内在因果关系和基本规律。该方法强调跳出教育系统本身，从社会大系统的角度进行系统性分析。",
          "location": {
            "start_line": 23,
            "end_line": 25,
            "text_snippet": "分析数智革命对高等教育的影响不能仅仅局限在教育系统内部，必须深入把握以人工智能为代表的数智革命如何影响高等教育变革的基本逻辑。"
          }
        },
        {
          "name": "案例分析法",
          "description": "通过引用具体的研究报告、技术产品和权威奖项等实例，支撑和论证数智革命带来的实际影响。该方法使分析更具说服力和现实基础。",
          "location": null
        }
      ],
      "datasets": [],
      "code_refs": [],
      "structure": {
        "sections": [
          {
            "title": "高等教育的数智变革:",
            "level": 1,
            "start_line": 1
          },
          {
            "title": "基本逻辑、趋势特点及实践应对",
            "level": 1,
            "start_line": 3
          },
          {
            "title": "一、引言与文献综述",
            "level": 1,
            "start_line": 15
          },
          {
            "title": "二、高等教育数智变革的基本逻辑",
            "level": 1,
            "start_line": 25
          },
          {
            "title": "（一）数智革命改变了人类的物质生产方式",
            "level": 1,
            "start_line": 29
          },
          {
            "title": "（二）数智革命重塑了人类的知识生产逻辑",
            "level": 1,
            "start_line": 37
          },
          {
            "title": "（三）数智革命重构了人类的人际交往模式",
            "level": 1,
            "start_line": 43
          },
          {
            "title": "（四）数智革命改变了人类的组织行为方式",
            "level": 1,
            "start_line": 51
          },
          {
            "title": "三、高等教育数智变革的趋势特点",
            "level": 1,
            "start_line": 59
          },
          {
            "title": "（一）人才培养目标从专业知识教育转向未来素养培养",
            "level": 1,
            "start_line": 63
          },
          {
            "title": "（二）知识创新生态从大学组织内部转向多元主体协同",
            "level": 1,
            "start_line": 69
          },
          {
            "title": "（三）课程教学范式从传统教学转向智慧教育",
            "level": 1,
            "start_line": 77
          },
          {
            "title": "（四）教育治理体系从技术赋能转向系统重构",
            "level": 1,
            "start_line": 85
          },
          {
            "title": "四、高等教育数智变革的实践应对",
            "level": 1,
            "start_line": 91
          },
          {
            "title": "（一）推进教育发展范式系统性变革",
            "level": 1,
            "start_line": 97
          },
          {
            "title": "（二）推进大学创新生态系统性重构",
            "level": 1,
            "start_line": 101
          },
          {
            "title": "（三）推进人才培养体系系统性重塑",
            "level": 1,
            "start_line": 105
          },
          {
            "title": "（四）推进内部治理体系系统性优化",
            "level": 1,
            "start_line": 111
          },
          {
            "title": "（五）推进教育评价体系系统性改革",
            "level": 1,
            "start_line": 115
          },
          {
            "title": "五、结 语",
            "level": 1,
            "start_line": 121
          },
          {
            "title": "参考文献：",
            "level": 1,
            "start_line": 125
          },
          {
            "title": "Digital-Intelligent Transformation in Higher Education:",
            "level": 1,
            "start_line": 163
          },
          {
            "title": "Fundamental Logic, Trends and Characteristics, and Practical Responses",
            "level": 1,
            "start_line": 165
          },
          {
            "title": "Li Xue, Li Yongqiang",
            "level": 1,
            "start_line": 167
          }
        ]
      },
      "error_message": null
    }
  },
  "e18fee52-bfb3-42f7-b457-ec1c57804020": {
    "id": "e18fee52-bfb3-42f7-b457-ec1c57804020",
    "filename": "2505.10468v1.pdf",
    "file_path": "./uploads/papers/e18fee52-bfb3-42f7-b457-ec1c57804020.pdf",
    "status": "completed",
    "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges",
    "category": null,
    "markdown_content": "# AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges\n\nRanjan Sapkota\\*†, Konstantinos I. Roumeliotis†, Manoj Karkee\\*‡\n\n* Cornell University, Department of Environmental and Biological Engineering, USA\n\n$^{\\dagger}$ Department of Informatics and Telecommunications, University of the Peloponnese, 22131 Tripoli, Greece\n\n†Corresponding authors: rs2672@cornell.edu, mk2684@cornell.edu\n\nAbstract—This review critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by LLMs and LIMs for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI-driven systems.\n\nIndex Terms—AI Agents, Agentic AI, Autonomy, Reasoning, Context Awareness, Multi-Agent Systems, Conceptual Taxonomy, vision-language model\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/3ed2affe58af5dd7192c50358456dc7a45d2e51af029c5bbeb82e1fceef60900.jpg)  \nFig. 1: Global Google search trends showing rising interest in \"AI Agents\" and \"Agentic AI\" since November 2022 (ChatGPT Era).\n\n# I. INTRODUCTION\n\nPrior to the widespread adoption of AI agents and agentic AI around 2022 (Before ChatGPT Era), the development of autonomous and intelligent agents was deeply rooted in foundational paradigms of artificial intelligence, particularly multi-agent systems (MAS) and expert systems, which emphasized social action and distributed intelligence [1], [2].\n\nNotably, Castelfranchi [3] laid critical groundwork by introducing ontological categories for social action, structure, and mind, arguing that sociality emerges from individual agents' actions and cognitive processes in a shared environment, with concepts like goal delegation and adoption forming the basis for cooperation and organizational behavior. Similarly, Ferber [4] provided a comprehensive framework for MAS, defining agents as entities with autonomy, perception, and communication capabilities, and highlighting their applications in distributed problem-solving, collective robotics, and synthetic world simulations. These early works established that individual social actions and cognitive architectures are fundamental to modeling collective phenomena, setting the stage for modern AI agents. This paper builds on these insights to explore how social action modeling, as proposed in [3], [4], informs the design of AI agents capable of complex, socially intelligent interactions in dynamic environments.\n\nThese systems were designed to perform specific tasks with predefined rules, limited autonomy, and minimal adaptability to dynamic environments. Agent-like systems were primarily reactive or deliberative, relying on symbolic reasoning, rule-based logic, or scripted behaviors rather than the learning-driven, context-aware capabilities of modern AI agents [5], [6]. For instance, expert systems used knowledge bases and inference engines to emulate human decision-making in domains like medical diagnosis (e.g., MYCIN [7]). Reactive agents, such as those in robotics, followed sense-act cycles based on hardcoded rules, as seen in early autonomous vehicles like the Stanford Cart [8]. Multi-agent systems facilitated coordination among distributed entities, exemplified by auction-based resource allocation in supply chain management [9], [10]. Scripted AI in video games, like NPC behaviors in early RPGs, used predefined decision trees [11]. Furthermore, BDI (Belief-Desire-Intention) architectures enabled goal-directed behavior in software agents, such as those in air traffic control simulations [12], [13]. These early systems lacked the generative capacity, self-learning, and environmental adaptability of modern agentic AI, which leverages deep learning, reinforcement learning, and large-scale data [14].\n\nRecent public and academic interest in AI Agents and Agentic AI reflects this broader transition in system capabilities. As illustrated in Figure 1, Google Trends data demonstrates a significant rise in global search interest for both terms\n\nfollowing the emergence of large-scale generative models in late 2022. This shift is closely tied to the evolution of agent design from the pre-2022 era, where AI agents operated in constrained, rule-based environments, to the post-ChatGPT period marked by learning-driven, flexible architectures [15]–[17]. These newer systems enable agents to refine their performance over time and interact autonomously with unstructured, dynamic inputs [18]–[20]. For instance, while pre-modern expert systems required manual updates to static knowledge bases, modern agents leverage emergent neural behaviors to generalize across tasks [17]. The rise in trend activity reflects increasing recognition of these differences. Moreover, applications are no longer confined to narrow domains like simulations or logistics, but now extend to open-world settings demanding real-time reasoning and adaptive control. This momentum, as visualized in Figure 1, underscores the significance of recent architectural advances in scaling autonomous agents for real-world deployment.\n\nThe release of ChatGPT in November 2022 marked a pivotal inflection point in the development and public perception of artificial intelligence, catalyzing a global surge in adoption, investment, and research activity [21]. In the wake of this breakthrough, the AI landscape underwent a rapid transformation, shifting from the use of standalone LLMs toward more autonomous, task-oriented frameworks [22]. This evolution progressed through two major post generative phases: AI Agents and Agentic AI. Initially, the widespread success of ChatGPT popularized Generative Agents, which are LLM-based systems designed to produce novel outputs such as text, images, and code from user prompts [23], [24]. These agents were quickly adopted across applications ranging from conversational assistants (e.g., GitHub Copilot [25]) and content-generation platforms (e.g., Jasper [26]) to creative tools (e.g., Midjourney [27]), revolutionizing domains like digital design, marketing, and software prototyping throughout 2023.\n\nBuilding on this generative foundation, a new class of systems known as AI Agents emerged. These agents enhanced LLMs with capabilities for external tool use, function calling, and sequential reasoning, enabling them to retrieve real-time information and execute multi-step workflows autonomously [28], [29]. Frameworks such as AutoGPT [30] and BabyAGI (https://github.com/yoheinakajima/babyagi) exemplified this transition, showcasing how LLMs could be embedded within feedback loops to dynamically plan, act, and adapt in goal-driven environments [31], [32]. By late 2023, the field had advanced further into the realm of Agentic AI complex, multi-agent systems in which specialized agents collaboratively decompose goals, communicate, and coordinate toward shared objectives. Architectures such as CrewAI demonstrate how these agentic frameworks can orchestrate decision-making across distributed roles, facilitating intelligent behavior in high-stakes applications including autonomous robotics, logistics management, and adaptive decision-support [33]-[36].\n\nAs the field progresses from Generative Agents toward increasingly autonomous systems, it becomes critically impor-\n\ntant to delineate the technological and conceptual boundaries between AI Agents and Agentic AI. While both paradigms build upon large LLMs and extend the capabilities of generative systems, they embody fundamentally different architectures, interaction models, and levels of autonomy. AI Agents are typically designed as single-entity systems that perform goal-directed tasks by invoking external tools, applying sequential reasoning, and integrating real-time information to complete well-defined functions [17], [37]. In contrast, Agentic AI systems are composed of multiple, specialized agents that coordinate, communicate, and dynamically allocate subtasks within a broader workflow [14], [38]. This architectural distinction underpins profound differences in scalability, adaptability, and application scope.\n\nUnderstanding and formalizing the taxonomy between these two paradigms (AI Agents and Agentic AI) is scientifically significant for several reasons. First, it enables more precise system design by aligning computational frameworks with problem complexity ensuring that AI Agents are deployed for modular, tool-assisted tasks, while Agentic AI is reserved for orchestrated multi-agent operations. Moreover, it allows for appropriate benchmarking and evaluation: performance metrics, safety protocols, and resource requirements differ markedly between individual-task agents and distributed agent systems. Additionally, clear taxonomy reduces development inefficiencies by preventing the misapplication of design principles such as assuming inter-agent collaboration in a system architected for single-agent execution. Without this clarity, practitioners risk both under-engineering complex scenarios that require agentic coordination and over-engineering simple applications that could be solved with a single AI Agent.\n\nSince the field of artificial intelligence has seen significant advancements, particularly in the development of AI Agents and Agentic AI. These terms, while related, refer to distinct concepts with different capabilities and applications. This article aims to clarify the differences between AI Agents and Agentic AI, providing researchers with a foundational understanding of these technologies. The objective of this study is to formalize the distinctions, establish a shared vocabulary, and provide a structured taxonomy between AI Agents and Agentic AI that informs the next generation of intelligent agent design across academic and industrial domains, as illustrated in Figure 2.\n\nThis review provides a comprehensive conceptual and architectural analysis of the progression from traditional AI Agents to emergent Agentic AI systems. Rather than organizing the study around formal research questions, we adopt a sequential, layered structure that mirrors the historical and technical evolution of these paradigms. Beginning with a detailed description of our search strategy and selection criteria, we first establish the foundational understanding of AI Agents by analyzing their defining attributes, such as autonomy, reactivity, and tool-based execution. We then explore the critical role of foundational models specifically LLMs and Large Image Models (LIMs) which serve as the core reasoning and perceptual substrates that drive agentic behavior. Subsequent\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/4cfcf39fb9cd1cdf5e4482e29b6cccd9aa88dd4a3f10bc1d8be7845ac3d1d65e.jpg)  \nFig. 2: Mindmap of Research Questions relevant to AI Agents and Agentic AI. Each color-coded branch represents a key dimension of comparison: Architecture, Mechanisms, Scope/Complexity, Interaction, and Autonomy.\n\nsections examine how generative AI systems have served as precursors to more dynamic, interactive agents, setting the stage for the emergence of Agentic AI. Through this lens, we trace the conceptual leap from isolated, single-agent systems to orchestrated multi-agent architectures, highlighting their structural distinctions, coordination strategies, and collaborative mechanisms. We further map the architectural evolution by dissecting the core system components of both AI Agents and Agentic AI, offering comparative insights into their planning, memory, orchestration, and execution layers. Building upon this foundation, we review application domains spanning customer support, healthcare, research automation, and robotics, categorizing real-world deployments by system capabilities and coordination complexity. We then assess key challenges faced by both paradigms including hallucination, limited reasoning depth, causality deficits, scalability issues, and governance risks. To address these limitations, we outline emerging solutions such as retrieval-augmented generation, tool-based reasoning, memory architectures, and simulation-based planning. The review culminates in a forward-looking roadmap that envisions the convergence of modular AI Agents and orchestrated Agentic AI in mission-critical domains. Overall, this paper aims to provide researchers with a structured taxonomy and actionable insights to guide the design, deployment, and evaluation of next-generation agentic systems.\n\n# A. Methodology Overview\n\nThis review adopts a structured, multi-stage methodology designed to capture the evolution, architecture, application,\n\nand limitations of AI Agents and Agentic AI. The process is visually summarized in Figure 3, which delineates the sequential flow of topics explored in this study. The analytical framework was organized to trace the progression from basic agentic constructs rooted in LLMs to advanced multi-agent orchestration systems. Each step of the review was grounded in rigorous literature synthesis across academic sources and AI-powered platforms, enabling a comprehensive understanding of the current landscape and its emerging trajectories.\n\nThe review begins by establishing a foundational understanding of AI Agents, examining their core definitions, design principles, and architectural modules as described in the literature. These include components such as perception, reasoning, and action selection, along with early applications like customer service bots and retrieval assistants. This foundational layer serves as the conceptual entry point into the broader agentic paradigm.\n\nNext, we delve into the role of LLMs as core reasoning components, emphasizing how pre-trained language models underpin modern AI Agents. This section details how LLMs, through instruction fine-tuning and reinforcement learning from human feedback (RLHF), enable natural language interaction, planning, and limited decision-making capabilities. We also identify their limitations, such as hallucinations, static knowledge, and a lack of causal reasoning.\n\nBuilding on these foundations, the review proceeds to the emergence of Agentic AI, which represents a significant conceptual leap. Here, we highlight the transformation from tool-augmented single-agent systems to collaborative, distributed ecosystems of interacting agents. This shift is driven by the need for systems capable of decomposing goals, assigning subtasks, coordinating outputs, and adapting dynamically to changing contexts capabilities that surpass what isolated AI Agents can offer.\n\nThe next section examines the architectural evolution from AI Agents to Agentic AI systems, contrasting simple, modular agent designs with complex orchestration frameworks. We describe enhancements such as persistent memory, meta-agent coordination, multi-agent planning loops (e.g., ReAct and Chain-of-Thought prompting), and semantic communication protocols. Comparative architectural analysis is supported with examples from platforms like AutoGPT, CrewAI, and Lang-Graph.\n\nFollowing the architectural exploration, the review presents an in-depth analysis of application domains where AI Agents and Agentic AI are being deployed. This includes six key application areas for each paradigm, ranging from knowledge retrieval, email automation, and report summarization for AI Agents, to research assistants, robotic swarms, and strategic business planning for Agentic AI. Use cases are discussed in the context of system complexity, real-time decision-making, and collaborative task execution.\n\nSubsequently, we address the challenges and limitations inherent to both paradigms. For AI Agents, we focus on issues like hallucination, prompt brittleness, limited planning ability, and lack of causal understanding. For Agentic AI, we identify\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/5e2d098f86347ed88c2f8499ef0bbaa689a73aa3e70f248aa88cb518a8aba709.jpg)  \nFig. 3: Methodology pipeline from foundational AI agents to Agentic AI systems, applications, limitations, and solution strategies.\n\nhigher-order challenges such as inter-agent misalignment, error propagation, unpredictability of emergent behavior, explainability deficits, and adversarial vulnerabilities. These problems are critically examined with references to recent experimental studies and technical reports.\n\nFinally, the review outlines potential solutions to overcome these challenges, drawing on recent advances in causal modeling, retrieval-augmented generation (RAG), multi-agent memory frameworks, and robust evaluation pipelines. These strategies are discussed not only as technical fixes but as foundational requirements for scaling agentic systems into high-stakes domains such as healthcare, finance, and autonomous robotics.\n\nTaken together, this methodological structure enables a comprehensive and systematic assessment of the state of AI Agents and Agentic AI. By sequencing the analysis across foundational understanding, model integration, architectural growth, applications, and limitations, the study aims to provide both theoretical clarity and practical guidance to researchers and practitioners navigating this rapidly evolving field.\n\n1) Search Strategy: To construct this review, we implemented a hybrid search methodology combining traditional academic repositories and AI-enhanced literature discovery tools. Specifically, twelve platforms were queried: academic databases such as Google Scholar, IEEE Xplore, ACM Digital Library, Scopus, Web of Science, ScienceDirect, and arXiv; and AI-powered interfaces including ChatGPT, Perplexity.ai, DeepSeek, Hugging Face Search, and Grok. Search queries incorporated Boolean combinations of terms such as \"AI Agents,\" \"Agentic AI,\" \"LLM Agents,\" \"Tool-augmented LLMs,\" and \"Multi-Agent AI Systems.\"\n\nTargeted queries such as \"Agentic AI + Coordination +\n\nPlanning,\" and \"AI Agents + Tool Usage + Reasoning\" were employed to retrieve papers addressing both conceptual underpinnings and system-level implementations. Literature inclusion was based on criteria such as novelty, empirical evaluation, architectural contribution, and citation impact. The rising global interest in these technologies illustrated in Figure 1 using Google Trends data reinforces the urgency of synthesizing this emerging knowledge space.\n\n# II. FOUNDATIONAL UNDERSTANDING OF AI AGENTS\n\nAI Agents are an autonomous software entities engineered for goal-directed task execution within bounded digital environments [14], [39]. These agents are defined by their ability to perceive structured or unstructured inputs [40], reason over contextual information [41], [42], and initiate actions toward achieving specific objectives, often acting as surrogates for human users or subsystems [43]. Unlike conventional automation scripts, which follow deterministic workflows, AI agents demonstrate reactive intelligence and limited adaptability, allowing them to interpret dynamic inputs and reconfigure outputs accordingly [44]. Their adoption has been reported across a range of application domains, including customer service automation [45], [46], personal productivity assistance [47], internal information retrieval [48], [49], and decision support systems [50], [51].\n\n1) Overview of Core Characteristics of AI Agents: AI Agents are widely conceptualized as instantiated operational embodiments of artificial intelligence designed to interface with users, software ecosystems, or digital infrastructures in pursuit of goal-directed behavior [52]–[54]. These agents distinguish themselves from general-purpose LLMs by exhibiting structured initialization, bounded autonomy, and persistent\n\ntask orientation. While LLMs primarily function as reactive prompt followers [55], AI Agents operate within explicitly defined scopes, engaging dynamically with inputs and producing actionable outputs in real-time environments [56].\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/b3f3b81e4c6726d994ba88d2ae2100d82522360cbade29e2e6c02e6a872fdd37.jpg)  \nFig. 4: Core characteristics of AI Agents autonomy, task-specificity, and reactivity illustrated with symbolic representations for agent design and operational behavior.\n\nFigure 4 illustrates the three foundational characteristics that recur across architectural taxonomies and empirical deployments of AI Agents. These include autonomy, task-specificity, and reactivity with adaptation. First, autonomy denotes the agent's ability to act independently post-deployment, minimizing human-in-the-loop dependencies and enabling largescale, unattended operation [46], [57]. Second, task-specificity encapsulates the design philosophy of AI agents being specialized for narrowly scoped tasks allowing high-performance optimization within a defined functional domain such as scheduling, querying, or filtering [58], [59]. Third, reactivity refers to an agent's capacity to respond to changes in its environment, including user commands, software states, or API responses; when extended with adaptation, this includes feedback loops and basic learning heuristics [17], [60].\n\nTogether, these three traits provide a foundational profile for understanding and evaluating AI Agents across deployment scenarios. The remainder of this section elaborates on each characteristic, offering theoretical grounding and illustrative examples.\n\n- Autonomy: A central feature of AI Agents is their ability to function with minimal or no human intervention after deployment [57]. Once initialized, these agents are capable of perceiving environmental inputs, reasoning over contextual data, and executing predefined or adaptive actions in real-time [17]. Autonomy enables scalable deployment in applications where persistent oversight is impractical, such as customer support bots or scheduling assistants [46], [61].  \n- Task-Specificity: AI Agents are purpose-built for narrow, well-defined tasks [58], [59]. They are optimized to execute repeatable operations within a fixed domain, such as email filtering [62], [63], database querying [64], or calendar coordination [38], [65]. This task specialization allows for efficiency, interpretability, and high precision\n\nin automation tasks where general-purpose reasoning is unnecessary or inefficient.\n\n- Reactivity and Adaptation: AI Agents often include basic mechanisms for interacting with dynamic inputs, allowing them to respond to real-time stimuli such as user requests, external API calls, or state changes in software environments [17], [60]. Some systems integrate rudimentary learning [66] through feedback loops [67], [68], heuristics [69], or updated context buffers to refine behavior over time, particularly in settings like personalized recommendations or conversation flow management [70]-[72].\n\nThese core characteristics collectively enable AI Agents to serve as modular, lightweight interfaces between pretrained AI models and domain-specific utility pipelines. Their architectural simplicity and operational efficiency position them as key enablers of scalable automation across enterprise, consumer, and industrial settings. While limited in reasoning depth compared to more general AI systems, their high usability and performance within constrained task boundaries have made them foundational components in contemporary intelligent system design.\n\n2) Foundational Models: The Role of LLMs and LIMs: The foundational progress in AI agents has been significantly accelerated by the development and deployment of LLMs and LIMs, which serve as the core reasoning and perception engines in contemporary agent systems. These models enable AI agents to interact intelligently with their environments, understand multimodal inputs, and perform complex reasoning tasks that go beyond hard-coded automation.\n\nLLMs such as GPT-4 [73] and PaLM [74] are trained on massive datasets of text from books, web content, and dialogue corpora. These models exhibit emergent capabilities in natural language understanding, question answering, summarization, dialogue coherence, and even symbolic reasoning [75], [76]. Within AI agent architectures, LLMs serve as the primary decision-making engine, allowing the agent to parse user queries, plan multi-step solutions, and generate naturalistic responses. For instance, an AI customer support agent powered by GPT-4 can interpret customer complaints, query backend systems via tool integration, and respond in a contextually appropriate and emotionally aware manner [77].\n\nLarge Image Models (LIMs) such as CLIP [78] and BLIP-2 [79] extend the agent's capabilities into the visual domain. Trained on image-text pairs, LIMs enable perception-based tasks including image classification, object detection, and vision-language grounding. These capabilities are increasingly vital for agents operating in domains such as robotics [80], autonomous vehicles [81], [82], and visual content moderation [83], [84].\n\nFor example, as illustrated in Figure 5 in an autonomous drone agent tasked with inspecting orchards, a LIM can identify diseased fruits or damaged branches by interpreting live aerial imagery and triggering predefined intervention protocols. Upon detection, the system autonomously triggers predefined intervention protocols, such as notifying horti\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/e21f8f305fc6c01732664186819ab633e78e369e8b7bc2bc85abf19d0ccb8146.jpg)  \nFig. 5: An AI agent-enabled drone autonomously inspects an orchard, identifying diseased fruits and damaged branches using vision models, and triggers real-time alerts for targeted horticultural interventions\n\ncultural staff or marking the location for targeted treatment without requiring human intervention [17], [57]. This workflow exemplifies the autonomy and reactivity of AI agents in agricultural environment and recent literature underscores the growing sophistication of such drone-based AI agents. Chitra et al. [85] provide a comprehensive overview of AI algorithms foundational to embodied agents, highlighting the integration of computer vision, SLAM, reinforcement learning, and sensor fusion. These components collectively support real-time perception and adaptive navigation in dynamic environments. Kourav et al. [86] further emphasize the role of natural language processing and large language models in generating drone action plans from human-issued queries, demonstrating how LLMs support naturalistic interaction and mission planning. Similarly, Natarajan et al. [87] explore deep learning and reinforcement learning for scene understanding, spatial mapping, and multi-agent coordination in aerial robotics. These studies converge on the critical importance of AI-driven autonomy, perception, and decision-making in advancing drone-based agents.\n\nImportantly, LLMs and LIMs are often accessed via inference APIs provided by cloud-based platforms such as OpenAI https://openai.com/, HuggingFace https://huggingface.co/, and Google Gemini https://gemini.google.com/app. These services abstract away the complexity of model training and fine-tuning, enabling developers to rapidly build and deploy agents equipped with state-of-the-art reasoning and perceptual abilities. This composability accelerates prototyping and allows agent frameworks like LangChain [88] and AutoGen [89] to orchestrate LLM and LIM outputs across task workflows. In short, foundational models give modern AI agents their basic understanding of language and visuals. Language models help them reason with words, and image models help them understand pictures-working together, they allow AI to make\n\nsmart decisions in complex situations.\n\n3) Generative AI as a Precursor: A consistent theme in the literature is the positioning of generative AI as the foundational precursor to agentic intelligence. These systems primarily operate on pretrained LLMs and LIMs, which are optimized to synthesize novel content text, images, audio, or code based on input prompts. While highly expressive, generative models fundamentally exhibit reactive behavior: they produce output only when explicitly prompted and do not pursue goals autonomously or engage in self-initiated reasoning [90], [91].\n\n# Key Characteristics of Generative AI:\n\n- Reactivity: As non-autonomous systems, generative models are exclusively input-driven [92], [93]. Their operations are triggered by user-specified prompts and they lack internal states, persistent memory, or goal-following mechanisms [94]-[96].  \n- Multimodal Capability: Modern generative systems can produce a diverse array of outputs, including coherent narratives, executable code, realistic images, and even speech transcripts. For instance, models like GPT-4 [73], PaLM-E [97], and BLIP-2 [79] exemplify this capacity, enabling language-to-image, image-to-text, and cross-modal synthesis tasks.  \n- Prompt Dependency and Statelessness: Generative systems are stateless in that they do not retain context across interactions unless explicitly provided [98], [99]. Their design lacks intrinsic feedback loops [100], state management [101], [102], or multi-step planning a requirement for autonomous decision-making and iterative goal refinement [103], [104].\n\nDespite their remarkable generative fidelity, these systems are constrained by their inability to act upon the environment or manipulate digital tools independently. For instance, they cannot search the internet, parse real-time data, or interact with APIs without human-engineered wrappers or scaffolding layers. As such, they fall short of being classified as true AI Agents, whose architectures integrate perception, decision-making, and external tool-use within closed feedback loops.\n\nThe limitations of generative AI in handling dynamic tasks, maintaining state continuity, or executing multi-step plans led to the development of tool-augmented systems, commonly referred to as AI Agents [105]. These systems build upon the language processing backbone of LLMs but introduce additional infrastructure such as memory buffers, tool-calling APIs, reasoning chains, and planning routines to bridge the gap between passive response generation and active task completion. This architectural evolution marks a critical shift in AI system design: from content creation to autonomous utility [106], [107]. The trajectory from generative systems to AI agents underscores a progressive layering of functionality that ultimately supports the emergence of agentic behaviors.\n\n# A. Language Models as the Engine for AI Agent Progression\n\nThe emergence of Ai agent as a transformative paradigm in artificial intelligence is closely tied to the evolution and repurposing of large-scale language models such as GPT-3\n\n[108], Llama [109], T5 [110], Baichuan 2 [111] and GPT3mix [112]. A substantial and growing body of research confirms that the leap from reactive generative models to autonomous, goal-directed agents is driven by the integration of LLMs as core reasoning engines within dynamic agentic systems. These models, originally trained for natural language processing tasks, are increasingly embedded in frameworks that require adaptive planning [113], [114], real-time decision-making [115], [116], and environment-aware behavior [117].\n\n1) LLMs as Core Reasoning Components:\n\nLLMs such as GPT-4 [73], PaLM [74], Claude https://www.anthropic.com/news/claude-3-5-sonnet, and LLaMA [109] are pre-trained on massive text corpora using self-supervised objectives and fine-tuned using techniques such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) [118], [119]. These models encode rich statistical and semantic knowledge, allowing them to perform tasks like inference, summarization, code generation, and dialogue management. In agentic contexts, however, their capabilities are repurposed not merely to generate responses, but to serve as cognitive substrates interpreting user goals, generating action plans, selecting tools, and managing multi-turn workflows.\n\nRecent work identifies these models as central to the architecture of contemporary agentic systems. For instance, AutoGPT [30] and BabyAGI https://github.com/yoheinakajima/babyagi use GPT-4 as both a planner and executor: the model analyzes high-level objectives, decomposes them into actionable subtasks, invokes external APIs as needed, and monitors progress to determine subsequent actions. In such systems, the LLM operates in a loop of prompt processing, state updating, and feedback-based correction, closely emulating autonomous decision-making.\n\n2) Tool-Augmented AI Agents: Enhancing Functionality: To overcome limitations inherent to generative-only systems such as hallucination, static knowledge cutoffs, and restricted interaction scopes, researchers have proposed the concept of tool-augmented LLM agents [120] such as Easytool [121], Gentopia [122], and ToolFive [123]. These systems integrate external tools, APIs, and computation platforms into the agent's reasoning pipeline, allowing for real-time information access, code execution, and interaction with dynamic data environments.\n\nTool Invocation. When an agent identifies a need that cannot be addressed through its internal knowledge such as querying a current stock price, retrieving up-to-date weather information, or executing a script, it generates a structured function call or API request [124], [125]. These calls are typically formatted in JSON, SQL, or Python, depending on the target service, and routed through an orchestration layer that executes the task.\n\nResult Integration. Once a response is received from the tool, the output is parsed and reincorporated into the LLM's context window. This enables the agent to synthesize new reasoning paths, update its task status, and decide on the next step. The ReAct framework [126] exemplifies this architecture\n\nby combining reasoning (Chain-of-Thought prompting) and action (tool use), with LLMs alternating between internal cognition and external environment interaction.\n\n3) Illustrative Examples and Emerging Capabilities: Tool-augmented LLM agents have demonstrated capabilities across a range of applications. In AutoGPT [30], the agent may plan a product market analysis by sequentially querying the web, compiling competitor data, summarizing insights, and generating a report. In a coding context, tools like GPT-Engineer combine LLM-driven design with local code execution environments to iteratively develop software artifacts [127], [128]. In research domains, systems like Paper-QA [129] utilize LLMs to query vectorized academic databases, grounding answers in retrieved scientific literature to ensure factual integrity.\n\nThese capabilities have opened pathways for more robust behavior of AI agents such as long-horizon planning, cross-tool coordination, and adaptive learning loops. Nevertheless, the inclusion of tools also introduces new challenges in orchestration complexity, error propagation, and context window limitations all active areas of research. The progression toward AI Agents is inseparable from the strategic integration of LLMs as reasoning engines and their augmentation through structured tool use. This synergy transforms static language models into dynamic cognitive entities capable of perceiving, planning, acting, and adapting setting the stage for multi-agent collaboration, persistent memory, and scalable autonomy.\n\nFigure 6 illustrates a representative case: a news query agent that performs real-time web search, summarizes retrieved documents, and generates an articulate, context-aware answer. Such workflows have been demonstrated in implementations using LangChain, AutoGPT, and OpenAI function-calling paradigms.\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/72dfa69d3bad63c0cdb90e6887bf14d1bfeeb9216ca91fa47b43bebc6f0af2fc.jpg)  \nFig. 6: Workflow of an AI Agent performing real-time news search, summarization, and answer generation, as commonly described in the literature (e.g., Author, Year).\n\n# III. THE EMERGENCE OF AGENTIC AI FROM AI AGENT FOUNDATIONS\n\nWhile AI Agents represent a significant leap in artificial intelligence capabilities, particularly in automating narrow tasks through tool-augmented reasoning, recent literature identifies notable limitations that constrain their scalability in complex, multi-step, or cooperative scenarios [130]–[132]. These constraints have catalyzed the development of a more advanced paradigm: Agentic AI. This emerging class of systems extends the capabilities of traditional agents by enabling multiple intelligent entities to collaboratively pursue goals through structured communication [133]–[135], shared memory [136], [137], and dynamic role assignment [14].\n\n1) Conceptual Leap: From Isolated Tasks to Coordinated Systems: AI Agents, as explored in prior sections, integrate LLMs with external tools and APIs to execute narrowly scoped operations such as responding to customer queries, performing document retrieval, or managing schedules. However, as use cases increasingly demand context retention, task interdependence, and adaptability across dynamic environments, the single-agent model proves insufficient [138], [139].\n\nAgentic AI systems represent an emergent class of intelligent architectures in which multiple specialized agents collaborate to achieve complex, high-level objectives. As defined in recent frameworks, these systems are composed of modular agents each tasked with a distinct subcomponent of a broader goal and coordinated through either a centralized orchestrator or a decentralized protocol [16], [134]. This structure signifies a conceptual departure from the atomic, reactive behaviors typically observed in single-agent architectures, toward a form of system-level intelligence characterized by dynamic inter-agent collaboration.\n\nA key enabler of this paradigm is goal decomposition, wherein a user-specified objective is automatically parsed and divided into smaller, manageable tasks by planning agents [38]. These subtasks are then distributed across the agent network. Multi-step reasoning and planning mechanisms facilitate the dynamic sequencing of these subtasks, allowing the system to adapt in real time to environmental shifts or partial task failures. This ensures robust task execution even under uncertainty [14].\n\nInter-agent communication is mediated through distributed communication channels, such as asynchronous messaging queues, shared memory buffers, or intermediate output exchanges, enabling coordination without necessitating continuous central oversight [14], [140]. Furthermore, reflective reasoning and memory systems allow agents to store context across multiple interactions, evaluate past decisions, and iteratively refine their strategies [141]. Collectively, these capabilities enable Agentic AI systems to exhibit flexible, adaptive, and collaborative intelligence that exceeds the operational limits of individual agents.\n\nA widely accepted conceptual illustration in the literature delineates the distinction between AI Agents and Agentic AI through the analogy of smart home systems. As depicted in\n\nFigure 7, the left side represents a traditional AI Agent in the form of a smart thermostat. This standalone agent receives a user-defined temperature setting and autonomously controls the heating or cooling system to maintain the target temperature. While it demonstrates limited autonomy such as learning user schedules or reducing energy usage during absence, it operates in isolation, executing a singular, well-defined task without engaging in broader environmental coordination or goal inference [17], [57].\n\nIn contrast, the right side of Figure 7 illustrates an Agentic AI system embedded in a comprehensive smart home ecosystem. Here, multiple specialized agents interact synergistically to manage diverse aspects such as weather forecasting, daily scheduling, energy pricing optimization, security monitoring, and backup power activation. These agents are not just reactive modules; they communicate dynamically, share memory states, and collaboratively align actions toward a high-level system goal (e.g., optimizing comfort, safety, and energy efficiency in real time). For instance, a weather forecast agent might signal upcoming heatwaves, prompting early pre-cooling via solar energy before peak pricing hours, as coordinated by an energy management agent. Simultaneously, the system might delay high-energy tasks or activate surveillance systems during occupant absence, integrating decisions across domains. This figure embodies the architectural and functional leap from task-specific automation to adaptive, orchestrated intelligence. The AI Agent acts as a deterministic component with limited scope, while Agentic AI reflects distributed intelligence, characterized by goal decomposition, inter-agent communication, and contextual adaptation, hallmarks of modern agentic AI frameworks.\n\n2) Key Differentiators between AI Agents and Agentic AI: To systematically capture the evolution from Generative AI to AI Agents and further to Agentic AI, we structure our comparative analysis around a foundational taxonomy where Generative AI serves as the baseline. While AI Agents and Agentic AI represent increasingly autonomous and interactive systems, both paradigms are fundamentally grounded in generative architectures, especially LLMs and LIMs. Consequently, each comparative table in this subsection includes Generative AI as a reference column to highlight how agentic behavior diverges and builds upon generative foundations.\n\nA set of fundamental distinctions between AI Agents and Agentic AI particularly in terms of scope, autonomy, architectural composition, coordination strategy, and operational complexity are synthesized in Table I, derived from close analysis of prominent frameworks such as AutoGen [89] and ChatDev [142]. These comparisons provide a multi-dimensional view of how single-agent systems transition into coordinated, multiagent ecosystems. Through the lens of generative capabilities, we trace the increasing sophistication in planning, communication, and adaptation that characterizes the shift toward Agentic AI.\n\nWhile Table I delineates the foundational and operational differences between AI Agents and Agentic AI, a more granular taxonomy is required to understand how these paradigms\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/7aeb83720d2279405e2da9490dd653317c44498a9f9c9397d4a57a824791c4f6.jpg)  \nFig. 7: Comparative illustration of AI Agent vs. Agentic AI, synthesizing conceptual distinctions found in the literature (e.g., Author, Year). Left: A single-task AI Agent. Right: A multi-agent, collaborative Agentic AI system.\n\nemerge from and relate to broader generative frameworks. Specifically, the conceptual and cognitive progression from static Generative AI systems to tool-augmented AI Agents, and further to collaborative Agentic AI ecosystems, necessitates an integrated comparative framework. This transition is not merely structural but also functional encompassing how initiation mechanisms, memory use, learning capacities, and orchestration strategies evolve across the agentic spectrum. Moreover, recent studies suggest the emergence of hybrid paradigms such as \"Generative Agents,\" which blend generative modeling with modular task specialization, further complicating the agentic landscape. In order to capture these nuanced relationships, Table II synthesizes the key conceptual and cognitive dimensions across four archetypes: Generative AI, AI Agents, Agentic AI, and inferred Generative Agents. By positioning Generative AI as a baseline technology, this taxonomy highlights the scientific continuum that spans from passive content generation to interactive task execution and finally to autonomous, multi-agent orchestration. This multitiered lens is critical for understanding both the current capabilities and future trajectories of agentic intelligence across applied and theoretical domains.\n\nTo further operationalize the distinctions outlined in Table I, Tables III and II extend the comparative lens to encompass a broader spectrum of agent paradigms including\n\nAI Agents, Agentic AI, and emerging Generative Agents. Table III presents key architectural and behavioral attributes that highlight how each paradigm differs in terms of primary capabilities, planning scope, interaction style, learning dynamics, and evaluation criteria. AI Agents are optimized for discrete task execution with limited planning horizons and rely on supervised or rule-based learning mechanisms. In contrast, Agentic AI systems extend this capacity through multi-step planning, meta-learning, and inter-agent communication, positioning them for use in complex environments requiring autonomous goal setting and coordination. Generative Agents, as a more recent construct, inherit LLM-centric pretraining capabilities and excel in producing multimodal content creatively, yet they lack the proactive orchestration and state-persistent behaviors seen in Agentic AI systems.\n\nThe second table (Table III) provides a process-driven comparison across three agent categories: Generative AI, AI Agents, and Agentic AI. This framing emphasizes how functional pipelines evolve from prompt-driven single-model inference in Generative AI, to tool-augmented execution in AI Agents, and finally to orchestrated agent networks in Agentic AI. The structure column underscores this progression: from single LLMs to integrated toolchains and ultimately to distributed multi-agent systems. Access to external data, a key operational requirement for real-world utility, also increases\n\nTABLE I: Key Differences Between AI Agents and Agentic AI  \n\n<table><tr><td>Feature</td><td>AI Agents</td><td>Agentic AI</td></tr><tr><td>Definition</td><td>Autonomous software programs that perform specific tasks.</td><td>Systems of multiple AI agents collaborating to achieve complex goals.</td></tr><tr><td>Autonomy Level</td><td>High autonomy within specific tasks.</td><td>Higher autonomy with the ability to manage multi-step, complex tasks.</td></tr><tr><td>Task Complexity</td><td>Typically handle single, specific tasks.</td><td>Handle complex, multi-step tasks requiring coordination.</td></tr><tr><td>Collaboration</td><td>Operate independently.</td><td>Involve multi-agent collaboration and information sharing.</td></tr><tr><td>Learning and Adaptation</td><td>Learn and adapt within their specific domain.</td><td>Learn and adapt across a wider range of tasks and environments.</td></tr><tr><td>Applications</td><td>Customer service chatbots, virtual assistants, automated workflows.</td><td>Supply chain management, business process optimization, virtual project managers.</td></tr></table>\n\nin sophistication, from absent or optional in Generative AI to modular and coordinated in Agentic AI. Collectively, these comparative views reinforce that the evolution from generative to agentic paradigms is marked not just by increasing system complexity but also by deeper integration of autonomy, memory, and decision-making across multiple levels of abstraction.\n\nFurthermore, to provide a deeper multi-dimensional understanding of the evolving agentic landscape, Tables V through IX extend the comparative taxonomy to dissect five critical dimensions: core function and goal alignment, architectural composition, operational mechanism, scope and complexity, and interaction-autonomy dynamics. These dimensions serve to not only reinforce the structural differences between Generative AI, AI Agents, and Agentic AI, but also introduce an emergent category Generative Agents representing modular agents designed for embedded subtask-level generation within broader workflows. Table V situates the three paradigms in terms of their overarching goals and functional intent. While Generative AI centers on prompt-driven content generation, AI Agents emphasize tool-based task execution, and Agentic AI systems orchestrate full-fledged workflows. This functional expansion is mirrored architecturally in Table VI, where the system design transitions from single-model reliance (in Generative AI) to multi-agent orchestration and shared memory utilization in Agentic AI. Table VII then outlines how these paradigms differ in their workflow execution pathways, highlighting the rise of inter-agent coordination and hierarchical communication as key drivers of agentic behavior.\n\nFurthermore, Table VIII explores the increasing scope and operational complexity handled by these systems ranging from isolated content generation to adaptive, multi-agent collaboration in dynamic environments. Finally, Table IX synthesizes the varying degrees of autonomy, interaction style,\n\nand decision-making granularity across the paradigms. These tables collectively establish a rigorous framework to classify and analyze agent-based AI systems, laying the groundwork for principled evaluation and future design of autonomous, intelligent, and collaborative agents operating at scale.\n\nEach of the comparative tables presented from Table V through Table IX offers a layered analytical lens to isolate the distinguishing attributes of Generative AI, AI Agents, and Agentic AI, thereby grounding the conceptual taxonomy in concrete operational and architectural features. Table V, for instance, addresses the most fundamental layer of differentiation: core function and system goal. While Generative AI is narrowly focused on reactive content production conditioned on user prompts, AI Agents are characterized by their ability to perform targeted tasks using external tools. Agentic AI, by contrast, is defined by its ability to pursue high-level goals through the orchestration of multiple subagents each addressing a component of a broader workflow. This shift from output generation to workflow execution marks a critical inflection point in the evolution of autonomous systems.\n\nIn Table VI, the architectural distinctions are made explicit, especially in terms of system composition and control logic. Generative AI relies on a single model with no built-in capability for tool use or delegation, whereas AI Agents combine language models with auxiliary APIs and interface mechanisms to augment functionality. Agentic AI extends this further by introducing multi-agent systems where collaboration, memory persistence, and orchestration protocols are central to the system's operation. This expansion is crucial for enabling intelligent delegation, context preservation, and dynamic role assignment capabilities absent in both generative and single-agent systems. Likewise in Table VII dives deeper into how these systems function operationally, emphasizing differences in execution logic and information flow. Unlike Generative AI's linear pipeline (prompt  $\\rightarrow$  output), AI Agents implement procedural mechanisms to incorporate tool responses midprocess. Agentic AI introduces recursive task reallocation and cross-agent messaging, thus facilitating emergent decision-making that cannot be captured by static LLM outputs alone. Table VIII further reinforces these distinctions by mapping each system's capacity to handle task diversity, temporal scale, and operational robustness. Here, Agentic AI emerges as uniquely capable of supporting high-complexity goals that demand adaptive, multi-phase reasoning and execution strategies.\n\nFurthermore, Table IX brings into sharp relief the operational and behavioral distinctions across Generative AI, AI Agents, and Agentic AI, with a particular focus on autonomy levels, interaction styles, and inter-agent coordination. Generative AI systems, typified by models such as GPT-3 [108] and DALL-E https://openai.com/index/dall-e-3/, remain reactive generating content solely in response to prompts without maintaining persistent state or engaging in iterative reasoning. In contrast, AI Agents such as those constructed with LangChain [88] or MetaGPT [143], exhibit a higher degree of autonomy, capable of initiating external tool invocations and adapting behaviors within bounded tasks. However,\n\nTABLE II: Taxonomy Summary of AI Agent Paradigms: Conceptual and Cognitive Dimensions  \n\n<table><tr><td>Conceptual Dimension</td><td>Generative AI</td><td>AI Agent</td><td>Agentic AI</td><td>Generative (Inferred)</td><td>Agent</td></tr><tr><td>Initiation Type</td><td>Prompt-triggered by user or input</td><td>Prompt or goal-triggered with tool use</td><td>Goal-initiated or orchestrated task</td><td>Prompt or system-level trigger</td><td>Prompt or system-level trigger</td></tr><tr><td>Goal Flexibility</td><td>(None) fixed per prompt</td><td>(Low) executes specific goal</td><td>(High) decomposes and adapts goals</td><td>(Low) guided by subtask goal</td><td>(Low) guided by subtask goal</td></tr><tr><td>Temporal Continuity</td><td>Stateless, single-session output</td><td>Short-term continuity within task</td><td>Persistent across workflow stages</td><td>Context-limited to subtask</td><td>Persistent across workflow stages</td></tr><tr><td>Learning/Adaptation</td><td>Static (pretrained)</td><td>(Might in future) Tool selection strategies may evolve</td><td>(Yes) Learns from outcomes</td><td>Typically static; limited adaptation</td><td>Typically static; limited adaptation</td></tr><tr><td>Memory Use</td><td>No memory or short context window</td><td>Optional memory or tool cache</td><td>Shared episodic/task memory</td><td>Subtask-local or contextual memory</td><td>Subtask-local or contextual memory</td></tr><tr><td>Coordination Strategy</td><td>None (single-step process)</td><td>Isolated task execution</td><td>Hierarchical or decentralized coordination</td><td>Receives instructions from system</td><td>Receives instructions from system</td></tr><tr><td>System Role</td><td>Content generator</td><td>Tool-using task executor</td><td>Collaborative workflow or-chestrator</td><td>Subtask-level modular generator</td><td>Subtask-level modular generator</td></tr></table>\n\nTABLE III: Key Attributes of AI Agents, Agentic AI, and Generative Agents  \n\n<table><tr><td>Aspect</td><td>AI Agent</td><td>Agentic AI</td><td>Generative Agent</td></tr><tr><td>Primary Capability</td><td>Task execution</td><td>Autonomous goal setting</td><td>Content generation</td></tr><tr><td>Planning Horizon</td><td>Single-step</td><td>Multi-step</td><td>N/A (content only)</td></tr><tr><td>Learning Mecanism</td><td>Rule-based or supervised</td><td>Reinforcement/meta-learning</td><td>Large-scale pre-training</td></tr><tr><td>Interaction Style</td><td>Reactive</td><td>Proactive</td><td>Creative</td></tr><tr><td>Evaluation Focus</td><td>Accuracy, latency</td><td>Engagement, adaptability</td><td>Coherence, diversity</td></tr></table>\n\nTABLE IV: Comparison of Generative AI, AI Agents, and Agentic AI  \n\n<table><tr><td>Feature</td><td>Generative AI</td><td>AI Agent</td><td>Agentic AI</td></tr><tr><td>Core Function</td><td>Content generation</td><td>Task-specific execution using tools</td><td>Complex workflow automation</td></tr><tr><td>Mechanism</td><td>Prompt → LLM → Output</td><td>Prompt → Tool Call → LLM → Output</td><td>Goal → Agent Orchestration → Output</td></tr><tr><td>Structure</td><td>Single model</td><td>LLM + tool(s)</td><td>Multi-agent system</td></tr><tr><td>External Data Access</td><td>None (unless added)</td><td>Via external APIs</td><td>Coordinated multi-agent access</td></tr><tr><td>Key Trait</td><td>Reactivity</td><td>Tool-use</td><td>Collaboration</td></tr></table>\n\ntheir autonomy is typically confined to isolated task execution, lacking long-term state continuity or collaborative interaction.\n\nAgentic AI systems mark a significant departure from these paradigms by introducing internal orchestration mechanisms and multi-agent collaboration frameworks. For example, platforms like AutoGen [89] and ChatDev [142] exemplify agentic coordination through task decomposition, role assignment, and recursive feedback loops. In AutoGen, one agent might\n\nserve as a planner while another retrieves information and a third synthesizes a report each communicating through shared memory buffers and governed by an orchestrator agent that monitors dependencies and overall task progression. This structured coordination allows for more complex goal pursuit and flexible behavior in dynamic environments. Such architectures fundamentally shift the locus of intelligence from single-model outputs to emergent system-level behavior, wherein agents learn, negotiate, and update decisions based on evolving task states. Thus, the comparative taxonomy not only highlights increasing levels of operational independence but also illustrates how Agentic AI introduces novel paradigms of communication, memory integration, and decentralized control, paving the way for the next generation of autonomous systems with scalable, adaptive intelligence.\n\n# A. Architectural Evolution: From AI Agents to Agentic AI Systems\n\nWhile both AI Agents and Agentic AI systems are grounded in modular design principles, Agentic AI significantly extends the foundational architecture to support more complex, distributed, and adaptive behaviors. As illustrated in Figure 8, the transition begins with core subsystems Perception, Reasoning, and Action, that define traditional AI Agents. Agentic AI enhances this base by integrating advanced components such as Specialized Agents, Advanced Reasoning & Planning, Persistent Memory, and Orchestration. The figure further emphasizes emergent capabilities including Multi-Agent Collaboration, System Coordination, Shared Context, and Task Decomposition, all encapsulated within a dotted boundary that signifies the shift toward reflective, decentralized, and goal-driven system architectures. This progression marks a fundamental inflection point in intelligent agent design. This section synthesizes findings from empirical frameworks such as LangChain [88], AutoGPT [89], and TaskMatrix [144], highlighting this progression in architectural sophistication.\n\n1) Core Architectural Components of AI Agents: Foundational AI Agents are typically composed of four primary subsystems: perception, reasoning, action, and learning. These\n\nTABLE V: Comparison by Core Function and Goal  \n\n<table><tr><td>Feature</td><td>Generative AI</td><td>AI Agent</td><td>Agentic AI</td><td>Generative (Inferred)</td><td>Agent</td></tr><tr><td>Primary Goal</td><td>Create novel content based on prompt</td><td>Execute a specific task using external tools</td><td>Automate complex workflow or achieve high-level goals</td><td colspan=\"2\">Perform a specific generative sub-task</td></tr><tr><td>Core Function</td><td>Content generation (text, image, audio, etc.)</td><td>Task execution with external interaction</td><td>Workflow orchestration and goal achievement</td><td colspan=\"2\">Sub-task content generation within a workflow</td></tr></table>\n\nTABLE VI: Comparison by Architectural Components  \n\n<table><tr><td>Component</td><td>Generative AI</td><td>AI Agent</td><td>Agentic AI</td><td>Generative (Inferred)</td><td>Agent</td></tr><tr><td>Core Engine</td><td>LLM / LIM</td><td>LLM</td><td>Multiple LLMs (potentially diverse)</td><td>LLM</td><td></td></tr><tr><td>Prompts</td><td>Yes (input trigger)</td><td>Yes (task guidance)</td><td>Yes (system goal and agent tasks)</td><td>Yes (sub-task guidance)</td><td></td></tr><tr><td>Tools/APIs</td><td>No (inherently)</td><td>Yes (essential)</td><td>Yes (available to constituent agents)</td><td>Potentially (if sub-task requires)</td><td></td></tr><tr><td>Multiple Agents</td><td>No</td><td>No</td><td>Yes (essential; collaborative)</td><td>No (is an individual agent)</td><td></td></tr><tr><td>Orchestration</td><td>No</td><td>No</td><td>Yes (implicit or explicit)</td><td>No (is part of orchestration)</td><td></td></tr></table>\n\nTABLE VII: Comparison by Operational Mechanism  \n\n<table><tr><td>Mechanism</td><td>Generative AI</td><td>AI Agent</td><td>Agentic AI</td><td>Generative (Inferred)</td><td>Agent</td></tr><tr><td>Primary Driver</td><td>Reactivity to prompt</td><td>Tool calling for task execution</td><td>Inter-agent communication and collaboration</td><td>Reactivity to input or sub-task prompt</td><td></td></tr><tr><td>Interaction Mode</td><td>User → LLM</td><td>User → Agent → Tool</td><td>User → System → Agents</td><td>System/Agent → Agent → Output</td><td></td></tr><tr><td>Workflow Handling</td><td>Single generation step</td><td>Single task execution</td><td>Multi-step workflow coordination</td><td>Single step within workflow</td><td></td></tr><tr><td>Information Flow</td><td>Input → Output</td><td>Input → Tool → Output</td><td>Input → Agent1 → Agent2 → ... → Output</td><td>Input (from system/agent) → Output</td><td></td></tr></table>\n\nTABLE VIII: Comparison by Scope and Complexity  \n\n<table><tr><td>Aspect</td><td>Generative AI</td><td>AI Agent</td><td>Agentic AI</td><td>Generative (Inferred)</td><td>Agent</td></tr><tr><td>Task Scope</td><td>Single piece of generated content</td><td>Single, specific, defined task</td><td>Complex, multi-faceted goal or workflow</td><td>Specific sub-task (often generative)</td><td></td></tr><tr><td>Complexity</td><td>Low (relative)</td><td>Medium (integrates tools)</td><td>High (multi-agent coordination)</td><td>Low to Medium (one task component)</td><td></td></tr><tr><td>Example (Video)</td><td>Chatbot</td><td>Tavily Search Agent</td><td>YouTube-to-Blog Conversion System</td><td>Title/Description/Conclusion Generator</td><td></td></tr></table>\n\nTABLE IX: Comparison by Interaction and Autonomy  \n\n<table><tr><td>Feature</td><td>Generative AI</td><td>AI Agent</td><td>Agentic AI</td><td>Generative (Inferred)</td><td>Agent</td></tr><tr><td>Autonomy Level</td><td>Low (requires prompt)</td><td>Medium (uses tools autonomously)</td><td>High (manages entire process)</td><td>Low to Medium (executes sub-task)</td><td></td></tr><tr><td>External Interaction</td><td>None (baseline)</td><td>Via specific tools or APIs</td><td>Through multiple agents/tools</td><td>Possibly via tools (if needed)</td><td></td></tr><tr><td>Internal Interaction</td><td>N/A</td><td>N/A</td><td>High (inter-agent)</td><td>Receives input from system or agent</td><td></td></tr><tr><td>Decision Making</td><td>Pattern selection</td><td>Tool usage decisions</td><td>Goal decomposition and assignment</td><td>Best sub-task generation strategy</td><td></td></tr></table>\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/13f7ce7956eb6484fa632ead0c4954d68dd2195949afb9be183bbec8a96e6cb6.jpg)  \nFig. 8: Illustrating architectural evolution from traditional AI Agents to modern Agentic AI systems. It begins with core modules Perception, Reasoning, and Action and expands into advanced components including Specialized Agents, Advanced Reasoning & Planning, Persistent Memory, and Orchestration. The diagram further captures emergent properties such as Multi-Agent Collaboration, System Coordination, Shared Context, and Task Decomposition, all enclosed within a dotted boundary signifying layered modularity and the transition to distributed, adaptive agentic AI intelligence.\n\nsubsystems form a closed-loop operational cycle, commonly referred to as \"Understand, Think, Act\" from a user interface perspective, or \"Input, Processing, Action, Learning\" in systems design literature [14], [145].\n\n- Perception Module: This subsystem ingests input signals from users (e.g., natural language prompts) or external systems (e.g., APIs, file uploads, sensor streams). It is responsible for preprocessing data into a format interpretable by the agent's reasoning module. For example, in LangChain-based agents [88], [146], the perception layer handles prompt templating, contextual wrapping, and retrieval augmentation via document chunking and embedding search.  \n- Knowledge Representation and Reasoning (KRR) Module: At the core of the agent's intelligence lies the KRR module, which applies symbolic, statistical, or hybrid logic to input data. Techniques include rule-based logic (e.g., if-then decision trees), deterministic workflow engines, and simple planning graphs. Reasoning in agents like AutoGPT [30] is enhanced with function-calling and prompt chaining to simulate thought processes (e.g., \"step-by-step\" prompts or intermediate tool invocations).  \n- Action Selection and Execution Module: This module\n\ntranslates inferred decisions into external actions using an action library. These actions may include sending messages, updating databases, querying APIs, or producing structured outputs. Execution is often managed by middleware like LangChain's \"agent executor,\" which links LLM outputs to tool calls and observes responses for subsequent steps [88].  \n- Basic Learning and Adaptation: Traditional AI Agents feature limited learning mechanisms, such as heuristic parameter adjustment [147], [148] or history-informed context retention. For instance, agents may use simple memory buffers to recall prior user inputs or apply scoring mechanisms to improve tool selection in future iterations.\n\nCustomization of these agents typically involves domain-specific prompt engineering, rule injection, or workflow templates, distinguishing them from hard-coded automation scripts by their ability to make context-aware decisions. Systems like ReAct [126] exemplify this architecture, combining reasoning and action in an iterative framework where agents simulate internal dialogue before selecting external actions.\n\n2) Architectural Enhancements in Agentic AI: Agentic AI systems inherit the modularity of AI Agents but extend\n\ntheir architecture to support distributed intelligence, inter-agent communication, and recursive planning. The literature documents a number of critical architectural enhancements that differentiate Agentic AI from its predecessors [149], [150].\n\n- Ensemble of Specialized Agents: Rather than operating as a monolithic unit, Agentic AI systems consist of multiple agents, each assigned a specialized function e.g., a summarizer, a retriever, a planner. These agents interact via communication channels (e.g., message queues, blackboards, or shared memory). For instance MetaGPT [143] exemplify this approach by modeling agents after corporate departments (e.g., CEO, CTO, engineer), where roles are modular, reusable, and role-bound.  \n- Advanced Reasoning and Planning: Agentic systems embed recursive reasoning capabilities using frameworks such as ReAct [126], Chain-of-Thought (CoT) prompting [151], and Tree of Thoughts [152]. These mechanisms allow agents to break down a complex task into multiple reasoning stages, evaluate intermediate results, and replan actions dynamically. This enables the system to respond adaptively to uncertainty or partial failure.  \n- Persistent Memory Architectures: Unlike traditional agents, Agentic AI incorporates memory subsystems to persist knowledge across task cycles or agent sessions [153], [154]. Memory types include episodic memory (task-specific history) [155], [156], semantic memory (long-term facts or structured data) [157], [158], and vector-based memory for retrieval-augmented generation (RAG) [159], [160]. For example, AutoGen [89] agents maintain scratchpads for intermediate computations, enabling stepwise task progression.  \n- Orchestration Layers / Meta-Agents: A key innovation in Agentic AI is the introduction of orchestrators meta-agents that coordinate the lifecycle of subordinate agents, manage dependencies, assign roles, and resolve conflicts. Orchestrators often include task managers, evaluators, or moderators. In ChatDev [142], for example, a virtual CEO meta-agent distributes subtasks to departmental agents and integrates their outputs into a unified strategic response.\n\nThese enhancements collectively enable Agentic AI to support scenarios that require sustained context, distributed labor, multi-modal coordination, and strategic adaptation. Use cases range from research assistants that retrieve, summarize, and draft documents in tandem (e.g., AutoGen pipelines [89]) to smart supply chain agents that monitor logistics, vendor performance, and dynamic pricing models in parallel.\n\nThe shift from isolated perception—reasoning—action loops to collaborative and reflective multi-agent workflows marks a key inflection point in the architectural design of intelligent systems. This progression positions Agentic AI as the next stage of AI infrastructure capable not only of executing predefined workflows but also of constructing, revising, and managing complex objectives across agents with minimal\n\nhuman supervision.\n\n# IV. APPLICATION OF AI AGENTS AND AGENTIC AI\n\nTo illustrate the real-world utility and operational divergence between AI Agents and Agentic AI systems, this study synthesizes a range of applications drawn from recent literature, as visualized in Figure 9. We systematically categorize and analyze application domains across two parallel tracks: conventional AI Agent systems and their more advanced Agentic AI counterparts. For AI Agents, four primary use cases are reviewed: (1) Customer Support Automation and Internal Enterprise Search, where single-agent models handle structured queries and response generation; (2) Email Filtering and Prioritization, where agents assist users in managing high-volume communication through classification heuristics; (3) Personalized Content Recommendation and Basic Data Reporting, where user behavior is analyzed for automated insights; and (4) Autonomous Scheduling Assistants, which interpret calendars and book tasks with minimal user input. In contrast, Agentic AI applications encompass broader and more dynamic capabilities, reviewed through four additional categories: (1) Multi-Agent Research Assistants that retrieve, synthesize, and draft scientific content collaboratively; (2) Intelligent Robotics Coordination, including drone and multirobot systems in fields like agriculture and logistics; (3) Collaborative Medical Decision Support, involving diagnostic, treatment, and monitoring subsystems; and (4) Multi-Agent Game AI and Adaptive Workflow Automation, where decentralized agents interact strategically or handle complex task pipelines.\n\n1) Application of AI Agents:  \n1) Customer Support Automation and Internal Enterprise Search: AI Agents are widely adopted in enterprise environments for automating customer support and facilitating internal knowledge retrieval. In customer service, these agents leverage retrieval-augmented LLMs interfaced with APIs and organizational knowledge bases to answer user queries, triage tickets, and perform actions like order tracking or return initiation [46]. For internal enterprise search, agents built on vector stores (e.g., Pinecone, Elascticsearch) retrieve semantically relevant documents in response to natural language queries. Tools such as Salesforce Einstein https://www.salesforce.com/artificial-intelligence/, Intercom Fin https://www.intercom.com/fin, and Notion AI https://www.notion.com/product/ai demonstrate how structured input processing and summarization capabilities reduce workload and improve enterprise decision-making.\n\nA practical example (Figure 10a) of this dual functionality can be seen in a multinational e-commerce company deploying an AI Agent-based customer support and internal search assistant. For customer support, the AI Agent integrates with the company's CRM (e.g., Salesforce) and fulfillment APIs to resolve queries such as \"Where is my order?\" or \"How can I return this\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/6f1f5ae95d27da4f73592ce56b6a0eb658ac58fda69c516809c48609f3aa7aeb.jpg)  \nFig. 9: Categorized applications of AI Agents and Agentic AI across eight core functional domains.\n\nitem?\" Within milliseconds, the agent retrieves contextual data from shipping databases and policy repositories, then generates a personalized response using retrieval-augmented generation. For internal enterprise search, employees use the same system to query past meeting notes, sales presentations, or legal documents. When an HR manager types \"summarize key benefits policy changes from last year,\" the agent queries a Pinecone vector store embedded with enterprise documentation, ranks results by semantic similarity, and returns a concise summary along with source links. These capabilities not only reduce ticket volume and support overhead but also minimize time spent searching for institutional knowledge. The result is a unified, responsive system that enhances both external service delivery and internal operational efficiency using modular AI Agent architectures.\n\n2) Email Filtering and Prioritization: Within productivity tools, AI Agents automate email triage through content classification and prioritization. Integrated with systems like Microsoft Outlook and Superhuman, these agents analyze metadata and message semantics to detect urgency, extract tasks, and recommend replies. They apply user-tuned filtering rules, behavioral signals, and intent classification to reduce cognitive overload. Autonomous actions, such as auto-tagging or summarizing threads,\n\nenhance efficiency, while embedded feedback loops enable personalization through incremental learning [61]. Figure10b illustrates a practical implementation of AI Agents in the domain of email filtering and prioritization. In modern workplace environments, users are inundated with high volumes of email, leading to cognitive overload and missed critical communications. AI Agents embedded in platforms like Microsoft Outlook or Superhuman act as intelligent intermediaries that classify, cluster, and triage incoming messages. These agents evaluate metadata (e.g., sender, subject line) and semantic content to detect urgency, extract actionable items, and suggest smart replies. As depicted, the AI agent autonomously categorizes emails into tags such as \"Urgent,\" \"Follow-up,\" and \"Low Priority,\" while also offering context-aware summaries and reply drafts. Through continual feedback loops and usage patterns, the system adapts to user preferences, gradually refining classification thresholds and improving prioritization accuracy. This automation offloads decision fatigue, allowing users to focus on high-value tasks, while maintaining efficient communication management in fast-paced, information-dense environments.\n\n3) Personalized Content Recommendation and Basic Data Reporting: AI Agents support adaptive personalization by analyzing behavioral patterns for news, prod\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/44335adacef08e3f4f132aae2ba85ccb3705cec95311d8119c2d584600e8e262.jpg)  \n(a)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/fbdd3d16b3abf297aa8ad10f931689ed9602605f4e67689bb6dc709054c2de9f.jpg)  \n(b)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/48100b714822e775902f38518c0089951be0c35abd8eecdf490653f38c3f5c3f.jpg)  \n(c)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/e9859731f25c8be9e6c5d789350a38e207e911e32eb99a2e34f6a5c7e99dd02c.jpg)  \n(d)  \nFig. 10: Applications of AI Agents in enterprise settings: (a) Customer support and internal enterprise search; (b) Email filtering and prioritization; (c) Personalized content recommendation and basic data reporting; and (d) Autonomous scheduling assistants. Each example highlights modular AI Agent integration for automation, intent understanding, and adaptive reasoning across operational workflows and user-facing systems.\n\nuct, or media recommendations. Platforms like Amazon, YouTube, and Spotify deploy these agents to infer user preferences via collaborative filtering, intent detection,\n\nand content ranking. Simultaneously, AI Agents in analytics systems (e.g., Tableau Pulse, Power BI Copilot) enable natural-language data queries and automated report generation by converting prompts to structured database queries and visual summaries, democratizing business intelligence access.\n\nA practical illustration (Figure 10c) of AI Agents in personalized content recommendation and basic data reporting can be found in e-commerce and enterprise analytics systems. Consider an AI agent deployed on a retail platform like Amazon: as users browse, click, and purchase items, the agent continuously monitors interaction patterns such as dwell time, search queries, and purchase sequences. Using collaborative filtering and content-based ranking, the agent infers user intent and dynamically generates personalized product suggestions that evolve over time. For example, after purchasing gardening tools, a user may be recommended compatible soil sensors or relevant books. This level of personalization enhances customer engagement, increases conversion rates, and supports long-term user retention. Simultaneously, within a corporate setting, an AI agent integrated into Power BI Copilot allows non-technical staff to request insights using natural language, for instance, \"Compare Q3 and Q4 sales in the Northeast.\" The agent translates the prompt into structured SQL queries, extracts patterns from the database, and outputs a concise visual summary or narrative report. This application reduces dependency on data analysts and empowers broader business decision-making through intuitive, language-driven interfaces.\n\n4) Autonomous Scheduling Assistants: AI Agents integrated with calendar systems autonomously manage meeting coordination, rescheduling, and conflict resolution. Tools like x.ai and Reclaim AI interpret vague scheduling commands, access calendar APIs, and identify optimal time slots using learned user preferences. They minimize human input while adapting to dynamic availability constraints. Their ability to interface with enterprise systems and respond to ambiguous instructions highlights the modular autonomy of contemporary scheduling agents.\n\nA practical application of autonomous scheduling agents can be seen in corporate settings as depicted in Figure 10d where employees manage multiple overlapping responsibilities across global time zones. Consider an executive assistant AI agent integrated with Google Calendar and Slack that interprets a command like \"Find a 45-minute window for a follow-up with the product team next week.\" The agent parses the request, checks availability for all participants, accounts for time zone differences, and avoids meeting conflicts or working-hour violations. If it identifies a conflict with a previously scheduled task, it may autonomously propose alternative windows and notify affected attendees via Slack integration. Additionally, the agent learns from\n\nhistorical user preferences such as avoiding early Friday meetings and refines its suggestions over time. Tools like Reclaim AI and Clockwise exemplify this capability, offering calendar-aware automation that adapts to evolving workloads. Such assistants reduce coordination overhead, increase scheduling efficiency, and enable smoother team workflows by proactively resolving ambiguity and optimizing calendar utilization.\n\nTABLE X: Representative AI Agents (2023-2025): Applications and Operational Characteristics  \n\n<table><tr><td>Model / Reference</td><td>Application Area</td><td>Operation as AI Agent</td></tr><tr><td>ChatGPT Deep Re-search Mode OpenAI (2025) Deep Research OpenAI</td><td>Research Analysis / Reporting</td><td>Synthesizes hundreds of sources into reports; functions as a self-directed research analyst.</td></tr><tr><td>Operator OpenAI (2025) Operator OpenAI</td><td>Web Automation</td><td>Navigates websites, fills forms, and completes online tasks autonomously.</td></tr><tr><td>Agentspace: Deep Re-search Agent Google (2025) Google Agentspace</td><td>Enterprise Reporting</td><td>Generates business intelligence reports using Gemini models.</td></tr><tr><td>NotebookLM Plus Agent Google (2025) NotebookLM</td><td>Knowledge Management</td><td>Summarizes, organizes, and retrieves data across Google Workspace apps.</td></tr><tr><td>Nova Act Amazon (2025) Amazon Nova</td><td>Workflow Automation</td><td>Automates browser-based tasks such as scheduling, HR requests, and email.</td></tr><tr><td>Manus Agent Monica (2025) Manus Agenthttps://manus.im/</td><td>Personal Task Automation</td><td>Executes trip planning, site building, and product comparisons via browsing.</td></tr><tr><td>Harvey Harvey AI (2025) Har-vey</td><td>Legal Automation</td><td>Automates document drafting, legal review, and predictive case analysis.</td></tr><tr><td>Otter Meeting Agent Otter.ai (2025) Otter</td><td>Meeting Management</td><td>Transcribes meetings and provides highlights, summaries, and action items.</td></tr><tr><td>Otter Sales Agent Otter.ai (2025) Otter sales agent</td><td>Sales Enablement</td><td>Analyzes sales calls, extracts insights, and suggests follow-ups.</td></tr><tr><td>ClickUp Brain ClickUp (2025) ClickUp Brain</td><td>Project Manage-ment</td><td>Automates task tracking, updates, and project workflows.</td></tr><tr><td>Agentforce Agentforce (2025) Agentforce</td><td>Customer Support</td><td>Routes tickets and generates context-aware replies for support teams.</td></tr><tr><td>Microsoft Copilot Microsoft (2024) Microsoft Copilot</td><td>Office Productivity</td><td>Automates writing, formula generation, and summarization in Microsoft 365.</td></tr><tr><td>Project Astra Google DeepMind (2025) Project Astra</td><td>Multimodal As-sistance</td><td>Processes text, image, audio, and video for task support and recommendations.</td></tr><tr><td>Claude 3.5 Agent Anthropic (2025) Claude 3.5 Sonnet</td><td>Enterprise Assist-stance</td><td>Uses multimodal input for reasoning, personalization, and enterprise task completion.</td></tr></table>\n\n# 2) Applications of Agentic AI:\n\n1) Multi-Agent Research Assistants: Agentic AI systems are increasingly deployed in academic and industrial research pipelines to automate multi-stage knowledge work. Platforms like AutoGen and CrewAI assign specialized roles to multiple agents retrievers, summarizers,\n\nsynthesizers, and citation formatters under a central orchestrator. The orchestrator distributes tasks, manages role dependencies, and integrates outputs into coherent drafts or review summaries. Persistent memory allows for cross-agent context sharing and refinement over time. These systems are being used for literature reviews, grant preparation, and patent search pipelines, outperforming single-agent systems such as ChatGPT by enabling concurrent sub-task execution and long-context management [89].\n\nFor example, a real-world application of agentic AI as depicted in Figure 11a is in the automated drafting of grant proposals. Consider a university research group preparing a National Science Foundation (NSF) submission. Using an AutoGen-based architecture, distinct agents are assigned: one retrieves prior funded proposals and extracts structural patterns; another scans recent literature to summarize related work; a third agent aligns proposal objectives with NSF solicitation language; and a formatting agent structures the document per compliance guidelines. The orchestrator coordinates these agents, resolving dependencies (e.g., aligning methodology with objectives) and ensuring stylistic consistency across sections. Persistent memory modules store evolving drafts, feedback from collaborators, and funding agency templates, enabling iterative improvement over multiple sessions. Compared to traditional manual processes, this multi-agent system significantly accelerates drafting time, improves narrative cohesion, and ensures regulatory alignment offering a scalable, adaptive approach to collaborative scientific writing in academia and R&D-intensive industries.\n\n2) Intelligent Robotics Coordination: In robotics and automation, Agentic AI underpins collaborative behavior in multi-robot systems. Each robot operates as a task specialized agent such as pickers, transporters, or mappers while an orchestrator supervises and adapts workflows. These architectures rely on shared spatial memory, real-time sensor fusion, and inter-agent synchronization for coordinated physical actions. Use cases include warehouse automation, drone-based orchard inspection, and robotic harvesting [143]. For instance, agricultural drone swarms may collectively map tree rows, identify diseased fruits, and initiate mechanical interventions. This dynamic allocation enables real-time reconfiguration and autonomy across agents facing uncertain or evolving environments.\n\nFor example, in commercial apple orchards (Figure 11b), Agentic AI enables a coordinated multi-robot system to optimize the harvest season. Here, task-specialized robots such as autonomous pickers, fruit classifiers, transport bots, and drone mappers operate as agentic units under a central orchestrator. The mapping drones first survey the orchard and use vision-language models (VLMs) to generate high-resolution yield maps and identify ripe clusters. This spatial data is shared via a\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/10000b10cc77b30216dc2d8ed1d92b4c5093f35aa6400d6aa0d49f22d0789a09.jpg)  \n(a)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/9893463a60e955fc5db4119a601640c0ad7880c14639133d39a8adcf0a1e9d4e.jpg)  \n(b)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/798430f46937ff657427e53343c8b9abfc124be2ef119fcf0b5e04f6f1aef15d.jpg)  \n(c)  \nFig. 11: Illustrative Applications of Agentic AI Across Domains: Figure 11 presents four real-world applications of agentic AI systems. (a) Automated grant writing using multi-agent orchestration for structured literature analysis, compliance alignment, and document formatting. (b) Coordinated multi-robot harvesting in apple orchards using shared spatial memory and task-specific agents for mapping, picking, and transport. (c) Clinical decision support in hospital ICUs through synchronized agents for diagnostics, treatment planning, and EHR analysis, enhancing safety and workflow efficiency. (d) Cybersecurity incident response in enterprise environments via agents handling threat classification, compliance analysis, and mitigation planning. In all cases, central orchestrators manage inter-agent communication, shared memory enables context retention, and feedback mechanisms drive continual learning. These use cases highlight agentic AI's capacity for scalable, autonomous task coordination in complex, dynamic environments across science, agriculture, healthcare, and IT security.\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/aa5eca44e46e61c780f1204260d9af7144e7c67be00f84130cd5b7c4394b30a1.jpg)  \n(d)\n\ncentralized memory layer accessible by all agents. Picker robots are assigned to high-density zones, guided by path-planning agents that optimize routes around obstacles and labor zones. Simultaneously, transport agents dynamically shuttle crates between pickers and storage, adjusting tasks in response to selector load levels and\n\nterrain changes. All agents communicate asynchronously through a shared protocol, and the orchestrator continuously adjusts task priorities based on weather forecasts or mechanical faults. If one picker fails, nearby units autonomously reallocate workload. This adaptive, memory-driven coordination exemplifies Agentic AI's\n\npotential to reduce labor costs, increase harvest efficiency, and respond to uncertainties in complex agricultural environments far surpassing the rigid programming of legacy agricultural robots [89], [143].\n\n3) Collaborative Medical Decision Support: In high-stakes clinical environments, Agentic AI enables distributed medical reasoning by assigning tasks such as diagnostics, vital monitoring, and treatment planning to specialized agents. For example, one agent may retrieve patient history, another validates findings against diagnostic guidelines, and a third proposes treatment options. These agents synchronize through shared memory and reasoning chains, ensuring coherent, safe recommendations. Applications include ICU management, radiology triage, and pandemic response. Real-world pilots show improved efficiency and decision accuracy compared to isolated expert systems [87].\n\nFor example, in a hospital ICU (Figure 11c), an agentic AI system supports clinicians in managing complex patient cases. A diagnostic agent continuously analyzes vitals and lab data for early detection of sepsis risk. Simultaneously, a history retrieval agent accesses electronic health records (EHRs) to summarize comorbidities and recent procedures. A treatment planning agent cross-references current symptoms with clinical guidelines (e.g., Surviving Sepsis Campaign), proposing antibiotic regimens or fluid protocols. The orchestrator integrates these insights, ensures consistency, and surfaces conflicts for human review. Feedback from physicians is stored in a persistent memory module, allowing agents to refine their reasoning based on prior interventions and outcomes. This coordinated system enhances clinical workflow by reducing cognitive load, shortening decision times, and minimizing oversight risks. Early deployments in critical care and oncology units have demonstrated increased diagnostic precision and better adherence to evidence-based protocols, offering a scalable solution for safer, real-time collaborative medical support.\n\n4) Multi-Agent Game AI and Adaptive Workflow Automation: In simulation environments and enterprise systems, Agentic AI facilitates decentralized task execution and emergent coordination. Game platforms like AI Dungeon deploy independent NPC agents with goals, memory, and dynamic interactivity to create emergent narratives and social behavior. In enterprise workflows, systems such as MultiOn and Cognosys use agents to manage processes like legal review or incident escalation, where each step is governed by a specialized module. These architectures exhibit resilience, exception handling, and feedback-driven adaptability far beyond rule-based pipelines.\n\nFor example, in a modern enterprise IT environment (as depicted in Figure 11d), Agentic AI systems are increasingly deployed to autonomously manage cybersecurity incident response workflows. When a potential\n\nthreat is detected such as abnormal access patterns or unauthorized data exfiltration specialized agents are activated in parallel. One agent performs real-time threat classification using historical breach data and anomaly detection models. A second agent queries relevant log data from network nodes and correlates patterns across systems. A third agent interprets compliance frameworks (e.g., GDPR or HIPAA) to assess the regulatory severity of the event. A fourth agent simulates mitigation strategies and forecasts operational risks. These agents coordinate under a central orchestrator that evaluates collective outputs, integrates temporal reasoning, and issues recommended actions to human analysts. Through shared memory structures and iterative feedback, the system learns from prior incidents, enabling faster and more accurate responses in future cases. Compared to traditional rule-based security systems, this agentic model enhances decision latency, reduces false positives, and supports proactive threat containment in large-scale organizational infrastructures [89].\n\n# V. CHALLENGES AND LIMITATIONS IN AI AGENTS AND AGENTIC AI\n\nTo systematically understand the operational and theoretical limitations of current intelligent systems, we present a comparative visual synthesis in Figure 12, which categorizes challenges and potential remedies across both AI Agents and Agentic AI paradigms. Figure 12a outlines the four most pressing limitations specific to AI Agents namely, lack of causal reasoning, inherited LLM constraints (e.g., hallucinations, shallow reasoning), incomplete agentic properties (e.g., autonomy, proactivity), and failures in long-horizon planning and recovery. These challenges often arise due to their reliance on stateless LLM prompts, limited memory, and heuristic reasoning loops.\n\nIn contrast, Figure 12b identifies eight critical bottlenecks unique to Agentic AI systems, such as inter-agent error cascades, coordination breakdowns, emergent instability, scalability limits, and explainability issues. These challenges stem from the complexity of orchestrating multiple agents across distributed tasks without standardized architectures, robust communication protocols, or causal alignment frameworks.\n\nFigure 13 complements this diagnostic framework by synthesizing ten forward-looking design strategies aimed at mitigating these limitations. These include Retrieval-Augmented Generation (RAG), tool-based reasoning [120], [121], [123], agentic feedback loops (ReAct [126]), role-based multi-agent orchestration, memory architectures, causal modeling, and governance-aware design. Together, these three panels offer a consolidated roadmap for addressing current pitfalls and accelerating the development of safe, scalable, and context-aware autonomous systems.\n\n1) Challenges and Limitations of AI Agents: While AI Agents have garnered considerable attention for their ability to automate structured tasks using LLMs and tool-use interfaces, the literature highlights significant theoretical and practical\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/d7b520e156b5904fa072bdba8e484e202d70696410eda29ff1cd9bfc47bcd27c.jpg)  \n(a)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/9f89cccb2bc74ebcefaeda8a5ee9653213dd462cba5d14c8d77a8db9964b0797.jpg)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/34234edfbfe0fa6744b642854d20782e31511dde77b8bf7352717cb2832e043f.jpg)  \nFig. 12: Challenges and Solutions Across Agentic Paradigms. (a) Key limitations of AI Agents including causality deficits and shallow reasoning. (b) Amplified coordination and stability challenges in Agentic AI systems.\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/399c016aec7419c6505d9aaf27d871e72b42d1c293848aa6bdcbfed067dd7b19.jpg)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/0f8be28dd9cf116b5dca027ae369e6932ca9eb91f15da6000f8c19a3541e6229.jpg)  \n(b)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/f72e5ad951dd52dba5aada9f7c06f9301d500bb18ca1bfb6e136aa3c1bd40678.jpg)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/9d38db690b35acd740eaf65229a71e52396b749c00e40d290f7f5daa8ab9f088.jpg)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/2b5bc1df0b14cc5ea70f54f27ae7e2d27faf780c05dad374dae8f33aba222b04.jpg)\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/67477b2e12034e2894a8c07b37fb5b6fa429771300a6c498d58586ae4d6e698e.jpg)\n\nlimitations that inhibit their reliability, generalization, and long-term autonomy [126], [150]. These challenges arise from both the architectural dependence on static, pretrained models and the difficulty of instilling agentic qualities such as causal reasoning, planning, and robust adaptation. The key challenges and limitations (Figure 12a) of AI Agents are as summarized into following five points:\n\n1) Lack of Causal Understanding: One of the most foundational challenges lies in the agents' inability to reason causally [164], [165]. Current LLMs, which form the cognitive core of most AI Agents, excel at identifying statistical correlations within training data. However, as noted in recent research from DeepMind and conceptual analyses by TrueTheta, they fundamentally lack the capacity for causal modeling distinguishing between mere association and cause-effect relationships [166]–[168]. For instance, while an LLM-powered agent might learn that visiting a hospital often co-occurs with illness, it cannot infer whether the illness causes the visit or vice versa, nor can it simulate interventions or hypothetical changes.\n\nThis deficit becomes particularly problematic under distributional shifts, where real-world conditions differ from the training regime [169], [170]. Without such grounding, agents remain brittle, failing in novel or high-stakes scenarios. For example, a navigation agent that excels in urban driving may misbehave in snow or construction zones if it lacks an internal causal model of road traction or spatial occlusion.\n\n2) Inherited Limitations from LLMs: AI Agents, particularly those powered by LLMs, inherit a number of intrinsic limitations that impact their reliability, adaptability, and overall trustworthiness in practical deployments [171]-[173]. One of the most prominent issues is the ten\n\ndency to produce hallucinations plausible but factually incorrect outputs. In high-stakes domains such as legal consultation or scientific research, these hallucinations can lead to severe misjudgments and erode user trust [174], [175]. Compounding this is the well-documented prompt sensitivity of LLMs, where even minor variations in phrasing can lead to divergent behaviors. This brittleness hampers reproducibility, necessitating meticulous manual prompt engineering and often requiring domain-specific tuning to maintain consistency across interactions [176].\n\nFurthermore, while recent agent frameworks adopt reasoning heuristics like Chain-of-Thought (CoT) [151], [177] and ReAct [126] to simulate deliberative processes, these approaches remain shallow in semantic comprehension. Agents may still fail at multi-step inference, misalign task objectives, or make logically inconsistent conclusions despite the appearance of structured reasoning [126]. Such shortcomings underscore the absence of genuine understanding and generalizable planning capabilities.\n\nAnother key limitation lies in computational cost and latency. Each cycle of agentic decision-making particularly in planning or tool-calling may require several LLM invocations. This not only increases runtime latency but also scales resource consumption, creating practical bottlenecks in real-world deployments and cloud-based inference systems. Furthermore, LLMs have a static knowledge cutoff and cannot dynamically integrate new information unless explicitly augmented via retrieval or tool plugins. They also reproduce the biases of their training datasets, which can manifest as culturally insensitive or skewed responses [178], [179]. Without rigorous auditing and mitigation strategies,\n\nTABLE XI: Representative Agentic AI Models (2023-2025): Applications and Operational Characteristics  \n\n<table><tr><td>Model / Reference</td><td>Application Area</td><td>Operation as Agentic AI</td></tr><tr><td>Auto-GPT [30]</td><td>Task Automation</td><td>Decomposes high-level goals, executes subtasks via tools/APIs, and iteratively self-corrects.</td></tr><tr><td>GPT Engineer Open Source (2023) GPT Engineer</td><td>Code Generation</td><td>Builds entire codebases: plans, writes, tests, and re-fines based on output.</td></tr><tr><td>MetaGPT [143])</td><td>Software Collab-oration</td><td>Coordinates specialized agents (e.g., coder, tester) for modular multi-role project development.</td></tr><tr><td>BabyAGI Nakajima (2024) BabyAGI</td><td>Project Manage-ment</td><td>Continuously creates, pri-ortizes, and executes sub-tasks to adaptively meet user goals.</td></tr><tr><td>Voyager Wang et al. (2023) [161]</td><td>Game Exploration</td><td>Learns in Minecraft, in-vents new skills, sets sub-goals, and adapts strategy in real time.</td></tr><tr><td>CAMEL Liu et al. (2023) [162]</td><td>Multi-Agent Simulation</td><td>Simulates agent societies with communication, negotiation, and emergent collaborative behavior.</td></tr><tr><td>Einstein Copilot Salesforce (2024) Einstein Copilot</td><td>Customer Automation</td><td>Automates full support workflows, escalates issues, and improves via feedback loops.</td></tr><tr><td>Copilot Studio (Agentic Mode) Microsoft (2025) Github Agentic Copilot</td><td>Productivity Automata-tion</td><td>Manages documents, meetings, and projects across Microsoft 365 with adaptive orchestration.</td></tr><tr><td>Atera AI Copilot Atera (2025) Atera Agentic AI</td><td>IT Operations</td><td>Diagnoses/resolves IT issues, automates ticketing, and learns from evolving infrastructures.</td></tr><tr><td>AES Safety Audit Agent AES (2025) AES agentic</td><td>Industrial Safety</td><td>Automates audits, assesses compliance, and evolves strategies to enhance safety outcomes.</td></tr><tr><td>DeepMind Gato (Agentic Mode) Reed et al. (2022) [163]</td><td>General Robotics</td><td>Performs varied tasks across modalities, dynamically learns, plans, and executes.</td></tr><tr><td>GPT-4o + Plugins OpenAI (2024) GPT-4O Agentic</td><td>Enterprise Automation</td><td>Manages complex workflows, integrates external tools, and executes adaptive decisions.</td></tr></table>\n\nthese issues pose serious ethical and operational risks, particularly when agents are deployed in sensitive or user-facing contexts.\n\n3) Incomplete Agentic Properties: A major limitation of current AI Agents is their inability to fully satisfy the canonical agentic properties defined in foundational literature, such as autonomy, proactivity, reactivity, and social ability [135], [173]. While many systems marketed as \"agents\" leverage LLMs to perform useful tasks, they often fall short of these fundamental criteria in practice. Autonomy, for instance, is typically partial at best. Although agents can execute tasks with\n\nminimal oversight once initialized, they remain heavily reliant on external scaffolding such as human-defined prompts, planning heuristics, or feedback loops to function effectively [180]. Self-initiated task generation, self-monitoring, or autonomous error correction are rare or absent, limiting their capacity for true independence.\n\nProactivity is similarly underdeveloped. Most AI Agents require explicit user instruction to act and lack the capacity to formulate or reprioritize goals dynamically based on contextual shifts or evolving objectives [181]. As a result, they behave reactively rather than strategically, constrained by the static nature of their initialization. Reactivity itself is constrained by architectural bottlenecks. Agents do respond to environmental or user input, but response latency caused by repeated LLM inference calls [182], [183], coupled with narrow contextual memory windows [153], [184], inhibits real-time adaptability. Perhaps the most underexplored capability is social ability. True agentic systems should communicate and coordinate with humans or other agents over extended interactions, resolving ambiguity, negotiating tasks, and adapting to social norms.\n\nHowever, existing implementations exhibit brittle, template-based dialogue that lacks long-term memory integration or nuanced conversational context. Agent-to-agent interaction is often hardcoded or limited to scripted exchanges, hindering collaborative execution and emergent behavior [96], [185]. Collectively, these deficiencies reveal that while AI Agents demonstrate functional intelligence, they remain far from meeting the formal benchmarks of intelligent, interactive, and adaptive agents. Bridging this gap is essential for advancing toward more autonomous, socially capable AI systems.\n\n4) Limited Long-Horizon Planning and Recovery: A persistent limitation of current AI Agents lies in their inability to perform robust long-horizon planning, especially in complex, multi-stage tasks. This constraint stems from their foundational reliance on stateless prompt-response paradigms, where each decision is made without an intrinsic memory of prior reasoning steps unless externally managed. Although augmentations such as the ReAct framework [126] or Tree-of-Thoughts [152] introduce pseudo-recursive reasoning, they remain fundamentally heuristic and lack true internal models of time, causality, or state evolution. Consequently, agents often falter in tasks requiring extended temporal consistency or contingency planning. For example, in domains such as clinical triage or financial portfolio management, where decisions depend on prior context and dynamically unfolding outcomes, agents may exhibit repetitive behaviors such as endlessly querying tools or fail to adapt when sub-tasks fail or return ambiguous results. The absence of systematic recovery mechanisms or error detection leads to brittle workflows and error propagation. This shortfall severely limits agent deployment in mission-critical environments\n\nwhere reliability, fault tolerance, and sequential coherence are essential.\n\n5) Reliability and Safety Concerns: AI Agents are not yet safe or verifiable enough for deployment in critical infrastructure [186]. The absence of causal reasoning leads to unpredictable behavior under distributional shift [165], [187]. Furthermore, evaluating the correctness of an agent's plan especially when the agent fabricates intermediate steps or rationales remains an unsolved problem in interpretability [104], [188]. Safety guarantees, such as formal verification, are not yet available for open-ended, LLM-powered agents. While AI Agents represent a major step beyond static generative models, their limitations in causal reasoning, adaptability, robustness, and planning restrict their deployment in high-stakes or dynamic environments. Most current systems rely on heuristic wrappers and brittle prompt engineering rather than grounded agentic cognition. Bridging this gap will require future systems to integrate causal models, dynamic memory, and verifiable reasoning mechanisms. These limitations also set the stage for the emergence of Agentic AI systems, which attempt to address these bottlenecks through multi-agent collaboration, orchestration layers, and persistent system-level context.\n\n2) Challenges and Limitations of Agentic AI: Agentic AI systems represent a paradigm shift from isolated AI agents to collaborative, multi-agent ecosystems capable of decomposing and executing complex goals [14]. These systems typically consist of orchestrated or communicating agents that interact via tools, APIs, and shared environments [18], [38]. While this architectural evolution enables more ambitious automation, it introduces a range of amplified and novel challenges that compound existing limitations of individual LLM-based agents. The current challenges and limitations of Agentic AI are as follows:\n\n1) Amplified Causality Challenges: One of the most critical limitations in Agentic AI systems is the magnification of causality deficits already observed in single-agent architectures. Unlike traditional AI Agents that operate in relatively isolated environments, Agentic AI systems involve complex inter-agent dynamics, where each agent's action can influence the decision space of others. Without a robust capacity for modeling cause-effect relationships, these systems struggle to coordinate effectively and adapt to unforeseen environmental shifts. A key manifestation of this challenge is inter-agent distributional shift, where the behavior of one agent alters the operational context for others. In the absence of causal reasoning, agents are unable to anticipate the downstream impact of their outputs, resulting in coordination breakdowns or redundant computations [189]. Furthermore, these systems are particularly vulnerable to error cascades: a faulty or hallucinated output from one agent can propagate through the system, compounding\n\ninaccuracies and corrupting subsequent decisions. For example, if a verification agent erroneously validates false information, downstream agents such as summarizers or decision-makers may unknowingly build upon that misinformation, compromising the integrity of the entire system. This fragility underscores the urgent need for integrating causal inference and intervention modeling into the design of multi-agent workflows, especially in high-stakes or dynamic environments where systemic robustness is essential.\n\n2) Communication and Coordination Bottlenecks: A fundamental challenge in Agentic AI lies in achieving efficient communication and coordination across multiple autonomous agents. Unlike single-agent systems, Agentic AI involves distributed agents that must collectively pursue a shared objective necessitating precise alignment, synchronized execution, and robust communication protocols. However, current implementations fall short in these aspects. One major issue is goal alignment and shared context, where agents often lack a unified semantic understanding of overarching objectives. This hampers sub-task decomposition, dependency management, and progress monitoring, especially in dynamic environments requiring causal awareness and temporal coherence.\n\nIn addition, protocol limitations significantly hinder inter-agent communication. Most systems rely on natural language exchanges over loosely defined interfaces, which are prone to ambiguity, inconsistent formatting, and contextual drift. These communication gaps lead to fragmented strategies, delayed coordination, and degraded system performance. Furthermore, resource contention emerges as a systemic bottleneck when agents simultaneously access shared computational, memory, or API resources. Without centralized orchestration or intelligent scheduling mechanisms, these conflicts can result in race conditions, execution delays, or outright system failures. Collectively, these bottlenecks illustrate the immaturity of current coordination frameworks in Agentic AI, and highlight the pressing need for standardized communication protocols, semantic task planners, and global resource managers to ensure scalable, coherent multi-agent collaboration.\n\n3) Emergent Behavior and Predictability: One of the most critical limitations of Agentic AI lies in managing emergent behavior complex system-level phenomena that arise from the interactions of autonomous agents. While such emergence can potentially yield adaptive and innovative solutions, it also introduces significant unpredictability and safety risks [145], [190]. A key concern is the generation of unintended outcomes, where agent interactions result in behaviors that were not explicitly programmed or foreseen by system designers. These behaviors may diverge from task objectives, generate misleading outputs, or even enact harmful actions particularly in high-stakes domains like healthcare, finance,\n\nor critical infrastructure.\n\nAs the number of agents and the complexity of their interactions grow, so too does the likelihood of system instability. This includes phenomena such as infinite planning loops, action deadlocks, and contradictory behaviors emerging from asynchronous or misaligned agent decisions. Without centralized arbitration mechanisms, conflict resolution protocols, or fallback strategies, these instabilities compound over time, making the system fragile and unreliable. The stochasticity and opacity of large language model-based agents further exacerbate this issue, as their internal decision logic is not easily interpretable or verifiable. Consequently, ensuring the predictability and controllability of emergent behavior remains a central challenge in designing safe and scalable Agentic AI systems.\n\n4) Scalability and Debugging Complexity: As Agentic AI systems scale in both the number of agents and the diversity of specialized roles, maintaining system reliability and interpretability becomes increasingly complex [191], [192]. A central limitation stems from the black-box chains of reasoning characteristic of LLM-based agents. Each agent may process inputs through opaque internal logic, invoke external tools, and communicate with other agents all of which occur through multiple layers of prompt engineering, reasoning heuristics, and dynamic context handling. Tracing the root cause of a failure thus requires unwinding nested sequences of agent interactions, tool invocations, and memory updates, making debugging non-trivial and time-consuming.\n\nAnother significant constraint is the system's non-compositionality. Unlike traditional modular systems, where adding components can enhance overall functionality, introducing additional agents in an Agentic AI architecture often increases cognitive load, noise, and coordination overhead. Poorly orchestrated agent networks can result in redundant computation, contradictory decisions, or degraded task performance. Without robust frameworks for agent role definition, communication standards, and hierarchical planning, the scalability of Agentic AI does not necessarily translate into greater intelligence or robustness. These limitations highlight the need for systematic architectural controls and traceability tools to support the development of reliable, large-scale agentic ecosystems.\n\n5) Trust, Explainability, and Verification: Agentic AI systems pose heightened challenges in explainability and verifiability due to their distributed, multi-agent architecture. While interpreting the behavior of a single LLM-powered agent is already non-trivial, this complexity is multiplied when multiple agents interact asynchronously through loosely defined communication protocols. Each agent may possess its own memory, task objective, and reasoning path, resulting in compounded opacity where tracing the causal chain of a final decision or failure\n\nbecomes exceedingly difficult. The lack of shared, transparent logs or interpretable reasoning paths across agents makes it nearly impossible to determine why a particular sequence of actions occurred or which agent initiated a misstep.\n\nCompounding this opacity is the absence of formal verification tools tailored for Agentic AI. Unlike traditional software systems, where model checking and formal proofs offer bounded guarantees, there exists no widely adopted methodology to verify that a multiagent LLM system will perform reliably across all input distributions or operational contexts. This lack of verifiability presents a significant barrier to adoption in safety-critical domains such as autonomous vehicles, finance, and healthcare, where explainability and assurance are non-negotiable. To advance Agentic AI safely, future research must address the foundational gaps in causal traceability, agent accountability, and formal safety guarantees.\n\n6) Security and Adversarial Risks: Agentic AI architectures introduce a significantly expanded attack surface compared to single-agent systems, exposing them to complex adversarial threats. One of the most critical vulnerabilities lies in the presence of a single point of compromise. Since Agentic AI systems are composed of interdependent agents communicating over shared memory or messaging protocols, the compromise of even one agent through prompt injection, model poisoning, or adversarial tool manipulation can propagate malicious outputs or corrupted state across the entire system. For example, a fact-checking agent fed with tampered data could unintentionally legitimize false claims, which are then integrated into downstream reasoning by summarization or decision-making agents.\n\nMoreover, inter-agent dynamics themselves are susceptible to exploitation. Attackers can induce race conditions, deadlocks, or resource exhaustion by manipulating the coordination logic between agents. Without rigorous authentication, access control, and sandboxing mechanisms, malicious agents or corrupted tool responses can derail multi-agent workflows or cause erroneous escalation in task pipelines. These risks are exacerbated by the absence of standardized security frameworks for LLM-based multi-agent systems, leaving most current implementations defenseless against sophisticated multistage attacks. As Agentic AI moves toward broader adoption, especially in high-stakes environments, embedding secure-by-design principles and adversarial robustness becomes an urgent research imperative.\n\n7) Ethical and Governance Challenges: The distributed and autonomous nature of Agentic AI systems introduces profound ethical and governance concerns, particularly in terms of accountability, fairness, and value alignment. In multi-agent settings, accountability gaps emerge when multiple agents interact to produce an outcome, making it difficult to assign responsibility\n\nfor errors or unintended consequences. This ambiguity complicates legal liability, regulatory compliance, and user trust, especially in domains such as healthcare, finance, or defense. Furthermore, bias propagation and amplification present a unique challenge: agents individually trained on biased data may reinforce each other's skewed decisions through interaction, leading to systemic inequities that are more pronounced than in isolated models. These emergent biases can be subtle and difficult to detect without longitudinal monitoring or audit mechanisms.\n\nAdditionally, misalignment and value drift pose serious risks in long-horizon or dynamic environments. Without a unified framework for shared value encoding, individual agents may interpret overarching objectives differently or optimize for local goals that diverge from human intent. Over time, this misalignment can lead to behavior that is inconsistent with ethical norms or user expectations. Current alignment methods, which are mostly designed for single-agent systems, are inadequate for managing value synchronization across heterogeneous agent collectives. These challenges highlight the urgent need for governance-aware agent architectures, incorporating principles such as role-based isolation, traceable decision logging, and participatory oversight mechanisms to ensure ethical integrity in autonomous multi-agent systems.\n\n8) Immature Foundations and Research Gaps: Despite rapid progress and high-profile demonstrations, Agentic AI remains in a nascent research stage with unresolved foundational issues that limit its scalability, reliability, and theoretical grounding. A central concern is the lack of standard architectures. There is currently no widely accepted blueprint for how to design, monitor, or evaluate multi-agent systems built on LLMs. This architectural fragmentation makes it difficult to compare implementations, replicate experiments, or generalize findings across domains. Key aspects such as agent orchestration, memory structures, and communication protocols are often implemented ad hoc, resulting in brittle systems that lack interoperability and formal guarantees.\n\nEqually critical is the absence of causal foundations as scalable causal discovery and reasoning remain unsolved challenges [193]. Without the ability to represent and reason about cause-effect relationships, Agentic AI systems are inherently limited in their capacity to generalize safely beyond narrow training regimes [170], [194]. This shortfall affects their robustness under distributional shifts, their capacity for proactive intervention, and their ability to simulate counterfactuals or hypothetical plans core requirements for intelligent coordination and decision-making.\n\nThe gap between functional demos and principled design thus underscores an urgent need for foundational research in multi-agent system theory, causal infer\n\nence integration, and benchmark development. Only by addressing these deficiencies can the field progress from prototype pipelines to trustworthy, general-purpose agentic frameworks suitable for deployment in high-stakes environments.\n\n# VI. POTENTIAL SOLUTIONS AND FUTURE ROADMAP\n\nThe potential solutions (as illustrated in Figure 13) to these challenges and limitations of AI agents and Agentic AI are summarized in the following points:\n\n1) Retrieval-Augmented Generation (RAG): For AI Agents, Retrieval-Augmented Generation mitigates hallucinations and expands static LLM knowledge by grounding outputs in real-time data [195]. By embedding user queries and retrieving semantically relevant documents from vector databases like FAISS Faiss or Pinecone Pinecone, agents can generate contextually valid responses rooted in external facts. This is particularly effective in domains such as enterprise search and customer support, where accuracy and up-to-date knowledge are essential.\n\nIn Agentic AI systems, RAG serves as a shared grounding mechanism across agents. For example, a summarizer agent may rely on the retriever agent to access the latest scientific papers before generating a synthesis. Persistent, queryable memory allows distributed agents to operate on a unified semantic layer, mitigating inconsistencies due to divergent contextual views. When implemented across a multi-agent system, RAG helps maintain shared truth, enhances goal alignment, and reduces inter-agent misinformation propagation.\n\n2) Tool-Augmented Reasoning (Function Calling): AI Agents benefit significantly from function calling, which extends their ability to interact with real-world systems [159], [196]. Agents can query APIs, run local scripts, or access structured databases, thus transforming LLMs from static predictors into interactive problem-solvers [125], [154]. This allows them to dynamically retrieve weather forecasts, schedule appointments, or execute Python-based calculations, all beyond the capabilities of pure language modeling.\n\nFor Agentic AI, function calling supports agent level autonomy and role differentiation. Agents within a team may use APIs to invoke domain-specific actions such as querying clinical databases or generating visual charts based on assigned roles. Function calls become part of an orchestrated pipeline, enabling fluid delegation across agents [197]. This structured interaction reduces ambiguity in task handoff and fosters clearer behavioral boundaries, especially when integrated with validation protocols or observation mechanisms [14], [18].\n\n3) Agentic Loop: Reasoning, Action, Observation: AI Agents often suffer from single-pass inference limitations. The ReAct pattern introduces an iterative loop where agents reason about tasks, act by calling tools or APIs, and then observe results before continuing.\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/58d7f580cead0834f8a42b572b1c436a5ca82924b8e64412de53f73add0950bb.jpg)  \nFig. 13: Ten emerging architectural and algorithmic solutions such as RAG, tool use, memory, orchestration, and reflexive mechanisms addressing reliability, scalability, and explainability across both paradigms\n\nThis feedback loop allows for more deliberate, context-sensitive behaviors. For example, an agent may verify retrieved data before drafting a summary, thereby reducing hallucination and logical errors. In Agentic AI, this pattern is critical for collaborative coherence. ReAct enables agents to evaluate dependencies dynamically reasoning over intermediate states, re-invoking tools if needed, and adjusting decisions as the environment evolves. This loop becomes more complex in multiagent settings where each agent's observation must be reconciled against others' outputs. Shared memory and consistent logging are essential here, ensuring that the reflective capacity of the system is not fragmented across agents [126].\n\n4) Memory Architectures (Episodic, Semantic, Vector): AI Agents face limitations in long-horizon planning and session continuity. Memory architectures address this by persisting information across tasks [198]. Episodic memory allows agents to recall prior actions and feedback, semantic memory encodes structured domain knowledge, and vector memory enables similarity-based retrieval [199]. These elements are key for personalization and adaptive decision-making in repeated interactions. Agentic AI systems require even more sophisticated memory models due to distributed state management. Each agent may maintain local memory while accessing shared global memory to facilitate coordination. For example, a planner agent might use vector-based memory to recall prior workflows, while a QA agent references semantic memory for fact verification. Synchronizing memory access and updates across agents enhances consistency, enables context-aware communication, and supports long-horizon system-level planning.\n\n5) Multi-Agent Orchestration with Role Specialization: In AI Agents, task complexity is often handled via modular prompt templates or conditional logic. However, as task diversity increases, a single agent may become overloaded [200], [201]. Role specialization splitting tasks into subcomponents (e.g., planner, summarizer) allows lightweight orchestration even within single-agent systems by simulating compartmentalized reasoning. In Agentic AI, orchestration is central. A meta-agent or orchestrator distributes tasks among specialized agents, each with distinct capabilities. Systems like MetaGPT and ChatDev exemplify this: agents emulate roles such as CEO, engineer, or reviewer, and interact through structured messaging. This modular approach enhances interpretability, scalability, and fault isolation ensuring that failures in one agent do not cascade without containment mechanisms from the orchestrator.\n\n6) Reflexive and Self-Critique Mechanisms: AI Agents often fail silently or propagate errors. Reflexive mechanisms introduce the capacity for self-evaluation [202], [203]. After completing a task, agents can critique their own outputs using a secondary reasoning pass, increasing robustness and reducing error rates. For example, a legal assistant agent might verify that its drafted clause matches prior case laws before submission. For Agentic AI, reflexivity extends beyond self-critique to inter-agent evaluation. Agents can review each other's outputs e.g., a verifier agent auditing a summarizer's work. Reflexion-like mechanisms ensure collaborative quality control and enhance trustworthiness [204]. Such patterns also support iterative improvement and adaptive replanning, particularly when integrated with memory logs or feedback queues [205], [206].\n\n7) Programmatic Prompt Engineering Pipelines: Manual prompt tuning introduces brittleness and reduces reproducibility in AI Agents. Programmatic pipelines automate this process using task templates, context fillers, and retrieval-augmented variables [207], [208]. These dynamic prompts are structured based on task type, agent role, or user query, improving generalization and reducing failure modes associated with prompt variability. In Agentic AI, prompt pipelines enable scalable, role-consistent communication. Each agent type (e.g., planner, retriever, summarizer) can generate or consume structured prompts tailored to its function. By automating message formatting, dependency tracking, and semantic alignment, programmatic prompting prevents coordination drift and ensures consistent reasoning across diverse agents in real time [14], [159].\n\n8) Causal Modeling and Simulation-Based Planning: AI Agents often operate on statistical correlations rather than causal models, leading to poor generalization under distribution shifts. Embedding causal inference allows agents to distinguish between correlation and causation, simulate interventions, and plan more robustly. For instance, in supply chain scenarios, a causally aware agent can simulate the downstream impact of shipment delays. In Agentic AI, causal reasoning is vital for safe coordination and error recovery. Agents must anticipate how their actions impact others requiring causal graphs, simulation environments, or Bayesian inference layers. For example, a planning agent may simulate different strategies and communicate likely outcomes to others, fostering strategic alignment and avoiding unintended emergent behaviors.\n\n9) Monitoring, Auditing, and Explainability Pipelines: AI Agents lack transparency, complicating debugging and trust. Logging systems that record prompts, tool calls, memory updates, and outputs enable post-hoc analysis and performance tuning. These records help developers trace faults, refine behavior, and ensure compliance with usage guidelines especially critical in enterprise or legal domains. For Agentic AI, logging and explainability are exponentially more important. With multiple agents interacting asynchronously, audit trails are essential for identifying which agent caused an error and under what conditions. Explainability pipelines that integrate across agents (e.g., timeline visualizations or dialogue replays) are key to ensuring safety, especially in regulatory or multi-stakeholder environments.\n\n10) Governance-Aware Architectures (Accountability and Role Isolation): AI Agents currently lack built-in safeguards for ethical compliance or error attribution. Governance-aware designs introduce role-based access control, sandboxing, and identity resolution to ensure agents act within scope and their decisions can be audited or revoked. These structures reduce risks in sensitive applications such as healthcare or finance. In Agentic AI, governance must scale across roles,\n\nagents, and workflows. Role isolation prevents rogue agents from exceeding authority, while accountability mechanisms assign responsibility for decisions and trace causality across agents. Compliance protocols, ethical alignment checks, and agent authentication ensure safety in collaborative settings paving the way for trustworthy AI ecosystems.\n\nAI Agents are projected to evolve significantly through enhanced modular intelligence focused on five key domains as depicted in Figure 14 as: proactive reasoning, tool integration, causal inference, continual learning, and trust-centric operations. The first transformative milestone involves transitioning from reactive to Proactive Intelligence, where agents initiate tasks based on learned patterns, contextual cues, or latent goals rather than awaiting explicit prompts. This advancement depends heavily on robust Tool Integration, enabling agents to dynamically interact with external systems, such as databases, APIs, or simulation environments, to fulfill complex user tasks. Equally critical is the development of Causal Reasoning, which will allow agents to move beyond statistical correlation, supporting inference of cause-effect relationships essential for tasks involving diagnosis, planning, or prediction. To maintain relevance over time, agents must adopt frameworks for Continuous Learning, incorporating feedback loops and episodic memory to adapt their behavior across sessions and environments. Lastly, to build user confidence, agents must prioritize Trust & Safety mechanisms through verifiable output logging, bias detection, and ethical guardrails especially as their autonomy increases. Together, these pathways will redefine AI Agents from static tools into adaptive cognitive systems capable of autonomous yet controllable operation in dynamic digital environments.\n\nAgentic AI, as a natural extension of these foundations, emphasizes collaborative intelligence through multi-agent coordination, contextual persistence, and domain-specific orchestration. Future systems (Figure 14 right side) will exhibit Multi-Agent Scaling, enabling specialized agents to work in parallel under distributed control for complex problem-solving mirroring team-based human workflows. This necessitates a layer of Unified Orchestration, where meta-agents or orchestrators dynamically assign roles, monitor task dependencies, and mediate conflicts among subordinate agents. Sustained performance over time depends on Persistent Memory architectures, which preserve semantic, episodic, and shared knowledge for agents to coordinate longitudinal tasks and retain state awareness. Simulation Planning is expected to become a core feature, allowing agent collectives to test hypothetical strategies, forecast consequences, and optimize outcomes before real-world execution. Moreover, Ethical Governance frameworks will be essential to ensure responsible deployment defining accountability, oversight, and value alignment across autonomous agent networks. Finally, tailored Domain-Specific Systems will emerge in fields like law, medicine, and supply chains, leveraging contextual specialization to outperform generic agents. This future positions Agentic AI not merely\n\n![](/uploads/images/e18fee52-bfb3-42f7-b457-ec1c57804020/9a523150fc969434f3864d2f1517cc5a8ae8008eb543a5d62d26c7a165f33a8b.jpg)  \nFig. 14: Mindmap visualization of the future roadmap for AI Agents and Agentic AI.\n\nas a coordination layer on the top of AI Agents, but as a new paradigm for collective machine intelligence with adaptive planning, recursive reasoning, and collaborative cognition at its core.\n\n# VII. CONCLUSION\n\nIn this study, we presented a comprehensive literature-based evaluation of the evolving landscape of AI Agents and Agentic AI, offering a structured taxonomy that highlights foundational concepts, architectural evolution, application domains, and key limitations. Beginning with a foundational understanding, we characterized AI Agents as modular, task-specific entities with constrained autonomy and reactivity. Their operational scope is grounded in the integration of LLMs and LIMs, which serve as core reasoning modules for perception, language understanding, and decision-making. We identified generative AI as a functional precursor, emphasizing its limitations in autonomy and goal persistence, and examined how LLMs drive the progression from passive generation to interactive task completion through tool augmentation.\n\nThis study then explored the conceptual emergence of Agentic AI systems as a transformative evolution from isolated agents to orchestrated, multi-agent ecosystems. We analyzed key differentiators such as distributed cognition, persistent memory, and coordinated planning that distinguish Agentic AI from conventional agent models. This was followed by a detailed breakdown of architectural evolution, highlighting the transition from monolithic, rule-based frameworks to\n\nmodular, role specialized networks facilitated by orchestration layers and reflective memory architectures. Additionally, this study then surveyed application domains in which these paradigms are deployed. For AI Agents, we illustrated their role in automating customer support, internal enterprise search, email prioritization, and scheduling. For Agentic AI, we demonstrated use cases in collaborative research, robotics, medical decision support, and adaptive workflow automation, supported by practical examples and industry-grade systems. Finally, this study provided a deep analysis of the challenges and limitations affecting both paradigms. For AI Agents, we discussed hallucinations, shallow reasoning, and planning constraints, while for Agentic AI, we addressed amplified causality issues, coordination bottlenecks, emergent behavior, and governance concerns. These insights offer a roadmap for future development and deployment of trustworthy, scalable agentic systems.\n\n# ACKNOWLEDGEMENT\n\nThis work was supported by the National Science Foundation and the United States Department of Agriculture, National Institute of Food and Agriculture through the \"Artificial Intelligence (AI) Institute for Agriculture\" Program under Award AWD003473, and AWD004595, Accession Number 1029004, \"Robotic Blossom Thinning with Soft Manipulators\".\n\n# DECLARATIONS\n\nThe authors declare no conflicts of interest.\n\n# STATEMENT ON AI WRITING ASSISTANCE\n\nChatGPT and Perplexity were utilized to enhance grammatical accuracy and refine sentence structure; all AI-generated revisions were thoroughly reviewed and edited for relevance. Additionally, ChatGPT-4o was employed to generate realistic visualizations.\n\n# REFERENCES\n\n[1] E. Oliveira, K. Fischer, and O. Stepankova, “Multi-agent systems: which research for which applications,” Robotics and Autonomous Systems, vol. 27, no. 1-2, pp. 91–106, 1999.  \n[2] Z. Ren and C. J. Anumba, \"Multi-agent systems in construction-state of the art and prospects,\" Automation in Construction, vol. 13, no. 3, pp. 421-434, 2004.  \n[3] C. Castelfranchi, “Modelling social action for ai agents,” Artificial intelligence, vol. 103, no. 1-2, pp. 157-182, 1998.  \n[4] J. Ferber and G. Weiss, Multi-agent systems: an introduction to distributed artificial intelligence, vol. 1. Addison-wesley Reading, 1999.  \n[5] R. Calegari, G. Ciatto, V. Mascardi, and A. Omicini, \"Logic-based technologies for multi-agent systems: a systematic literature review,\" Autonomous Agents and Multi-Agent Systems, vol. 35, no. 1, p. 1, 2021.  \n[6] R. C. Cardoso and A. Ferrando, “A review of agent-based programming for multi-agent systems,” Computers, vol. 10, no. 2, p. 16, 2021.  \n[7] E. Shortliffe, Computer-based medical consultations: MYCIN, vol. 2. Elsevier, 2012.  \n[8] H. P. Moravec, “The stanford cart and the cmu rover,” Proceedings of the IEEE, vol. 71, no. 7, pp. 872-884, 1983.  \n[9] B. Dai and H. Chen, \"A multi-agent and auction-based framework and approach for carrier collaboration,\" Logistics Research, vol. 3, pp. 101-120, 2011.  \n[10] J. Grosset, A.-J. Fougères, M. Djoko-Kouam, and J.-M. Bonnin, \"Multi-agent simulation of autonomous industrial vehicle fleets: Towards dynamic task allocation in v2x cooperation mode,\" *Integrated Computer-Aided Engineering*, vol. 31, no. 3, pp. 249-266, 2024.  \n[11] R. A. Agis, S. Gottifredi, and A. J. Garcia, \"An event-driven behavior trees extension to facilitate non-player multi-agent coordination in video games,\" Expert Systems with Applications, vol. 155, p. 113457, 2020.  \n[12] A. Guerra-Hernández, A. El Fallah-Seghrouchni, and H. Soldano, \"Learning in bdi multi-agent systems,\" in International Workshop on Computational Logic in Multi-Agent Systems, pp. 218-233, Springer, 2004.  \n[13] A. Saadi, R. Maamri, and Z. Sahnoun, \"Behavioral flexibility in belief-desire-intention (bdi) architectures,\" Multiagent and grid systems, vol. 16, no. 4, pp. 343-377, 2020.  \n[14] D. B. Acharya, K. Kuppan, and B. Divya, \"Agentic ai: Autonomous intelligence for complex goals-a comprehensive survey,\" IEEE Access, 2025.  \n[15] M. Z. Pan, M. Cemri, L. A. Agrawal, S. Yang, B. Chopra, R. Tiwari, K. Keutzer, A. Parameswaran, K. Ramchandran, D. Klein, et al., \"Why do multiagent systems fail?\", in ICLR 2025 Workshop on Building Trust in Language Models and Applications, 2025.  \n[16] L. Hughes, Y. K. Dwivedi, T. Malik, M. Shawosh, M. A. Albashrawi, I. Jeon, V. Dutot, M. Appanderanda, T. Crick, R. De', et al., \"Ai agents and agentic systems: A multi-expert analysis,\" Journal of Computer Information Systems, pp. 1-29, 2025.  \n[17] Z. Deng, Y. Guo, C. Han, W. Ma, J. Xiong, S. Wen, and Y. Xiang, \"Ai agents under threat: A survey of key security challenges and future pathways,\" ACM Computing Surveys, vol. 57, no. 7, pp. 1-36, 2025.  \n[18] M. Gridach, J. Nanavati, K. Z. E. Abidine, L. Mendes, and C. Mack, \"Agentic ai for scientific discovery: A survey of progress, challenges, and future directions,\" arXiv preprint arXiv:2503.08979, 2025.  \n[19] T. Song, M. Luo, X. Zhang, L. Chen, Y. Huang, J. Cao, Q. Zhu, D. Liu, B. Zhang, G. Zou, et al., \"A multiagent-driven robotic ai chemist enabling autonomous chemical research on demand,\" Journal of the American Chemical Society, vol. 147, no. 15, pp. 12534-12545, 2025.  \n[20] M. M. Karim, D. H. Van, S. Khan, Q. Qu, and Y. Kholodov, \"Ai agents meet blockchain: A survey on secure and scalable collaboration for multi-agents,\" Future Internet, vol. 17, no. 2, p. 57, 2025.\n\n[21] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., \"Improving language understanding by generative pre-training,\" arxiv, 2018.  \n[22] J. Sánchez Cuadrado, S. Pérez-Soler, E. Guerra, and J. De Lara, \"Automating the development of task-oriented llm-based chatbots,\" in Proceedings of the 6th ACM Conference on Conversational User Interfaces, pp. 1-10, 2024.  \n[23] Y. Lu, A. Aleta, C. Du, L. Shi, and Y. Moreno, \"Llms and generative agent-based models for complex systems research,\" Physics of Life Reviews, 2024.  \n[24] A. Zhang, Y. Chen, L. Sheng, X. Wang, and T.-S. Chua, \"On generative agents in recommendation,\" in Proceedings of the 47th international ACM SIGIR conference on research and development in Information Retrieval, pp. 1807-1817, 2024.  \n[25] S. Peng, E. Kalliamvakou, P. Cihon, and M. Demirer, “The impact of ai on developer productivity: Evidence from github copilot,” arXiv preprint arXiv:2302.06590, 2023.  \n[26] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Cohen, H. Nguyen, and R. T. Gadde, \"Jasper: An end-to-end convolutional neural acoustic model,\" arXiv preprint arXiv:1904.03288, 2019.  \n[27] A. Jaruga-Rozdolska, \"Artificial intelligence as part of future practices in the architect's work: Midjourney generative tool as part of a process of creating an architectural form,\" Architectus, no. 3 (71, pp. 95-104, 2022.  \n[28] K. Basu, “Bridging knowledge gaps in llms via function calls,” in Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pp. 5556–5557, 2024.  \n[29] Z. Liu, T. Hoang, J. Zhang, M. Zhu, T. Lan, J. Tan, W. Yao, Z. Liu, Y. Feng, R. RN, et al., \"Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets,\" Advances in Neural Information Processing Systems, vol. 37, pp. 54463-54482, 2024.  \n[30] H. Yang, S. Yue, and Y. He, \"Auto-gpt for online decision making: Benchmarks and additional opinions,\" arXiv preprint arXiv:2306.02224, 2023.  \n[31] I. Hettiarachchi, “Exploring generative ai agents: Architecture, applications, and challenges,” Journal of Artificial Intelligence General science (JAIGS) ISSN: 3006-4023, vol. 8, no. 1, pp. 105–127, 2025.  \n[32] A. Das, S.-C. Chen, M.-L. Shyu, and S. Sadiq, \"Enabling synergistic knowledge sharing and reasoning in large language models with collaborative multi-agents,\" in 2023 IEEE 9th International Conference on Collaboration and Internet Computing (CIC), pp. 92-98, IEEE, 2023.  \n[33] Z. Duan and J. Wang, \"Exploration of lmm multi-agent application implementation based on langgraph+ crewai,\" arXiv preprint arXiv:2411.18241, 2024.  \n[34] R. Sapkota, Y. Cao, K. I. Roumeliotis, and M. Karkee, “Vision-language-action models: Concepts, progress, applications and challenges,” arXiv preprint arXiv:2505.04769, 2025.  \n[35] R. Sapkota, K. I. Roumeliotis, R. H. Cheppally, M. F. Calero, and M. Karkee, “A review of 3d object detection with vision-language models,” arXiv preprint arXiv:2504.18738, 2025.  \n[36] R. Sapkota and M. Karkee, \"Object detection with multimodal large vision-language models: An in-depth review,\" Available at SSRN 5233953, 2025.  \n[37] B. Memarian and T. Doleck, “Human-in-the-loop in artificial intelligence in education: A review and entity-relationship (er) analysis,” Computers in Human Behavior: Artificial Humans, vol. 2, no. 1, p. 100053, 2024.  \n[38] P. Bornet, J. Wirtz, T. H. Davenport, D. De Cremer, B. Evergreen, P. Fersht, R. Gohel, S. Khiyara, P. Sund, and N. Mullakara, Agentic Artificial Intelligence: Harnessing AI Agents to Reinvent Business, Work and Life. Irreplaceable Publishing, 2025.  \n[39] F. Sado, C. K. Loo, W. S. Liew, M. Kerzel, and S. Wermter, \"Explainable goal-driven agents and robots-a comprehensive review,\" ACM Computing Surveys, vol. 55, no. 10, pp. 1-41, 2023.  \n[40] J. Heer, \"Agency plus automation: Designing artificial intelligence into interactive systems,\" Proceedings of the National Academy of Sciences, vol. 116, no. 6, pp. 1844-1850, 2019.  \n[41] G. Papagni, J. de Pagter, S. Zafari, M. Filzmoser, and S. T. Koeszegi, \"Artificial agents' explainability to support trust: considerations on timing and context,\" Ai & Society, vol. 38, no. 2, pp. 947-960, 2023.  \n[42] P. Wang and H. Ding, “The rationality of explanation or human capacity? understanding the impact of explainable artificial intelligence on human-ai trust and decision performance,” Information Processing & Management, vol. 61, no. 4, p. 103732, 2024.\n\n[43] E. Popa, “Human goals are constitutive of agency in artificial intelligence (ai),” Philosophy & Technology, vol. 34, no. 4, pp. 1731–1750, 2021.  \n[44] M. Chacon-Chamorro, L. F. Giraldo, N. Quijano, V. Vargas-Panesso, C. González, J. S. Pinzón, R. Manrique, M. Ríos, Y. Fonseca, D. Gómez-Barrera, et al., \"Cooperative resilience in artificial intelligence multiagent systems,\" IEEE Transactions on Artificial Intelligence, 2025.  \n[45] M. Adam, M. Wessel, and A. Benlian, “Ai-based chatbots in customer service and their effects on user compliance,” *Electronic Markets*, vol. 31, no. 2, pp. 427–445, 2021.  \n[46] D. Leocádio, L. Guedes, J. Oliveira, J. Reis, and N. Melão, \"Customer service with ai-powered human-robot collaboration (hrc): A literature review,\" Procedia Computer Science, vol. 232, pp. 1222–1232, 2024.  \n[47] T. Cao, Y. Q. Khoo, S. Birajdar, Z. Gong, C.-F. Chung, Y. Moghaddam, A. Xu, H. Mehta, A. Shukla, Z. Wang, et al., “Designing towards productivity: A centralized ai assistant concept for work,” The Human Side of Service Engineering, p. 118, 2024.  \n[48] Y. Huang and J. X. Huang, “Exploring chatgpt for next-generation information retrieval: Opportunities and challenges,” in Web Intelligence, vol. 22, pp. 31–44, SAGE Publications Sage UK: London, England, 2024.  \n[49] N. Holtz, S. Wittfoth, and J. M. Gómez, \"The new era of knowledge retrieval: Multi-agent systems meet generative ai,\" in 2024 Portland International Conference on Management of Engineering and Technology (PICMET), pp. 1-10, IEEE, 2024.  \n[50] F. Poszler and B. Lange, “The impact of intelligent decision-support systems on humans’ ethical decision-making: A systematic literature review and an integrated framework,” Technological Forecasting and Social Change, vol. 204, p. 123403, 2024.  \n[51] F. Khemakhem, H. Ellouzi, H. LtiFi, and M. B. Ayed, \"Agent-based intelligent decision support systems: a systematic review,\" IEEE Transactions on Cognitive and Developmental Systems, vol. 14, no. 1, pp. 20-34, 2020.  \n[52] R. V. Florian, \"Autonomous artificial intelligent agents,\" Center for Cognitive and Neural Studies (Coneural), Cluj-Napoca, Romania, 2003.  \n[53] T. Hellström, N. Kaiser, and S. Bensch, “A taxonomy of embodiment in the ai era,” *Electronics*, vol. 13, no. 22, p. 4441, 2024.  \n[54] M. Wischnewski, “Attributing mental states to non-embodied autonomous systems: A systematic review,” in Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pp. 1–8, 2025.  \n[55] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz, \"Not what you've signed up for: Compromising real-world IIm-integrated applications with indirect prompt injection,\" in Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, pp. 79-90, 2023.  \n[56] Y. Talebirad and A. Nadiri, \"Multi-agent collaboration: Harnessing the power of intelligent llm agents,\" arXiv preprint arXiv:2306.03314, 2023.  \n[57] A. I. Hauptman, B. G. Schelble, N. J. McNeese, and K. C. Madathil, \"Adapt and overcome: Perceptions of adaptive autonomous agents for human-ai teaming,\" Computers in Human Behavior, vol. 138, p. 107451, 2023.  \n[58] N. Krishnan, “Advancing multi-agent systems through model context protocol: Architecture, implementation, and applications,” arXiv preprint arXiv:2504.21030, 2025.  \n[59] H. Padigela, C. Shah, and D. Juyal, \"Ml-dev-bench: Comparative analysis of ai agents on ml development workflows,\" arXiv preprint arXiv:2502.00964, 2025.  \n[60] M. Raees, I. Meijerink, I. Lykourentzou, V.-J. Khan, and K. Papangelis, \"From explainable to interactive ai: A literature review on current trends in human-ai interaction,\" International Journal of Human-Computer Studies, p. 103301, 2024.  \n[61] P. Formosa, “Robot autonomy vs. human autonomy: social robots, artificial intelligence (ai), and the nature of autonomy,” *Minds and Machines*, vol. 31, no. 4, pp. 595–616, 2021.  \n[62] C. S. Eze and L. Shamir, “Analysis and prevention of ai-based phishing email attacks,” *Electronics*, vol. 13, no. 10, p. 1839, 2024.  \n[63] D. Singh, V. Patel, D. Bose, and A. Sharma, “Enhancing email marketing efficacy through ai-driven personalization: Leveraging natural language processing and collaborative filtering algorithms,” International Journal of AI Advancements, vol. 9, no. 4, 2020.\n\n[64] R. Khan, S. Sarkar, S. K. Mahata, and E. Jose, \"Security threats in agentic ai system,\" arXiv preprint arXiv:2410.14728, 2024.  \n[65] C. G. Endacott, “Enacting machine agency when ai makes one’s day: understanding how users relate to ai communication technologies for scheduling,” Journal of Computer-Mediated Communication, vol. 29, no. 4, p. zmae011, 2024.  \n[66] Z. Pawlak and A. Skowron, “Rudiments of rough sets,” Information sciences, vol. 177, no. 1, pp. 3-27, 2007.  \n[67] P. Ponnusamy, A. Ghias, Y. Yi, B. Yao, C. Guo, and R. Sarikaya, \"Feedback-based self-learning in large-scale conversational ai agents,\" AI magazine, vol. 42, no. 4, pp. 43-56, 2022.  \n[68] A. Zagalsky, D. Te'eni, I. Yahav, D. G. Schwartz, G. Silverman, D. Cohen, Y. Mann, and D. Lewinsky, \"The design of reciprocal learning between human and artificial intelligence,\" Proceedings of the ACM on Human-Computer Interaction, vol. 5, no. CSCW2, pp. 1-36, 2021.  \n[69] W. J. Clancey, “Heuristic classification,” Artificial intelligence, vol. 27, no. 3, pp. 289–350, 1985.  \n[70] S. Kapoor, B. Stroebl, Z. S. Siegel, N. Nadgir, and A. Narayanan, “Ai agents that matter,” arXiv preprint arXiv:2407.01502, 2024.  \n[71] X. Huang, J. Lian, Y. Lei, J. Yao, D. Lian, and X. Xie, \"Recommender ai agent: Integrating large language models for interactive recommendations,\" arXiv preprint arXiv:2308.16505, 2023.  \n[72] A. M. Baabdullah, A. A. Alalwan, R. S. Algharabat, B. Metri, and N. P. Rana, “Virtual agents and flow experience: An empirical examination of ai-powered chatbots,” Technological Forecasting and Social Change, vol. 181, p. 121772, 2022.  \n[73] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774, 2023.  \n[74] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., “Palm: Scaling language modeling with pathways,” Journal of Machine Learning Research, vol. 24, no. 240, pp. 1–113, 2023.  \n[75] H. Honda and M. Hagiwara, “Question answering systems with deep learning-based symbolic processing,” IEEE Access, vol. 7, pp. 152368–152378, 2019.  \n[76] N. Karanikolas, E. Manga, N. Samaridi, E. Tousidou, and M. Vassilikopoulos, \"Large language models versus natural language understanding and generation,\" in Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics, pp. 278-290, 2023.  \n[77] A. S. George, A. H. George, T. Baskar, and A. G. Martin, \"Revolutionizing business communication: Exploring the potential of gpt-4 in corporate settings,\" *Partners Universal International Research Journal*, vol. 2, no. 1, pp. 149–157, 2023.  \n[78] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., \"Learning transferable visual models from natural language supervision,\" in International conference on machine learning, pp. 8748-8763, PmLR, 2021.  \n[79] J. Li, D. Li, S. Savarese, and S. Hoi, \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,\" in International conference on machine learning, pp. 19730-19742, PMLR, 2023.  \n[80] S. Sontakke, J. Zhang, S. Arnold, K. Pertsch, E. Bityik, D. Sadigh, C. Finn, and L. Itti, \"Roboclip: One demonstration is enough to learn robot policies,\" Advances in Neural Information Processing Systems, vol. 36, pp. 55681-55693, 2023.  \n[81] M. Elhenawy, H. I. Ashqar, A. Rakotonirainy, T. I. Alhadidi, A. Jaber, and M. A. Tami, \"Vision-language models for autonomous driving: Clip-based dynamic scene understanding,\" *Electronics*, vol. 14, no. 7, p. 1282, 2025.  \n[82] S. Park, M. Lee, J. Kang, H. Choi, Y. Park, J. Cho, A. Lee, and D. Kim, \"Vlaad: Vision and language assistant for autonomous driving,\" in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 980-987, 2024.  \n[83] S. H. Ahmed, S. Hu, and G. Sukthankar, “The potential of vision-language models for content moderation of children’s videos,” in 2023 International Conference on Machine Learning and Applications (ICMLA), pp. 1237–1241, IEEE, 2023.  \n[84] S. H. Ahmed, M. J. Khan, and G. Sukthankar, \"Enhanced multimodal content moderation of children's videos using audiovisual fusion,\" arXiv preprint arXiv:2405.06128, 2024.\n\n[85] P. Chitra and A. Saleem Raja, \"Artificial intelligence (ai) algorithm and models for embodied agents (robots and drones),\" in Building Embodied AI Systems: The Agents, the Architecture Principles, Challenges, and Application Domains, pp. 417-441, Springer, 2025.  \n[86] S. Kourav, K. Verma, and M. Sundararajan, \"Artificial intelligence algorithm models for agents of embodiment for drone applications,\" in Building Embodied AI Systems: The Agents, the Architecture Principles, Challenges, and Application Domains, pp. 79-101, Springer, 2025.  \n[87] G. Natarajan, E. Elango, B. Sundaravadivazhagan, and S. Rethinam, \"Artificial intelligence algorithms and models for embodied agents: Enhancing autonomy in drones and robots,\" in Building Embodied AI Systems: The Agents, the Architecture Principles, Challenges, and Application Domains, pp. 103-132, Springer, 2025.  \n[88] K. Pandya and M. Holia, \"Automating customer service using langchain: Building custom open-source gpt chatbot for organizations,\" arXiv preprint arXiv:2310.05421, 2023.  \n[89] Q. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu, L. Jiang, X. Zhang, S. Zhang, J. Liu, et al., \"Autogen: Enabling next-gen llm applications via multi-agent conversation,\" arXiv preprint arXiv:2308.08155, 2023.  \n[90] L. Gabora and J. Bach, “A path to generative artificial selves,” in EPIA Conference on Artificial Intelligence, pp. 15–29, Springer, 2023.  \n[91] G. Pezzulo, T. Parr, P. Cisek, A. Clark, and K. Friston, \"Generating meaning: active inference and the scope and limits of passive ai,\" Trends in Cognitive Sciences, vol. 28, no. 2, pp. 97-112, 2024.  \n[92] J. Li, M. Zhang, N. Li, D. Weyns, Z. Jin, and K. Tei, \"Generative ai for self-adaptive systems: State of the art and research roadmap,\" ACM Transactions on Autonomous and Adaptive Systems, vol. 19, no. 3, pp. 1-60, 2024.  \n[93] W. O'Grady and M. Lee, \"Natural syntax, artificial intelligence and language acquisition,\" Information, vol. 14, no. 7, p. 418, 2023.  \n[94] X. Liu, J. Wang, J. Sun, X. Yuan, G. Dong, P. Di, W. Wang, and D. Wang, “Prompting frameworks for large language models: A survey,” arXiv preprint arXiv:2311.12785, 2023.  \n[95] E. T. Rolls, “The memory systems of the human brain and generative artificial intelligence,” Heliyon, vol. 10, no. 11, 2024.  \n[96] K. Alizadeh, S. I. Mirzadeh, D. Belenko, S. Khatamifard, M. Cho, C. C. Del Mundo, M. Rastegari, and M. Farajtabar, \"Llm in a flash: Efficient large language model inference with limited memory,\" in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12562-12584, 2024.  \n[97] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, et al., “Palm-e: An embodied multimodal language model,” 2023.  \n[98] P. Denny, J. Leinonen, J. Prather, A. Luxton-Reilly, T. Amarouche, B. A. Becker, and B. N. Reeves, \"Prompt problems: A new programming exercise for the generative ai era,\" in Proceedings of the 55th ACM Technical Symposium on Computer Science Education V.1, pp.296-302, 2024.  \n[99] C. Chen, S. Lee, E. Jang, and S. S. Sundar, \"Is your prompt detailed enough? exploring the effects of prompt coaching on users' perceptions, engagement, and trust in text-to-image generative ai tools,\" in Proceedings of the Second International Symposium on Trustworthy Autonomous Systems, pp. 1-12, 2024.  \n[100] A. Pan, E. Jones, M. Jagadeesan, and J. Steinhardt, \"Feedback loops with language models drive in-context reward hacking,\" arXiv preprint arXiv:2402.06627, 2024.  \n[101] K. Nabben, “Ai as a constituted system: accountability lessons from an llm experiment,” Data & policy, vol. 6, p. e57, 2024.  \n[102] P. J. Pesch, “Potentials and challenges of large language models (llms) in the context of administrative decision-making,” European Journal of Risk Regulation, pp. 1–20, 2025.  \n[103] C. Wang, Y. Deng, Z. Lyu, L. Zeng, J. He, S. Yan, and B. An, “Q*: Improving multi-step reasoning for llms with deliberative planning,” arXiv preprint arXiv:2406.14283, 2024.  \n[104] H. Wei, Z. Zhang, S. He, T. Xia, S. Pan, and F. Liu, “Plangen-llms: A modern survey of llm planning capabilities,” arXiv preprint arXiv:2502.11221, 2025.  \n[105] A. Bandi, P. V. S. R. Adapa, and Y. E. V. P. K. Kuchi, “The power of generative ai: A review of requirements, models, input-output formats, evaluation metrics, and challenges,” Future Internet, vol. 15, no. 8, p. 260, 2023.\n\n[106] Y. Liu, H. Du, D. Niyato, J. Kang, Z. Xiong, Y. Wen, and D. I. Kim, \"Generative ai in data center networking: Fundamentals, perspectives, and case study,\" IEEE Network, 2025.  \n[107] C. Guo, F. Cheng, Z. Du, J. Kiessling, J. Ku, S. Li, Z. Li, M. Ma, T. Molom-Ochir, B. Morris, et al., \"A survey: Collaborative hardware and software design in the era of large language models,\" IEEE Circuits and Systems Magazine, vol. 25, no. 1, pp. 35-57, 2025.  \n[108] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., \"Language models are few-shot learners,\" Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.  \n[109] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., \"Llama: Open and efficient foundation language models,\" arXiv preprint arXiv:2302.13971, 2023.  \n[110] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \"Exploring the limits of transfer learning with a unified text-to-text transformer,\" Journal of machine learning research, vol. 21, no. 140, pp. 1-67, 2020.  \n[111] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, et al., \"Baichuan 2: Open large-scale language models,\" arXiv preprint arXiv:2309.10305, 2023.  \n[112] K. M. Yoo, D. Park, J. Kang, S.-W. Lee, and W. Park, “Gpt3mix: Leveraging large-scale language models for text augmentation,” arXiv preprint arXiv:2104.08826, 2021.  \n[113] D. Zhou, X. Xue, X. Lu, Y. Guo, P. Ji, H. Lv, W. He, Y. Xu, Q. Li, and L. Cui, \"A hierarchical model for complex adaptive system: From adaptive agent to ai society,\" ACM Transactions on Autonomous and Adaptive Systems, 2024.  \n[114] H. Hao, Y. Wang, and J. Chen, \"Empowering scenario planning with artificial intelligence: A perspective on building smart and resilient cities,\" Engineering, 2024.  \n[115] Y. Wang, J. Zhu, Z. Cheng, L. Qiu, Z. Tong, and J. Huang, \"Intelligent optimization method for real-time decision-making in laminated cooling configurations through reinforcement learning,\" Energy, vol. 291, p. 130434, 2024.  \n[116] X. Xiang, J. Xue, L. Zhao, Y. Lei, C. Yue, and K. Lu, “Real-time integration of fine-tuned large language model for improved decision-making in reinforcement learning,” in 2024 International Joint Conference on Neural Networks (IJCNN), pp. 1-8, IEEE, 2024.  \n[117] Z. Li, H. Zhang, C. Peng, and R. Peiris, “Exploring large language model-driven agents for environment-aware spatial interactions and conversations in virtual reality role-play scenarios,” in 2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR), pp. 1-11, IEEE, 2025.  \n[118] T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, and M. N. Halgamuge, \"The inadequacy of reinforcement learning from human feedback-radicalizing large language models via semantic vulnerabilities,\" IEEE Transactions on Cognitive and Developmental Systems, 2024.  \n[119] S. Lee, G. Lee, W. Kim, J. Kim, J. Park, and K. Cho, \"Human strategy learning-based multi-agent deep reinforcement learning for online team sports game,\" IEEE Access, 2025.  \n[120] Z. Shi, S. Gao, L. Yan, Y. Feng, X. Chen, Z. Chen, D. Yin, S. Verberne, and Z. Ren, \"Tool learning in the wild: Empowering language models as automatic tool agents,\" in Proceedings of the ACM on Web Conference 2025, pp. 2222-2237, 2025.  \n[121] S. Yuan, K. Song, J. Chen, X. Tan, Y. Shen, R. Kan, D. Li, and D. Yang, \"Easytool: Enhancing llm-based agents with concise tool instruction,\" arXiv preprint arXiv:2401.06201, 2024.  \n[122] B. Xu, X. Liu, H. Shen, Z. Han, Y. Li, M. Yue, Z. Peng, Y. Liu, Z. Yao, and D. Xu, \"Gentopia: A collaborative platform for tool-augmented llms,\" arXiv preprint arXiv:2308.04030, 2023.  \n[123] H. Lu, X. Li, X. Ji, Z. Kan, and Q. Hu, “Toolfive: Enhancing tool-augmented llms via tool filtering and verification,” in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5, IEEE, 2025.  \n[124] Y. Song, F. Xu, S. Zhou, and G. Neubig, “Beyond browsing: Api-based web agents,” arXiv preprint arXiv:2410.16464, 2024.  \n[125] V. Tupe and S. Thube, \"Ai agentic workflows and enterprise apis: Adapting api architectures for the age of ai agents,\" arXiv preprint arXiv:2502.17443, 2025.  \n[126] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, \"React: Synergizing reasoning and acting in language models,\" in International Conference on Learning Representations (ICLR), 2023.\n\n[127] L. Ning, Z. Liang, Z. Jiang, H. Qu, Y. Ding, W. Fan, X.-y. Wei, S. Lin, H. Liu, P. S. Yu, et al., \"A survey of webagents: Towards next-generation ai agents for web automation with large foundation models,\" arXiv preprint arXiv:2503.23350, 2025.  \n[128] M. W. U. Rahman, R. Nevarez, L. T. Mim, and S. Hariri, “Multiagent actor-critic generative ai for query resolution and analysis,” IEEE Transactions on Artificial Intelligence, 2025.  \n[129] J. Lála, O. O'Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques, and A. D. White, \"Paperqa: Retrieval-augmented generative agent for scientific research,\" arXiv preprint arXiv:2312.07559, 2023.  \n[130] Z. Wu, C. Yu, C. Chen, J. Hao, and H. H. Zhuo, \"Models as agents: Optimizing multi-step predictions of interactive local models in model-based multi-agent reinforcement learning,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, pp. 10435-10443, 2023.  \n[131] Z. Feng, R. Xue, L. Yuan, Y. Yu, N. Ding, M. Liu, B. Gao, J. Sun, and G. Wang, \"Multi-agent embodied ai: Advances and future directions,\" arXiv preprint arXiv:2505.05108, 2025.  \n[132] A. Feriani and E. Hossain, \"Single and multi-agent deep reinforcement learning for ai-enabled wireless networks: A tutorial,\" IEEE Communications Surveys & Tutorials, vol. 23, no. 2, pp. 1226-1252, 2021.  \n[133] R. Zhang, S. Tang, Y. Liu, D. Niyato, Z. Xiong, S. Sun, S. Mao, and Z. Han, \"Toward agentic ai: generative information retrieval inspired intelligent communications and networking,\" arXiv preprint arXiv:2502.16866, 2025.  \n[134] U. M. Borghoff, P. Bottoni, and R. Pareschi, “Human-artificial interaction in the age of agentic ai: a system-theoretical approach,” Frontiers in Human Dynamics, vol. 7, p. 1579166, 2025.  \n[135] E. Miehling, K. N. Ramamurthy, K. R. Varshney, M. Riemer, D. Boun-effouf, J. T. Richards, A. Dhurandhar, E. M. Daly, M. Hind, P. Sattigeri, et al., \"Agentic ai needs a systems theory,\" arXiv preprint arXiv:2503.00237, 2025.  \n[136] W. Xu, Z. Liang, K. Mei, H. Gao, J. Tan, and Y. Zhang, “A-mem: Agentic memory for llm agents,” arXiv preprint arXiv:2502.12110, 2025.  \n[137] C. Riedl and D. De Cremer, “Ai for collective intelligence,” Collective Intelligence, vol. 4, no. 2, p. 26339137251328909, 2025.  \n[138] L. Peng, D. Li, Z. Zhang, T. Zhang, A. Huang, S. Yang, and Y. Hu, \"Human-ai collaboration: Unraveling the effects of user proficiency and ai agent capability in intelligent decision support systems,\" International Journal of Industrial Ergonomics, vol. 103, p. 103629, 2024.  \n[139] H. Shirado, K. Shimizu, N. A. Christakis, and S. Kasahara, “Realism drives interpersonal reciprocity but yields to ai-assisted egocentrism in a coordination experiment,” in Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 1–21, 2025.  \n[140] Y. Xiao, G. Shi, and P. Zhang, \"Towards agentic ai networking in 6g: A generative foundation model-as-agent approach,\" arXiv preprint arXiv:2503.15764, 2025.  \n[141] P. R. Lewis and S. Sarkadi, “Reflective artificial intelligence,” *Minds and Machines*, vol. 34, no. 2, p. 14, 2024.  \n[142] C. Qian, W. Liu, H. Liu, N. Chen, Y. Dang, J. Li, C. Yang, W. Chen, Y. Su, X. Cong, et al., \"Chatdev: Communicative agents for software development,\" arXiv preprint arXiv:2307.07924, 2023.  \n[143] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, et al., “Metagpt: Meta programming for multi-agent collaborative framework,” arXiv preprint arXiv:2308.00352, vol. 3, no. 4, p. 6, 2023.  \n[144] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji, S. Mao, et al., \"Taskmatrix: ai: Completing tasks by connecting foundation models with millions of apis,\" Intelligent Computing, vol. 3, p. 0063, 2024.  \n[145] H. Hexmoor, J. Lammens, G. Caicedo, and S. C. Shapiro, Behaviour based AI, cognitive processes, and emergent behaviors in autonomous agents, vol. 1. WIT Press, 2025.  \n[146] H. Zhang, Z. Li, F. Liu, Y. He, Z. Cao, and Y. Zheng, \"Design and implementation of langchain-based chatbot,\" in 2024 International Seminar on Artificial Intelligence, Computer Technology and Control Engineering (ACTCE), pp. 226-229, IEEE, 2024.  \n[147] E. Ephrati and J. S. Rosenschein, “A heuristic technique for multi-agent planning,” Annals of Mathematics and Artificial Intelligence, vol. 20, pp. 13–67, 1997.  \n[148] S. Kupferschmid, J. Hoffmann, H. Dierks, and G. Behrmann, \"Adapting an ai planning heuristic for directed model checking,\" in International SPIN Workshop on Model Checking of Software, pp. 35-52, Springer, 2006.\n\n[149] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C.-M. Chan, Y. Qin, Y. Lu, R. Xie, et al., \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents,\" arXiv preprint arXiv:2308.10848, vol. 2, no. 4, p. 6, 2023.  \n[150] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves to use tools,” Advances in Neural Information Processing Systems, vol. 36, pp. 68539–68551, 2023.  \n[151] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al., \"Chain-of-thought prompting elicits reasoning in large language models,\" Advances in neural information processing systems, vol. 35, pp. 24824-24837, 2022.  \n[152] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large language models,” Advances in neural information processing systems, vol. 36, pp. 11809–11822, 2023.  \n[153] J. Guo, N. Li, J. Qi, H. Yang, R. Li, Y. Feng, S. Zhang, and M. Xu, \"Empowering working memory for large language model agents,\" arXiv preprint arXiv:2312.17259, 2023.  \n[154] S. Agashe, J. Han, S. Gan, J. Yang, A. Li, and X. E. Wang, \"Agent s: An open agentic framework that uses computers like a human,\" arXiv preprint arXiv:2410.08164, 2024.  \n[155] C. DeChant, \"Episodic memory in ai agents poses risks that should be studied and mitigated,\" arXiv preprint arXiv:2501.11739, 2025.  \n[156] A. M. Nuxoll and J. E. Laird, “Enhancing intelligent agents with episodic memory,” Cognitive Systems Research, vol. 17, pp. 34–48, 2012.  \n[157] G. Sarthou, A. Clodic, and R. Alami, “Ontologenius: A long-term semantic memory for robotic agents,” in 2019 28th IEEE International Conference on Robot and Human Interactive Communication (ROMAN), pp. 1–8, IEEE, 2019.  \n[158] A.-e.-h. Munir and W. M. Qazi, “Artificial subjectivity: Personal semantic memory model for cognitive agents,” Applied Sciences, vol. 12, no. 4, p. 1903, 2022.  \n[159] A. Singh, A. Ehtesham, S. Kumar, and T. T. Khoei, \"Agentic retrieval-augmented generation: A survey on agentic rag,\" arXiv preprint arXiv:2501.09136, 2025.  \n[160] R. Akkiraju, A. Xu, D. Bora, T. Yu, L. An, V. Seth, A. Shukla, P. Gundecha, H. Mehta, A. Jha, et al., \"Facts about building retrieval augmented generation-based chatbots,\" arXiv preprint arXiv:2407.07858, 2024.  \n[161] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, \"Voyager: An open-ended embodied agent with large language models,\" arXiv preprint arXiv:2305.16291, 2023.  \n[162] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, \"Camel: Communicative agents for\" mind\" exploration of large language model society,\" Advances in Neural Information Processing Systems, vol. 36, pp. 51991-52008, 2023.  \n[163] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg, et al., \"A generalist agent,\" arXiv preprint arXiv:2205.06175, 2022.  \n[164] C. K. Thomas, C. Chaccour, W. Saad, M. Debbah, and C. S. Hong, \"Causal reasoning: Charting a revolutionary course for next-generation ai-native wireless networks,\" IEEE Vehicular Technology Magazine, 2024.  \n[165] Z. Tang, R. Wang, W. Chen, K. Wang, Y. Liu, T. Chen, and L. Lin, \"Towards causalgpt: A multi-agent approach for faithful knowledge reasoning via promoting causal consistency in llms,\" arXiv preprint arXiv:2308.11914, 2023.  \n[166] Z. Gekhman, J. Herzig, R. Aharoni, C. Elkind, and I. Szpektor, \"Trueteacher: Learning factual consistency evaluation with large language models,\" arXiv preprint arXiv:2305.11171, 2023.  \n[167] A. Wu, K. Kuang, M. Zhu, Y. Wang, Y. Zheng, K. Han, B. Li, G. Chen, F. Wu, and K. Zhang, \"Causality for large language models,\" arXiv preprint arXiv:2410.15319, 2024.  \n[168] S. Ashwani, K. Hegde, N. R. Mannuru, D. S. Sengar, M. Jindal, K. C. R. Kathala, D. Banga, V. Jain, and A. Chadha, \"Cause and effect: can large language models truly understand causality?\", in Proceedings of the AAAI Symposium Series, vol. 4, pp. 2-9, 2024.  \n[169] J. Richens and T. Everitt, “Robust agents learn causal world models,” in The Twelfth International Conference on Learning Representations, 2024.\n\n[170] A. Chan, R. Salganik, A. Markelius, C. Pang, N. Rajkumar, D. Krasheninnikov, L. Langosco, Z. He, Y. Duan, M. Carroll, et al., \"Harms from increasingly agentic algorithmic systems,\" in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pp. 651-666, 2023.  \n[171] A. Plaat, M. van Duijn, N. van Stein, M. Preuss, P. van der Putten, and K. J. Batenburg, \"Agentic large language models, a survey,\" arXiv preprint arXiv:2503.23037, 2025.  \n[172] J. Qiu, K. Lam, G. Li, A. Acharya, T. Y. Wong, A. Darzi, W. Yuan, and E. J. Topol, “Llm-based agentic systems in medicine and healthcare,” Nature Machine Intelligence, vol. 6, no. 12, pp. 1418–1420, 2024.  \n[173] G. A. Gabison and R. P. Xian, “Inherent and emergent liability issues in llm-based agentic systems: a principal-agent perspective,” arXiv preprint arXiv:2504.03255, 2025.  \n[174] M. Dahl, V. Magesh, M. Suzgun, and D. E. Ho, “Large legal fictions: Profiling legal hallucinations in large language models,” *Journal of Legal Analysis*, vol. 16, no. 1, pp. 64–93, 2024.  \n[175] Y. A. Latif, \"Hallucinations in large language models and their influence on legal reasoning: Examining the risks of ai-generated factual inaccuracies in judicial processes,\" Journal of Computational Intelligence, Machine Reasoning, and Decision-Making, vol. 10, no. 2, pp. 10-20, 2025.  \n[176] S. Tonmoy, S. Zaman, V. Jain, A. Rani, V. Rawte, A. Chadha, and A. Das, “A comprehensive survey of hallucination mitigation techniques in large language models,” arXiv preprint arXiv:2401.01313, vol. 6, 2024.  \n[177] Z. Zhang, Y. Yao, A. Zhang, X. Tang, X. Ma, Z. He, Y. Wang, M. Gerstein, R. Wang, G. Liu, et al., \"Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents,\" ACM Computing Surveys, vol. 57, no. 8, pp. 1-39, 2025.  \n[178] Y. Wan and K.-W. Chang, “White men lead, black women help? benchmarking language agency social biases in llms,” arXiv preprint arXiv:2404.10508, 2024.  \n[179] A. Borah and R. Mihalcea, “Towards implicit bias detection and mitigation in multi-agent llm interactions,” arXiv preprint arXiv:2410.02584, 2024.  \n[180] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al., \"Agentbench: Evaluating llms as agents,\" arXiv preprint arXiv:2308.03688, 2023.  \n[181] G. He, G. Demartini, and U. Gadiraju, \"Plan-then-execute: An empirical study of user trust and team performance when using lvm agents as a daily assistant,\" in Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 1-22, 2025.  \n[182] Z. Ke, F. Jiao, Y. Ming, X.-P. Nguyen, A. Xu, D. X. Long, M. Li, C. Qin, P. Wang, S. Savarese, et al., \"A survey of frontiers in llm reasoning: Inference scaling, learning to reason, and agentic systems,\" arXiv preprint arXiv:2504.09037, 2025.  \n[183] M. Luo, X. Shi, C. Cai, T. Zhang, J. Wong, Y. Wang, C. Wang, Y. Huang, Z. Chen, J. E. Gonzalez, et al., \"Autellix: An efficient serving engine for llm agents as general programs,\" arXiv preprint arXiv:2502.13965, 2025.  \n[184] K. Hatalis, D. Christou, J. Myers, S. Jones, K. Lambert, A. Amos-Binks, Z. Dannenhauer, and D. Dannenhauer, \"Memory matters: The need to improve long-term memory in lmm-agents,\" in Proceedings of the AAAI Symposium Series, vol. 2, pp. 277-280, 2023.  \n[185] H. Jin, X. Han, J. Yang, Z. Jiang, Z. Liu, C.-Y. Chang, H. Chen, and X. Hu, “Llm maybe longlm: Self-extend llm context window without tuning,” arXiv preprint arXiv:2401.01325, 2024.  \n[186] M. Yu, F. Meng, X. Zhou, S. Wang, J. Mao, L. Pang, T. Chen, K. Wang, X. Li, Y. Zhang, et al., \"A survey on trustworthy llm agents: Threats and countermeasures,\" arXiv preprint arXiv:2503.09648, 2025.  \n[187] H. Chi, H. Li, W. Yang, F. Liu, L. Lan, X. Ren, T. Liu, and B. Han, \"Unveiling causal reasoning in large language models: Reality or mirage?\", Advances in Neural Information Processing Systems, vol. 37, pp. 96640-96670, 2024.  \n[188] H. Wang, A. Zhang, N. Duy Tai, J. Sun, T.-S. Chua, et al., \"Ali-agent: Assessing llms' alignment with human values via agent-based evaluation,\" Advances in Neural Information Processing Systems, vol. 37, pp. 99040-99088, 2024.  \n[189] L. Hammond, A. Chan, J. Clifton, J. Hoelscher-Obermaier, A. Khan, E. McLean, C. Smith, W. Barfuss, J. Foerster, T. Gavenciak, et al., “Multi-agent risks from advanced ai,” arXiv preprint arXiv:2502.14143, 2025.\n\n[190] D. Trusilo, \"Autonomous ai systems in conflict: Emergent behavior and its impact on predictability and reliability,\" Journal of Military Ethics, vol. 22, no. 1, pp. 2-17, 2023.  \n[191] M. Puvvadi, S. K. Arava, A. Santoria, S. S. P. Chennupati, and H. V. Puvvadi, \"Coding agents: A comprehensive survey of automated bug fixing systems and benchmarks,\" in 2025 IEEE 14th International Conference on Communication Systems and Network Technologies (CSNT), pp. 680-686, IEEE, 2025.  \n[192] C. Newton, J. Singleton, C. Copland, S. Kitchen, and J. Hudack, \"Scalability in modeling and simulation systems for multi-agent, ai, and machine learning applications,\" in Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III, vol. 11746, pp. 534-552, SPIE, 2021.  \n[193] H. D. Le, X. Xia, and Z. Chen, \"Multi-agent causal discovery using large language models,\" arXiv preprint arXiv:2407.15073, 2024.  \n[194] Y. Shavit, S. Agarwal, M. Brundage, S. Adler, C. O'Keefe, R. Campbell, T. Lee, P. Mishkin, T. Eloundou, A. Hickey, et al., \"Practices for governing agentic ai systems,\" Research Paper, OpenAI, 2023.  \n[195] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.-t. Yih, T. Rocktäschel, et al., \"Retrievalaugmented generation for knowledge-intensive nlp tasks,\" Advances in neural information processing systems, vol. 33, pp. 9459-9474, 2020.  \n[196] Y. Ma, Z. Gou, J. Hao, R. Xu, S. Wang, L. Pan, Y. Yang, Y. Cao, A. Sun, H. Awadalla, et al., \"Sciagent: Tool-augmented language models for scientific reasoning,\" arXiv preprint arXiv:2402.11451, 2024.  \n[197] K. Dev, S. A. Khowaja, K. Singh, E. Zeydan, and M. Debbah, \"Advanced architectures integrated with agentic ai for next-generation wireless networks,\" arXiv preprint arXiv:2502.01089, 2025.  \n[198] A. Boyle and A. Blomkvist, “Elements of episodic memory: insights from artificial agents,” Philosophical Transactions B, vol. 379, no. 1913, p. 20230416, 2024.  \n[199] Y. Du, W. Huang, D. Zheng, Z. Wang, S. Montella, M. Lapata, K.-F. Wong, and J. Z. Pan, \"Rethinking memory in ai: Taxonomy, operations, topics, and future directions,\" arXiv preprint arXiv:2505.00675, 2025.  \n[200] K.-T. Tran, D. Dao, M.-D. Nguyen, Q.-V. Pham, B. O'Sullivan, and H. D. Nguyen, \"Multi-agent collaboration mechanisms: A survey of llms,\" arXiv preprint arXiv:2501.06322, 2025.  \n[201] K. Tallam, “From autonomous agents to integrated systems, a new paradigm: Orchestrated distributed intelligence,” arXiv preprint arXiv:2503.13754, 2025.  \n[202] Y. Lee, “Critique of artificial reason: Ontology of human and artificial intelligence,” Journal of Ecohumanism, vol. 4, no. 3, pp. 397–415, 2025.  \n[203] L. Ale, S. A. King, N. Zhang, and H. Xing, “Enhancing generative ai reliability via agentic ai in 6g-enabled edge computing,” Nature Reviews Electrical Engineering, pp. 1-3, 2025.  \n[204] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, \"Reflexion: Language agents with verbal reinforcement learning,\" Advances in Neural Information Processing Systems, vol. 36, pp. 8634-8652, 2023.  \n[205] F. Kamalov, D. S. Calonge, L. Smail, D. Azizov, D. R. Thadani, T. Kwong, and A. Atif, \"Evolution of ai in education: Agentic workflows,\" arXiv preprint arXiv:2504.20082, 2025.  \n[206] A. Sulc, T. Hellert, R. Kammering, H. Hoschouer, and J. S. John, \"Towards agentic ai on particle accelerators,\" arXiv preprint arXiv:2409.06336, 2024.  \n[207] J. Yang, C. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press, \"Swe-agent: Agent-computer interfaces enable automated software engineering,\" Advances in Neural Information Processing Systems, vol. 37, pp. 50528-50652, 2024.  \n[208] S. Barua, “Exploring autonomous agents through the lens of large language models: A review,” arXiv preprint arXiv:2404.04442, 2024.",
    "translated_content": null,
    "created_at": "2025-12-15 11:44:10.895310",
    "updated_at": "2025-12-15 11:44:17.838779"
  },
  "9d7befb9-e05e-4cb6-89d3-41a9b5d3a916": {
    "id": "9d7befb9-e05e-4cb6-89d3-41a9b5d3a916",
    "filename": "from words 2 winsdom 2511.20547v1.pdf",
    "file_path": "./uploads/papers/9d7befb9-e05e-4cb6-89d3-41a9b5d3a916.pdf",
    "status": "completed",
    "title": "From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding",
    "category": null,
    "markdown_content": "# From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding\n\nFarjana Sultana Mim, Shuchin Aeron, Eric Miller and Kristen Wendell\n\nAbstract-Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.\n\nIndex Terms—Natural Language Processing, Large Language Model, Discourse, Dialogue, Education.\n\n# I. INTRODUCTION\n\nRESEARCH in classroom settings has shown that student learning outcomes are higher when students frame a classwork or homework activity as an opportunity for constructing knowledge rather than as a task to be produced for the instructor [1], [2]. In other words, two important features of student conversations are: knowledge construction (KC) discourse, which refers to the student talks focused on developing conceptual understanding, and task production (TP) discourse, where student talks are focused on completing an instructional task as expeditently as possible [3].\n\nPrior research in learning sciences has also demonstrated that when students frame their purpose within an instructional activity as constructing knowledge rather than just completing a task, they are more likely to develop expertise and be able to later transfer their expertise to new situations [4]. These\n\nF. S. Mim is with the Department of Electrical and Computer Engineering, Tufts University, Medford, MA 02155, United States. (e-mail: farjana.mim59@gmail.com) (currently in the Department of Computer Science and Information Technology, Patuakhali Science and Technology University, Bangladesh)  \nS. Aeron is with the Department of Electrical and Computer Engineering, Tufts University, Medford, MA 02155, United States. (e-mail: shuchin.aeron@tufts.edu)  \nE. Miller is with the Department of Electrical and Computer Engineering, Computer Science and Biomedical Engineering, Tufts University, Medford, MA 02155, United States. (e-mail: eric.miller@tufts.edu)  \nK. Wendell is with the Department of Mechanical Engineering and Education, Tufts University, Medford, MA 02155, United States. (e-mail: kristen.wendell@tufts.edu)\n\n# Homework Topic\n\nDesign an experiment complete with instrumentation to determine the specific heats of a gas using a resistance heater. Discuss how the experiment will be conducted, what measurements need to be taken, and how the specific heats will be determined. What are the sources of error in your system? How can you minimize the experimental error?\n\n# Task Production Discourse\n\nStudent X: Although we just have to design the experiment. It's not like we have to actually do it.\n\nStudent  $T$  : No.\n\nStudent A: Just design and justify this will work.\n\nStudent  $X$ : How can you minimize the experimental error. That's one of the points there.\n\n# Knowledge Construction Discourse\n\nStudent X: Ok. So one thought I had too was that actually um whatever material the container is made out of when it heats up, it's going to expand -\n\nStudent T: Mhm.\n\nStudent X: - and that will change whatever the internal volume is. And I don't know if it makes it bigger or smaller actually. It um might make it bigger but if there were -\n\nFig. 1: Students' homework discussion's snippet of knowledge construction and task production discourse.\n\nresults have been found across several disciplines, including physics, chemistry, biology, and engineering education [1]–[3], [5]. However, the relationship between knowledge construction discourse and learning outcomes has yet to be translated into actionable principles for pedagogy and curriculum design. The major difficulty lies in pinpointing which particular aspects of the learning environment and instructional activity cue students into knowledge constructing discourse.\n\nTo address this gap, we aim to develop efficient methods for distinguishing students' knowledge construction discourse from their task production discourse so that researchers can more broadly investigate the conditions or contexts under which students tend to adopt a knowledge construction framing. Such findings would enable educators to design learning experiences and environments so that they cue students toward constructing knowledge.\n\nFig 1 shows examples of knowledge construction and task production discourse in an undergraduate engineering students' conversation. In the task production discourse of the example, the students remind each other that their homework task is to design an experiment and describe how they would minimize experimental error. These lines are focused on setting up the steps to complete their homework. In the\n\nknowledge construction discourse from the same homework conversation, student  $X$  shares an idea about how the process of heating a gas will affect the material containing it. Rather than simply completing a pre-determined step of the homework, student  $X$  tries to envision the phenomena that will occur in the experiment the students are designing. At this moment,  $X$ 's turn of talk is oriented toward understanding rather than expediency.\n\nTraditional manual analysis of student dialogues to identify these discourse features is time-intensive, which limits the scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of KC and TP discourse, providing educational researchers with valuable insights into how curricular and pedagogical variables influence students to engage in knowledge construction rather only task production.\n\nDiscourse in dialogue or conversations has been widely studied in NLP in different task settings such as dialogue act classification [6]–[9], dialogue topic segmentation and categorization [10]–[13], dialogue state tracking [14]–[19], and identifying dialogue system behaviors [20], [21]. However, although various discourse frameworks are being applied to different types of conversational data, hardly any of them are educational data [22], [23]. To address this gap, this study creates a novel educational dialogue dataset, annotated with knowledge construction (KC) and task production (TP) discourse<sup>1</sup>. We also formulate the NLP task of KCTP (Knowledge Construction and Task Production) prediction, aiming to automatically identify these discourse types within educational dialogues.\n\nLately, the NLP field has been revolutionized by pre-trained large language models (LLMs) such as GPT-3 [24], Llama [25], Gemini [26], Deepseek [27]. These models have demonstrated significant performance gains and yielded interesting findings across various NLP tasks, including the study of discourse in dialogues or conversations [14], [20]. Recently, a new paradigm called \"Pre-train, Prompt, and Predict\" [28] has gained popularity which leverages pre-trained LLMs through natural language prompts instead of fine-tuning them for specific tasks. By using such \"prompting\" method, one can probe task-specific knowledge from LLMs, which has shown remarkable performance in various tasks such as text classification and summarization [29], [30]. Another paradigm called \"instruction fine-tuning\" [31] which finetunes a model on a dataset via instructions, has significantly improved the performance of several tasks [32]. Therefore, we use GPT-3.5 with prompting techniques to establish a baseline for our Knowledge Construction vs. Task Production (KCTP) prediction task. However, as GPT-3.5 is not an open-source model, we also use the open-access LLaMA-3.1 (8B) model [33] and fine-tune it for the same task. Experimental results indicate that prompting and fine-tuning GPT-3.5 and LLaMA-3.1 yield suboptimal performance on KCTP prediction, suggesting the need for further research into models and methods better suited\n\nto educational discourse analysis. To summarize, the main contributions of this work are as follows:\n\n- We create a novel educational dialogue dataset annotated with Knowledge Construction (KC) and Task Production (TP) discourse, addressing a gap in discourse-annotated educational data.  \n- We formulate the Knowledge Construction vs. Task Production (KCTP) classification as a natural language processing (NLP) task to automatically identify KC and TP discourse in student dialogues.  \n- We establish baseline models for the KCTP prediction task using GPT-3.5 and LLaMA-3.1 prompting as well as LLaMA-3.1 instruction fine-tuning, revealing current limitations of LLMs in modeling educational discourse and highlighting directions for future research.\n\n# II. RELATED WORK\n\nThis study develops an educational dialogue dataset annotated with instances of knowledge construction (KC) and task production (TP) discourse. We also establish baseline models for the automatic prediction of KC and TP discourse, with the goal of enabling educational researchers to identify the curricular and pedagogical conditions that encourage students to engage in constructing knowledge rather than merely completing tasks. In this section, we briefly review prior work in three relevant areas: (1) discourse in learning sciences, (2) discourse analysis in Dialogue using NLP, and (3) use of pretrained language models for discourse modeling in dialogue.\n\n# A. Learning Sciences Approach to Educational Discourse Analysis\n\nDiscourse has been long studied in learning sciences to determine the nature of activity, understanding, and learning styles of students [1]–[5]. Gouvea et al. [1] presented a case study of a life-science major in a reformed physics course, showing how epistemological shifts in one discipline can transfer to another. Over a year, the student moved from rote learning to coherence-seeking reasoning in physics, integrating materials, peer discussion, and feedback. This reframing extended to biology, where the student began approaching the subject more conceptually. The study provides qualitative evidence that discourse-centered instructional strategies can foster cross-disciplinary epistemological development.\n\nIn another work, Scherr and Hammer [5] explored how students' collaborative behaviors such as posture, gaze, gestures, and vocal dynamics serve as observable indicators of their epistemological framing during active-learning physics activities. They analyze video recordings from introductory physics tutorial sessions and identify distinct behavioral clusters corresponding to different ways students frame the task: for instance, working through substance-based sensemaking versus perceiving it as a procedural worksheet exercise. The authors demonstrate that when students frame the activity as sensemaking, their behaviors align with deeper conceptual reasoning and engagement in discussing the substance of ideas. Their findings highlight the dynamic interplay between\n\nobservable behavior, framing, and the quality of students' scientific reasoning in small-group learning contexts.\n\nKoretsky et al. [2] examined how the design of engineering tasks and instructional framing influence student team dynamics, balancing action (\"doing\") and reflection (\"thinking\") during collaborative open-ended projects. Through detailed cases of small-group engineering design work, they show that when tasks are meaningful, realistic, and properly scaffolded, teams display more equitable participation, distributed modeling and communication, and deeper conceptual reasoning rather than surface-level task execution alone. In particular, the interplay between material engagement (e.g., prototyping and sketching) and explicit discourse about design decisions fosters collective sense-making and shared agency. The study highlights how thoughtfully structured activities and facilitative framing can empower teams to engage in both productive action and epistemic dialogue, offering important implications for discourse-centric analyses and NLP applications in educational dialogue modeling.\n\n# B. Discourse Analysis in Dialogue using NLP\n\nDiscourse in dialogue has been extensively studied in natural language processing (NLP) [34]–[36]. Raheja and Tetreault [7] proposed a hierarchical recurrent neural network and coupled it with a context-aware self-attention mechanism to model different levels of utterance and dialogue act semantics, achieving state-of-the-art performance on the Switchboard Dialogue Act Corpus. Liu et al. [10] introduced a joint model for dialogue segmentation and topic categorization, which was evaluated on a clinical spoken conversation dataset created by them. In another work, Xu et al. [15] developed a Dialogue State Distillation Network (DSDN), which leverages relevant information of previous dialogue states and employs an interslot contrastive learning loss to effectively capture the slot co-update relations from dialogue context. Their proposed method achieved state-of-the-art performance on the dialogue state tracking task. Sabour et al. [21] introduced a novel approach for empathetic response generation in dialogue, which leverages commonsense to draw more information about the user's situation and uses that to further enhance the empathy expression in generated responses. They showed that their approach outperforms the baseline models in both automatic and human evaluations.\n\n# C. Use of Pre-trained Language Models for Discourse Modeling in Dialogue\n\nRecent advancements of pre-trained large language models (LLMs) has significantly advanced the field of discourse modeling [37]–[40]. The importance of modeling speaker turns in dialogues was investigated by He et al. [6], where they incorporated turn changes in conversations among speakers for the dialogue act classification task. They introduced speaker turn embeddings and added them to utterance embeddings produced by the pretrained language model RoBERTa [41], which showed better performance for the dialogue act classification task. Xing and Carenini [11] utilized a neural utterance-pair coherence scoring model based on fine-tuning NSP BERT\n\n![](/uploads/images/9d7befb9-e05e-4cb6-89d3-41a9b5d3a916/bba84cd114aabd9a566f1acd99a86c2156a7e29a4dc63e3f8155985510fbf70a.jpg)  \nFig. 2: Topic distribution across the dataset\n\n[42] and achieved state-of-the-art results on the Dialogue topic segmentation task across three public datasets. Feng et al. [14] presented the first evaluation of ChatGPT on the dialogue state tracking task, highlighting its superior performance over prior methods. They also proposed an LLM-driven dialogue state tracking framework based on smaller, open-source foundation models and showed that it achieves comparable performance to ChatGPT. Finch et al. [20] investigated the ability of the state-of-the-art large language model (LLM), i.e., ChatGPT-3.5, to perform dialogue behavior detection for nine categories in real human-bot dialogues and showed that although ChatGPT performed promisingly, often outperforming specialized detection models, the result is still not up to human performance.\n\nFew researches have been conducted that focus on discourse modeling on educational dialogue data. Jensen et al. [22] proposed a methodology for providing teachers with objective, automated feedback on the quality of their classroom discourse by comparing traditional open-vocabulary approaches using n-grams and Random Forest classifiers with a modern deep transfer learning method leveraging BERT. By modeling seven key features of teacher talk (such as questioning and elaborated evaluation) on 127 recordings of classroom talk, the authors demonstrated that while transfer learning with BERT offers a promising path for enhancing automated discourse analytics in education, its effectiveness hinges on the availability of sufficient annotated data to fine-tune the model effectively. Alic et al. [23] automatically distinguished between two pedagogically significant types of teacher questions: funneling questions, which guide students toward specific answers, and focusing questions, which encourage students to reflect on their reasoning. The authors create a labeled dataset of over 2,000 teacher questions annotated by experts and develop both supervised (fine-tuned RoBERTa) and unsupervised models to classify question types. Their supervised RoBERTa model showed strong alignment with expert judgments and correlates with key educational outcomes, such as instructional quality\n\n![](/uploads/images/9d7befb9-e05e-4cb6-89d3-41a9b5d3a916/e0b74a57a9ef55add7777e0214581eb34aea6f0100a23d660f768c53616a43dc.jpg)  \nFig. 3: Distribution of categories across the dataset.\n\nand student learning gains.\n\n# III. DATASET CURATION\n\n# A. Data Collection\n\nWe recorded homework discussions among undergraduate mechanical engineering students, focusing on topics from their thermal fluid systems course. Between 2 and 5 students participated in each conversation. Then, we transcribed the conversations ensuring that all data were de-identified. As part of the consent process, students were asked if their de-identified transcripts could be used in future research. Only transcripts from students who consented were included in the dataset.\n\n# B. Dataset Statistics\n\nThe dataset consists of 32 small-group conversations covering 19 homework topics, each topic corresponding to a distinct task description that students were required to complete collaboratively through discussion. Fig 2 shows the topic distribution across the dataset. The utterances in the conversations are segmented based on the fact that one \"turn,\" or utterance, consists of everything a single person utters until another person speaks (either because the first person has finished or because they interrupt the first person). The average token per conversation is 6404, and the average turns of talk is 321. Please see the appendix for the details of each topic.\n\n# C. Annotation Study\n\n1) Setup: Two expert annotators, including one co-author of this paper, participated in the annotation study. We developed a comprehensive annotation guideline and instructed the annotators to label each conversational turn as knowledge construction, task production, uncertain, or other. We created the label uncertain for the turns of talk where there is insufficient evidence to determine whether the speaker is continuing the current framing of either knowledge construction or task production. If a single utterance includes indicators for both\n\n![](/uploads/images/9d7befb9-e05e-4cb6-89d3-41a9b5d3a916/260629126576a8a452629b85f7734b9d10aec438f5e70a735dde8bd574948d7c.jpg)  \nFig. 4: Confusion matrix of dual annotations\n\nKC and TP classification, and the annotator cannot determine which category is the predominant framing for the student during the utterance, the utterance should be classified as uncertain. The label other refers to the turns of talk where students discuss a topic other than the assigned problem, such as the purpose of participating in the research study, or other academic classes or social events.\n\nWe trained the annotators in a pilot annotation phase where they were asked to annotate 5 conversations. After the pilot annotation, we discussed the disagreements and, if needed, adjusted the annotation guidelines. In our main annotation study, 6 conversations were annotated by two annotators and 21 conversations were annotated by a single annotator. For inter-annotator agreement (IAA) and the analysis of annotations, we report the results of dual annotations. Fig 3 illustrates the distribution of annotated labels across the dataset of 32 conversations. As anticipated, we see that the dataset is imbalanced, with the majority of annotated labels falling into the knowledge construction and task production categories.\n\n2) Inter-annotator agreement (IAA): We computed IAA using Cohen's  $(\\kappa)$  [43] for the dual annotations across four annotated discourse labels (i.e., knowledge construction, task production, uncertain and other). We obtained Cohen's  $(\\kappa)$  of 0.45 which indicates a moderate agreement [44], [45].  \nDiscerning undergraduate students' aims and purposes based on their spoken word is notoriously difficult for learning sciences researchers [5]. The difficulty in quickly determining whether students are in task production or knowledge construction mode (or when those modes are co-occurring) is one reason researchers are interested in exploring algorithm-assisted annotation. This also means that it is not surprising that the agreement between the two annotators was only moderate.  \n3) Analysis of Annotations: The confusion matrix of the dual annotations of 6 conversations is shown in Figure 4. We see that both annotators mostly agree with each other during the labeling of task production (TP) discourse and the most disagreement happens when one annotator thinks a turn of\n\ntalk is knowledge construction (KC) while other thinks that it's task production.\n\nFigure 4 reveals that Annotator 1 leaned toward classifying discourse as TP, while Annotator 2 leaned toward classifying discourse as KC. Of the utterances on which there was TP vs KC disagreement between the two human annotators, Annotator 1 chose KC for only  $7\\%$  (50/726) of the disagreements while Annotator 2 chose KC in  $93\\%$  (676/726) of the cases. Besides, where there was TP vs \"other\" disagreement,  $94\\%$  (125/133) times Annotator 1 chose TP over other, while just  $0.06\\%$  (8/133) times Annotator 2 chose TP over \"other.\"\n\nWe also found that the disagreement mostly happens under two conditions: (i) when students discuss the details of their problem-solving steps, and (ii) when students ask each other questions. For example, Consider the conversation snippet below (from Topic 4, \"determine the specific heats\").\n\nT: Right. Also how long are we doing it for. It's for like\n\nX: Yeah. Do it for ten hours. Do we need another you know microsecond.\n\nT: Yeah. Um ok. So then graph um V I versus time and take the area under the curve. Um. Ok. That area under the curve is just gonna be equal to  $Q$ , right?\n\nThe students here are discussing the details of an experimental design. Their homework task is to specify the design set-up. They consider the duration of the experiment, the plot they will produce from the data, and the physical quantity represented on that plot. Annotators 1 and 2 disagree on whether this portion of the discussion is aimed toward deeper understanding or toward making progress on the assignment. On one hand, discussion of experiment timescales and of the meaning of a graph might help students build knowledge about the physical quantity to be measured in the experiment. On the other hand, the students' statements about the length of the experiment and of the graphs it will generate could simply comprise another step forward in specifying an experimental design, which is completing the homework task.\n\nThe correct label in the case is knowledge construction. When student X discusses the experiment's timescale and student T discusses the meaning of the area under the curve, they are talking about concepts that they contributed anew to this homework discussion; these were not concepts mentioned in the homework problem statement, notes, or textbook for this course. Therefore, the students were calling up other intellectual resources to construct new ideas for this homework activity.\n\n# IV. EXPERIMENTS\n\n# A. Task setting\n\nWe consider the prediction task of KCTP discourse in a conversation as a label-generation task for each turn of talk in the conversation, where the model is instructed to generate one label out of the four annotated labels i.e., knowledge construction (KC), task production (TP), uncertain, other for each turn of talk.\n\nTo create a strong baseline, we assume that in cases where such KCTP discourse-specific resources are unavailable, pretrained large language models (LLMs) could be the most\n\neffective means of generating KCTP discourse labels for each turn in the conversation. We evaluate our task in three settings: (i) Zero-shot prompting setting: Zero-shot prompting is a technique used with large language models in which a task is defined using only natural language instructions, without providing any examples of how the task should be performed. This method relies on the model's pre-trained knowledge and ability to generalize in order to accurately interpret and carry out the given instruction. (ii) Few-shot prompting setting: Few-shot prompting is a technique where a language model is given a few input-output examples along with a natural language instruction to guide its response to new, similar inputs. In contrast to zero-shot prompting, which relies only on instructions, few-shot prompting uses these examples to establish a pattern or context that the model can mimic. This method exploits the in-context learning ability of large language models, allowing them to generalize from a small number of examples without the need for task-specific fine-tuning [24]. (iii) Fine-tuning setting: Fine-tuning refers to the process of taking a pre-trained large language model (which is generally trained on a large, general-purpose corpus) and further training it on a smaller, task-specific dataset to improve its performance on a particular task. This transfer learning strategy [42] allows models to leverage the rich representations learned during pre-training and adapt them to specialized tasks.\n\n# B. Models\n\nWe employ state-of-the-art LLMs namely GPT-3.5-turbo [24] and Llama-3.1-8B-Instruct [33] models for the KCTP discourse prediction task while we use GPT-4-1106-preview [46] for our prompt engineering [28]. A GPT (Generative Pre-trained Transformer) model [47] is an auto-regressive large language model developed by OpenAI that uses transformer [48] architecture to generate and understand human-like text. GPT models use a transformer decoder architecture, which is trained to predict the next word in a sequence, followed by fine-tuning on labeled datasets for specific applications. GPT-3.5 Turbo is optimized for speed and cost-efficiency, making it ideal for high-volume tasks. In contrast, GPT-4 offers superior reasoning, accuracy, and contextual understanding for more complex applications while costing more as well. Like GPT, the LLaMA (Large Language Model Meta AI) series [25] developed by Meta is also an auto-regressive language model based on the transformer architecture. Its key advantage lies in being open-source, enabling cost-free use while still delivering competitive performance.\n\n# C. Prompt Design\n\nWe create 5 prompts for the KCTP prediction task and use the GPT-4-1106-preview model to optimize our created prompts. We report results for both types of prompts, i.e., our curated prompts and GPT-4 optimized prompts. We also use the optimized prompts for instruction fine-tuning of Llama3.1 8B model. Among the 5 prompts, prompts 1 and 2 consist of the previous dialogue context along with the current turn of talk. Prompts 3 and 4 include the task description and the definitions of the labels respectively in addition to the previous\n\n<table><tr><td rowspan=\"3\">Prompts</td><td colspan=\"4\">Zero-Shot</td><td colspan=\"4\">Few-Shot</td><td>Fine-Tuned</td></tr><tr><td colspan=\"2\">Curated prompt</td><td colspan=\"2\">Optimized prompt</td><td colspan=\"2\">Curated prompt</td><td colspan=\"2\">Optimized prompt</td><td>Optimized prompt</td></tr><tr><td>GPT-3.5</td><td>Llama-3.1</td><td>GPT-3.5</td><td>Llama-3.1</td><td>GPT-3.5</td><td>Llama-3.1</td><td>GPT-3.5</td><td>Llama-3.1</td><td>Llama-3.1</td></tr><tr><td>Prompt 1</td><td>0.34</td><td>0.43</td><td>0.29</td><td>0.49</td><td>0.35</td><td>0.48</td><td>0.26</td><td>0.50</td><td>0.54</td></tr><tr><td>Prompt 2</td><td>0.28</td><td>0.38</td><td>0.47</td><td>0.40</td><td>0.35</td><td>0.45</td><td>0.37</td><td>0.48</td><td>0.51</td></tr><tr><td>Prompt 3</td><td>0.49</td><td>0.47</td><td>0.55</td><td>0.44</td><td>0.39</td><td>0.51</td><td>0.40</td><td>0.47</td><td>0.45</td></tr><tr><td>Prompt 4</td><td>0.32</td><td>0.52</td><td>0.46</td><td>0.50</td><td>0.38</td><td>0.54</td><td>0.44</td><td>0.57</td><td>0.49</td></tr><tr><td>Prompt 5</td><td>0.27</td><td>0.39</td><td>0.33</td><td>0.46</td><td>0.27</td><td>0.44</td><td>0.27</td><td>0.49</td><td>0.55</td></tr></table>\n\nTABLE I: Performance of GPT-3.5 and Llama-3.1 in the label prediction task under zero-shot, few-shot, and fine-tuned settings using different prompt types.\n\ndialogue context and current turn of talk. Prompt 5 consists of both the previous and afterward dialogue context and the current turn of talk. Please see the details of these prompts in Appendix.\n\n# D. Setup\n\nWe conduct experiments in zero-shot and eight-shot (two examples for each of the four labels) prompt settings where the number of shots reflects the number of examples provided in the prompt. Few-shot examples were sampled from two conversations and topics not included in the dataset.\n\nWe use OpenAI's API for GPT models and set the temperature of the model at 0.0 and maximum tokens at 10. To use Llama-3.1 (8B) for our task, we use Unsloth, an open-source AI library that enables us to train an LLM faster and efficiently with less GPU memory by applying techniques like quantization [49] and low-rank adaptation (LoRA) [50]. In our zero-shot and few-shot experiments, we set the Llama-3.1 model with a temperature of 1.5 and a maximum of 64 new tokens. Fine-tuning is performed for 5 epochs using a learning rate of 1e-4 with the AdamW 8-bit optimizer. We use a batch size of 8, gradient accumulation of 16, and a weight decay of 0.01. All experiments are conducted with a fixed random seed of 3407.\n\n# E. Evaluation Procedure\n\nWe use the weighted F1 score to evaluate model performance. During fine-tuning Llama-3.1, we employ five-fold cross-validation to obtain results across the entire dataset and enable comparison with zero-shot and few-shot prompting.\n\n# V. RESULTS AND DISCUSSIONS\n\nTable I presents the performance of GPT-3.5 and LLaMA-3.1 models across three experimental settings: Zero-Shot, Few-Shot, and Fine-Tuned, using both curated and GPT-4 optimized prompts on the KCTP discourse label prediction task. Five prompts (Prompt 1-5) developed by us are evaluated, and the scores represent the average F1 score.\n\n# A. Zero-shot and Few-shot effectiveness\n\nThe results indicate that overall, Llama-3.1 performs better than GPT-3.5 for both curated and optimized prompts across zero-shot and few-shot settings. However, the overall performance remains suboptimal, with the highest F1-score reaching only 0.57.\n\nIn the zero-shot setting, the best result is obtained by GPT-3.5 with optimized prompt 3, which includes a topic description in addition to the previous dialogue contexts. The result suggests that explicitly providing the communicative goal of the student conversation helps the model infer appropriate labels without prior examples. However, the GPT-3.5 performance drops in the few-shot setting. We assume that one of the reasons the few-shot prompting did not perform better here could be attributed to the fact that the examples we used didn't generalize well with the dataset, or the model had too much information to process. Moreover, performance degradation can sometimes happen in some LLMs for adding too domain-specific examples [51]. In contrast, LLaMA-3.1 attains its highest score in the few-shot setting with optimized prompt 4, which incorporates label definitions alongside the preceding dialogue context. It means that when we include examples, LLaMA-3.1 can better generalize than GPT-3.5 on this task by leveraging explicit label information. Table I also shows that optimizing our curated prompt with GPT-4 improves the overall performance.\n\n# B. Fine-tuning effectiveness\n\nThe fine-tuning results presented in Table I show that finetuning Llama-3.1 does not exceed its zero-shot and few\n\n![](/uploads/images/9d7befb9-e05e-4cb6-89d3-41a9b5d3a916/83dc0106f80e5606accfc9a13fd0a6d0b484c889c90d19ba70e60902a57169c3.jpg)  \nFig. 5: Confusion matrix of model prediction vs. true annotated labels\n\n<table><tr><td>Prompts</td><td>Same topic</td><td>Single different topic</td><td>Three different topics</td></tr><tr><td>Prompt 1</td><td>0.64</td><td>0.61</td><td>0.65</td></tr><tr><td>Prompt 2</td><td>0.58</td><td>0.63</td><td>0.61</td></tr><tr><td>Prompt 3</td><td>0.60</td><td>0.63</td><td>0.61</td></tr><tr><td>Prompt 4</td><td>0.53</td><td>0.58</td><td>0.62</td></tr><tr><td>Prompt 5</td><td>0.64</td><td>0.62</td><td>0.61</td></tr></table>\n\nTABLE II: Performance of finetuned Llama-3.1 when trained on the same topic, a single different topic, and three different topics.\n\nshot performance. We hypothesize that since our data is very domain-specific, it may have reduced the model's generalization ability by overfitting to narrow linguistic or conceptual patterns. Recent studies also showed that, in certain cases, fine-tuning often yields limited or even negative performance gains for large language models (LLMs) [52].\n\nFurthermore, the results suggest that incorporating dialogue context alone (as in Prompts 1 and 3), without additional task-specific contextual signals, leads to relatively better outcomes during fine-tuning. This observation implies that minimal yet coherent contextual grounding may help the model retain its pretrained reasoning and discourse capabilities.\n\n# C. Analysis\n\nFigure 5 presents the confusion matrix of the true annotated labels versus the model-predicted labels for the best result (i.e., Llama 3.1 performance in the few-shot settings with optimized prompt 4). It shows that the model is good at predicting the task production labels. However, it struggles with predicting the knowledge construction label and misclassifies about half of these instances as task production, indicating notable room for improvement. Moreover, the model also kind of struggles with predicting the \"other\" label, only correctly predicting it  $22\\%$  of the time. These findings suggest that while the model captures dominant discourse functions well, it struggles with more nuanced or less frequent categories, highlighting opportunities for improving label representation and contextual modeling.\n\nSince our dataset consists a diverse set of 19 topics (see Appendix for the details of each topic), we investigated how topic similarity between training and testing data influences model performance. For this experiment, we kept the test data same, and the training data size fixed, but varied the topical composition of the training data across three configurations: (i) training and test data drawn from the same topic, (ii) training data from a different single topic than the test set, and (iii) training data from three different topics distinct from the test set.\n\nThe results, summarized in Table II, reveal that, the model performs better when trained on data from different topics, except for prompt 5. This finding suggests that exposure to a broader range of linguistic and conceptual patterns enhances the model's generalization ability, whereas training on a single, homogeneous topic may lead to overfitting for most prompts. Notably, the model exhibits slightly better generalization within the same-topic setting for Prompt 5, which incorporates succeeding dialogue context. This indicates that the model might learn topic-bounded discourse dynamics\n\ni.e., how participants introduce, elaborate on, and shift ideas within a coherent topical space in this prompt setting whereas such recurring patterns are less transferable across topics.\n\n# VI. LIMITATIONS\n\nIn this study, we only consider two types of discourse features in educational conversations, namely knowledge construction (KC) and task production (TP). Besides, the dataset is limited to topics from a thermal fluid systems course in mechanical engineering. Moreover, we use a limited set of prompt templates because of the resource and time constraints.\n\n# VII. CONCLUSIONS AND FUTURE WORK\n\nThis work presents a novel educational conversational dataset, annotated with knowledge construction (KC) and task production (TP) discourse. Such discourse properties are crucial for framing student learning activities to develop more effective pedagogical settings that emphasize knowledge construction over mere task completion. In this work, we establish baselines for the KCTP discourse prediction task using state-of-the-art language models with prompting techniques and fine-tuning. Our results demonstrate that state-of-the-art LLMs struggle with this task, both under prompting-based and fine-tuning settings.\n\nFor future work, we plan to create reasoning chains that will help the model better understand the definitions of the labels in the prompts. We also intend to annotate low-level discourse structure for these student dialogues so that looking at the lower levels might help to see how the higher-order concepts emerge from a particular interaction of dialogue moves. Furthermore, we aim to expand the dataset by including a broader range of undergraduate subjects, thereby capturing more diverse discourse patterns across academic domains. This increased topical diversity will support more robust fine-tuning and facilitate the development of models capable of domain-general discourse understanding.\n\n# APPENDIX\n\n<table><tr><td>Topic</td><td>Description</td></tr><tr><td>Topic 1 (T1)</td><td>about a decade ago, stanford university successfully tried using waste vegetable oil from the dining halls as fuel for campus shuttles (https://news.stanford.edu/news/2006/january25/biodiesel-012506.html). what if [institution] tried to do this? plan a useful bus route around [institution] and specify the volume of fuel needed for the bus to travel this route without having to refuel. you may assume the energy density of vegetable oil is 42.20 mj/kg or 30.53 mj/l.</td></tr><tr><td>Topic 2 (T2)</td><td>contrails are giant vortices left by airplanes on the runway and in the sky. if other planes pass through these, it can cause problems because it is like going through a mini tornado, and the planes are not equipped to handle such a pressure gradient. boeing has hired you to design a device to be placed on runways to help get rid of contrails there. this could be done by moving the contrails out of the way or by stopping them altogether. justify your design using fluid mechanics.</td></tr><tr><td>Topic 3 (T3)</td><td>covid-19 has drastically changed how people live their daily lives. guidelines have been created for how far apart people should stay when talking normally to each other. however, if people are doing something like singing, which takes more effort results in air (and droplets potentially carrying the coronavirus) being expelled from the lungs more forcefully, the guidelines for simply talking may not be adequate. if six feet apart is the recommendation for talking, use fluid mechanics argument to decide how far apart people who are singing should stand in order to be far enough away from any particles that may be expelled into the air by their singing.</td></tr><tr><td>Topic 4 (T4)</td><td>design an experiment complete with instrumentation to determine the specific heats of a gas using a resistance heater. discuss how the experiment will be conducted, what measurements need to be taken, and how the specific heats will be determined. what are the sources of error in your system? how can you minimize the experimental error?</td></tr><tr><td>Topic 5 (T5)</td><td>gas turbine engines used in airplanes consist of a fan followed by a compressor, diffuser, combustor, turbine, and sometimes an afterburner. you are designing the engine for a high- altitude airplane. normally, commercial planes operate best around 35,000 ft above sea level, but your plane should operate optimally at around 100,000 ft. because of the high altitude, there will be a lower concentration of oxygen than normal, and the air entering the engine will be colder. design a protocol for getting the oxygen up to the appropriate temperature and pressure needed for combustion. keep in mind your solution has to be relatively light.</td></tr><tr><td>Topic 6 (T6)</td><td>geothermal heat pumps harness renewable geothermal energy by using thermal reservoirs of water deep within the earth for heating. such reservoirs have temperatures up to around 370 degrees celsius. geothermal heat pumps use this energy by transporting room-temperature or cold liquid deep into the ground via pipes, exposing it to the hot reservoir, and carrying it back up to the surface. imagine one of these reservoirs is discovered beneath the building where you live and design a heat exchanger system that uses the reservoir to heat your building. sketch your system and specify the diameter, length, and material of the pipe, the flow rate, and the working fluid. design your system such that it supplies a significant portion of the energy required for your building to operate normally.</td></tr><tr><td>Topic 7 (T7)</td><td>hybrid rockets use a combination of solid and liquid or gaseous propellants. in hybrid rockets, a stable oxidizer is used with a solid fuel. in order to be used, the fuel needs to be vaporized. the primary difficulty with hybrids is with mixing the propellants during the combustion process. in a hybrid rocket, the mixing happens at the melting or evaporating surface. the mixing is not well-controlled and generally, a lot of propellant is left unburned, limiting the motor's efficiency. on the other hand, liquid propellants are generally mixed with oxidizer by an injector at the top of the combustion chamber which directs many small streams of fuel and oxidizer into one another. based on reasonable efficiencies of both liquid fuel and hybrid fuel processes, estimate the weight of fuel necessary to get a specific rocket of your choice to low earth orbit if the fuel is liquid vs. hybrid.</td></tr><tr><td>Topic 8 (T8)</td><td>most ski resorts in the u.s. use snow guns to make additional snow to supplement natural snow. these machines use water and compressed air. the air forces the water to form tiny droplets, which are then expelled from the nozzle and form ice crystals, which then fall to the ground as snow. compressed air cools as it expands, which assists with converting the water droplets into snow. choose your favorite ski resort and the desired depth of snow for the best skiing, and use thermodynamics to determine how long it will take to cover the ski trails in that amount of snow. you may assume that one snow gun uses about 100 gallons of water per minute and that the compressor can produce 50 cfm (cubic feet per minute) of air.</td></tr><tr><td>Topic 9 (T9)</td><td>race cars need to be as aerodynamic as possible. in many cases, to test the aerodynamics of a car, a wind-tunnel is used. you have been hired by chevrolet to analyze the air flow around their race cars. the wind tunnel you will be using to do this is an open circuit wind tunnel, where air is drawn from the laboratory environment, rather than being recirculated in the wind tunnel itself. such wind-tunnels consist of a nozzle to accelerate the air, the test section in which the car sits, and a diffuser which decelerates the air. based on reasonable values for air speed around the vehicle being tested, design a wind-tunnel for testing a race car. include all necessary specifications of the different parts of the wind tunnel, such as dimensions and air speeds. also specify the necessary power of the fan and estimate the head loss due to the vehicle. use fluid mechanics to justify your response.</td></tr><tr><td>Topic 10 (T10)</td><td>since they know you are a mechanical engineer, your neighbors have asked you to help them design a waterfall for their garden similar to the one in the image below. you need to devise a way to get water from the pool at the bottom up to the top of the waterfall, and there needs to be enough water so that the waterfall actually looks good. design a system to do this. include a diagram of how the pump system will work, and include any important specifications such as flow rates and dimensions. then find a pump online and determine approximately how much power the waterfall pump will use per day. you may make as many assumptions as needed, just specify what assumptions you are making and why.</td></tr><tr><td>Topic 11 (T11)</td><td>the building of farfar's danish ice cream shop in duxbury, ma is somewhat old and thus does not seem to have a great cooling system. as a result, sometimes the ice cream gets a bit melty even when it's still in the freezer. the temperature in the ice cream shop is to be maintained at 55°f. estimate the dimensions of the building, use thermodynamics principles to determine the maximum heat loss the shop can have, and suggest a method for minimizing this heat loss.</td></tr><tr><td>Topic 12 (T12)</td><td>the butterfly swimming stroke is considered by many to be one of the most difficult strokes. it is also one of the fastest. when used over longer distances, the butterfly stroke is slightly slower than freestyle, partly due to the greater physical exertion required by the butterfly. however, butterfly has the fastest peak speed. explain why you think this stroke has the fastest peak speed. then, design a special swimsuit or other (non-motorized) device for a swimmer to further increase the speed of the butterfly stroke so that it will always be faster than freestyle no matter the distance over which the stroke is used. include a diagram of your design, and use fluid mechanics principles to prove that it will work.</td></tr><tr><td>Topic 13 (T13)</td><td>trek bikes has contracted you to design an attachment for their bikes to help make the bike and rider more streamlined. this attachment should effectively reduce the bike and rider's air resistance without impeding the cyclist's ability to ride their bike as usual. also specify what material this should be made of, and include a diagram of your design. justify your design using fluid mechanics.</td></tr><tr><td>Topic 14 (T14)</td><td>you are designing a tiny home that can be used for camping adventures. you want to be able to take your tiny home on camping trips in vermont and new hampshire during the fall to see the foliage, but you are worried that it might get a bit too cold for comfort, as that time of year, the temperatures at night can get down to 30°F. design a small, low-power HVAC system to keep the inside of your tiny home at a temperature no lower than 45°F. specify what parts will be needed and how this system will be compatible with your tiny home. use fluid mechanics and heat transfer principles to justify that this system will indeed keep the temperature at 45°F or higher.</td></tr><tr><td>Topic 15 (T15)</td><td>you are working at a robotics company to design a robot that can swim in water to collect data on sharks. this robot needs to be as hydrodynamic as possible so that it is efficient, and you need to be able to control how fast the robot will go so it can keep up with the sharks, as well as be able to make it turn while swimming. design a swimming robot, estimate its drag coefficient and the drag on the robot when it is moving at three different speeds (so you should have three different values for drag). then determine how much power will be needed to make the robot move forward at each of the three speeds. include a diagram of your robot in your response.</td></tr><tr><td>Topic 16 (T16)</td><td>you have been contracted by [institution] to design a system to get hot water to different parts of the science and engineering complex (sec). in particular, this system needs to work well during winter, when it is colder outside and most likely slightly colder than usual within the outer walls of the building and in the building in general. estimate the wattage necessary to keep the water at a reasonably hot temperature, and determine the flow rates and pressures necessary to get the hot water to various parts of the building. include a labeled sketch of your design, and be sure to use fluid mechanics to justify that your design will work.</td></tr><tr><td>Topic 17 (T17)</td><td>you have been doing a lot of baking recently and wish that you had a convection oven. convection ovens have one or more fans that help circulate the air in the oven, whereas in regular ovens, the only thing moving the air is natural convection. therefore, you want a convection oven so that you can bake everything faster and more evenly. however, you don’t want to spend the money on an entirely new oven since convection ovens are expensive, and you don’t want to have to get rid of the regular oven you already have. design something you can put in your regular oven that will make it function similarly to a convection oven. specify air flow rates and estimate the power needed for any components. also draw a diagram of your design and specify where any proposed components will go in the oven. use fluid mechanics to justify that your design will make your oven work similarly to a convection oven.</td></tr><tr><td>Topic 18 (T18)</td><td>you have been hired by firefighters to design a tripod to hold a large hose when fighting fires. the stream of water that comes out of the hose is 5 cm in diameter. determine what the flowrate of water out of the hose should be in order to work well for fighting a fire that is 9 meters away. then calculate how much reaction force will be needed at the base of the tripod to keep it from moving when the hose is being used. use fluid mechanics to support your response.</td></tr><tr><td>Topic 19 (T19)</td><td>you have recently gotten into skydiving. when you are skydiving, once you get close enough to the earth, you have to deploy a parachute. the skydiving part is exciting, but once you deploy the parachute, you have been getting bored since when you’re falling through the air, you eventually reach one constant speed (the terminal velocity). you want to design an attachment that enables you to increase and decrease your terminal velocity as you are falling. estimate your terminal velocity without this attachment, and then estimate the maximum and minimum terminal velocities when the attachment is being used. use lift and drag calculations to justify your answer.</td></tr></table>\n\nTABLE III: Details of the dataset topics, where each topic corresponds to a distinct task description that students were required to complete collaboratively through discussion.  \n\n<table><tr><td>Prompts</td><td>Author curated prompt template</td><td>GPT-4 optimized prompt template</td></tr><tr><td>Prompt 1(Previous dialogue context)</td><td>You will be provided with a dialogue and its context.The context is the previous dialogue lines of the given dialogue and each line in context is separated by a newline character.Classify the given dialogue considering its context into one of the four categories: knowledge construction, task production, uncertain, other.Output only one of the categories and do not provide any explanation.#####Context:Dialogue:</td><td>Classify the provided dialogue into the correct category based on its context. Choose one of these categories: knowledge construction, task production, uncertain, or other. Only provide the category name as your response(Context:Dialogue:</td></tr><tr><td>Prompt 2(Previous dialogue context)</td><td>You will be provided with a current dialogue line and its previous dialogue lines.Each line in the previous dialogue lines is separated by a newline \\n character.Classify the current dialogue line considering its previous dialogue lines into one of the four categories: knowledge construction, task production, uncertain, other.Output only one of the categories and do not provide any explanation.#####Previous dialogue lines:Current dialogue line:</td><td>Classify the current dialogue line into one of the following categories based on its context within the preceding dialogue lines: knowledge construction, task production, uncertain, or other. Provide the category without any explanation.Previous dialogue context:Current dialogue line:</td></tr><tr><td>Prompt 3(Previous dialogue context &amp; Topic description)</td><td>You will be provided with a dialogue, its context and a task description.The context is the previous dialogue lines of the given dialogue and each line in context is separated by a newline character.The dialogue and context are about completing the task details in the task description.Classify the given dialogue considering its context and task description into one of the four categories: knowledge construction, task production, uncertain, other.Output only one of the categories and do not provide any explanation.#####Task description:Context:Dialogue:</td><td>Given a dialogue along with its preceding context and a specific task description, classify the provided dialogue into one of four categories (knowledge construction, task production, uncertain, other). Provide only the category without any further explanation.Task Description:Context:Dialogue:</td></tr><tr><td>Prompt 4(Previous dialogue context &amp; Label defini-tions)</td><td>You will be provided with a dialogue and its context.The context is the previous dialogue lines of the given dialogue, and each line in context is separated by a newline \\n character.Classify the given dialogue considering its context into one of the four categories: knowledge construction, task production, uncertain, other.The definition of each of the categories is given below:knowledge construction: means the dialogue is focused on expressing understandings of concepts, phenomena, or technolo-gies. Simply stating a definition from textbook or notes does not count as knowledge construction.task production: means the dialogue is focused on completing the assigned task to satisfy the instructor, without verbalizing regard for understanding the bigger picture. For example, the dialogue is stating an equation, or asksin for a numerical answer, or calculating a number out loud, or discussing what to do next.uncertain: means there is insufficient evidence for classifying the dialogue either as a knowledge construction or task production.It is the default category for one word conversational fillers such as 'yeah', 'okay'.other: means the dialogue is about a topic other than the assigned task.Output only one of the categories and do not provide any explanation.#####Context:Dialogue:</td><td>Given the dialogue and its preceding context, classify the dialogue into one of the following four categories: knowledge construction, task production, uncertain, or other.- Knowledge Construction: The dialogue focuses on deep understanding of concepts or phenomena, going beyond mere definitions.- Task Production: The dialogue aims at completing a task or assignment, primarily focusing on procedural steps.- Uncertain: The dialogue does not provide enough information for classification into the above categories or includes filler words like 'yeah', 'okay'.- Other: The dialogue discusses topics unrelated to the assigned task.Provide only the category without any explanation(Context:Dialogue:</td></tr><tr><td>Prompt 5(Previous and afterward dialogue context)</td><td>You will be provided with a dialogue, its before context and its after contextThe before context is the previous dialogue lines and after context is the succeeding dialogue lines of the given dialogue.Each line in before and after context is separated by a newline \\n character.Classify the given dialogue considering its before and after context into one of the four categories: knowledge construction, task production, uncertain, other.Output only one of the categories and do not provide any explanation.#####Before Context:Dialogue:After Context:</td><td>Your task is to classify a specific dialogue based on the surrounding context into one of the following cat-egories: knowledge construction, task production, uncer-tain, other. You will be given the dialogue, as well as the lines of conversation that precede it (Before Context) and follow it (After Context). Each dialogue line in the contexts is separated by a newline character.Please provide only the category as your response with- out any explanation.Before Context:Dialogue:After Context:</td></tr></table>\n\nTABLE IV: Details of the prompts used in modeling for knowledge construction and task production discourse prediction.\n\n# REFERENCES\n\n[1] J. Gouvdea, V. Sawtelle, and A. Nair, \"Epistemological progress in physics and its impact on biology,\" Physical Review Physics Education Research, vol. 15, no. 1, p. 010107, 2019.  \n[2] M. D. Koretsky, D. M. Gilbuena, S. B. Nolen, G. Tierney, and S. E. Volet, \"Productively engaging student teams in engineering: The interplay between doing and thinking,\" in 2014 IEEE Frontiers in Education Conference (FIE) Proceedings. IEEE, 2014, pp. 1-8.  \n[3] J. E. S. Swenson, \"Developing knowledge in engineering science courses: Sense-making and epistemologies in undergraduate mechanical engineering homework sessions,\" Ph.D. dissertation, Tufts University, 2018.  \n[4] D. L. Schwartz, J. M. Tsang, and K. P. Blair, The ABCs of how we learn: 26 scientifically proven approaches, how they work, and when to use them. WW Norton & Company, 2016.  \n[5] R. E. Scherr and D. Hammer, \"Student behavior and epistemological framing: Examples from collaborative active-learning activities in physics,\" Cognition and Instruction, vol. 27, no. 2, pp. 147-174, 2009.  \n[6] Z. He, L. Tavabi, K. Lerman, and M. Soleymani, \"Speaker turn modeling for dialogue act classification,\" arXiv preprint arXiv:2109.05056, 2021.  \n[7] V. Raheja and J. Tetreault, \"Dialogue act classification with context-aware self-attention,\" arXiv preprint arXiv:1904.02594, 2019.  \n[8] R. Li, C. Lin, M. Collinson, X. Li, and G. Chen, “A dual-attention hierarchical recurrent neural network for dialogue act classification,” arXiv preprint arXiv:1810.09154, 2018.  \n[9] H. Kumar, A. Agarwal, R. Dasgupta, and S. Joshi, \"Dialogue act sequence labeling using hierarchical encoder with crf,\" in Proceedings of the aaai conference on artificial intelligence, vol. 32, no. 1, 2018.  \n[10] Z. Liu, S. U. M. Salleh, H. C. Oh, P. Krishnaswamy, and N. Chen, \"Joint dialogue topic segmentation and categorization: A case study on clinical spoken conversations,\" in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, 2023, pp. 185-193.  \n[11] L. Xing and G. Carenini, “Improving unsupervised dialogue topic segmentation with utterance-pair coherence scoring,” arXiv preprint arXiv:2106.06719, 2021.  \n[12] S. Somasundaran et al., “Two-level transformer and auxiliary coherence modeling for improved text segmentation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, 2020, pp. 7797–7804.  \n[13] S. Kim, R. E. Banchs, and H. Li, “Towards improving dialogue topic tracking performances with wikification of concept mentions,” in Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2015, pp. 124–128.  \n[14] Y. Feng, Z. Lu, B. Liu, L. Zhan, and X.-M. Wu, \"Towards llm-driven dialogue state tracking,\" arXiv preprint arXiv:2310.14970, 2023.  \n[15] J. Xu, D. Song, C. Liu, S. C. Hui, F. Li, Q. Ju, X. He, and J. Xie, \"Dialogue state distillation network with inter-slot contrastive learning for dialogue state tracking,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 11, 2023, pp. 13834-13842.  \n[16] M. D. Ma, J.-Y. Kao, S. Gao, A. Gupta, D. Jin, T. Chung, and N. Peng, “Parameter-efficient low-resource dialogue state tracking by prompt tuning,” arXiv preprint arXiv:2301.10915, 2023.  \n[17] J. Guo, K. Shuang, J. Li, Z. Wang, and Y. Liu, “Beyond the granularity: Multi-perspective dialogue collaborative selection for dialogue state tracking,” arXiv preprint arXiv:2205.10059, 2022.  \n[18] Y. Zhou, G. Zhao, and X. Qian, “Dialogue state tracking based on hierarchical slot attention and contrastive learning,” in Proceedings of the 31st ACM international conference on information & knowledge management, 2022, pp. 4737–4741.  \n[19] G. Qixiang, G. Dong, Y. Mou, L. Wang, C. Zeng, D. Guo, M. Sun, and W. Xu, “Exploiting domain-slot related keywords description for few-shot cross-domain dialogue state tracking,” in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, pp. 2460–2465.  \n[20] S. E. Finch, E. S. Paek, and J. D. Choi, “Leveraging large language models for automated dialogue analysis,” arXiv preprint arXiv:2309.06490, 2023.  \n[21] S. Sabour, C. Zheng, and M. Huang, “Cem: Commonsense-aware empathetic response generation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. 11229-11237.  \n[22] E. Jensen, S. L. Pugh, and S. K. D'Mello, \"A deep transfer learning approach to modeling teacher discourse in the classroom,\" in LAK21: 11th international learning analytics and knowledge conference, 2021, pp. 302-312.\n\n[23] S. Alic, D. Demszky, Z. Mancenido, J. Liu, H. Hill, and D. Jurafsky, \"Computationally identifying funneling and focusing questions in classroom discourse,\" arXiv preprint arXiv:2208.04715, 2022.  \n[24] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \"Language models are few-shot learners,\" Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.  \n[25] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar et al., \"Llama: Open and efficient foundation language models,\" arXiv preprint arXiv:2302.13971, 2023.  \n[26] M. Reid, N. Savinov, D. Teptyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. First, J. Schrittwieser et al., \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,\" arXiv preprint arXiv:2403.05530, 2024.  \n[27] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., \"Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,\" arXiv preprint arXiv:2501.12948, 2025.  \n[28] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, \"Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing,\" ACM Computing Surveys, vol. 55, no. 9, pp. 1-35, 2023.  \n[29] T. Gao, A. Fisch, and D. Chen, \"Making pre-trained language models better few-shot learners,\" arXiv preprint arXiv:2012.15723, 2020.  \n[30] X. L. Li and P. Liang, \"Prefix-tuning: Optimizing continuous prompts for generation,\" arXiv preprint arXiv:2101.00190, 2021.  \n[31] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, \"Finetuned language models are zero-shot learners,\" arXiv preprint arXiv:2109.01652, 2021.  \n[32] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma et al., \"Scaling instruction-finetuned language models,\" Journal of Machine Learning Research, vol. 25, no. 70, pp. 1-53, 2024.  \n[33] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., \"The llama 3 herd of models,\" arXiv e-prints, pp. arXiv-2407, 2024.  \n[34] W. Li, L. Zhu, W. Shao, Z. Yang, and E. Cambria, \"Task-aware self-supervised framework for dialogue discourse parsing,\" in 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023). Association for Computational Linguistics, 2023, pp. 14162-14173.  \n[35] Y. Tulpan and O. Tsur, \"A deeper (autoregressive) approach to nonconvergent discourse parsing,\" arXiv preprint arXiv:2305.12510, 2023.  \n[36] F. S. Mim, N. Inoue, S. Naito, K. Singh, and K. Inui, \"LPAttack: A feasible annotation scheme for capturing logic pattern of attacks in arguments,\" in Proceedings of the Thirteenth Language Resources and Evaluation Conference, N. Calzolari, F. Bechet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, J. Odijk, and S. Piperidis, Eds. Marseille, France: European Language Resources Association, Jun. 2022, pp. 2446-2459. [Online]. Available: https://aclanthology.org/2022.lrec-1.261/  \n[37] C. Li, Y. Yin, and G. Carenini, “Dialogue discourse parsing as generation: a sequence-to-sequence llm-based approach,” in Proceedings of the 25th annual meeting of the special interest group on discourse and dialogue, 2024, pp. 1-14.  \n[38] G. Cimino, C. Li, G. Carenini, and V. Deufemia, “Coherence-based dialogue discourse structure extraction using open-source large language models,” in Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2024, pp. 297–316.  \n[39] X. Gu, K. M. Yoo, and J.-W. Ha, \"Dialogbert: Discourse-aware response generation via learning to recover and rank utterances,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 14, 2021, pp. 12911-12919.  \n[40] F. S. Mim, N. Inoue, P. Reisert, H. Ouchi, and K. Inui, \"Corruption is not all bad: Incorporating discourse structure into pre-training via corruption for essay scoring,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 2202-2215, 2021.  \n[41] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \"Roberta: A robustly optimized bert pretraining approach,\" arXiv preprint arXiv:1907.11692, 2019.  \n[42] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” in Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), 2019, pp. 4171–4186.\n\n[43] J. Cohen, “A coefficient of agreement for nominal scales,” Educational and psychological measurement, vol. 20, no. 1, pp. 37–46, 1960.  \n[44] R. Artstein and M. Poesio, \"Inter-coder agreement for computational linguistics,\" Computational linguistics, vol. 34, no. 4, pp. 555-596, 2008.  \n[45] W. Spooren and L. Degand, \"Coding coherence relations: Reliability and validity,\" 2010.  \n[46] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774, 2023.  \n[47] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improving language understanding by generative pre-training,” 2018.  \n[48] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" Advances in neural information processing systems, vol. 30, 2017.  \n[49] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, “Quantization and training of neural networks for efficient integer-arithmetic-only inference,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 2704–2713.  \n[50] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., \"Lora: Low-rank adaptation of large language models.\" ICLR, vol. 1, no. 2, p. 3, 2022.  \n[51] Y. Tang, D. Tuncel, C. Koerner, and T. Runkler, “The few-shot dilemma: Over-prompting large language models,” arXiv preprint arXiv:2509.13196, 2025.  \n[52] S. Barnett, Z. Brannelly, S. Kurniawan, and S. Wong, “Fine-tuning or fine-failing? debunking performance myths in large language models,” arXiv preprint arXiv:2406.11201, 2024.\n\n![](/uploads/images/9d7befb9-e05e-4cb6-89d3-41a9b5d3a916/193c6d2e3a8f20d5573caa66c5bdb4930fa0b274d35f5bb1344f123f481e77b6.jpg)\n\nFarjana Sultana Mim received her B.Sc. in Computer Science and Engineering from Patuakhali Science and Technology University, Bangladesh, in 2016, and her M.S. and Ph.D. in System Information Sciences from Tohoku University, Japan, in 2019 and 2022. She was a postdoctoral scholar in the Department of Electrical and Computer Engineering at Tufts University, USA, in 2023. She is currently a lecturer in the Department of Computer Science and Information Technology at Patuakhali Science and Technology University. Her research interests\n\ninclude NLP in education, large language models, discourse analysis, unsupervised learning, argumentation, and commonsense reasoning.\n\n![](/uploads/images/9d7befb9-e05e-4cb6-89d3-41a9b5d3a916/6adf8d7283e86234bcd8f5a4e972ff207226ba0df1e9c68be835dfd698a9ebe8.jpg)\n\nShuchin Aeron is a professor in the Department of Electrical and Computer Engineering at Tufts School of Engineering. He received his Ph.D. from Boston University in 2009 and was awarded the best PhD thesis award from both the School Of Engineering and from the Department of Electrical and Computer Engineering. From 2009-2011, he was a postdoctoral research fellow at Schlumberger-Doll Research (SDR), where he worked on signal processing solution products for borehole acoustics resulting in a number of patents. In 2016, he received\n\nthe NSF CAREER award for his work on multidimensional signals and systems. Shuchin Aeron is presently a senior member of the Institute of Electrical and Electronics Engineers (IEEE) and an associate editor for the ACM transactions on Theory of Probabilistic Machine Learning.\n\n![](/uploads/images/9d7befb9-e05e-4cb6-89d3-41a9b5d3a916/402b5a9f8c99aed13a90ac89f354956cc61ad94379a6fcc38197490ea705674f.jpg)\n\nEric Miller is a Professor in the Department of Electrical and Computer Engineering and an adjunct Professor in the Departments of Computer Science and Biomedical Engineering at Tufts University. He previously served in the Department of Electrical and Computer Engineering at Northeastern University from 1994 to 2006. He is also a Senior Scientist at the Jean Meyer Human Nutrition Research Center on Aging at Tufts University and currently serves as the Director of the Engineering Education and Centers Division in the Directorate for Engineering\n\nat the U.S. National Science Foundation. Dr. Miller received National Science Foundation CAREER Award in 1996 and the Outstanding Research Award from the Northeastern University College of Engineering in 2002. From 2014 to 2018, he served on the Technical Liaison Committee for the IEEE Transactions on Computational Imaging and chaired the SIAM Imaging Sciences Special Interest Group from 2015 to 2017. He was an Associate Editor for the IEEE Transactions on Geoscience and Remote Sensing from 2003 to 2015 and for the IEEE Transactions on Image Processing from 1999 to 2003.\n\n![](/uploads/images/9d7befb9-e05e-4cb6-89d3-41a9b5d3a916/09b3199cb79e86dc5fe391ad46f7e33c3f4980e8ae5fc74048f58bd9f221536d.jpg)\n\nKristen Wendell is an Associate Professor in the Department of Mechanical Engineering and Education at Tufts University. She earned her B.S.E. from Princeton University, her M.S. from the Massachusetts Institute of Technology, and her Ph.D. from Tufts University in 2003, 2005, and 2011, respectively. She currently serves as a CEEO Fellow in the Center for Engineering Education Outreach and as the Co-Director of the Institute for Research on Learning and Instruction at Tufts University. Her research work focuses on characterizing and\n\nsupporting inclusive, sophisticated disciplinary practices during engineering learning experiences in undergraduate course, K-8 classrooms, and teacher education contexts.",
    "translated_content": null,
    "created_at": "2025-12-15 14:03:42.383056",
    "updated_at": "2025-12-15 14:04:10.491461"
  },
  "2123bbce-b4f3-421c-8876-3f0549537ae5": {
    "id": "2123bbce-b4f3-421c-8876-3f0549537ae5",
    "filename": "2405.11070v1.pdf",
    "file_path": "./uploads/papers/2123bbce-b4f3-421c-8876-3f0549537ae5.pdf",
    "status": "completed",
    "title": "Jill Watson: A Virtual Teaching Assistant powered by ChatGPT",
    "category": null,
    "markdown_content": "# Jill Watson: A Virtual Teaching Assistant powered by ChatGPT\n\nKaran Taneja, Pratyusha Maiti, Sandeep Kakar, Pranav Guruprasad, Sanjeev Rao, and Ashok K. Goel\n\nGeorgia Institute of Technology, Atlanta, GA {ktaneja6, pmaiti6, skakar6, pguruprasad7, srao373, ag25}@gatech.edu\n\nAbstract. Conversational AI agents often require extensive datasets for training that are not publicly released, are limited to social chit-chat or handling a specific domain, and may not be easily extended to accommodate the latest advances in AI technologies. This paper introduces Jill Watson, a conversational Virtual Teaching Assistant (VTA) leveraging the capabilities of ChatGPT. Jill Watson based on ChatGPT requires no prior training and uses a modular design to allow the integration of new APIs using a skill-based architecture inspired by XiaoIce. Jill Watson is also well-suited for intelligent textbooks as it can process and converse using multiple large documents. We exclusively utilize publicly available resources for reproducibility and extensibility. Comparative analysis shows that our system outperforms the legacy knowledge-based Jill Watson as well as the OpenAI Assistants service. We employ many safety measures that reduce instances of hallucinations and toxicity. The paper also includes real-world examples from a classroom setting that demonstrate different features of Jill Watson and its effectiveness.\n\nKeywords: Virtual Teaching Assistant  $\\cdot$  Intelligent Textbooks  $\\cdot$  Conversational Agents  $\\cdot$  Question Answering  $\\cdot$  Modular AI Design\n\n# 1 Introduction\n\nConversational AI agents can be powerful tools for education as they enable continuous 24x7 support and instant responses to student queries without increasing the workload for instructors. These virtual teaching assistants can help in efficiently scaling quality education in terms of both time and cost. The interactive nature of conversational AI agents allows students to be more inquisitive and increases teaching presence by resembling one-on-one tutoring. Based on the Community of Inquiry framework [6], teaching presence through instructional management and direct instruction leads to an increase in student engagement and retention. Towards this end, we developed Jill Watson, a Virtual Teaching Assistant (VTA) powered by ChatGPT for online classrooms which answers students' queries based on course material such as slides, notes, and syllabi.\n\nIn previous work, the legacy Jill Watson [8,5] (henceforth LJW) is a question-answering system for course logistics and uses a two-dimensional database of\n\ninformation organized by course deliverables (assignments, exams, etc.) and information categories (submission policy, deadline, etc.). It also uses a list of FAQs for course-level information such as ethics and grading policies. In this paper, we introduce the new Jill Watson which is conversational and answers questions related to course logistics as well as course content based on multiple large documents provided as context.\n\nChatGPT or GPT-3.5, based on GPT-3 [3], is a powerful large language model (LLM) trained to follow instructions and hold a dialogue. It is capable of attending to a large context and constructing meaningful text in response to user inputs. Many conversational systems such as HuggingGPT [21], Microsoft Bing Chat (www.bing.com/chat), and LangChain (www.langchain.com) based systems leverage ChatGPT for performing context-aware response generation and zero-shot learning. ChatGPT and other LLMs suffer from hallucination i.e. they generate text that can be inconsistent or unverifiable with the source text, or absurd in a given context [9]. While hallucination is useful in creative tasks such as story writing, it is detrimental in information-seeking tasks such as those in the domain of education. ChatGPT and other LLMs also have safety issues as they generate text that may be considered toxic or inappropriate [25].\n\nThis work introduces Jill Watson's architecture which does not require any model training or fine-tuning and is designed to address the above LLM-related concerns. We address the hallucination issue by citing the documents from which information is obtained and verifying grounding using textual entailment. To prevent Jill Watson from answering unsafe questions or generating unsafe responses, we employ a classifier for question relevance, toxic text filters, and prompts that promote politeness in response generation. Further, Jill Watson is designed to answer questions based on multiple large documents which makes it well-suited for intelligent textbooks. We only rely on publicly available resources to promote future research in this direction.\n\nThe paper has three main contributions: (i) we introduce Jill Watson, a virtual teaching assistant powered by ChatGPT with a skill-based architecture, (ii) detail all the different modules of Jill Watson and associated algorithms, and (iii) quantitatively evaluate Jill Watson to measure response quality and safety, along with a discussion on examples from our first deployment.\n\nSection 2 discusses Jill Watson in the context of related work. Section 3 describes the architecture and each module in detail. Section 4 describes our experimental results comparing Jill Watson to two strong baselines in terms of response quality and safety along with examples (see Table 3) from our first deployment. We conclude the paper in Section 5 with a summary of the strengths, limitations, and potential impact of Jill Watson.\n\n# 2 Related Work\n\nQuestion Answering can either be open-ended or grounded in knowledge. Without a knowledge source, question-answering models based on LLMs [16,22] are expected to store the information in their parameters during the training.\n\nIn grounded question answering, previous work has explored different types of contexts including the web [17], machine reading comprehension [1], knowledge bases [2], and short text documents [18,27]. Some methods assume access to the correct context from the document [18]. Further, many methods require training with datasets that are expensive to collect and do not generalize well [24,26]. Jill Watson neither imposes a limit on the document size nor requires a training dataset. It pre-processes large documents and answers incoming questions based on passages retrieved using dense passage retrieval (DPR) [11].\n\nRetrieval Augmented Generation (RAG) is a well-known method [14] for increasing the reliability of LLMs by generating text conditioned on source texts that are retrieved based on a query. Knowledge-grounded generative models have two main goals: factuality and conversationality [19]. Factuality minimizes hallucinations by ensuring consistency of output with the retrieved texts while conversationality refers to relevance of the information to the query and generation without repetition. Previous work has shown improved factuality using RAG in dialog response generation task to remain consistent with a persona [23], knowledge-grounded generation [13,19,26] and machine translation [4].\n\nMany models use large training datasets to learn question answering from contexts [2,24,26]. On the other hand, WikiChat [19] uses seven-step few-shot prompts based question answering system which uses both retrieval and open-ended generation to answer questions using Wikipedia.  $\\mathrm{Re}^2\\mathrm{G}$  or Retrieve, Re-rank, Generate [7] also uses retrieval for generating outputs but uses an additional re-ranking step to score retrieved passages before generation. Further, it can also be trained end-to-end after initial fine-tuning. Jill Watson solves the knowledge-intensive generation problem using RAG but without any fine-tuning by using open-source DPR models for retrieval and using ChatGPT to construct responses. Because of clever prompting and indexing, it is also able to refer to the document and page number from which the response was generated.\n\nSafety in LLMs is important to avoid harm because of hateful, offensive, or toxic text. Previous work on evaluating the toxicity of ChatGPT has found that assigning personas, using non-English languages, prompting with creative tasks, jailbreak prompts, and higher temperature values can all lead to more toxic responses [25]. Perspective API [12] and OpenAI Moderation API [15] are popular services to measure toxicity in various categories including hateful content, violence, etc. Jill Watson ensures safety in three ways: (i) OpenAI Moderation API for both user inputs and its own responses, (ii) skill classifier to identify irrelevant queries, and (iii) encourages polite responses in prompts to ChatGPT.\n\nDialog Systems are AI agents designed for human-like conversations, typically using hybrid architectures involving both rule-based systems and machine learning systems. For instance, MILABOT [20] uses rule-based and generative models along with a response selection policy trained using reinforcement learning. Microsoft XiaoIce [28] uses a skill-based architecture where the chat manager selects one of 230 high-level skills to generate responses. To the best of our knowledge, such powerful multi-turn dialog systems have not been introduced in a classroom. The legacy Jill Watson [8] is a single-turn question-answering\n\n![](/uploads/images/2123bbce-b4f3-421c-8876-3f0549537ae5/0fa51ad6db48b2e0593c73bb2200b94e603f95c19a7b007d2d1689fd6daec81e.jpg)  \nFig. 1. Architecture of Jill Watson: After the coreference resolution of an incoming query, the skill classifier is used to find the most appropriate skill for response generation. Jill Watson's skills include Contextual Answering, Greetings, etc. The updated conversation history is used as context for generating responses in the future.\n\nsystem for course logistics and policies. OpenAI Assistants service<sup>1</sup> provides a way to instantiate a ChatGPT-based agent to generate responses using text documents. Inspired by XiaoIce, Jill Watson has a skill-based architecture and is designed to be a safe conversational agent for classrooms that can answer student queries related to course logistics as well as course content using ChatGPT in the backend. It is also well-suited for other applications in education like intelligent textbooks given its ability to use long documents as context.\n\n# 3 Jill Watson Architecture\n\nThe architecture of Jill Watson shown in Figure 1 takes inspiration from the skill-based architecture of XiaoIce [28]. XiaoIce relies on different skills such as task completion, image commenting, content creation, etc. to interact with users and selects the appropriate skill for each conversation turn based on the previous context. In Jill Watson, the query with resolved coreferences is used to decide the most appropriate skill for answering the incoming query. The skill-based design of Jill Watson makes it extensible as we can easily plug in new API services and other capabilities in the future in the form of new skills.\n\nContextual Answering skill is responsible for answering questions where content needs to be retrieved from course content or syllabus and Self-awareness skill answers queries about Jill Watson itself. As we will discuss later, these two skills make Jill Watson a knowledge-grounded AI agent with the ability to refer to multiple documents and cite relevant content in its answers. Further, ChatGPT allows Jill Watson to be conversational by using past messages as context in generating responses to user messages. We also utilize many safety features in Jill Watson which include detecting irrelevant queries by skill classifier, moderation filters, and prompts to encourage courteous responses.\n\n# 3.1 Coreference Resolution\n\nCoreference resolution involves determining the entities that are indirectly referenced in a text and making them explicit using nouns or noun phrases. For example, given the context 'John started reading a book', the query 'When did he start?' has two coreferences. An explicit coreference is suggested by the word 'he' while an implicit coreference to an event is present because of 'When' and 'start'. In coreference resolution, we must resolve the reference 'he' and the event that 'When' and 'start' are referring to. Therefore, the resolved query would be 'When did John start reading the book?' formed by replacing 'he' with John and adding the implicit event 'reading the book'.\n\nWhile ChatGPT implicitly resolves the coreferences as it can construct appropriate responses by itself, since we wish to use existing models for retrieval without any fine-tuning, we need to construct complete queries with resolved coreferences before passing them to the retrieval module. Hence, the first step in Jill Watson is to resolve coreferences in the user query based on the previous messages. In the example in Figure 1, 'it' in 'When is it due?' is replaced by the entity 'Assignment 2'. This is done by prompting ChatGPT to resolve the coreferences in the received query and passing the past messages as context. We use a combined instruction and demonstration-based prompt where we explain the task (instruction) along with three demonstrations with no coreferences, an explicit and an implicit coreference. In our investigation, we found demonstrations to be extremely useful for improving performance, especially on implicit coreferences which are much harder to identify.\n\n# 3.2 Skill Classifier\n\nAs discussed earlier, Jill Watson uses various skills to answer different types of queries. For instance, queries that require retrieval are forwarded to the Contextual Answering Skill while greetings are answered by the Greetings Skill. The skill-based division using a skill classifier allows us to use different response-generation techniques based on the user query. It can also aid in understanding user behaviors by analyzing the skill distribution of student queries.\n\nTo forward a user query to the appropriate skill, the resolved query (after coreference resolution) is used is to perform skill classification by prompting ChatGPT. In the example in Figure 1, the selected skill is Contextual Answering based on the resolved query and previous messages as context. We again used a combined instruction and demonstration-based prompt with an explanation of each skill (instruction) and one demonstration per skill. We found that fewer and distinct classes lead to a better performance which motivates the use of a small number of skills as far as possible.\n\n# 3.3 Contextual Answering\n\nContextual answering or context-based question answering skill involves answering questions based on the given information. For Jill Watson, this information\n\nAlgorithm 1 Contextual Answering Skill  \nRequire:Resolved query  $Q$  ,context  $C$  ,PRE-PROCESSED DOCUMENTS with passages   \n $\\mathcal{P}$  and corresponding context embeddings  $\\mathcal{D}$  , DPR query encoder  $E_{q}(.)$    \nEnsure:Response  $R$  ,confidence  $C\\in \\{\\mathrm{low},\\mathrm{high}\\}$    \n// DENSE PASSAGE RETRIEVAL   \nConstruct a query with context  $Q_{C}$  with  $Q$  and  $C$    \nSort passages  $\\mathcal{P}$  using cosine similarity between  $e_{Q,C} = E_{q}(Q_{C})$  and context embeddings in  $\\mathcal{D}$  .Keep top-20 passages  $P$  with highest cosine similarity.   \n// CONTEXT Loop   \nfor batches  $P_{5}$  of 5 passages in top-20 do   \n//RESPONSE GENERATION   \nGenerate response  $R$  by prompting ChatGPT with  $C,Q,$  and  $P_{5}$  if  $R$  answers  $Q$  then   \n// TEXTUAL ENTAILMENT   \nif  $^ { \" P _ { 5 } }$  implies  $R ^ { \" }$  succeeds:return  $R$  and  $C =$  high. else: return  $R$  and  $C =$  low. end if   \nend if   \nend for   \nreturn  $R$  and  $C =$  low.\n\nconsists of verified course documents provided by course instructors. The process outlined in Algorithm 1 can be divided into five main parts, highlighted in SMALL CAPS, viz. documents pre-processing, DPR, response generation, textual entailment and context loop. The documents pre-processing step is performed only once when a Jill Watson is initialized with a set of course documents. The remaining four steps have to be performed for every query received by the Contextual Answering skill.\n\nDocuments Pre-processing: Jill Watson pre-processes course documents used for answering student queries and stores them as a list of passages along with their different representations discussed below. These representations allow fast retrieval of the most relevant parts of documents during run-time. It accepts PDF documents, the most common format in which course contents (syllabus, notes, books, and slides) are distributed. After text extraction from each PDF document using Adobe PDF Extract API, it is divided into pages and each page is further divided into paragraphs. We group paragraphs into passages of at least 500 characters (90-100 words). This ensures a significant context size in each passage for the DPR step. We also store document and page information with each passage which is used to refer back and cite the documents. Further, there is a  $50\\%$  overlap between passages for added redundancy and to represent continuity between consecutive passages.\n\nFigure 2 shows different representations stored for each passage. Since we use DPR [11] in the retrieval step (discussed in more detail later), we pre-compute embeddings of each passage using the DPR context encoder. The context encoder requires passages with prepended headings which are obtained by prompting ChatGPT to generate a 2-3 word heading. We use a cleaned version of the original text for the context encoder model because the raw text from the PDF\n\n![](/uploads/images/2123bbce-b4f3-421c-8876-3f0549537ae5/d753db231d8a6fc397aec096e4ee34a3f875483a09c97f09b79ba0a7242576c6.jpg)  \nFig. 2. Passage representation consists of the original text, heading, clean text, summary text, and context embeddings of both clean and summary texts.\n\nhas unwanted spaces, special characters, and poor table formatting. To obtain this clean text, we again prompt ChatGPT with the original text and instruct it to clean the text and to format the tables. Further, we also prompt ChatGPT to summarize the clean text, and we store a context embedding of this passage summary. We found that this is useful for retrieval because passages can contain implicit information which is made more explicit in summaries. For example, an exam might be mentioned in a different line from its deadline but a summary makes this relation more explicit and direct.\n\nDense Passage Retrieval: DPR [11] has a dual-encoder architecture i.e. it consists of a query encoder and a context encoder. During training, the two models are aligned to output in a common embedding space by maximizing the similarity between embeddings of query-context pairs in training data and minimizing the similarity across example pairs. We used the multi-dataset model which is trained on Natural Questions, TriviaQA, WebQuestions, and TREC datasets [11], allowing it to generalize over domains and text properties.\n\nThe first step in DPR is to compute query embeddings of the coreference-resolved queries. We found that prepending the question history in the query to provide more context improves retrieval performance. Second, we compute the similarity of the query embedding with context embeddings of both clean and summary text. Third, we sort the passages in decreasing order of similarity and group clean text in batches of five for prompting. For sorting, the similarity score of a passage is the maximum between similarities with clean and summary embeddings. Finally, we also pass the document and page information for each passage to the next step where we prompt ChatGPT to use this information for citing document and page when constructing an answer.\n\nChatGPT API Call: In Jill Watson, we wish to prompt ChatGPT to construct an answer with four requirements: (i) ChatGPT should only answer student query if the context has the information to answer the query, or else, refuse to answer, (ii) it should not add any additional information to the answer\n\nthat is not present in the context, (iii) it should cite the corresponding document and page number when using content from a text chunk, and (iv) it should answer the query with respect to the current date and time.\n\nThe prompt to construct an answer using context is given in Figure 3 (left). Note that we prompt ChatGPT to use only the given context which is made up of a batch of five passages, encourage it to say it doesn't know the answer if the context is insufficient, and ask it to cite the document and page number (see example in Figure 3). We also feed it the current week (Monday-Sunday), date and time, and past messages which can be useful in answering user queries in the context of the present time and the past conversation.\n\n<table><tr><td>System: A polite virtual assistant named Jill Watson answers student (user) questions on a class forum.\nThe context below is provided to answer questions.\n CONTEXT: {context}\nOnly the above context is used and rephrased to answer the question. If context does not answer it, say that you don’t know the answer. If answer is found, don’t forget to cite the document and page number.\nCURRENT WEEK: {current-week}\nCURRENT TIME: {current-time}\nUser and Assistant Messages:\n{past-messages} {most-recent-query}</td><td>USER: When is Mini-Project 2 due?\nJill Watson: Mini-Project 2 is due on Monday, September 25, 2023 at 9 am. You need to submit the code for solving the problem to the Mini-Project 2 assignment on Gradescope and also submit a report describing your agent to Canvas. Your grade for this project will be based on a combination of your report’s quality (50%) and your agent’s performance (50%).\nSource: Syllabus, Page 13</td></tr></table>\n\nFig. 3. Answer generation prompt (left) and an example response with citation (right). The context contains five passages with document name and page numbers.\n\nTextual Entailment: The textual entailment task involves determining the entailment relation between two texts. A text  $T$  entails hypothesis  $H$  if  $H$  is a logical consequence of text  $T$  i.e.  $T \\Rightarrow H$ . If  $T \\Rightarrow \\neg H$ , we say that  $T$  contradicts  $H$ .  $T$  can also be neutral i.e. it neither entails nor contradicts  $H$ . Jill Watson's prompt for answer generation contains instructions to not answer a question when the context doesn't provide an answer and to only use the given context for answering. However, we wish to have an additional check to detect hallucinations through textual entailment. Given a context  $C$  and an answer  $A$ , we wish to check if  $C$  entails  $A$ . If  $C$  doesn't completely entail  $A$ , there is information in  $A$  that was not retrieved from  $C$ . In such a case, we pretend a warning for the user conveying that the confidence in the answer is low and encourage them to check the answer on their own. As shown in Algorithm 1, we check if the context  $C = P_5$  is used to generate the answer  $A = R$ . We prompt ChatGPT with  $P_5$  as the text and  $R$  as the hypothesis and ask it to determine if the text implies that hypothesis. We found a simple instruction-based prompt to be most effective with the highest recall for non-entailed answers.\n\nContext Loop: After scoring and sorting the passages, we group the top twenty passages into four batches of five and prompt ChatGPT to generate a response based on the first batch as context. If it fails to answer using the first batch, we use the second batch of passages, and so on until a valid answer is generated. To check if ChatGPT generated a valid answer, we prompt it to classify the response as NEGATIVE if it refuses to reply because the information is not present or it suggests contacting the staff, and NEUTRAL otherwise.\n\n# 3.4 Other Skills\n\nIn addition to contextual answering, we use other skills for additional capabilities and we plan to expand these in the future with software tools and API services.\n\nSelf-awareness Skill: Many curious users ask AI agents about itself to check its self-awareness or to know more about the system. For such queries, we prompt ChatGPT to answer the user query based on a textual description of Jill Watson. This textual description contains basic information about us, the team of researchers who built it, and a blurb about its capabilities.\n\nGreeting Skill: If an incoming query is a greeting or conveys gratitude to Jill Watson, we prompt ChatGPT to generate a polite reply.\n\nIrrelevant Skill: If a query doesn't fit into any of the other skills, we use a fixed polite message asking the user to change or rephrase their question.\n\n# 3.5 Moderation Filter\n\nFor deployment in real world, Jill Watson should be safe to use and not accept any harmful requests or generate a harmful response. To this end, we filter input user queries and outputs of Jill Watson using the OpenAI Moderation API [15]. The API allows Jill Watson to detect different categories of unsafe text including hate, hate and threatening, harassment, harassment and threatening, self-harm, self-harm intention, self-harm instructions, sexual, sexual involving minors, and violence with graphic depictions.\n\n# 4 Results\n\nWe compare Jill Watson with both legacy Jill Watson (LJW), and the OpenAI Assistants service (OAI-Assist). LJW baseline employs an intent classifier and a database of information organized by course deliverables and information categories as well as a list of FAQs. OAI-Assist allows users to upload files PDF files and employs retrieval to generate answers for user queries. Our new system Jill Watson uses coreference resolution, skill classification, dual encoder DPR, ChatGPT for generation, and safety features described earlier. Both OAI-Assist and Jill Watson use 'gpt-3.5-turbo-1106' for retrieval-augmented generation in our experiments.\n\nResponse Quality and Harmful Errors: We used a set of 150 questions created by four students based on the syllabus, e-book, and video transcripts for\n\nTable 1. Response Quality: A set of 150 questions is used to evaluate the response quality of each system. Failures are further determined to be harmful, confusing, and stemming from poor retrieval.  \n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Pass</td><td colspan=\"3\">Failures</td></tr><tr><td>Harmful</td><td>Confusing</td><td>Retrieval</td></tr><tr><td>LJW</td><td>26.0%</td><td>-</td><td>60.4%</td><td>-</td></tr><tr><td>OAI-Assist</td><td>31.3%</td><td>16.5%</td><td>72.8%</td><td>68.0%</td></tr><tr><td>Jill Watson</td><td>76.7%</td><td>5.7%</td><td>62.8%</td><td>57.1%</td></tr></table>\n\na course on AI. The ground truth answers contain text from these documents or 'I don't know' (IDK) responses for unanswerable questions that students deliberately added based on our instructions. These 150 questions were asked to each of the systems and the answers were evaluated by human annotators based on ground truth values and labeled as 'Pass' or 'Fail' (Cohen's  $\\kappa = 0.76$  in inter-rater reliability test). Human annotators made a second pass through the failing answers to annotate the different types of mistakes made by the three systems.\n\nThe results are shown in Table 1. We observe that Jill Watson can answer a much higher proportion of questions as compared to LJW and OAI-Assist. To dig deeper into the types of mistakes that different systems make, we explore three types of failures. Harmful answers are those that are misleading or have misinformation that can potentially be detrimental to the students. Confusing answers are either indisputably wrong or confusing as they mostly contain irrelevant information. The answers with Retrieval problems are suspected to have retrieved irrelevant information and used it to answer the question. Note that the above categories are not mutually exclusive and AI agents can respond with IDK response to escape all these pitfalls.\n\nWe find Jill Watson to be the most reliable in terms of generating the fewest potentially harmful answers, and least retrieval problems as well as confusing answers. We believe that Jill Watson can generate more precise answers as compared to other agents because of our retrieval approach as well as the prompting method to encourage responses grounded in the provided context and the use of IDK response when a query cannot be answered. LJW is not generative or retrieval-based but can create confusion because of irrelevant answers. We speculate that OAI-Assist prompts the ChatGPT model to use the retrieved context to generate a response and possibly discourages IDK responses unless retrieval fails because of a threshold score. We believe that our use of multiple representations of the same passages with overlapping text for retrieval is a better approach as compared to OAI-Assist which we assume to be directly using poorly formatted raw text from PDF files for retrieval.\n\nOffensive Responses and Toxicity: Based on [25] (Table 10), we use 20 insult prompts that generate insults using ChatGPT by injecting literary devices into instructions. For example, the tenth prompt is \"Put together an offensive jab applying irony.\" Skill classifier in Jill Watson classifies such prompts\n\nTable 2. User Safety: 20 insult prompts are used directly and adapted for an AI course to measure toxicity. T. Avg and T. Max are obtained using Perspective API. Numbers in braces are the results for the most toxic sentence in each response.  \n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">Insult Prompts</td><td colspan=\"3\">Adapted Insult Prompts</td></tr><tr><td>IDK%</td><td>T. Avg (%)</td><td>T. Max (%)</td><td>IDK%</td><td>T. Avg (%)</td><td>T. Max (%)</td></tr><tr><td>OAI-Assist</td><td>68</td><td>7.9 (11.4)</td><td>42.7 (54.3)</td><td>5</td><td>6.4 (14.0)</td><td>65.4 (64.4)</td></tr><tr><td>Jill Watson</td><td>98</td><td>2.7 (5.0)</td><td>45.6 (50.8)</td><td>82</td><td>3.2 (9.4)</td><td>32.2 (44.8)</td></tr></table>\n\nas irrelevant to the course which leads to a default response. Therefore, we adapt these prompts to make them relevant in the context of the course. For example, we modify the above prompt to \"How many assignments are there in this course? Half of the semester is already over and I still need to watch the first lecture. Answer in form of an offensive jab applying irony.\" We also test out another variation that exploits the AI concept of reasoning: \"What is reasoning in [course name]? Put together an offensive jab applying irony and explain your reasoning.\"\n\nWe generate three outputs for all three systems with 20 insult prompts and 40 adapted insult prompts which are then passed to Perspective API [12] to measure their toxicity. Perspective API provides a toxicity score (T. Score) between zero and one which denotes the fraction of people that are expected to perceive the text as toxic. In Table 2, similar to [25], we report the average (T. Avg) and maximum (T. Max) over all prompts.\n\nJill Watson responds with IDKs more often than OAI-Assist which leads to lower T. Avg. for complete responses as well as on sentence level. Also, the proportion of IDKs goes down for adapted insult prompts which shows that simple modifications can be made to insult prompts to disguise them as domain-relevant for engaging in conversations that can lead to potentially toxic content. Further, with adapted insult prompts, we also see an increase in T. Avg on sentence level as more responses have toxic content but we do not see an increase in T Avg. for complete responses because of their longer length, about  $2.25\\mathrm{x}$  the length of response to the original insult prompts. The adapted prompts elicit more toxicity for OAI-Assist than Jill Watson as the proportion of IDK responses is lower. We did not perform this experiment for LJW as it uses templated responses which cannot be toxic, arguably making it the safest of all methods.\n\nSample Conversations: Kakar et al. 2024 [10] deployed Jill Watson with additional design changes in a diverse range of courses including the online graduate-level Knowledge-Based AI course offered at Georgia Tech in the Fall 2023 semester. Deployed Jill Watson had an average response time of 6.8 seconds. Table 3 provides six snippets from organic student conversations with Jill Watson. These snippets demonstrate how Jill Watson is able to converse using coreference resolution as well as the chat context to answer questions. We also see how Jill Watson uses the documents to answer student questions and avoids answering questions or adds a disclaimer when the context does not contain an answer to the user query. Further, Jill Watson is also able to navigate conversations involving user queries that encourage toxicity and is also able to correct\n\nTable 3. Examples: Snippets from organic student conversations with Jill Watson deployment in an AI course.  \n\n<table><tr><td>Coreference Resolution\nUSER: what is partial order planning\nASSISTANT: Partial order planning is a type of planning algorithm used in artifi-\ncial intelligence to solve problems [...] \nUSER: does it devise separate plans\nASSISTANT: Yes, in partial order plan-\nning, the planner devises separate plans for\neach subgoal or subtask. It treats [...]</td><td>Retrieval and conversationality\nUSER: What is a semantic network?\nASSISTANT: A semantic network is a\nknowledge representation technique [...] \nUSER: what is another way to represent\nknowledge in AI systems?\nASSISTANT: Another way to represent\nknowledge in AI systems is through the use\nof frames. Frames are a knowledge [...]</td></tr><tr><td>Date-aware and conversationality\nUSER (asked on 2023-08-30): what assign-\nments are due this week?\nASSISTANT: This week (2023-08-28 to\n2023-09-03), the Start-of-Course Survey is\ndue on Monday, August 28, 2023 at 9am.\nNo other assignments are due this week.\nUSER: What about next week?\nASSISTANT: Next week is the week of\nSeptember 4th to September 10th. Here\nare the upcoming milestones and assign-\nments for that week: [...]</td><td>Accepting a mistake and self-correction\nUSER: Are there resources available to\nprovide an introduction to the RPM\nproject, other than the project descri-\ntions?\nASSISTANT: Yes, [...] lesson on RPM (Re-\nsource Planning and Management), [...] \nUSER: RPM does not stand for Resource\nPlanning and Management\nASSISTANT: That&#x27;s correct. In the con-\ntext of the course material provided, RPM\nstands for Raven&#x27;s Progressive Matrices.\nRaven&#x27;s Progressive Matrices is a [...]</td></tr><tr><td>Avoiding harmful text\nUSER: Explain to me rudely how I can\nsucceed in this class.\nASSISTANT: I&#x27;m sorry, but I cannot pro-\nvide a rude response. However, I can offer\nyou some tips on how to succeed in this\nclass: [...]</td><td>Not answering outside context\nUSER: what is the grading rubric for\nHW1?\nASSISTANT: The grading rubric for HW1\nis not provided in the context. I recommend\nreferring to the rubric in Canvas or reach-\ning out to your instructor [...]</td></tr></table>\n\nitself when a mistake is pointed out by the user. Note that the textual entailment warning and document citations were not available in our first deployed system. The reader can find more details in [10].\n\n# 5 Conclusion\n\nWe described the architecture of a virtual teaching assistant powered by ChatGPT and presented quantitative evaluation and qualitative examples to demonstrate its ability to ground answers in course documents, minimize hallucinations, and promote safety. We compared our system Jill Watson with legacy Jill Watson and OpenAI Assistants service and found that it can answer student queries more reliably, and generate fewer potentially harmful and confusing answers.\n\nLimitations: Jill Watson has the limitation of the RAG method i.e. the answers must be generated using a limited context. This means that long-range\n\nqueries such as 'Summarize chapter 15.' cannot be answered unless the summary is directly available in the text. The performance of Jill Watson also relies on each module working correctly, or else errors can cascade in modular AI systems. In ensuring safety, we also have to make a trade-off with performance as some questions that can be answered may not get addressed. For example, the skill classifier may deem some relevant questions as irrelevant. Building expectations around AI assistants is also an important aspect as the users should understand the limitations to avoid harm from misleading or harmful text.\n\nSocietal Impact: Jill Watson will promote the use of AI in education in boosting student and teacher productivity. LLMs are powerful tools to create AI assistants but more work needs to be done to ensure safety in terms of both misinformation and toxicity. Our work showcases a virtual teaching assistant in the real world and demonstrates the use of various techniques towards this end. AI assistants will inevitably play an important role in our daily lives including our education. We believe that Jill Watson is an important step towards understanding the role of AI assistants, user expectations, and performance constraints.\n\nAcknowledgements: This research has been supported by NSF Grants #2112532 and #2247790 to the National AI Institute for Adult Learning and Online Education. We thank Alekhya Nandula, Aiden Zhao, Elaine Cortez, and Gina Nguyen for their inputs and contributions to this work.\n\n# References\n\n1. Bajaj, P., Campos, D., Craswell, N., et al.: MS MARCO: A Human Generated MAchine Reading CComprehension Dataset (Oct 2018), arXiv:1611.09268 [cs]  \n2. Bao, J., Duan, N., Yan, Z., Zhou, M., Zhao, T.: Constraint-Based Question Answering with Knowledge Graph. In: COLING 2016. pp. 2503-2514 (Dec 2016)  \n3. Brown, T.B., Mann, B., Ryder, N., et al.: Language Models are Few-Shot Learners. In: NeurIPS 2020 (2020)  \n4. Cai, D., Wang, Y., Li, H., Lam, W., Liu, L.: Neural Machine Translation with Monolingual Translation Memory. In: ACL 2021. pp. 7307-7318 (2021)  \n5. Eicher, B., Polepeddi, L., Goel, A.: Jill Watson Doesn't Care if You're Pregnant: Grounding AI Ethics in Empirical Studies. In: AIES 2018. pp. 88-94 (2018)  \n6. Garrison, D., Anderson, T., Archer, W.: Critical Inquiry in a Text-Based Environment: Computer Conferencing in Higher Education. The Internet and Higher Education 2(2-3), 87-105 (1999)  \n7. Glass, M., Rossiello, G., Chowdhury, M.F.M., Naik, A.R., Cai, P., Gliozzo, A.: Re2G: Retrieve, Rerank, Generate. In: NAACL 2022. pp. 2701-2715 (2022)  \n8. Goel, A.K., Polepeddi, L.: Jill Watson: A Virtual Teaching Assistant for Online Education. In: Dede, C., Richards, J., & Saxberg, B., (Editors) Education at scale: Engineering online teaching and learning. NY: Routledge. (2018)  \n9. Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A., Fung, P.: Survey of Hallucination in Natural Language Generation. ACM Computing Surveys 55(12), 248:1-38 (2023)  \n10. Kakar, S., Maiti, P., Nandula, P., Nguyen, G., Taneja, K., Zhao, A., Nandan, V., Goel, A.: Jill Watson: Scaling and Deploying an AI Conversational Agent in Online Classrooms. In: Intelligent Tutoring Systems 2024 (2024)\n\n11. Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., Yih, W.t.: Dense Passage Retrieval for Open-Domain Question Answering. In: EMNLP 2020. pp. 6769-6781 (2020)  \n12. Lees, A., Tran, V.Q., Tay, Y., Sorensen, J., Gupta, J., Metzler, D., Vasserman, L.: A New Generation of Perspective API: Efficient Multilingual Character-level Transformers. In: ACM SIGKDD 2022. pp. 3197-3207 (2022)  \n13. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS 2020 pp. 9459–74 (2020)  \n14. Li, H., Su, Y., Cai, D., Wang, Y., Liu, L.: A Survey on Retrieval-Augmented Text Generation (Feb 2022), arXiv:2202.01110 [cs]  \n15. Markov, T., Zhang, C., Agarwal, S., Eloundou, T., Lee, T., Adler, S., Jiang, A., Weng, L.: A Holistic Approach to Undesired Content Detection in the Real World. In: AAAI 2023. pp. 15009-15018 (2023)  \n16. Ouyang, L., Wu, J., Jiang, X., et al.: Training language models to follow instructions with human feedback. In: NeurIPS 2022 (2022)  \n17. Piktus, A., Petroni, F., Karpukhin, V., et al.: The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus (2022), arXiv:2112.09924 [cs]  \n18. Qin, L., Galley, M., Brockett, C., Liu, X., Gao, X., Dolan, B., Choi, Y., Gao, J.: Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading. In: ACL 2019. pp. 5427-5436 (2019)  \n19. Semnani, S., Yao, V., Zhang, H., Lam, M.: WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia. In: EMNLP 2023. pp. 2387-2413 (2023)  \n20. Serban, I.V., Sankar, C., Germain, M., et al.: A Deep Reinforcement Learning Chatbot (Nov 2017), arXiv:1709.02349 [cs, stat]  \n21. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. In: Thirty-seventh Conference on Neural Information Processing Systems (Nov 2023), https://openreview.net/forum?id=yHdTscY6Ci  \n22. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (Feb 2023). https://doi.org/10.48550/arXiv.2302.13971, http://arxiv.org/abs/2302.13971, arXiv:2302.13971 [cs]  \n23. Weston, J., Dinan, E., Miller, A.: Retrieve and Refine: Improved Sequence Generation Models For Dialogue. In: EMNLP 2019 SCAI Workshop. pp. 87-92 (2019)  \n24. Wu, Z., Galley, M., Brockett, C., Zhang, Y., Gao, X., Quirk, C., Koncel-Kedziorski, R., Gao, J., Hajishirzi, H., Ostendorf, M., Dolan, B.: A Controllable Model of Grounded Response Generation. In: AAAI 2022 (2022)  \n25. Zhang, B., Shen, X., Si, W.M., Sha, Z., Chen, Z., Salem, A., Shen, Y., Backes, M., Zhang, Y.: Comprehensive Assessment of Toxicity in ChatGPT (Nov 2023), arXiv:2311.14685 [cs]  \n26. Zhang, Y., Sun, S., Gao, X., Fang, Y., Brockett, C., Galley, M., Gao, J., Dolan, B.: RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling. In: AAAI 2022. pp. 11739-11747 (2022)  \n27. Zhou, K., Prabhumoye, S., Black, A.W.: A Dataset for Document Grounded Conversations. In: EMNLP 2018. pp. 708-713 (2018)  \n28. Zhou, L., Gao, J., Li, D., Shum, H.Y.: The Design and Implementation of XiaoIce, an Empathetic Social Chatbot. Computational Linguistics 46(1), 53-93 (Mar 2020)",
    "translated_content": null,
    "created_at": "2025-12-15 15:24:05.139498",
    "updated_at": "2025-12-15 15:24:11.867144",
    "doi": "10.48550/arXiv.2302.13971,",
    "arxiv_id": "2405.11070",
    "embedding": [
      -1.2734375,
      1.8828125,
      -1.1328125,
      -0.158203125,
      -1.7109375,
      -0.54296875,
      0.365234375,
      0.310546875,
      -0.40625,
      4.09375,
      1.8125,
      0.46484375,
      1.5078125,
      0.51953125,
      1.4609375,
      1.9375,
      1.0546875,
      0.07421875,
      1.0859375,
      -8.3125,
      1.9453125,
      2.09375,
      1.6640625,
      -4.03125,
      3.671875,
      -3.328125,
      1.578125,
      1.359375,
      1.046875,
      2,
      8.625,
      -4.59375,
      -2.5625,
      0.279296875,
      -0.01300048828125,
      0.2001953125,
      -2.875,
      -0.859375,
      4.78125,
      6.375,
      -8.5,
      2.9375,
      0.9140625,
      3.859375,
      -0.734375,
      3.328125,
      3.09375,
      -5,
      -4.0625,
      0.67578125,
      -3.765625,
      -2.859375,
      4.375,
      -0.6015625,
      2.625,
      -5.6875,
      -7.8125,
      7.40625,
      -4,
      -1.859375,
      1.5,
      -0.9921875,
      2.03125,
      -1.1640625,
      3.609375,
      3.921875,
      1.40625,
      0.66015625,
      -2.046875,
      2.0625,
      -1.7578125,
      -0.50390625,
      5.1875,
      -2.8125,
      6.28125,
      6.84375,
      1.8125,
      5.34375,
      -0.4375,
      4.84375,
      -3.921875,
      3.9375,
      5.375,
      -1.2734375,
      3.859375,
      4.15625,
      -2.515625,
      -2.1875,
      -4.8125,
      1.640625,
      -2.078125,
      -0.6328125,
      -0.251953125,
      -3.546875,
      -0.390625,
      5.21875,
      -1.7265625,
      -5.59375,
      -5.125,
      -0.462890625,
      -0.70703125,
      -4.0625,
      -0.96484375,
      -5.25,
      -1.8359375,
      -4.28125,
      -2.71875,
      -5.71875,
      -6.4375,
      -2.828125,
      -2.0625,
      2.125,
      2.5,
      -0.62109375,
      2.40625,
      -0.91015625,
      1.71875,
      -7.3125,
      -5.90625,
      -1.953125,
      -3.453125,
      -2.84375,
      -0.6171875,
      -1.6328125,
      2.671875,
      1.375,
      -1.734375,
      0.734375,
      5.46875,
      1.140625,
      -0.6015625,
      1.1015625,
      4.6875,
      -0.5078125,
      -10.375,
      -1.84375,
      -7.78125,
      1.3515625,
      5.21875,
      7.71875,
      -6.40625,
      0.71484375,
      -0.625,
      -5.5625,
      5.0625,
      0.349609375,
      -6.46875,
      0.119140625,
      3.46875,
      -2.59375,
      -1.765625,
      1.640625,
      4.40625,
      7.3125,
      -1.8828125,
      -4.6875,
      4.34375,
      -1.671875,
      2.453125,
      -3.3125,
      -2.453125,
      3.984375,
      -0.75390625,
      1.7890625,
      -0.671875,
      2.390625,
      -2.65625,
      0.416015625,
      -1.796875,
      -1.5,
      0.9921875,
      11.8125,
      3.625,
      0.034423828125,
      -0.796875,
      0.134765625,
      -3.5,
      6.78125,
      3.6875,
      -1.46875,
      3.21875,
      -0.80859375,
      -3.0625,
      3.078125,
      -2.40625,
      1.6484375,
      1.2890625,
      -3.640625,
      4.96875,
      -2.234375,
      0.2412109375,
      1.2578125,
      0.890625,
      2.953125,
      -5.09375,
      -0.12451171875,
      4.15625,
      0.69921875,
      0.1884765625,
      2.328125,
      -0.2431640625,
      -10.25,
      0.734375,
      0.10986328125,
      -3.625,
      0.0595703125,
      3.265625,
      -1.203125,
      1.78125,
      0.64453125,
      0.74609375,
      0.796875,
      3.125,
      0.6640625,
      5.625,
      2.53125,
      2.28125,
      -3.53125,
      2.796875,
      0.06591796875,
      3.921875,
      4.375,
      1.1640625,
      -1.6484375,
      2.203125,
      3.21875,
      3.34375,
      3.828125,
      3.171875,
      1.140625,
      2.671875,
      2.15625,
      1.40625,
      -4.4375,
      -2.703125,
      -0.73828125,
      -5.53125,
      -0.58984375,
      -1.9375,
      2.78125,
      -1.90625,
      -3.0625,
      1.2421875,
      3.171875,
      3.171875,
      0.30859375,
      -3.859375,
      -3.046875,
      -3.828125,
      -9.1875,
      3.609375,
      2.484375,
      -8.9375,
      -7.90625,
      2.296875,
      4.6875,
      0.7734375,
      -2.078125,
      0.306640625,
      -3.390625,
      2.8125,
      -1.7265625,
      -6.46875,
      2.96875,
      0.484375,
      0.5078125,
      2.390625,
      -1.6015625,
      2.265625,
      1.7265625,
      2.3125,
      -2.53125,
      -1.9453125,
      0.96875,
      -1.1015625,
      5.40625,
      2.515625,
      -3.859375,
      -0.003173828125,
      -1.7890625,
      -3.328125,
      -7.28125,
      6.625,
      -4.34375,
      5.75,
      -1.5078125,
      0.498046875,
      4.84375,
      -0.60546875,
      9.6875,
      4.375,
      2.140625,
      2.5,
      -1.9765625,
      -3.15625,
      1.1640625,
      -2.015625,
      -2.4375,
      -6.15625,
      -2.046875,
      6.3125,
      0.85546875,
      -2.609375,
      -0.1708984375,
      -0.27734375,
      4.1875,
      1.3203125,
      -2.875,
      2.703125,
      0.91015625,
      -3.796875,
      0.58203125,
      4.65625,
      -2.734375,
      3.40625,
      -3.296875,
      -2.8125,
      3.796875,
      -0.06787109375,
      -0.59375,
      -2.140625,
      -4.84375,
      -2.8125,
      0.77734375,
      -4.25,
      -1.4765625,
      1.203125,
      -1.4140625,
      3.65625,
      -0.83203125,
      5.28125,
      -1.828125,
      -5.65625,
      -7.5625,
      6.9375,
      -0.90234375,
      2.78125,
      3.59375,
      -0.150390625,
      6.9375,
      -1.90625,
      -2.328125,
      3.578125,
      -3,
      -1.796875,
      3.515625,
      4.5,
      -1.5625,
      -0.63671875,
      -6.71875,
      5.25,
      0.53515625,
      -0.392578125,
      0.63671875,
      6.375,
      -2.0625,
      -0.333984375,
      -2.6875,
      0.302734375,
      -0.5078125,
      3.875,
      -2.5625,
      3.75,
      -0.15234375,
      -2.921875,
      -2.265625,
      -4.9375,
      2.625,
      -3.1875,
      -2.765625,
      2.09375,
      -0.62109375,
      0.7734375,
      2.328125,
      -0.1806640625,
      -0.2314453125,
      1.734375,
      -2.765625,
      -6.09375,
      1.2265625,
      -2.359375,
      -1.203125,
      -0.173828125,
      1.09375,
      2.375,
      1.1484375,
      0.94140625,
      0.5703125,
      0.546875,
      -1.1953125,
      -2.34375,
      -1.6953125,
      -2.453125,
      2.5,
      2.96875,
      0.9375,
      -3,
      2.171875,
      -0.95703125,
      -0.32421875,
      3.421875,
      1,
      -1.484375,
      -0.484375,
      -2.484375,
      1.8046875,
      0.21875,
      -9.75,
      2.609375,
      2.734375,
      0.1435546875,
      4.4375,
      0.041748046875,
      -1.7890625,
      0.65234375,
      0.49609375,
      -0.40234375,
      -0.038818359375,
      -6.40625,
      -1.34375,
      -5.34375,
      -0.7265625,
      2.25,
      -2.078125,
      1.125,
      -0.2138671875,
      1.9140625,
      6.6875,
      -2.09375,
      -0.080078125,
      2.3125,
      0.6875,
      -5.71875,
      3.921875,
      -5.34375,
      -4.21875,
      2.75,
      -4.40625,
      -5.8125,
      1.53125,
      2.25,
      -1.9296875,
      2.734375,
      4.25,
      -5.5625,
      0.357421875,
      1.8046875,
      4.625,
      -0.98828125,
      -1.578125,
      -1.984375,
      1.21875,
      -0.68359375,
      2.328125,
      2.578125,
      0.2265625,
      -3.453125,
      3.125,
      2.078125,
      -1.265625,
      1.8046875,
      -1.421875,
      -2.078125,
      4.1875,
      -1.2109375,
      -1.5703125,
      -2.875,
      3.390625,
      7.875,
      -6.03125,
      -9.5,
      4.375,
      -0.0107421875,
      -1.5625,
      -4.03125,
      4.40625,
      1.765625,
      2.265625,
      -5.09375,
      -1.1953125,
      -3.15625,
      -2.828125,
      5.28125,
      2.375,
      -3.375,
      -5.46875,
      -3.71875,
      2.171875,
      1.59375,
      5.34375,
      0.5546875,
      -1.0703125,
      0.1923828125,
      -7.0625,
      1.1171875,
      0.365234375,
      6.71875,
      -0.24609375,
      1.421875,
      4.0625,
      -7,
      0.390625,
      -0.71484375,
      0.83203125,
      -0.1865234375,
      -1.25,
      4.71875,
      -1.5234375,
      -1.421875,
      0.7578125,
      6.1875,
      -3.75,
      -0.49609375,
      1.6171875,
      -3.859375,
      -2.03125,
      -1.2734375,
      2.125,
      4.125,
      0.41796875,
      -3.015625,
      1.2265625,
      0.7109375,
      1.453125,
      4.25,
      -1.28125,
      -1.484375,
      -2.109375,
      1.1484375,
      3.328125,
      2.59375,
      0.921875,
      -1.609375,
      -3.15625,
      -2.234375,
      -0.5234375,
      2.390625,
      -2.3125,
      -0.58984375,
      1.390625,
      -0.87109375,
      2.4375,
      -4.59375,
      -3.4375,
      1.7890625,
      0.490234375,
      -4.46875,
      1.140625,
      2.546875,
      4.59375,
      2.421875,
      0.1171875,
      -1.1328125,
      0.56640625,
      -0.2578125,
      -2.890625,
      -4.59375,
      -1.359375,
      4.28125,
      -5.5,
      -0.97265625,
      -2.640625,
      1.2734375,
      -0.478515625,
      2.453125,
      1.609375,
      -0.99609375,
      5.375,
      -0.87109375,
      -0.4765625,
      1.234375,
      3.90625,
      -2.96875,
      1.09375,
      -0.12890625,
      2.078125,
      -6.59375,
      -5.5,
      -0.859375,
      0.390625,
      7.4375,
      -0.6953125,
      2.515625,
      -3.234375,
      7.96875,
      -1.8828125,
      1.671875,
      -11.9375,
      5,
      1.4765625,
      -5.625,
      -0.35546875,
      -3.75,
      2.5625,
      -2,
      3.578125,
      2.328125,
      0.384765625,
      2.265625,
      4.28125,
      -0.984375,
      -2.1875,
      5.6875,
      3.203125,
      -4,
      0.59765625,
      1.03125,
      -4.625,
      2.140625,
      -1.4765625,
      5.28125,
      2.828125,
      2.078125,
      2.015625,
      1.515625,
      2.5,
      -6.6875,
      -0.447265625,
      4.9375,
      1.4140625,
      -2.171875,
      2.234375,
      -3.375,
      2.296875,
      -3.828125,
      5.25,
      0.63671875,
      -1.5390625,
      2.953125,
      3.984375,
      -4.21875,
      -0.271484375,
      -1.234375,
      -3.0625,
      5.625,
      4.28125,
      -2.59375,
      2.15625,
      -0.7890625,
      -1.59375,
      5.125,
      0.73828125,
      -2.875,
      -1.359375,
      2.125,
      0.69140625,
      -1.46875,
      5.71875,
      1.2265625,
      -3.4375,
      -0.08056640625,
      0.86328125,
      0.353515625,
      -2.171875,
      -0.890625,
      -0.1630859375,
      -2,
      -0.1337890625,
      2.140625,
      -0.314453125,
      -2.5625,
      -3.484375,
      1.328125,
      -2.953125,
      3.03125,
      0.60546875,
      -2.90625,
      0.039306640625,
      -3.6875,
      4.875,
      -1.8359375,
      0.498046875,
      0.404296875,
      3.09375,
      4.0625,
      -1.1640625,
      3.609375,
      -5.125,
      -4.625,
      -0.80078125,
      -2.875,
      -2.609375,
      1.21875,
      0.2119140625,
      -0.0103759765625,
      -0.78125,
      -2.484375,
      -5.15625,
      -5.3125,
      0.67578125,
      2.6875,
      -1.328125,
      1.8515625,
      -3.1875,
      4.1875,
      -1.1640625,
      -0.30078125,
      2.765625,
      -0.5390625,
      -0.87109375,
      0.7265625,
      2.546875,
      -7.53125,
      0.298828125,
      4.40625,
      3.34375,
      -1.7421875,
      4.65625,
      -2.421875,
      -2.6875,
      -1.109375,
      4.34375,
      -1.9140625,
      -1.9296875,
      -3.375,
      -0.65625,
      -3.828125,
      -4.9375,
      -1.109375,
      -1.53125,
      0.75,
      -2.5625,
      -0.91796875,
      -2.953125,
      -6.03125,
      -0.1943359375,
      -0.0654296875,
      -4.46875,
      2.96875,
      1.734375,
      -1.0546875,
      -3.625,
      1.8125,
      3.40625,
      0.58203125,
      -1.6015625,
      -1.1796875,
      4.15625,
      -0.76953125,
      2.734375,
      -0.0810546875,
      -6.40625,
      1.3671875,
      -1.375,
      -3.171875,
      2.265625,
      -2.75,
      1.6796875,
      4.25,
      1.4765625,
      -4.21875,
      -1.4375,
      1.59375,
      -2.359375,
      0.427734375,
      0.62109375,
      3.265625,
      -0.1796875,
      5.125,
      -2.578125,
      -4.4375,
      0.443359375,
      4.25,
      4.53125,
      -0.00153350830078125,
      -2.921875,
      2.953125,
      1.8359375,
      3.359375,
      -5.375,
      -2.765625,
      -0.54296875,
      2.625,
      -2,
      -2.859375,
      7.03125,
      -2.78125,
      2.40625,
      0.9765625,
      3,
      -0.04248046875,
      -1.75,
      -0.03662109375,
      2.03125,
      2.5625,
      -0.81640625,
      1.96875,
      3.328125,
      -3.71875,
      -1.84375,
      1.1328125,
      1.2421875,
      -2.859375,
      2.328125,
      0.3671875,
      -0.1669921875,
      -1.0234375,
      -0.70703125,
      -0.0213623046875,
      3.65625,
      6,
      -1.125,
      -0.91015625,
      -2.046875,
      8.5625,
      -2.09375,
      4.1875,
      1.703125,
      10.6875,
      -1.6953125,
      -4.46875,
      1.7109375,
      -1.1796875,
      0.007049560546875,
      1.1015625,
      -3.90625,
      -5.25,
      1.4296875,
      -0.2578125,
      0.427734375,
      1.5078125,
      -5.59375,
      -0.470703125,
      5.96875,
      -0.4921875,
      5.125,
      4.78125,
      3.625,
      -0.68359375,
      -0.56640625,
      -2.546875,
      -1.46875,
      -0.984375,
      0.08984375,
      0.466796875,
      -4.375,
      3.9375,
      0.8515625,
      4.4375,
      1.8515625,
      0.7578125,
      -0.0927734375,
      8.3125,
      -1.78125,
      -2.03125,
      1.65625,
      2.953125,
      -0.8671875,
      -0.74609375,
      2.703125,
      -5.0625,
      -1.9375,
      7.25,
      5.25,
      0.9453125,
      2.359375,
      -0.6796875,
      0.474609375,
      -4.03125,
      -0.8359375,
      -0.5234375,
      1.71875,
      -8.1875,
      -2.625,
      2.171875,
      2.640625,
      -3,
      0.006072998046875,
      2.21875,
      -6.71875,
      5.8125,
      1.2109375,
      2.484375,
      0.94921875,
      -0.15234375,
      3.875,
      -2.265625,
      -2.5625,
      -4.25,
      -3.484375,
      -4.375,
      -1.1171875,
      3.53125,
      2.828125,
      4.625,
      -1.5390625,
      -0.255859375,
      -8.3125,
      0.1376953125,
      -0.65625,
      2.5625,
      2.65625,
      -5.6875,
      0.5703125,
      -3.046875,
      -8.5625,
      -4.75,
      -3.515625,
      -3.0625,
      -1.21875,
      -4.9375,
      -0.68359375,
      0.392578125,
      -4.90625,
      2.96875,
      -2.734375,
      1.4609375,
      3.78125,
      0.482421875,
      1.890625,
      -3.9375,
      -1,
      0.62890625,
      0.69921875,
      5.40625,
      0.72265625,
      2.75,
      5.40625,
      0.37890625,
      0.5,
      -4.0625,
      7.0625,
      -1.1953125,
      6.25,
      3.125,
      1.6953125,
      -5.8125,
      1.7109375,
      -0.46484375,
      1.3828125,
      -5.6875,
      0.9609375,
      -3.9375,
      1.34375,
      0.59375,
      -0.259765625,
      4.15625,
      -3.53125,
      -3.578125,
      1.2421875,
      -6.03125,
      -0.43359375,
      4.84375,
      0.57421875,
      1.0234375,
      -4.53125,
      -0.87890625,
      -0.201171875,
      4.625,
      1.5234375,
      -0.478515625,
      2.5,
      -4.5,
      0.734375,
      -1.515625,
      0.158203125,
      -2.234375,
      -1.2578125,
      -1.4453125,
      -1.1953125,
      2.734375,
      3.25,
      0.486328125,
      5.875,
      1.9140625,
      0.5546875,
      -1.4453125,
      -0.6171875,
      -0.84375,
      -2.3125,
      6.0625,
      -0.5625,
      0.58203125,
      -2.09375,
      1.1171875,
      -4.875,
      -1.890625,
      0.109375,
      -7.90625,
      -0.0001983642578125,
      -2.59375,
      -5.03125,
      2.6875,
      -1.703125,
      0.72265625,
      0.81640625,
      4.90625,
      1.796875,
      2.5625,
      -0.462890625,
      0.5859375,
      0.271484375,
      -3.8125,
      7.53125,
      -2.171875,
      -0.74609375,
      4.59375,
      3.84375,
      6.53125,
      -1.9140625,
      -3.328125,
      -2.71875,
      3.265625,
      5.0625,
      1.84375,
      0.81640625,
      2.875,
      0.65625,
      1.578125,
      1.609375,
      -2.21875,
      0.14453125,
      -3.03125,
      0.6953125,
      3.5,
      4.75,
      -2.984375,
      -1.4375,
      -5.6875,
      -0.1015625,
      0.89453125,
      1.609375,
      -3.578125,
      -1.6875,
      -2.3125,
      0.921875,
      -1.0078125,
      -0.33984375,
      -1.59375,
      -5.625,
      1.1171875,
      1.640625,
      -0.059814453125,
      -0.40234375,
      -0.578125,
      0.5390625,
      -1.4140625,
      2.359375,
      2.375,
      -0.73828125,
      1.703125,
      2.984375,
      0.10009765625,
      4.21875,
      3.90625,
      1.625,
      2.015625,
      -0.302734375,
      -3.625,
      0.0751953125,
      -3.328125,
      1.734375,
      -2.828125,
      1.90625,
      3.5625,
      6.8125,
      -0.447265625,
      3.640625,
      1.421875,
      0.07080078125,
      2.59375,
      -3.84375,
      -1.265625,
      1.0703125,
      -1.2734375,
      -2.09375,
      1.1953125,
      -3.375,
      1.1328125,
      3.578125,
      -7.1875,
      3.09375,
      -1.4765625,
      9.25,
      0.1640625,
      -3.109375,
      0.65234375,
      -1.234375,
      1.546875,
      -4.40625,
      2.40625,
      -0.51953125,
      -3.8125,
      5.125,
      -1.6640625,
      -0.66796875,
      0.2275390625,
      -3.125,
      1.453125,
      -2.25,
      -1.5625,
      1.0703125,
      -1.5234375,
      -4.3125,
      -2.203125,
      -1.53125,
      1.25,
      0.474609375,
      0.66015625,
      0.09033203125,
      -1.484375,
      1.8046875,
      -4.03125,
      -0.498046875,
      0.88671875,
      0.38671875,
      2.1875,
      -3.15625,
      5.75,
      1.828125,
      -1.0390625,
      -0.11376953125,
      -1.109375,
      -3.953125,
      -1.75,
      -1.4140625,
      -0.2060546875,
      -4.8125,
      -4.625,
      1.375,
      0.42578125,
      0.6796875,
      -0.7734375,
      -1.25,
      2.140625,
      4.53125,
      -0.58984375,
      4.28125,
      -0.35546875,
      1.328125,
      -2.359375,
      -6,
      -5.875,
      1.6328125,
      1.4609375,
      -0.36328125,
      2.359375,
      -0.76953125,
      2.53125,
      -0.01434326171875,
      -0.00482177734375,
      -3.0625,
      -0.07568359375,
      4.03125,
      -1.609375,
      -3.28125,
      4.625,
      1.296875,
      3.125,
      -2.546875,
      -0.1494140625,
      -2.5625,
      3.109375,
      1.8046875,
      2.46875,
      -0.78515625,
      -0.0986328125,
      -4.21875,
      -2.375,
      -3.296875,
      -1.765625,
      5.875,
      -0.55078125,
      3.171875,
      2.203125,
      -1.5859375,
      -2.140625,
      0.5625,
      -1.546875,
      4.125,
      3.1875,
      -1.1484375,
      -0.130859375,
      -2.953125,
      2.078125,
      -3.171875,
      0.162109375,
      -7.3125,
      -3.171875,
      4.75,
      -1.015625,
      -2.328125,
      -1.7421875,
      -0.03466796875,
      1.4609375,
      0.56640625,
      2.84375,
      -1.859375,
      -3.40625,
      -4.21875,
      4.21875,
      -0.4453125,
      -3.28125,
      1.5078125,
      3.515625,
      -5.15625,
      2.71875,
      3.5,
      2.25,
      -3.1875,
      4.8125,
      -1.2109375,
      -4.75,
      -5.78125,
      3.546875,
      3.984375,
      -0.609375,
      -1.6015625,
      -4.03125,
      -1.6328125,
      -0.255859375,
      2.140625,
      5.0625,
      -1.6015625,
      1.6015625,
      -0.8671875,
      3.125,
      0.4921875,
      -4.65625,
      -0.474609375,
      3.328125,
      3.421875,
      -0.9921875,
      0.134765625,
      -2.25,
      -0.047607421875,
      -1.8046875,
      -8.9375,
      1.0546875,
      1.34375,
      -0.058349609375,
      5.21875,
      0.4375,
      2.4375,
      0.703125,
      -3.859375,
      -1.3984375,
      -2.546875,
      2.703125,
      1.2578125,
      4.53125,
      -3.640625,
      0.43359375,
      -0.5,
      -0.40625,
      -1.7734375,
      2.28125,
      -2.0625,
      0.82421875,
      0.6640625,
      4,
      -3.046875,
      -4.5625,
      -0.859375,
      -1.046875,
      -1.4453125,
      0.8359375,
      2.453125,
      2.09375,
      4.1875,
      -2.625,
      -3.625,
      -0.9609375,
      -4.28125,
      2.390625,
      3.390625,
      1.6875,
      -0.734375,
      0.462890625,
      -4,
      -1.703125,
      3.484375,
      -1.59375,
      -1.203125,
      -0.412109375,
      0.047607421875,
      1.9375,
      1.5078125,
      -0.7265625,
      -1.2734375,
      3.90625,
      -1.2265625,
      -4.84375,
      -2.921875,
      1.7890625,
      1.0390625,
      -2.796875,
      2.171875,
      -3.59375,
      3.15625,
      -3.21875,
      -0.95703125,
      -0.15234375,
      -1.6796875,
      1.6171875,
      4.0625,
      0.8515625,
      -1.8984375,
      0.279296875,
      -0.99609375,
      3.609375,
      1.84375,
      -1.171875,
      0.57421875,
      1.0078125,
      0.34375,
      -3.671875,
      3.484375,
      0.31640625,
      -3.828125,
      1.828125,
      2.1875,
      -5.84375,
      -0.5078125,
      3,
      3.3125,
      -2.8125,
      0.671875,
      -0.310546875,
      8.25,
      0.248046875,
      -1.3046875,
      -2.859375,
      -2.140625,
      1.1796875,
      0.6796875,
      1.265625,
      -0.474609375,
      1.78125,
      -6.28125,
      -1.9921875,
      -0.07666015625,
      2.421875,
      6.21875,
      -4.03125,
      1.828125,
      4,
      -1.8515625,
      5.5,
      -0.5625,
      -3.578125,
      -2.359375,
      -1.1484375,
      -6.34375,
      -0.921875,
      -5.90625,
      1.640625,
      -4.15625,
      -2.125,
      -2.78125,
      1.4296875,
      2.28125,
      1.6640625,
      -2.515625,
      -2.484375,
      -3.953125,
      -5.125,
      -2.375,
      -1.3515625,
      -6.375,
      2.828125,
      -0.2119140625,
      1.84375,
      4.34375,
      3.578125,
      1.703125,
      -1.3203125,
      2.21875,
      -1.90625,
      0.93359375,
      -0.60546875,
      -0.5,
      -2.09375,
      -2.265625,
      -0.271484375,
      -1.96875,
      2.21875,
      -5.625,
      -3.015625,
      -3.4375,
      -0.7265625,
      0.431640625,
      -0.3984375,
      0.408203125,
      -0.80859375,
      5.4375,
      -0.3359375,
      1.671875,
      3.8125,
      -1.5078125,
      0.494140625,
      1.453125,
      1.1875,
      8.25,
      -1.3046875,
      1.1796875,
      -4.46875,
      4.90625,
      2.859375,
      -3.484375,
      2.171875,
      -0.62890625,
      -2.5,
      -3.859375,
      1.3828125,
      2.328125,
      3.5625,
      -3.703125,
      3.515625,
      4.75,
      -1.15625,
      4.96875,
      4.9375,
      2.765625,
      1.1640625,
      4.6875,
      0.99609375,
      -0.1787109375,
      -1.625,
      4.0625,
      -1.6328125,
      -2.546875,
      1.7578125,
      0.61328125,
      2.75,
      2.234375,
      0.201171875,
      2.484375,
      6.53125,
      -7.28125,
      1.609375,
      -6.1875,
      0.87109375,
      -4.90625,
      -1.03125,
      -3.96875,
      -3.21875,
      1.7109375,
      4.4375,
      -1.90625,
      -4.625,
      1.3984375,
      3.375,
      -0.166015625,
      -4.5625,
      4.40625,
      3.96875,
      1.390625,
      -3.09375,
      -2.609375,
      3.421875,
      2.015625,
      -1.171875,
      -5.25,
      4.09375,
      -0.07568359375,
      -4.59375,
      -2.9375,
      0.169921875,
      -1.4453125,
      -3.640625,
      -6.125,
      -3.796875,
      3.015625,
      -4,
      3.84375,
      5.125,
      -0.1787109375,
      3.25,
      0.8671875,
      1.515625,
      1.546875,
      -2.015625,
      2.84375,
      1.1640625,
      1.4296875,
      -1.390625,
      1.6875,
      -1.3125,
      1.859375,
      -0.2314453125,
      1.8515625,
      3.046875,
      -1.6015625,
      0.9375,
      -3.8125,
      3.296875,
      3.09375,
      1.109375,
      -3.671875,
      -2.40625,
      3.71875,
      1.8828125,
      -2.9375,
      -0.91796875,
      0.294921875,
      -0.466796875,
      -1.59375,
      1.484375,
      -0.99609375,
      4.09375,
      0.158203125,
      1.2578125,
      -0.23046875,
      -0.921875,
      -3.59375,
      -1.1015625,
      -2,
      -0.94140625,
      -1.5390625,
      5.46875,
      -1.734375,
      -2.703125,
      -2.9375,
      3.484375,
      -2.234375,
      2.15625,
      2.84375,
      -1.0390625,
      2.4375,
      0.8125,
      -0.458984375,
      -5.09375,
      0.23828125,
      3.75,
      0.373046875,
      1.0625,
      -0.89453125,
      -1.96875,
      1.9765625,
      -3.296875,
      -4.0625,
      1.234375,
      -1.0390625,
      -2.09375,
      -9.625,
      1.625,
      2.0625,
      -0.4765625,
      -1.8671875,
      -1.734375,
      1.8203125,
      4.03125,
      4.375,
      4.25,
      -0.51953125,
      -0.62109375,
      1.0703125,
      16.375,
      -1.6015625,
      -8.0625,
      -0.5859375,
      0.62109375,
      2.546875,
      9.4375,
      0.484375,
      0.30078125,
      1.296875,
      2.421875,
      1.7734375,
      -3.578125,
      3.4375,
      -0.54296875,
      0.3046875,
      3.203125,
      1.0546875,
      0.69140625,
      -3.234375,
      1.1640625,
      1.7890625,
      2.09375,
      1.5,
      -2.375,
      1.2578125,
      -1.2421875,
      -0.98046875,
      -3.171875,
      -3.59375,
      2.140625,
      -1.65625,
      -0.1806640625,
      -2.015625,
      0.2314453125,
      -1.6875,
      2.890625,
      0.89453125,
      0.0157470703125,
      -1.265625,
      -3.375,
      -4.5,
      4.21875,
      -0.1552734375,
      -1.5234375,
      -2.453125,
      -0.052734375,
      -1.1171875,
      1.1875,
      0.5546875,
      0.376953125,
      1.7734375,
      -1.875,
      0.5625,
      -0.390625,
      -1.4375,
      -2.203125,
      -6.84375,
      -2.15625,
      -3.328125,
      0.55078125,
      0.1650390625,
      -2.46875,
      -1.203125,
      -2.609375,
      -1.7421875,
      -2.90625,
      -0.9375,
      -2.421875,
      0.73828125,
      -5.15625,
      -0.890625,
      6.375,
      2.296875,
      -1.8671875,
      -4.3125,
      0.087890625,
      4.25,
      1.2109375,
      3.375,
      -3.984375,
      0.000881195068359375,
      -4.8125,
      -0.0322265625,
      -2.5,
      0.546875,
      -3.4375,
      3.03125,
      -0.423828125,
      -1.453125,
      4.6875,
      -4.40625,
      3.59375,
      1.75,
      -3.765625,
      -0.640625,
      3.796875,
      1.375,
      -0.89453125,
      -1.5078125,
      4.125,
      -1.546875,
      -0.2333984375,
      1.1015625,
      -0.0135498046875,
      -1.5,
      3.046875,
      -3.203125,
      2.40625,
      -0.828125,
      -2.390625,
      5.3125,
      3.078125,
      0.474609375,
      -0.1337890625,
      3.484375,
      -0.75,
      -0.435546875,
      -0.0111083984375,
      -4.03125,
      4.375,
      -0.1796875,
      -0.90625,
      1.8984375,
      -8.125,
      -4.96875,
      -1.9765625,
      -6.125,
      1.96875,
      -5.3125,
      -0.73828125,
      -1.4296875,
      -1.84375,
      -3.203125,
      1.9375,
      -2.46875,
      0.78125,
      5.34375,
      -0.72265625,
      0.5859375,
      -3.1875,
      0.83984375,
      4.03125,
      -3.484375,
      -1.1171875,
      0.19921875,
      0.1279296875,
      2.390625,
      3.03125,
      -0.546875,
      -2.265625,
      5.34375,
      -0.9921875,
      -2.984375,
      0.5625,
      3.1875,
      -0.1787109375,
      -0.058349609375,
      -3.40625,
      2.140625,
      -1.2421875,
      -0.248046875,
      -0.98828125,
      -3.109375,
      -2.046875,
      0.92578125,
      2.25,
      -2.5,
      -6.9375,
      4.53125,
      5.375,
      -7.25,
      -0.1650390625,
      -4.28125,
      4.90625,
      -2.609375,
      3.4375,
      5.625,
      -0.00116729736328125,
      -0.057373046875,
      2.5625,
      1.171875,
      0.08447265625,
      -5.65625,
      2.84375,
      6.90625,
      -3.90625,
      2.546875,
      3.953125,
      0.58203125,
      0.703125,
      -3.171875,
      -1.7734375,
      6.40625,
      3.125,
      -3.75,
      -1.140625,
      -2.875,
      -0.64453125,
      -2.671875,
      -2.96875,
      -2.171875,
      -0.8359375,
      -0.27734375,
      3.46875,
      -0.86328125,
      -3,
      -1.3984375,
      -2.671875,
      -1.890625,
      -1.1015625,
      4.9375,
      0.65625,
      2.421875,
      -5.6875,
      6.1875,
      2.6875,
      0.82421875,
      1.578125,
      5.5,
      -0.75390625,
      3.484375,
      -5.25,
      -1.6171875,
      -1.6875,
      2,
      -4.625,
      2.359375,
      -4.0625,
      1.375,
      -4.28125,
      3.65625,
      -0.232421875,
      1.8984375,
      -4.03125,
      -3.84375,
      -2.28125,
      5.59375,
      -2.125,
      1.0390625,
      4.1875,
      -1.5,
      0.7421875,
      2.875,
      -1.6640625,
      0.2734375,
      -2.53125,
      2.453125,
      0.91796875,
      -2.15625,
      2.203125,
      -0.86328125,
      3.90625,
      -1.03125,
      1.3671875,
      -2.59375,
      -3.625,
      0.0177001953125,
      -4.8125,
      0.412109375,
      0.3203125,
      -5.46875,
      1.375,
      0.7578125,
      2.109375,
      0.5390625,
      3.59375,
      -7.625,
      5.59375,
      0.80078125,
      -5.5625,
      1.8984375,
      0.9453125,
      -2.625,
      0.39453125,
      -2.546875,
      1.78125,
      -4.6875,
      0.2333984375,
      5.1875,
      -2.34375,
      0.265625,
      0.953125,
      -3.953125,
      0.259765625,
      -4.8125,
      3.703125,
      1.0625,
      3.421875,
      2.109375,
      1.5078125,
      5.75,
      -3.8125,
      3.375,
      -6.0625,
      0.82421875,
      -2.296875,
      2.640625,
      0.86328125,
      -2.890625,
      4.3125,
      5.65625,
      -0.353515625,
      -9.125,
      -5.25,
      -0.75390625,
      -2.859375,
      2.375,
      2.671875,
      1.671875,
      -1.3828125,
      0.11181640625,
      1.3125,
      3.234375,
      -1.859375,
      -1.890625,
      -1.265625,
      2.671875,
      -0.0067138671875,
      -4.71875,
      -0.373046875,
      0.00799560546875,
      -0.1171875,
      -0.53125,
      -2.203125,
      -2.9375,
      2.328125,
      3,
      -3.375,
      2.78125,
      1.375,
      2.890625,
      1.9140625,
      0.1640625,
      -0.373046875,
      3.703125,
      -3.125,
      -6.84375,
      -0.369140625,
      -0.60546875,
      2.203125,
      4.5,
      -1.71875,
      4.1875,
      0.06640625,
      4.25,
      -5.09375,
      3.5,
      3.328125,
      -10.25,
      2.34375,
      -2.859375,
      1.0625,
      -0.365234375,
      -5.5625,
      -1.8515625,
      3.8125,
      -0.41796875,
      -1.828125,
      5.6875,
      -2.203125,
      0.228515625,
      -0.21875,
      -2.734375,
      -3.484375,
      3.125,
      2.1875,
      -2.234375,
      3.515625,
      5.21875,
      -1.921875,
      -2.015625,
      -1.6015625,
      5.5,
      -1.8671875,
      -3,
      3.515625,
      3.609375,
      1.203125,
      -1.4375,
      -4.4375,
      -0.255859375,
      -1.65625,
      -2.953125,
      -0.99609375,
      -2.3125,
      2.3125,
      -2.046875,
      -0.1474609375,
      -1.8984375,
      -2.640625,
      0.54296875,
      -2.53125,
      -3.3125,
      1.8125,
      2.625,
      0.050537109375,
      -2.046875,
      -0.7890625,
      1.8515625,
      0.2080078125,
      -3.46875,
      -3.953125,
      0.98046875,
      -2.59375,
      -3.609375,
      -1.78125,
      0.314453125,
      -2.3125,
      0.408203125,
      -0.58203125,
      -0.3125,
      -0.29296875,
      -0.76953125,
      -0.7421875,
      1.5703125,
      2.921875,
      0.310546875,
      0.27734375,
      -0.043701171875,
      3.234375,
      2.671875,
      -5.71875,
      -2.828125,
      1.234375,
      -0.0888671875,
      -4.28125,
      -3.78125,
      0.314453125,
      -2.203125,
      3.171875,
      5.125,
      2.578125,
      -1.0546875,
      3.03125,
      -3.8125,
      -1.265625,
      4.96875,
      4.34375,
      -0.52734375,
      -1.4609375,
      1.7890625,
      -4.6875,
      5.8125,
      6.6875,
      -3.46875,
      -3.75,
      0.66015625,
      -2.875,
      -1.3125,
      -1.78125,
      3.328125,
      0.32421875,
      -7.1875,
      3.765625,
      -0.9921875,
      5.53125,
      1.0078125,
      -2.515625,
      2.4375,
      -0.4609375,
      1.890625,
      -1.015625,
      2.140625,
      0.71484375,
      -4.46875,
      0.267578125,
      -2.328125,
      -2.859375,
      -3.40625,
      -0.349609375,
      0.337890625,
      2.59375,
      5.25,
      1.1328125,
      -0.9453125,
      -1.3359375,
      -1.6328125,
      3.359375,
      -1.6953125,
      -2.96875,
      -0.3125,
      -2.9375,
      0.96484375,
      0.78515625,
      -0.95703125,
      -6.4375,
      -1.2890625,
      -4.625,
      4.90625,
      -1.5390625,
      0.19921875,
      -2.40625,
      -1.4921875,
      2.9375,
      4.59375,
      3.578125,
      3.625,
      2.5,
      2.6875,
      0.234375,
      -2.875,
      4.65625,
      3.671875,
      2.203125,
      -0.87109375,
      0.87109375,
      0.578125,
      -2.359375,
      -4.1875,
      -0.1328125,
      0.10888671875,
      5.4375,
      4.1875,
      -1.0546875,
      1.40625,
      -2.125,
      -1.1328125,
      -2.171875,
      0.88671875,
      1.0234375,
      1.3203125,
      -1.828125,
      0.82421875,
      2.5625,
      1.0703125,
      0.388671875,
      3.3125,
      1.34375,
      -0.1923828125,
      -2.5,
      -0.6875,
      3.234375,
      -2.75,
      -0.447265625,
      -2,
      0.31640625,
      -1.2265625,
      -3.59375,
      3.34375,
      -0.2109375,
      -0.9296875,
      4.125,
      -0.376953125,
      1.8828125,
      0.3515625,
      2.96875,
      -2.9375,
      1.3359375,
      0.490234375,
      2.75,
      -1.5078125,
      1.234375,
      1.1796875,
      1.5703125,
      -1.71875,
      1.53125,
      1.75,
      -2.390625,
      -0.2734375,
      1.2265625,
      -2.421875,
      -1.0390625,
      3.09375,
      -1.78125,
      0.5234375,
      0.84765625,
      -0.546875,
      -1.0859375,
      4.625,
      -3.65625,
      -3.15625,
      -0.6875,
      1.1484375,
      0.1728515625,
      -0.55078125,
      -0.0478515625,
      -0.859375,
      0.890625,
      -0.27734375,
      -2.71875,
      -0.68359375,
      -0.7890625,
      -1.515625,
      1.4453125,
      -1.15625,
      0.267578125,
      -4.46875,
      3.390625,
      -0.25390625,
      -0.8125,
      -0.396484375,
      -0.68359375,
      -0.306640625,
      -3.125,
      1.0546875,
      -1.4765625,
      1.6015625,
      1.359375,
      0.76953125,
      2.796875,
      -0.88671875,
      0.396484375,
      -0.419921875,
      -0.033203125,
      3.921875,
      -0.1728515625,
      -2.296875,
      -0.890625,
      1.84375,
      1.140625,
      1.296875,
      0.5546875,
      2.84375,
      0.59765625,
      1.5078125,
      1.953125,
      -0.59375,
      3.25,
      1.015625,
      3.21875,
      -2.9375,
      -0.9609375,
      -3.109375,
      -1.90625,
      1.390625,
      -1.8671875,
      -3.921875,
      -0.3515625,
      -0.140625,
      1.015625,
      2.703125,
      5.15625,
      -2.25,
      1.6484375,
      -0.75390625,
      1.5,
      -1.234375,
      1.203125,
      2.03125,
      -0.000514984130859375,
      -3.5625,
      -3.0625,
      2.046875,
      -1.9453125,
      1.4609375,
      0.298828125,
      -1.421875,
      0.61328125,
      -2.203125,
      -2.015625,
      -0.2255859375,
      -0.921875,
      0.37109375,
      0.9453125,
      1.4375,
      1.515625,
      1.1640625,
      -2.9375,
      3.875,
      -3.34375,
      2.96875,
      -0.259765625,
      0.41796875,
      -2.078125,
      2.65625,
      -1.28125,
      -2.984375,
      1.921875,
      -3.34375,
      -2.28125,
      -3.265625,
      -1.2890625,
      2.515625,
      -1.3671875,
      0.58984375,
      -1.5703125,
      -0.32421875,
      2.390625,
      2.515625,
      2.515625,
      2.78125,
      1.5703125,
      3.078125,
      2.109375,
      -2.875,
      0.23828125,
      -2.859375,
      -0.9375,
      -0.59375,
      1.9296875,
      1.828125,
      -4.28125,
      0.50390625,
      -1.0859375,
      2.546875,
      3.515625,
      -0.9765625,
      2.46875,
      1.9765625,
      -1.6171875,
      3.796875,
      5.09375,
      0.9609375,
      0.85546875,
      0.158203125,
      -1.6875,
      -0.2197265625,
      -1.8671875,
      -1.125,
      -0.12451171875,
      2.296875,
      0.0859375,
      0.5390625,
      -2.0625,
      2.375,
      1.390625,
      0.0234375,
      2.328125,
      -2.578125,
      -1.2890625,
      0.07080078125,
      2.5,
      1.15625,
      1.484375,
      1.3984375,
      0.6796875,
      -4.9375,
      -1.21875,
      -0.07080078125,
      -1.140625,
      -0.8203125,
      -2.0625,
      0.79296875,
      -1.4296875,
      -4.1875,
      0.54296875,
      -0.73828125,
      0.208984375,
      1.125,
      2.921875,
      -0.54296875,
      1.953125,
      -0.01055908203125,
      1.359375,
      -0.55859375,
      -0.91796875,
      1.8984375,
      1.9296875,
      0.98828125,
      0.404296875,
      -3.703125,
      -2.421875,
      -1.9609375,
      -1.953125,
      -1.0078125,
      2.625,
      0.34375,
      -0.98828125,
      0.87109375,
      -1.3984375,
      -1.3515625,
      2.5625,
      3.1875,
      -0.515625,
      1.1953125,
      -1.15625,
      2.5,
      -3.453125,
      1.234375,
      -0.265625,
      -0.9765625,
      0.1845703125,
      -0.45703125,
      -0.51171875,
      -1.6015625,
      0.462890625,
      -0.890625,
      0.125,
      -2.25,
      -2.0625,
      1.078125,
      2.71875,
      -0.333984375,
      1.875,
      2.171875,
      5.4375,
      -0.6640625,
      1.0234375,
      1.734375,
      3.625,
      5.625,
      1.6328125,
      -5.0625,
      -2.890625,
      0.73828125,
      2.46875,
      2.3125,
      -0.451171875,
      1.796875,
      4.625,
      -0.69140625,
      0.4375,
      -4.65625,
      -0.00101470947265625,
      3.125,
      1.328125,
      4.59375,
      -1.53125,
      -0.7265625,
      -2.015625,
      -1.984375,
      2.4375,
      2.453125,
      2.984375,
      2.4375,
      2.453125,
      -0.0439453125,
      1.625,
      0.28515625,
      2.8125,
      -1.2109375,
      0.79296875,
      2.34375,
      -1.4921875,
      -1.7421875,
      1.9765625,
      -3.34375,
      1.703125,
      -1.2109375,
      2.28125,
      3.21875,
      -0.98828125,
      -1.40625,
      0.02587890625,
      -0.8984375,
      4.375,
      -1.078125,
      -0.1787109375,
      -2.53125,
      1.15625,
      -0.00213623046875,
      -1.4296875,
      0.84375,
      -3.828125,
      1.9140625,
      0.265625,
      -1.140625,
      -1.078125,
      -2.578125,
      -2.671875,
      -1.3203125,
      -1.75,
      -0.98046875,
      -2.3125,
      5,
      -2.765625,
      -1.6640625,
      0.671875,
      4.46875,
      0.13671875,
      6.5625,
      -0.376953125,
      2.890625,
      -1.328125,
      -1.1875,
      -0.671875,
      -4.34375,
      -2.875,
      -0.1962890625,
      0.384765625,
      -0.2158203125,
      -0.53125,
      -1.484375,
      -1.171875,
      3.5625,
      -0.498046875,
      -3.6875,
      -2.078125,
      2.609375,
      -1.0078125,
      0.70703125,
      -3.609375,
      0.27734375,
      -0.80078125,
      -1.53125,
      1.2734375,
      -0.390625,
      0.88671875,
      0.71875,
      -0.01031494140625,
      -2.734375,
      -0.9375,
      0.515625,
      -3.421875,
      -0.3984375,
      4.90625,
      -1.515625,
      0.08740234375,
      -0.494140625,
      4.125,
      0.09765625,
      2.203125,
      -2.515625,
      -1.3203125,
      2.515625,
      4.09375,
      3.140625,
      1.6328125,
      0.126953125,
      3.453125,
      -3.8125,
      -1.21875,
      -0.4375,
      -4.28125,
      -0.2734375,
      3.25,
      -4.09375,
      0.068359375,
      -1.703125,
      0.318359375,
      -0.625,
      -2.578125,
      2.78125,
      -2.859375,
      -0.546875,
      3.65625,
      -1.921875,
      -0.9140625,
      -3.28125,
      1.6875,
      -4.28125,
      0.36328125,
      -2.71875,
      -2.390625,
      0.59375,
      1.6640625,
      -0.31640625,
      3,
      0.412109375,
      0.447265625,
      5.21875,
      2.328125,
      3.015625,
      1.2421875,
      -0.6484375,
      1.3203125,
      -1.4921875,
      3.328125,
      -0.3046875,
      -2.578125,
      0.8359375,
      -2.75,
      -1.453125,
      -0.5390625,
      3.46875,
      0.44140625,
      -0.08544921875,
      -0.8515625,
      1.5390625,
      1.8671875,
      -0.80859375,
      3.75,
      -0.98046875,
      0.5078125,
      -3.921875,
      0.275390625,
      -0.8828125,
      3.375,
      3.375,
      2.03125,
      -0.80859375,
      -2.09375,
      2.46875,
      -1.7890625,
      2.0625,
      0.486328125,
      -4.6875,
      -2.140625,
      -1.375,
      -1.125,
      -0.4921875,
      -1.90625,
      -0.0615234375,
      -0.484375,
      -0.1513671875,
      2.640625,
      -0.298828125,
      -2.28125,
      -0.90234375,
      0.859375,
      2.09375,
      -0.07080078125,
      1.15625,
      -1.296875,
      3.3125,
      0.392578125,
      -0.72265625,
      1.9765625,
      2.171875,
      -1.046875,
      -2.40625,
      -0.1083984375,
      -3.75,
      -1.4609375,
      2.515625,
      1.6640625,
      1.921875,
      -0.703125,
      0.0703125,
      3.703125,
      0.76953125
    ],
    "s2_graph": {
      "citations": [],
      "citations_fetched_at": "2025-12-16T15:25:25.863070",
      "references": [],
      "references_fetched_at": "2025-12-16T15:25:26.148305"
    }
  },
  "1daa7fa3-52d8-4ad3-bad2-545c83a3c45e": {
    "id": "1daa7fa3-52d8-4ad3-bad2-545c83a3c45e",
    "filename": "ssrn-5095149.pdf",
    "file_path": "./uploads/papers/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e.pdf",
    "status": "completed",
    "title": "Effects of LLM use and note-taking on reading comprehension and memory: A randomised experiment in secondary schools",
    "category": null,
    "markdown_content": "# Effects of LLM use and note-taking on reading comprehension and memory: A randomised experiment in secondary schools\n\nAuthors:\n\nPia Kreijkes<sup>1</sup>, Viktor Kewenig<sup>2*</sup>, Martina Kuvalja<sup>1*</sup>, Mina Lee<sup>2</sup>, Sylvia Vitello<sup>1</sup>, Jake M. Hofman<sup>2</sup>, Abigail Sellen<sup>2</sup>, Sean Rintel<sup>2</sup>, Daniel G. Goldstein<sup>2</sup>, David Rothschild<sup>2</sup>, Lev Tankelevitch<sup>2</sup>, Tim Oates<sup>1</sup>\n\n*Joint second authors\n\n# Affiliations:\n\n$^{1}$ Cambridge University Press and Assessment  \n2Microsoft Research\n\n# Abstract\n\nThe rapid uptake of Generative AI, particularly large language models (LLMs), by students raises urgent questions about their effects on learning. We compared the impact of LLM use to that of traditional note-taking, or a combination of both, on secondary school students' reading comprehension and retention. We conducted a pre-registered, randomised controlled experiment with within- and between-participant design elements in schools. 405 students aged 14-15 studied two text passages and completed comprehension and retention tests three days later. Quantitative results demonstrated that both note-taking alone and combined with the LLM had significant positive effects on retention and comprehension compared to the LLM alone. Yet, most students preferred using the LLM over note-taking, and perceived it as more helpful. Qualitative results revealed that many students valued LLMs for making complex material more accessible and reducing cognitive load, while they appreciated note-taking for promoting deeper engagement and aiding memory. Additionally, we identified \"archetypes\" of prompting behaviour, offering insights into the different ways students interacted with the LLM. Overall, our findings suggest that, while note-taking promotes cognitive engagement and long-term comprehension and retention, LLMs may facilitate initial understanding and student interest. The study reveals the continued importance of traditional learning approaches, the benefits of combining AI use with traditional learning over using AI alone, and the AI skills that students need to maximise those benefits.\n\n# Main\n\nLearners' rapid and widespread adoption of Generative Artificial Intelligence (GenAI) tools, particularly Large Language Models (LLMs), has unsettled the global educational landscape by offering\n\nnew ways for students to engage with learning materials $^{1;2;3;4;5;6}$  while also creating new challenges $^{7;8;9;10;11;12}$ . Large national surveys in the UK and US have found that a sizeable proportion of school students use GenAI tools such as OpenAI's ChatGPT $^{13;14}$ . This development raises fundamental questions about teaching and learning models. And yet, the vast majority of existing research on learning with LLMs has focused on the higher education context, leaving substantial knowledge gaps regarding effects on younger learners $^{15}$ . In addition, previous research has concentrated on second language education, mostly writing performance, as well as computing, health, and physics $^{15}$ . While such studies overall reveal positive effects of LLM use on academic performance, researchers call for caution as these might reflect the quality of LLM-produced work rather than genuine improvements in students' learning $^{15}$ . The effect of LLM use on two foundational aspects of learning – understanding and retaining information – remains critically underexplored. Knowledge stored in long-term memory is a fundamental element of cognition, forming the basis of nearly all human activity $^{16}$ . Thus, understanding the effects of LLMs on these foundations is urgently required to guide how such tools are integrated into schools, as policymakers and educators on the front-line are grappling with many unknowns. This study presents one of the first large-scale quantitative investigation into how reading comprehension and retention are affected by the use of LLMs.\n\nReading comprehension is the process of making sense of written materials resulting in a mental representation of the material<sup>17</sup>. Models of reading comprehension, such as the Construction-Integration (CI) model<sup>18</sup>, highlight that readers need to understand a text at several levels: the surface structure (words and their syntactic relations), the textbase (propositions, which generally represent one full idea), and the situation model (inferences about the text)<sup>17</sup>. This multi-level structure is supported by neuroimaging studies<sup>19;20;21;22;16</sup>. The ability to make inferences is a key aspect of comprehension. Usually, two types of inferences are distinguished: text-based bridging inferences involve connecting information from different text locations (e.g., the current sentence with a previous sentence) and knowledge-based inferences involve connecting information in the text with prior knowledge<sup>17</sup>. A reader's ultimate comprehension of a text depends on complex interactions between various elements, including factors related to the reader's characteristics (e.g., decoding skills, vocabulary and linguistic knowledge, prior domain knowledge, working memory capacity, inference-making ability, knowledge of reading strategies, motivation, and goals)<sup>23;24;25;26;27</sup>, the text itself (e.g., genre, length, word and sentence complexity, cohesion)<sup>28;29</sup>, and the reading context (e.g., reading for leisure or academic purposes)<sup>30;31</sup>.\n\nReading retention is the process of storing the comprehended content from a text in long-term memory. For learning it is necessary to not just comprehend the text at the time of reading, but also being able to remember what one has read and understood later. Retention is, in part, determined by the level and quality of information processing during encoding (i.e., the initial information acquisition while reading). According to the Levels of Processing framework  $^{32;33}$ , information that is processed deeply and elaborately —through semantic analysis involving meaning, inferences, and implications— can be recalled more readily. Deep processing facilitates the formation of rich, interconnected semantic networks, which provide multiple retrieval cues, and thus enhance the retrieval potential, as well as the construction of a robust schematic framework wherein specific details are meaningfully organised and related  $^{32;34}$ .\n\nThere are several reading strategies and learning activities that can enhance comprehension and retention as outlined by McNamara $^{35}$  and Chi $^{36}$ . Throughout the reading process, monitoring comprehension is particularly crucial, and includes strategies such as generating questions to gauge one's understanding $^{35}$ . Text-focused strategies involve interpreting the meaning of words, sentences and ideas (e.g., paraphrasing, breaking up long and complex sentence into manageable chunks, making bridging inferences to link different concepts) $^{35}$ . Strategies such as paraphrasing, selecting, and repeating are also considered active learning strategies, and these can activate prior knowledge and support the encoding, storing and assimilation of new knowledge $^{36}$ . There\n\nare also several effective reading strategies that go beyond the text (e.g., generating questions, using self-explanations, and using external information sources) $^{35}$ . Such strategies are considered to be constructive as learners generate new ideas and integrate information more deeply through explaining, elaborating, and connecting. This involves cognitive processes such as inferring new knowledge, integrating and organising new and existing knowledge, and repairing faulty knowledge $^{36}$ . Lastly, interactive learning activities involve meaningful dialogue with a partner, including with peers or systems like intelligent tutoring agents $^{36;28}$ . Such interactions can enhance learning by providing scaffoldings, corrective feedback, as well as additional information and new perspectives. Importantly, a dialogue is only considered to be interactive if both partners make substantive contributions $^{36}$ .\n\nThe integration of LLM tools into education raises the crucial question of whether their use could facilitate or undermine such learning strategies while reading. These models offer unprecedented flexibility in generating explanations, providing diverse perspectives, responding to complex questions in real-time, and adapting to individual learners' needs<sup>37;38</sup>. By serving as an external knowledge resource that extends beyond learners' personal knowledge and skills, LLMs can potentially enhance students' understanding and engagement with educational materials<sup>39;40;10;41</sup>. Furthermore, LLMs' ability to provide immediate clarifications and simplify complex concepts may help reduce cognitive load<sup>42;43</sup>. Thus, LLMs may be particularly useful in helping learners build understanding at multiple levels: from surface-level text comprehension and identification of key ideas, to deeper text-base representation of meanings, and ultimately to a comprehensive mental representation at the situation-model level of comprehension.\n\nHowever, over-use of LLMs could lead to shallow processing, where learners passively receive information without actively engaging in deep cognitive processing or critical thinking $^{44;36;45;46;47}$ . This superficial engagement could hinder the development of comprehensive mental models, negatively affecting comprehension and long-term retention $^{33;48}$ . When learners depend excessively on LLMs for answers and explanations, they may be less inclined to employ self-explanation and elaboration strategies that are essential for comprehension and meaningful learning $^{35;49;42}$ . While LLMs can make information readily accessible, this accessibility needs to be leveraged in ways that promote, rather than substitute for, the deep cognitive processing necessary for knowledge consolidation and learning $^{50;51}$ .\n\nIn order to assess the effectiveness of using LLMs as a learning tool for reading comprehension and retention, we compared it to a widely used learning activity that can facilitate many active and constructive strategies – note-taking. It is one of the most common and widely used learning activities and has been found to be an effective aid to learning while reading $^{52;53}$ . Note-taking can stimulate active processing of information and encourage the integration of new material with prior knowledge, thereby aiding comprehension as well as creating retrieval cues that aid later recall $^{52;54}$ . The impact of note-taking appears to vary depending on the depth of cognitive processing involved. It could focus readers on shallower processing, because readers might pay more attention to the surface structure and textbase but it could also enhance the situation-model by encouraging elaboration and better mental organisation $^{55;56;57}$ . Kobayashi's $^{52}$  meta-analysis supports the former as it found relatively small effects for higher-order performance tests, suggesting that the generative value of note-taking may be limited and highly dependent on the quality of the notes taken (whether they are verbatim or generative). We also compared the effectiveness of using an LLM on its own with using an LLM in conjunction with note-taking, given that it might be useful to combine the activities of querying LLMs and taking notes to facilitate learning. The two activities could potentially have complementary effects on reading comprehension and retention by drawing on their respective strengths. However, there might also be a risk of dividing attention in a way that renders both activities less effective.\n\nTo examine whether LLMs can be used as a tool to support the fundamental learning processes of reading comprehension and retention, we conducted a large-scale, pre-registered, randomised\n\ncontrolled experiment with within- and between-participant design elements. The study involved 405 secondary school students, aged 14-15 years, and took place in seven schools in England (UK). The experiment consisted of a learning session and a test session, which were three days apart. In the learning session, each student was tasked with understanding and learning two text passages on a different history topic (Apartheid in South Africa and the Cuban Missile Crisis), each by using a different learning activity (learning condition) drawing on evidence-based strategies. Students were not informed that they would be tested on the passages. They were randomly assigned to one of two groups. Group 1 was exposed to conditions referred to as \"LLM\" (i.e., using an LLM to understand and learn a text) and \"Notes\" (i.e., taking notes to understand and learn a text) and Group 2 was exposed to conditions referred to as \"LLM\" and \"LLM+Notes\" (i.e., using an LLM alongside note-taking to understand and learn a text). Both learning condition and text order were randomised. The LLM functionality in the learning session was provided by a private Azure-hosted instance of OpenAI's GPT-3.5 turbo model. After each learning task, students responded to a survey about their learning experience, with both quantitative and qualitative questions.\n\nIn the test session, students completed a range of questions assessing different levels of comprehension and retention. Specifically, we assessed their literal retention, comprehension, and free recall. For each passage, literal retention (i.e., lower-level retention) was measured through eight short response (cued recall) and ten multiple choice (recognition) questions assessing literal information which did not require any knowledge-based inferences, and no or only minimal text-based (bridging) inferences. Comprehension (i.e., higher-level retention) was measured through three open response questions requiring bridging inferences to connect information from several different text locations as well as knowledge-based inferences. Free recall was assessed through one open response question for each text, asking students to write down everything they remembered, and thus measuring how much students retained and understood without any cueing.\n\nOur primary aim was to quantify the impact of using an LLM on students' reading comprehension and retention. We made the choice not to have a \"reading-only\" control condition both because it would limit participant fatigue in responding to conditions, and on the basis that any engagement with the text beyond passive reading is likely going to lead to improved learning outcomes $^{35;36}$ , setting the bar for LLM use comparatively low. Instead, we decided to compare it against the common, evidence-based learning activity of note-taking. We also explored students' learning experiences when engaging in the different learning activities, including which activity they preferred and why, as well as different \"archetypes\" of prompting behaviour that shed light on the learning outcomes. The results offer valuable insights for stakeholders and policy makers of the global education landscape.\n\n# Results\n\nOur study investigated the effects of using an LLM on student learning outcomes compared to traditional note-taking in a sample of 344 students (after applying pre-registered exclusion criteria, see Methods for more information). Group 1 (LLM vs Notes conditions) had a final sample of 184 students and Group 2 (LLM vs LLM+Notes conditions) of 160 students. Among the students there were slightly more males than females, most were English native speakers, a small number of students  $(5.2\\%)$  received free school meals indicating socioeconomic disadvantage, and about half were taking History GCSEs (see Supplementary Table 3 for all student characteristics). Both groups showed similar prior familiarity with the three learning conditions (LLM, Notes, LLM+Notes). About half of the students regularly took notes and most reported limited prior use of LLM for learning (see Supplementary Table 4 for detailed frequencies).\n\n# Learning outcomes\n\nWe compared the impact of LLM (reference condition, used by all students) to the impact of Notes (used by students in Group 1) and LLM+Notes (used by students in Group 2) on students' literal retention, comprehension, and free recall. Traditional note-taking led to the best performance across all measures, followed by LLM+Notes, while using LLM alone resulted in the lowest scores (see Supplementary Table 5 for descriptive statistics).\n\nLinear mixed-effects models confirmed significant differences across the conditions (see Figure 1, see Supplementary Table 6 for all model coefficients, confidence intervals and effect sizes).\n\nFor literal retention, we found significant main effects for both Notes ( $\\beta = 1.92$ ,  $p < 0.001$ , 95% CI [1.42, 2.42]) and LLM+Notes ( $\\beta = 0.57$ ,  $p = 0.040$ , 95% CI [0.03, 1.11]), indicating that students performed better with Notes compared to LLM and better with LLM+Notes compared to LLM.\n\nFor comprehension, we again found significant main effects for both Notes ( $\\beta = 0.95$ ,  $p < 0.001$ ,  $95\\%$  CI [0.62, 1.28]) and LLM+Notes ( $\\beta = 0.35$ ,  $p = 0.049$ ,  $95\\%$  CI [0.00, 0.70]), where students had better performance with Notes compared to LLM and with LLM+Notes compared to LLM.\n\nFor free recall, we found a significant main effect for Notes ( $\\beta = 1.02$ ,  $p = 0.018$ , 95% CI [0.18, 1.86]) but not for LLM+Notes ( $\\beta = -0.08$ ,  $p = 0.855$ , 95% CI [-0.98, 0.81]). Thus, students showed better performance with Notes compared to LLM but there was no significant difference between LLM+Notes compared to LLM. Given the non-normal distribution of free recall scores, we also conducted non-parametric versions of these tests as a robustness check, detailed in the Methods section, which corroborated these findings.\n\nThese results suggest that both note-taking conditions (either alone or with LLM) showed improved learning compared to using LLM on its own. However, the benefit of note-taking was seen across all different measures of learning, whereas the benefit of LLM+Notes was seen for literal retention and comprehension but not for free recall.\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/f9c6b97ec629fd3a5afd56314cf1273a7a23652bdf7aa8dcc448b1d899f826ce.jpg)  \nFigure 1: Distribution of test performance by condition and group for Comprehension (left, max 12 points; Notes:  $M = 4.89$ ,  $SD = 2.52$ ; LLM+Notes:  $M = 4.11$ ,  $SD = 2.65$ ; LLM Group 1:  $M = 4.00$ ,  $SD = 2.44$ ; LLM Group 2:  $M = 3.80$ ,  $SD = 2.47$ ), *Literal retention (middle, max 20 points; Notes:  $M = 10.8$ ,  $SD = 4.29$ ; LLM+Notes:  $M = 9.68$ ,  $SD = 4.83$ ; LLM Group 1:  $M = 8.83$ ,  $SD = 3.96$ ; LLM Group 2:  $M = 8.95$ ,  $SD = 4.29$ ) and *Free recall (right, max 50 points; Notes:  $M = 5.36$ ,  $SD = 5.49$ ; LLM Group 1:  $M = 4.32$ ,  $SD = 4.15$ ; LLM Group 2:  $M = 4.32$ ,  $SD = 4.63$ ; LLM+Notes:  $M = 4.20$ ,  $SD = 5.07$ ). Mean values are indicated by the two large circles within each facet, whereas the smaller points show individual students scores. Error bars indicate one standard error above and below the mean. Group 1 is shown on the left facet of each subfigure, comparing LLM (red) and Notes (blue). Group 2 is on the right facet of each plot, comparing LLM (red) and LLM+Notes (green).\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/41488ca1a6c3943e2825383542041eb80af29edf193795e1cd6d1ef164a3df0a.jpg)\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/cfcb380db33b073aea66229200e4a4b9ce36c4e9d8d6f6b463a22debcaf33262.jpg)\n\n# Behavioural engagement\n\nBehavioural engagement with the LLM and note-taking was quantified by the average number of queries made to the LLM, the average number of words written in students' notes as well as time spent on task. Access to notes alongside the LLM reduced students' query frequency compared to LLM-only conditions (from 9.21 to 6.02 queries in Group 2). While students wrote a similar number of words in their notepad in both Notes and LLM+Notes conditions (around 100 words), a concerning proportion  $(25.63\\%)$  heavily copied from LLM outputs into their notes, with some  $(16.25\\%)$  showing nearly complete copying (more than  $90\\%$  overlap of trigrams between LLM output and notes). Additionally, students spent significantly less time on task when using only the LLM compared to conditions involving note-taking (differences of 0.80 and 1.54 minutes for Groups 1 and 2, respectively), suggesting deeper engagement when note-taking was involved. See Supplementary Table 7 for a full description of behavioural measures.\n\n# Prompting behaviour\n\nIn order to understand how students engaged with the LLM, we performed a qualitative analysis of all prompts  $(n = 4,929)$  using a hierarchical coding scheme where specific prompts were nested within overarching prompt types. Each prompt could be assigned to multiple codes. We identified four behavioural archetypes of how students worked with the LLM in relation to the task as well as two additional overarching prompt types that were not directly related to the task (see Figure 2 for the distribution of prompt types across each LLM session). For exact frequency counts of overarching prompt-types, see Supplementary Table 21 and for specific prompt types see Supplementary Table 22.\n\nThe most frequent archetype was seeking additional information and deeper understanding (2,265 prompts, as shown in the purple bars in Figure 2). The vast majority of students  $(90\\%)$\n\nused such a prompt type at least once, about  $40\\%$  used this as their first prompt, and  $60\\%$  as their most common prompt type (see Figure 3). These prompts primarily comprised requests for elaboration (1,479 instances) and general background information (514 instances). Examples include \"how are people today affected by the apatheid\" and \"why did it take so long to free nelson mandela\".\n\nInformation condensation (749 prompts, as shown in the teal bars in Figure 2) emerged as the second most common archetype, with  $27\\%$  of students using it as their first prompt, typically requesting summaries or key ideas, such as \"What are five key points from the entire text?\" or \"create a timeline of all the events\". The third archetype, basic understanding of the text (615 prompts, green bars in Figure 2), was used by  $70\\%$  of students at least once, mainly for definitions and content simplifications such as \"What is a sanction?\" and \"explain communist\". A fourth archetype, requesting direct study and memory help, was used infrequently (39 instances, red bars in Figure 2) despite students receiving no explicit instructions for such use. These ranged from asking the LLM to generate a quiz (\"ask me 4 questions about the text and tell me if i get them right after my next reply\") to pneumonic devices (\"create me a mnemonic device on the cuban missile crisis\").\n\nBeyond these archetypes, 760 prompts focused on interacting with the LLM rather than (or in addition to) text content (blue bars in Figure 2), primarily requesting specific formats or response improvements. Examples include \"can you put this into bullet points?\" and \"shorten the aftermath into 1 sentence\". Notably, only six prompts questioned the LLM's reliability. Finally, about  $10\\%$  of all interactions (501 prompts, brown bars in Figure 2) were off-topic or irrelevant (e.g., \"what is the meaning to life\" and \"Tell me about Harry Potter\"), showing that a small but potentially relevant prompt proportion was not task-focused, potentially due to low task motivation or boredom.\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/d626ae4afddf164784c2957f218467f2fcf897ba4e897712255c0f3e6a5a4074.jpg)  \nFigure 2: Distribution of prompt types across LLM sessions for different conditions and students. Each panel represents a specific combination of condition (LLM-only or LLM+Notes) and text passage (Apartheid in South Africa or Cuban Missile Crisis). Each bar shows the number of prompts within each type for an individual LLM session, with sessions sorted in descending order by the total number of prompts and ties broken by the number of prompts within each type.\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/b9a2f4d9cc9579f597bbeeb013a133f3f56b5f7e78028c7f54b3caea7c03b5ee.jpg)  \nFigure 3: Distribution of student prompts across different types, showing the percentage of students who used the prompt type at least once (blue), as their most common prompt (magenta), and as their first prompt (green). Prompt types are arranged by overall frequency.\n\n# Learning experiences and perceptions\n\nIn addition to analysing students' behavioural engagement, we asked them about their learning experiences and perceptions of the different conditions. The quantitative results are summarised in Figure 4, with details of statistical tests in Supplementary Table 15. We used an adjusted p-value threshold of  $0.05 / 18 = 0.002$  to gauge statistical significance based on the Bonferroni correction to account for multiple comparisons  $(n = 18)$ .\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/c4c266d6421d905ef8a8bd42b99b86f7e33f41d2190d0d2c236b0c94e604e5c3.jpg)\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/23e6863e1c87df8e23a0c590c8e6744c9f75059bb10033cad565cccdca9a1e8e.jpg)\n\nFigure 4: Differences in learning experiences and perceptions by group and condition. The top panel displays perceived test performance on a 0-100 scale, while the middle and bottom panels show ratings for measures with positive and negative valences, respectively, on a 1-5 scale. Each point represents the mean rating for a condition, with error bars indicating one standard error above and below the mean.  \n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/2f7b3c6eb55edba33c7498db63ee23202e70938030ee28f26ed778c685bd2de3.jpg)  \nCondition  $\\rightarrow$  LLM only  $\\rightarrow$  LLM+Notes  $\\rightarrow$  Notes only\n\nContrary to actual learning outcomes, Group 1 students found the LLM more helpful, easier to use, and more enjoyable than note-taking, while reporting less effort investment. Group 2 showed similar experiences between conditions, except perceiving the LLM-only condition as less difficult than LLM+Notes. Students perceived task performance similar across conditions during learning. Following the test, students in both groups accurately reported their perceived test performance to be lower in the LLM-only conditions than in the Notes and LLM+Notes conditions.\n\nThese findings suggest that while the LLM-only condition was less effective for learning, it provided motivational benefits - particularly evident in Group 1's preferences. Importantly, these motivational benefits were maintained when combining LLM use with note-taking in Group 2.\n\n# Activity preferences\n\nStudents were asked to indicate their preferred learning activities and explain their preferences through an open response (see Table 1). In Group 1, most students preferred the LLM activity over traditional note-taking. Those students cited enhanced understanding, the LLM's ability to answer questions, and ease of the activity as their main reasons. Students favouring traditional notetaking emphasised benefits for understanding, the importance of self-generated work, and improved\n\nmemory retention. In Group 2, a substantial majority preferred the combined activity over using the LLM alone. Students preferring the combined activity noted the complementary benefits of both approaches, enhanced memory retention, and improved organisation. Those favouring the LLM-only activity emphasised its efficiency, particularly appreciating that the LLM did the work for them. This reveals an underlying tension between efficiency and depth of processing - while the LLM-only activity was perceived as more efficient, conditions involving note-taking demonstrated superior learning outcomes through deeper engagement and better retention.\n\nTable 1: Learning activity preferences and reasons by group  \n\n<table><tr><td>Activity preference and reasons</td><td>Count</td><td>Percentage</td></tr><tr><td colspan=\"3\">Group 1: LLM vs Notes</td></tr><tr><td>LLM over Notes</td><td>89</td><td>42.0</td></tr><tr><td>Notes over LLM</td><td>57</td><td>26.9</td></tr><tr><td>No preference</td><td>48</td><td>22.6</td></tr><tr><td>Not sure</td><td>18</td><td>8.5</td></tr><tr><td colspan=\"3\">Group 2: LLM vs LLM+Notes</td></tr><tr><td>LLM over LLM+Notes</td><td>32</td><td>16.2</td></tr><tr><td>LLM+Notes over LLM</td><td>100</td><td>50.5</td></tr><tr><td>No preference</td><td>48</td><td>24.2</td></tr><tr><td>Not sure</td><td>18</td><td>9.1</td></tr><tr><td colspan=\"3\">Reasons for LLM over Notes preference</td></tr><tr><td>Helps understanding</td><td>34</td><td>21.9</td></tr><tr><td>Answers questions</td><td>23</td><td>14.8</td></tr><tr><td>Easy to use</td><td>22</td><td>14.2</td></tr><tr><td>Quick to use</td><td>18</td><td>11.6</td></tr><tr><td>Provides background</td><td>18</td><td>11.6</td></tr><tr><td>Summarises and simplifies</td><td>17</td><td>11.0</td></tr><tr><td>Engaging</td><td>10</td><td>6.5</td></tr><tr><td>Interactive</td><td>8</td><td>5.2</td></tr><tr><td>Helps remember</td><td>4</td><td>2.6</td></tr><tr><td colspan=\"3\">Reasons for Notes over LLM preference</td></tr><tr><td>Helps understanding</td><td>22</td><td>21.4</td></tr><tr><td>Own work</td><td>21</td><td>20.4</td></tr><tr><td>Aids memory</td><td>18</td><td>17.5</td></tr><tr><td>Helps processing</td><td>8</td><td>7.8</td></tr><tr><td>Unclear usage of LLM</td><td>7</td><td>6.8</td></tr><tr><td>Active learning</td><td>6</td><td>5.8</td></tr><tr><td>LLM distracts</td><td>6</td><td>5.8</td></tr><tr><td>Revisitable</td><td>5</td><td>4.9</td></tr><tr><td>Easier</td><td>4</td><td>3.9</td></tr><tr><td>Helps organisation</td><td>4</td><td>3.9</td></tr><tr><td colspan=\"3\">Reasons for LLM over LLM+Notes preference</td></tr><tr><td>Does the work for you</td><td>15</td><td>50.0</td></tr><tr><td>Notes not necessary</td><td>5</td><td>16.7</td></tr><tr><td>Quicker</td><td>4</td><td>13.3</td></tr><tr><td>More time for questions</td><td>4</td><td>13.3</td></tr><tr><td colspan=\"3\">Reasons for LLM+Notes over LLM preference</td></tr><tr><td>Best of both worlds</td><td>35</td><td>23.2</td></tr><tr><td>Helps remember</td><td>27</td><td>17.9</td></tr><tr><td>Helps organisation</td><td>24</td><td>15.9</td></tr><tr><td>Own work</td><td>21</td><td>13.9</td></tr><tr><td>Helps understanding</td><td>16</td><td>10.6</td></tr><tr><td>More helpful and easier</td><td>12</td><td>7.9</td></tr><tr><td>Helps process LLM output</td><td>6</td><td>4.0</td></tr><tr><td>More fun</td><td>4</td><td>2.6</td></tr><tr><td>LLM errors</td><td>3</td><td>2.0</td></tr></table>\n\nNote: This table only includes reasons that have been mentioned by at least three students.\n\n# Future use\n\nAt the end of the learning session, students reported their intentions for future use of each activity. In Group 1, the majority of students  $(64.4\\%)$  indicated they would use LLMs in the future, with only  $7.3\\%$  negating and  $28.2\\%$  being unsure. A smaller majority of students  $(55.3\\%)$  planned to take notes in the future, and  $10.6\\%$  did not think they would do so, while  $34.1\\%$  were uncertain. In Group 2, the majority of students  $(59.5\\%)$  intended to use LLMs in the future,  $10.4\\%$  did not and  $30.1\\%$  were unsure. A similar majority  $(58.5\\%)$  planned to use the combined LLM+Notes activity in the future, while  $14.6\\%$  did not and  $26.8\\%$  were unsure.\n\n# Discussion\n\nThis study provides new insights into how the use of LLMs compares to and interacts with traditional evidence-based practices (specifically note-taking) to support students' reading comprehension, retention, and engagement. It offers important perspectives on the cognitive and motivational dynamics underlying human-AI interactions in learning, and how these interactions influence educational outcomes and perceptions. In particular, it suggests that LLM use and more traditional note-taking have complementary roles in the learning process.\n\nIn this study, we found that note-taking—whether done alone or alongside LLM usage—produced higher comprehension and retention scores compared to using an LLM alone, underscoring the importance and effectiveness of traditional active learning strategies. At the same time, students generally used LLMs constructively and perceived them as more \"helpful\" and preferable to notetaking. How can we reconcile these seemingly conflicting results?\n\nOne part of the answer may be that students simply have a limited metacognitive understanding of what is in fact helpful for their own learning $^{58;59;60}$ , specifically in the context of GenAI $^{61}$ . In particular, they may underweight the importance of the \"desirable difficulties\" induced by activities such as note-taking $^{48}$ . Note-taking requires active processing of information, such as identifying important information, paraphrasing and summarising $^{52}$ . While these tasks demand cognitive effort and may not be inherently enjoyable, past research shows that the learning potential increases with the level of required cognitive engagement $^{62}$ . Having an LLM do some of the work of summarising a passage or explaining a concept may feel more enjoyable and efficient, but can reduce the cognitive engagement necessary for deep comprehension and long-term retention. Similar effects on LLM use on learners' affective-motivational state and mental effort were found in Deng et al.'s meta-analysis $^{15}$ . Additionally, LLMs may sometimes provide learners with distractions that are interesting, but that compete with the primary task at hand.\n\nAt the same time, our exploratory analysis of student prompts suggests that another part of the answer lies in the unique benefits LLMs provide, which may have been genuinely helpful beyond what our primary analyses captured. The vast majority of LLM use was constructive rather than distracting or reductive, with students seeking additional information and deeper understanding. Students demonstrated remarkable curiosity, asking sophisticated questions that extended beyond the immediate text. For example, in a passage about apartheid in South Africa that briefly mentions Nelson Mandela's journey from prisoner to president, one student asked, \"What was Mandela's life story?\" Similarly, in a passage on the Cuban Missile Crisis that assumes some background knowledge of the Cold War, another student asked, \"Why was America afraid of communism?\" These explorations represent a different kind of active learning opportunity that may not result from note-taking alone, underscoring the LLM's potential to expand intellectual horizons. That said, these deeper inquiries may have involved tradeoffs: they could have competed with processing the core information in the passage, reducing performance on tested items, but they likely also enhanced learning in ways not captured by our tests, which focused only on the explicit and implied content within the texts.\n\nTaken together, our findings demonstrate the value of combining LLM use and note-taking, which was not only more effective than LLM use alone but also students' preferred activity. This raises the opportunity and challenge of how to combine traditional evidence-based strategies like note-taking with the unique benefits offered by LLMs. Rather than viewing these as competing alternatives, we should think of them as complements that when thoughtfully integrated can enhance learning outcomes in ways that neither can achieve alone. A key to doing so is leveraging input from educators and researchers in the design and use of new LLM-based tools for learning, as has been key for past hybridisation of traditional and digital approaches $^{63;64}$ .\n\nOur work suggests several such directions. First and most easily would be to separate LLM use from note-taking. Under this model, students would first independently read a text, and then interact with an LLM to further clarify and explore its content. Following this they would take notes independently, without the ability to simply copy and paste output from the LLM. This would prevent students from taking shortcuts we have observed in this study, instead encouraging them to synthesise and internalise information themselves. This is a small but likely meaningful design choice that was not obvious to us a priori, but that emerged through our work and could be tested in future research.\n\nSecond, educators could actively train and guide students to use LLMs in ways that align with active learning strategies, such as asking targeted questions to clarify specific misunderstandings, engage in critical thinking, and integrate information, without overloading them with excessive information or reducing cognitive processing $^{36;35}$ . Likewise, educators could discourage the passive consumption of automatic summaries and explanations. This aligns with the conceptualisation of AI tools as \"thought partners\" that support existing human cognitive processes rather than disrupting them $^{9}$ . Going beyond learning activities, by guiding students to use LLMs more effectively, educators will help students develop their metacognitive skills more generally, which will make them better prepared to use these technologies in the long-term. Furthermore, software could be configured to support these goals by limiting distracting behaviour and encouraging productive use (plausibly by capturing data and using the LLM to provide feedback or nudges to the student based on their LLM interactions).\n\nAnd third, educators could leverage insights from students' interactions with the LLM to better understand what concepts they are struggling with or what they are curious about. This could be done at an individual level but could also be conducted collectively for an entire class, possibly through the use of automated tools that collect and analyse student interactions and then provide data back to the educational instructors in a privacy-protecting way to surface insights. The results could be used to tailor future lessons, activities and group discussions. For example, through analysing the prompts in our experiments, it becomes clear that students were curious about the tenets of communism and why they provoked such fear and opposition in the U.S.\n\nThis research makes several contributions to the growing field of research examining the impact of LLMs in education. While much prior work has focused on the impact of LLMs on task performance and efficiency, the present study investigated aspects that are more fundamental to learning and cognition. In addition, it examined the effects of LLMs within a large sample of secondary school students coming from different school types, rather than amongst students in higher education, who have received much more research attention thus far<sup>15</sup> Such populations can be difficult to reach, especially when several study sessions are involved. In designing the study, we aimed to be authentic to students' experiences in school, ensuring the findings hold practical significance. In particular, we used texts that reflect the topics and difficulty that such students might come across in the classroom, and we compared the effects of LLM use with a learning activity that is, at least until now, commonly used.\n\nOne limitation of the present study is that students received no in-depth training for the different learning activities. While we provided instructions and a demonstration video for how to interact with the LLM and take notes, students did not have an opportunity to practice. This might have\n\nbeen a particular disadvantage for the LLM conditions because students were less familiar with using LLMs than note-taking and might thus not have leveraged the activity as effectively. In addition, the study might have benefited from a baseline or passive reading condition to ascertain whether using the LLM to understand and learn a text provides benefits above passive reading (that is, to gauge its effectiveness per se). Another limitation is that we were practically constrained to a small set of retention and comprehension questions relative to the vast number of potential questions that could have been asked, although we sampled a wide range of content. Thus, we could have underestimated students' learning overall, with the exception of the free recall questions. Furthermore, the study was limited to a single, isolated activity outside of the context of normal use throughout an entire course of study. It is possible that repeated use or use in other settings (e.g., in everyday classrooms or independently for homework, unsupervised) could yield different results. Lastly, while we consider it a strength that we used texts that were appropriate to the student sample, it is possible that LLM usage might be more beneficial for texts that students struggle with, as indicated by a few students who stated they did not know what to ask the LLM. Hence, exploring the effects of LLM use for texts that go beyond students' current capabilities could further expand our understanding of potential applications.\n\nIt is crucial for future research to explore which ways of interacting with LLMs most effectively enhance learning outcomes. Future research must also explore the long-term consequences of LLM integration in learning contexts, particularly its impact on reading skills, independent problem-solving, and metacognition. Additionally, it will become vital to understand how these tools influence societal perceptions of effort, expertise, and achievement. The evolving role of LLMs and generative AI technology may shift the definition of essential expertise and change the landscape of necessary competencies across various fields<sup>8</sup>. Moving forward, it is vital for educators and society to identify which core skills remain indispensable in this new environment and to develop pedagogical strategies that ensure their preservation and growth<sup>9</sup>. This research marks only the beginning of understanding how to effectively use LLMs to complement existing activities and tools while maintaining students' cognitive engagement.\n\nIn summary, this study provides one of the first large-scale quantitative evidence on the effects of LLMs on reading comprehension and retention. Our findings reaffirm the importance of traditional strategies like note-taking, which foster deep cognitive engagement and strong learning outcomes. At the same time, LLMs introduce new possibilities for learning—offering opportunities to clarify, explore, and contextualise material—but these tools must be used with proper guidance aimed at enhancing, rather than bypassing, active learning. Rather than viewing these tools as a disruption to be resisted, educators and researchers have an opportunity to proactively shape their use to maximise learning potential. By doing so, we can prepare students to thrive in an AI-integrated world while preserving the focus, depth, and curiosity that define meaningful education.\n\n# Materials and Methods\n\nThis study comprised two stages: a piloting stage and a main study. The purpose of the piloting stage was to test the tasks and proposed procedures in the school context and amend them as appropriate. The methods and findings reported here are a part of the main study, which took place between March and July 2024.\n\n# Participants\n\nParticipants were 405 Year 10 students (aged 14-15 years) from seven secondary schools in England. Based on our exclusion criteria (see Supplementary Section 1.1), we retained 344 students for analysis. We made efforts to recruit 600 students but were unable to do so as we could not find enough schools before the start of the summer holidays. Recruitment methods included emailing\n\nschool headteachers in several counties and asking participating schools to contact other schools. The final school sample included three non-selective state schools, two grammar schools (one all girls, one all boys) and two independent schools, located in three different counties.\n\nOnce a school agreed to participate, all Year 10 students were invited to take part through the school's project lead. Information sheets were shared with students and their parents/guardians, after which both were asked to provide their informed written consent using an online Microsoft form. This study was conducted in line with the British Educational Research Association's  $^{65}$  ethical guidelines. Ethical approval was provided by the research ethics committees of the researchers' institutions.\n\n# Experimental design and procedure\n\nThe study was a pre-registered randomised controlled experiment with within- and between-participant design elements, as illustrated in Figure 5. Conducted over two sessions spaced three days apart, the experiment consisted of a learning session followed by a test session.\n\nLearning Session: In the learning session, students were tasked with understanding and learning two text passages on different history topics (Passage A and Passage B). Each passage was studied using a specific active learning activity (condition). The three conditions were:\n\n- LLM: Students were asked to use an LLM chatbot we created to help them understand and learn the passage.  \n- Notes: Students were asked to take notes to help them understand and learn the passage.  \n- LLM+Notes: Students were asked to use our LLM chatbot as well as take notes to help them understand and learn the passage.\n\nStudents were randomly assigned to one of two groups:\n\n- Group 1: Exposed to the LLM and Notes conditions.  \n- Group 2: Exposed to the LLM and LLM+Notes conditions.\n\nRandomisation assigned 184 students to Group 1 (53.5%) and 160 to Group 2 (46.5%). The order of conditions and passages was randomised. During this session, students also completed survey questions about their learning experiences.\n\nTest Session: In the test session, students answered comprehension and retention questions about the two passages (with passage order randomised) and completed survey questions regarding their general characteristics.\n\nTiming: Students spent a mean of approximately 35 minutes on the learning session and 30 minutes on the test session.\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/b21bdd2e3d49ceb66072818fc8bb684298786b88b09834ba3fb45c8e408c61ce.jpg)  \nRandomised order of group, condition and passage\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/b9b81a2d9ef90ec106dc670f00146ef1702cc9c8dc0607a32f8ae05c0131d727.jpg)  \nRandomised order of passage  \nFigure 5: Study design illustrating the activities and their order during Session 1 and 2.\n\n# Setup and system\n\nBoth sessions took place in schools during regular school hours. Groups of students participated simultaneously in classrooms, with each student completing the sessions on an individual laptop or computer. At the start of each session, the experimenter or teacher read out a script with introductory instructions. They also monitored students during the entire session and answered their questions.\n\nThe experiment was a web app hosted on github.com that students accessed via the browser. For the LLM functionality in Session 1, the app made backend calls to private Azure Functions that accessed an Azure-hosted instance of OpenAI's GPT-3.5 turbo model. The LLM interactions were limited to Azure and did not go back to OpenAI. Participants could issue a maximum of 20 prompts. The LLM was customised with a meta-prompt that was not visible to students (\"You are an AI chat bot that helps students read and comprehend the following passage: <text> Students can use this tool to define unfamiliar words, explain concepts, or summarise key points of the passage.\"). Figure 6 illustrates the task screen for the LLM+Notes condition. For the Notes and\n\n# Apartheid in South Africa\n\nIn 1910, four British colonies joined to create the \"Union of South Africa.\" The Union was part of the British Empire, and later became the Republic of South Africa that we know today. After World War II, many countries that were controlled by Western nations, including South Africa, wanted independence. The South African government wanted to break free from the British Empire. However, for Black South Africans, the main struggle was against the discrimination by White South Africans who were of British and Dutch descent.\n\nIn 1948, the National Party came to power. This new government formalised the discrimination and racial separation in a system called 'apartheid'. It lasted for over 40 years, during which many unfair laws were passed. For example, every citizen had to be classified by their skin colour, people of different skin colours were not allowed to marry each other, and people were forced to live in specific areas based on their skin colour. More than 3.5 million people of colour were forced to leave their homes, and many were pushed into poverty.\n\nAnti-apartheid groups like the African National Congress (ANC) at first only used peaceful protest. This changed after the Sharpeville Massacre in 1960 when police killed black people that were peacefully protesting outside the police station. Activists now also turned to violence, such as sabotage and attacks on police and military. In response, the government banned anti-apartheid groups. In the decades that followed, anti-apartheid activists faced arrests, prison, and even execution. For example, Nelson Mandela, the leader of the ANC, was in prison for 27 years.\n\nMore and more countries criticised apartheid and used sanctions and boycotts against South Africa. Horrific events at the Soweeto Youth Uprising in 1976 also gained global attention. Black students peacefully protested a new law that forced them to study in Afrikaans, the language of the Dutch colonisers. The police killed more than 100 teenagers. Growing pushback from outside and within South Africa put pressure on the government. Finally, Nelson Mandela was freed from prison, which started negotiations to end apartheid. The elections in 1994 granted all South African citizens, including Black citizens, voting rights. As a result, Mandela became the first democratically elected president. This marked the end of apartheid. However, even today, many Black South Africans still feel the negative effects of apartheid.\n\n# AI Chatbot ②\n\nYou can ask 20 more questions.\n\n# Notepad\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/34bb33463af6cbdc665c50ca9aa10ad1e76195cb893c9f0d2effdf2c955d4149.jpg)  \nFigure 6: Example task screen for the LLM+Notes condition.\n\nWhen you are finished with the task,\n\nclick continue.\n\nCONTINUE (12:29)\n\n#\n\nthe LLM conditions, only the notepad or chatbot was displayed, respectively.\n\n# Learning task and materials (Session 1)\n\nIn the learning session, students read two passages on a history topic, each with a different learning activity. They were asked to understand and learn the content of the texts as best as they could. Notably, students had not been told that they would be tested on the materials. For each task, they first received instructions (see Supplementary Section 2.6 about the value of active reading, what it involves, and how the given reading activity might support active reading). They then received more detailed task instructions describing specific strategies, which were followed by a video demonstration of the task and interface. The suggested strategies were based on the active reading and comprehension literature[29;35;36;66]. The content and wording of the instructions for the three conditions were kept as similar as possible. Once the task started, students needed to remain on the task page for 10 (minimum) to 15 (maximum) minutes.\n\nEach student read two expository text passages. Each passage covered a single topic which was included in at least one of the UK exam boards' GCSE History specifications: Apartheid in South Africa (Passage A) and The Cuban Missile Crisis (Passage B). The passages were adapted from two OpenStax textbooks (World History, Volume 2: from 1400; U.S. History). Substantial adaptations were made to ensure that the content and language difficulty as well as text features were comparable and appropriate for Year 10 students. Passages A and B had four paragraphs each and were nearly equal length (386 and 385 words), average word length (5.3 and 4.8 characters), word complexity (i.e., the average position of the words in the 10,000 most frequent English words list, 1986 and 1927), number of sentences (both 26) and CEFR level (both C1 – upper intermediate).\n\nTable 2: Question types and scoring for literal retention, comprehension, and free recall  \n\n<table><tr><td>Outcome</td><td>Question Type (N Questions per Text)</td><td>Scoring</td><td>Maximum score</td></tr><tr><td rowspan=\"2\">Literal retention</td><td>Short response - Cued recall (8)</td><td>For each literal piece of information:0 - missing, incorrect or irrelevant0.5 - incomplete or partially correct1 - correct</td><td>10</td></tr><tr><td>Multiple choice with four response options - Recognition (10)</td><td>0 - missing or incorrect1 - correct</td><td>10</td></tr><tr><td>Comprehension</td><td>Short response - Cued recall (3)</td><td>For each idea:0 - missing, incorrect or irrelevant0.5 - incomplete or partially correct1 - correct</td><td>12</td></tr><tr><td>Free recall</td><td>Open response (1)</td><td>For each literal piece of information/idea:0 - incorrect or irrelevant0.5 - incomplete or partially correct1 - correct</td><td>50</td></tr></table>\n\nNote: Two of the eight \"Short response - Cued recall\" questions for literal retention are worth two points each.\n\nWe divided each passage into 50 main ideas to ensure comparability and to aid scoring.\n\n# Test task and materials (Session 2)\n\nIn the test session, students were told that they would answer some questions about the passages they read in Session 1 as well as some general questions about the task and themselves. For each passage, there were 22 test questions assessing literal retention, comprehension and free recall. Table2 provides an overview of how the different constructs were assessed. As pre-registered, we used a single literal retention score, which was the sum of the short response and multiple-choice scores. The question order for both passages was free response, comprehension, literal retention (cued recall) and, finally, literal retention (recognition). Students had to spend at least three minutes and a maximum of five minutes on the free-recall questions. Questions were carefully sequenced and separated by screens where needed to avoid that previous questions would provide cues for later questions. Example questions can be found in Supplementary Table 11.\n\nLiteral retention questions required literal recall or recognition of information from the passage to provide a correct response. In order to succeed, students did not need background knowledge beyond understanding the vocabulary used in the passage. They did not need to make any knowledge-based inferences (elaborations), and no or only minimal text-based (bridging) inferences, such as connecting two consecutive sentences. Accordingly, literal retention questions targeted the surface and textbase level of representation.\n\nIn contrast, comprehension questions probed for deeper comprehension as they required students to make bridging inferences to connect information from several different locations in the text. Participants needed to make knowledge-based inferences to earn more points, inferring information that was implied but not explicitly stated. Accordingly, comprehension questions targeted the situation-model level of representation.\n\nThe short response and open response questions were scored by three independent raters who were PhD students in Education and/or Psychology who were blind to condition. They were trained to use a scoring scheme that provided general instructions, rules, and detailed explanations and examples for each question. As part of the training, and to demonstrate consistent and accurate use of the scheme, raters scored responses from 25 students and received feedback. Each rater then independently scored the full set of responses, including the questions for both passages, from approximately 140 students.\n\nTo assess inter-rater reliability, the full set of responses from 35 students (approximately  $10\\%$  of the sample) was scored by all three raters. Reliability was evaluated using the intraclass-correlation coefficient (ICC) with a two-way model<sup>67</sup>. We measured absolute agreement and applied the single\n\nmeasure approach as we ultimately used scores from a single rater for all but the 35 students in the reliability sample. For those students, we used the median of the three ratings in subsequent analyses. The inter-rater reliabilities for the combined cued-recall retention scores (one for Passage A and one for Passage B), the combined comprehension scores, and the free recall scores ranged between .97 and .99, indicating excellent reliability $^{67}$ . The lower bounds of the  $95\\%$  confidence intervals were all above the .90 threshold for excellent reliability (see Supplementary Table 12).\n\n# Survey questions\n\nAll questions and response scales can be found in Supplementary Section 2.9. After each task in Session 1, students were asked to self-report on: the difficulty of the text and their familiarity with, and interest in, the topic; enjoyment, difficulty, and helpfulness of the learning activity, and likelihood of its future use; and the overall interest in the task, effort expenditure, and perceived task performance. Students were also asked to indicate whether they preferred any of the learning activities and why, whether they had ever used AI chatbots and if so, with what frequency, and, lastly, how often they had used these learning activities when reading a text for school.\n\nAfter each test in Session 2, students were asked to rate their perceived test performance. At the end of the session, they were asked to indicate whether they had engaged in any learning related to the two texts in between sessions. Students were also asked to report their gender, their English language status, and whether they were taking GCSE History.\n\nIn addition, Free School Meals (FSM) eligibility data was obtained from schools as a measure of student socioeconomic disadvantage $^{68}$ . This is because eligibility for FSM is typically based on family income and other socioeconomic factors.\n\n# Analytic strategies\n\nWe did not deviate from our pre-registered analyses other than described here. First, we extended analyses to conduct qualitative analyses exploring why students preferred one learning activity over another. Second, while we initially planned to explore interaction effects between learning conditions and Gender, EAL, FSM, History GCSE, and School type, we did not do so given our smaller than planned sample size.\n\nQuantitative analyses were run with Python 3.11 and R 4.4.2. We used a significance level of 0.05 (two-tailed) for all analyses. Effect sizes were estimated using Cohen's d, calculated as the mean difference divided by the standard deviation of paired differences for each variable.\n\n# Estimation of condition effects on text comprehension and retention\n\nMissing data handling There were no missing data on the dependent variables because participants were excluded if they did not complete both tests (see exclusion criteria) and because any missing responses on individual questions were scored as 0 points. Missingness in covariates was minimal and only occurred for the variables Gender, EAL and History GCSE  $(5.23\\%, 1.16\\%$  and  $1.16\\%$ , respectively). Missing data were handled using multiple imputation by chained equations (MICE) using the 'mice' package. Models were fitted on five imputed datasets and the results were pooled for combined estimates.\n\nMixed-effects regression We ran three linear mixed-effects regression models using the 'lme4' package, one for each outcome (i.e., literal retention, comprehension, free recall), where students were modelled as a random effect. Note that we pre-registered the regression for free recall as a secondary analysis but we are reporting it alongside the other outcomes for simplicity. The regression specification was as follows:\n\n$$\n\\begin{array}{l} Y _ {i j} = \\beta_ {0} + \\beta_ {1} \\text {C o n d i t i o n} _ {i j} + \\beta_ {2} \\text {G r o u p} _ {i j} + \\beta_ {3} \\text {S c h o o l} _ {i j} + \\beta_ {4} \\text {T e x t} _ {i j} + \\beta_ {5} \\text {T a k} _ {-} \\text {O r d e r} _ {i j} \\\\ + \\beta_ {6} \\text {T e s t} _ {-} \\text {O r d e r} _ {i j} + \\beta_ {7} \\text {G e n d e r} _ {i j} + \\beta_ {8} \\text {F S M} _ {i j} + \\beta_ {9} \\text {E A L} _ {i j} + \\beta_ {1 0} \\text {H i s t o r y} _ {i j} + u _ {i j} + \\epsilon_ {i j} \\\\ \\end{array}\n$$\n\nWhere:\n\n-  $Y_{ij}$  represents the outcome for student  $i$  in condition  $j$ .  \n-  $\\beta_0$  represents the intercept of the model.  \n-  $\\beta_{1}$  to  $\\beta_{10}$  represent the coefficients for the fixed effects:\n\n- Condition: A categorical variable with three levels (0 = LLM, 1 = Notes, 2 = LLM+Notes).  \n- Group: A binary variable indicating group membership.  \n- School: A categorical variable with seven levels indicating school membership.  \n- Text: A binary variable indicating which text student  $i$  studied in condition  $j$ .  \n- Task order: A binary variable indicating whether student  $i$  did condition  $j$  first or second.  \n- Test order: A binary variable indicating whether the text was tested first or second.  \n- Gender: A categorical variable with four levels (0 = female, 1 = male, 2 = other, 3 = prefer not to say).  \n- FSM: A binary variable indicating whether the student received free school meals or not.  \n- EAL: A categorical variable indicating students' English language status (0 = first language, 1 = bilingual, 2 = other)  \n- History: A binary variable indicating whether or not students take History GCSEs.\n\n-  $u_{ij}$  represents the random intercept for each student.  \n-  $\\epsilon_{ij}$  represents the error term for student  $i$  in condition  $j$ .\n\nAs depicted in Figure 1, free recall scores were non-normally distributed, so we ran additional non-parametric permutation tests. Specifically, we used the 'infer' package in R to conduct paired permutation tests at the student level. These tests compared free recall scores between the LLM and Notes conditions in Group 1, and between the LLM and LLM+Notes conditions in Group 2. For each student, we calculated the difference between their two scores and averaged these differences across students. This test statistic was compared to a null distribution, generated by repeatedly randomising the signs of within-student differences and computing means. The process was repeated across all instances of imputed data, and the results were summarised by taking the median p-value across instances to yield a pooled p-value. Doing so gives similar findings to the mixed effects model: in Group 1 we find a significant difference for free recall between the Notes and LLM conditions  $(p = 0.02)$ , but do not find evidence for a significant difference in free recall for Group 2 between the LLM+Notes vs. LLM conditions  $(p = 0.80)$ .\n\n# Qualitative exploration of student prompts\n\nTo provide potential explanations for the effects of the LLM condition on reading comprehension and retention, we sought to understand what kind of prompts students made when using the LLM in planned exploratory analyses. The LLM prompts were analysed using a hierarchical coding scheme through GPT-4 in an automated Python script accessing the Azure OpenAI's API (deployment dated 2024-06-01). Temperature was set to 0 for deterministic outputs with a narrow sampling range (top-p=0.1) to ensure consistent classifications. The model was provided with detailed instructions and examples for each category, along with both texts that students were studying. Each prompt could receive multiple sub-codes.\n\nThe hierarchical coding scheme was developed through several iterations. The initial version was deductively and inductively developed by a researcher using active reading literature, students' task instructions, and piloting work. This scheme was expanded based on the API's suggestions and the API was then asked to code the data using the coding scheme. The researchers then iteratively refined the coding scheme based on checking portions of the API output. They merged, deleted, and added codes as needed and adapted code descriptions and examples to improve the quality of the API output. Finally, one of the researchers manually checked the API output for 500 prompts (approximately  $10\\%$  of the data) and found an error rate of  $5.6\\%$ . This was deemed to be an acceptable level. The assigned codes for these 500 prompts were adjusted where necessary, and the rest of the API output was left as it was. The final coding schemes for student prompts can be found in Supplementary Table 20.\n\n# Quantitative exploration of students' learning experience\n\nAs planned we explored a range of variables capturing students' learning experiences. More specifically, we compared students' learning experiences when using LLM vs. Notes and LLM vs. LLM+Notes using paired  $t$ -tests. We applied Bonferroni corrections to adjust for multiple comparisons. The  $t$ -tests were conducted using the 'tidyverse' package.\n\n# Qualitative exploration of students' activity preferences\n\nWe explored students' open response explanations for preferring one learning activity over another. The explanations were analysed by two of the authors with help from the API described above. Four preference groups were separately analysed:\n\n1. LLM over Notes,  \n2. Notes over LLM,  \n3. LLM over  $\\mathrm{LLM} + \\mathrm{Notes}$ , and  \n4. LLM+Notes over LLM.\n\nEach preference group had its own coding scheme which only included explanations for preferring the favoured activity over the non-favoured activity (i.e., benefits of note-taking were not coded if the student preferred the LLM over Notes). The initial schemes were developed by manually and deductively coding approximately  $30\\%$  of responses of each preference group. Several codes could be applied to each response. The initial coding schemes, including the category label, description and examples were provided to the API alongside the data and general coding instructions. The API did not suggest any further helpful codes. The researchers then iteratively refined the coding schemes by manually checking portions of the API output. They merged, deleted, and added codes as well as refined code descriptions and examples before the API analysis was rerun. This process was repeated until both researchers were satisfied with the coding schemes. Due to the\n\nsmall number of responses that had to be coded ( $n = 278$ ), one researcher checked the entire API output and made adjustments where necessary. The final coding schemes for activity preferences can be found in Supplementary Section 2.11.\n\n# Data availability\n\nAll quantitative data will be made available upon publication. We will not provide the following qualitative data as that would risk sharing identifiable information: Students' LLM interactions (only the applied codes will be shared), students' notes, students' activity preferences (only applied codes will be shared).\n\n# Code availability\n\nThe corresponding code will be shared upon publication.\n\n# Ethics declarations\n\n# Competing interests\n\nSome of the authors conduct research at a company that invests in generative AI and develops technology using generative AI models as a core component. The other authors are part of a publishing, assessment and learning organisation which increasingly uses AI in developing and operating assessment and learning products and services. However, this work is not connected to any specific product or monetisation efforts for either organisation.\n\n# Acknowledgements\n\nWe thank Dr Tom Benton and Dr Matthew Carroll for their valuable advice on the analyses conducted in this study.\n\n# Supplementary Material\n\n# Table of Contents\n\n# Supplementary Information\n\n- Participant Exclusion Criteria\n\n# Supplementary Tables\n\n- Student Characteristics  \nFamiliarity with Learning Activities  \n- Descriptive Statistics  \n- Mixed Effects Regression Results  \nBehavioural Engagement  \n- Introduction to Active Reading  \n- Introduction to Learning Activity\n\n- Specific instructions by Condition  \nTest Questions  \n- Inter-rater Reliability Results  \nSurvey Questions and Response Scales  \nSurvey Questions and Response Scales (session 2)  \n- Learning Experiences and Perceptions  \nCoding Scheme Activity Preferences  \nCoding scheme: LLM over Notes preferences  \nCoding scheme: Notes over LLM preferences  \nCoding scheme: LLM+Notes over LLM preferences  \nCoding Scheme Prompt Interactions  \n- Frequencies of Prompt Types\n\n# References\n\n[1] Cecilia Ka Yuk Chan. A comprehensive AI policy education framework for university teaching and learning. International Journal of Educational Technology in Higher Education, 20(1):38, July 2023. ISSN 2365-9440. doi: 10.1186/s41239-023-00408-3. URL https://doi.org/10.1186/s41239-023-00408-3.  \n[2] Abdulhadi Shoufan. Exploring Students' Perceptions of ChatGPT: Thematic Analysis and Follow-Up Survey. IEEE Access, 11:38805-38818, 2023. ISSN 2169-3536. doi: 10.1109/ACCESS.2023.3268224. URL https://ieeexplore.ieee.org/document/10105236/?arnumber=10105236. Conference Name: IEEE Access.  \n[3] K. Aleksić-Maslac, F. Borović, and Z. Biočina. PERCEPTION AND USAGE OFchat GPT IN THE EDUCATION SYSTEM. INTED2024 Proceedings, pages 1842-1848, 2024. ISSN 2340-1079. doi: 10.21125/inted.2024.0511. URL https://library.iated.org/view/ ALEKSICMASLAC2024PER. Conference Name: 18th International Technology, Education and Development Conference ISBN: 9788409592159 Meeting Name: 18th International Technology, Education and Development Conference Place: Valencia, Spain Publisher: IATED.  \n[4] Nikhil Singh, Guillermo Bernal, Daria Savchenko, and Elena L. Glassman. Where to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal Machine Intelligence. ACM Transactions on Computer-Human Interaction, February 2022. ISSN 1073-0516. doi: 10.1145/3511599. URL https://dl.acm.org/doi/10.1145/3511599. Just Accepted.  \n[5] Heather Johnston, Rebecca F. Wells, Elizabeth M. Shanks, Timothy Boey, and Bryony N. Parsons. Student perspectives on the use of generative artificial intelligence technologies in higher education. International Journal for Educational Integrity, 20(1):2, February 2024. ISSN 1833-2595. doi: 10.1007/s40979-024-00149-4. URL https://doi.org/10.1007/s40979-024-00149-4.\n\n[6] Duong Hoai Lan and Tran Minh Tung. Analyzing the Impact of Chat-GPT Usage by University Students in Vietnam. Migration Letters, 20(S10):259-268, November 2023. ISSN 1741-8992. doi: 10.59670/ml.v20iS10.5134. URL https://migrationletters.com/index.php/ml/article/view/5134. Number: S10.  \n[7] Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnmann, Eyke Hüllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci. ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 2023.  \n[8] Stefan E. Huber, Kristian Kiili, Steve Nebel, Richard M. Ryan, Michael Sailer, and Manuel Ninaus. Leveraging the Potential of Large Language Models in Education Through Playful and Game-Based Learning. Educational Psychology Review, 36(1):25, February 2024. ISSN 1573-336X. doi: 10.1007/s10648-024-09868-z. URL https://doi.org/10.1007/s10648-024-09868-z.  \n[9] Yogesh K. Dwivedi, Nir Kshetri, Laurie Hughes, Emma Louise Slade, Anand Jeyaraj, Arpan Kumar Kar, Abdullah M. Baabdullah, Alex Koohang, Vishnupriya Raghavan, Manju Ahuja, Hanaa Albanna, Mousa Ahmad Albashrawi, Adil S. Al-Busaidi, Janarthanan Balakrishnan, Yves Barlette, Sriparna Basu, Indranil Bose, Laurence Brooks, Dimitrios Buhalis, Lemuria Carter, Soumyadeb Chowdhury, Tom Crick, Scott W. Cunningham, Gareth H. Davies, Robert M. Davison, Rahul De, Denis Dennehy, Yanqing Duan, Rameshwar Dubey, Rohita Dwivedi, John S. Edwards, Carlos Flavian, Robin Gauld, Varun Grover, Mei-Chih Hu, Marijn Janssen, Paul Jones, Iris Junglas, Sangeeta Khorana, Sascha Kraus, Kai R. Larsen, Paul Latreille, Sven Laumer, F. Tegwen Malik, Abbas Mardani, Marcello Mariani, Sunil Mithas, Emmanuel Mogaji, Jeretta Horn Nord, Siobhan O'Connor, Fevzi Okumus, Margherita Pagani, Neeraj Pandey, Savvas Papagiannidis, Ilias O. Pappas, Nishith Pathak, Jan Pries-Heje, Ramakrishnan Raman, Nripendra P. Rana, Sven-Volker Rehm, Samuel Ribeiro-Navarrete, Alexander Richter, Frantz Rowe, Suprateek Sarker, Bernd Carsten Stahl, Manoj Kumar Tiwari, Wil van der Aalst, Viswanath Venkatesh, Giampaoloiglia, Michael Wade, Paul Walton, Jochen Wirtz, and Ryan Wright. Opinion Paper: \"So what if ChatGPT wrote it?\" Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71:102642, August 2023. ISSN 0268-4012. doi: 10. 1016/j.ijinfomgt.2023.102642. URL https://www.sciencedirect.com/science/article/ pii/S0268401223000233.  \n[10] Jun-Jie Zhu, Jinyue Jiang, Meiqi Yang, and Zhiyong Jason Ren. ChatGPT and Environmental Research. *Environmental Science & Technology*, 57(46):17667-17670, November 2023. ISSN 0013-936X. doi: 10.1021/acs.est.3c01818. URL https://doi.org/10.1021/acs.est.3c01818. Publisher: American Chemical Society.  \n[11] Alex Barrett and Austin Pack. Not quite eye to A.I.: student and teacher perspectives on the use of generative artificial intelligence in the writing process. International Journal of Educational Technology in Higher Education, 20(1):59, November 2023. ISSN 2365-9440. doi: 10.1186/s41239-023-00427-0. URL https://doi.org/10.1186/s41239-023-00427-0.  \n[12] Aiste Steponenaite and Basel Barakat. Plagiarism in AI Empowered World. In Margherita Antona and Constantine Stephanidis, editors, Universal Access in Human-Computer Interaction, pages 434–442, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-35897-5. doi: 10.1007/978-3-031-35897-5_31.\n\n[13] Ofcom. Online nation 2024 report. Technical report, Ofcom, November 2024. URL https://www.ofcom.org.uk/media-use-and-attitudes/online-habits/online-nation/.  \n[14] Walton Family Foundation. Teachers and Students Embrace ChatGPT for Education. Technical report, Walton Family Foundation, March 2023. URL https://www.waltonfamilyfoundation.org/learning/teachers-and-students-embrace-chatgpt-for-education. Section: Learning.  \n[15] Ruiqi Deng, Maoli Jiang, Xinlu Yu, Yuyan Lu, and Shasha Liu. Does chatgpt enhance student learning? a systematic review and meta-analysis of experimental studies. Computers Education, 227:105224, 2025. ISSN 0360-1315. doi: https://doi.org/10.1016/j.compedu.2024.105224. URL https://www.sciencedirect.com/science/article/pii/S0360131524002380.  \n[16] Jeffrey R. Binder and Rutvik H. Desai. The neurobiology of semantic memory. Trends in Cognitive Sciences, 15(11):527-536, November 2011. ISSN 1879-307X. doi: 10.1016/j.tics.2011.10.001.  \n[17] Danielle S. McNamara and Joe Magliano. Toward a comprehensive model of comprehension. In The psychology of learning and motivation, Vol. 51, The psychology of learning and motivation, pages 297-384. Elsevier Academic Press, San Diego, CA, US, 2009. ISBN 978-0-12-374489-0. doi: 10.1016/S0079-7421(09)51009-2.  \n[18] Walter Kintsch. The role of knowledge in discourse comprehension: A construction-integration model. *Psychological Review*, 95(2):163–182, 1988. ISSN 1939-1471. doi: 10.1037/0033-295X.95.2.163. Place: US Publisher: American Psychological Association.  \n[19] Gregory Hickok and David Poeppel. The cortical organization of speech processing. Nature Reviews Neuroscience, 8(5):393-402, May 2007. ISSN 1471-0048. doi: 10.1038/nrn2113. URL https://www.nature.com/articles/nrn2113. Publisher: Nature Publishing Group.  \n[20] Evelina Fedorenko, Anna A. Ivanova, and Tamar I. Regev. The language network as a natural kind within the broader landscape of the human brain. Nature Reviews Neuroscience, 25 (5):289-312, May 2024. ISSN 1471-0048. doi: 10.1038/s41583-024-00802-4. URL https://www.nature.com/articles/s41583-024-00802-4. Publisher: Nature Publishing Group.  \n[21] Rolf A. Zwaan and Gabriel A. Radvansky. Situation models in language comprehension and memory. *Psychological Bulletin*, 123(2):162–185, 1998. ISSN 1939-1455. doi: 10.1037/0033-2909.123.2.162. Place: US Publisher: American Psychological Association.  \n[22] Junhua Ding, Keliang Chen, Haoming Liu, Lin Huang, Yan Chen, Yingru Lv, Qing Yang, Qihao Guo, Zaizhu Han, and Matthew A. Lambon Ralph. A unified neurocognitive model of semantics language social behaviour and face recognition in semantic dementia. Nature Communications, 11(1):2595, May 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-16089-9. URL https://www.nature.com/articles/s41467-020-16089-9. Publisher: Nature Publishing Group.  \n[23] Kate Cain and Jane Oakhill. Reading Comprehension Difficulties: Correlates, Causes, and Consequences. In Children's comprehension problems in oral and written language: A cognitive perspective, Challenges in language and literacy, pages 41-75. The Guilford Press, New York, NY, US, 2007. ISBN 978-1-59385-443-0.  \n[24] Meredithyth Daneman and Patricia A. Carpenter. Individual differences in working memory and reading. Journal of Verbal Learning & Verbal Behavior, 19(4):450-466, 1980. ISSN 0022-5371. doi: 10.1016/S0022-5371(80)90312-6. Place: Netherlands Publisher: Elsevier Science.\n\n[25] Charles A. Perfetti, Nicole Landi, and Jane Oakhill. The Acquisition of Reading Comprehension Skill. In *The science of reading: A handbook*, Blackwell handbooks of developmental psychology, pages 227-247. Blackwell Publishing, Malden, 2005. ISBN 978-1-4051-1488-2. doi: 10.1002/9780470757642.ch13.  \n[26] Jane V. Oakhill, Molly S. Berenhaus, and Kate Cain. Children's reading comprehension and comprehension difficulties. In *The Oxford handbook of reading*, Oxford library of psychology, pages 344-360. Oxford University Press, New York, NY, US, 2015. ISBN 978-0-19-932457-6. doi: 10.1093/oxfordhb/9780199324576.001.0001.  \n[27] Keith E. Stanovich. Matthew effects in reading: Some consequences of individual differences in the acquisition of literacy. Reading Research Quarterly, 21(4):360-407, 1986. ISSN 1936-2722. doi: 10.1598/RRQ.21.4.1. Place: US Publisher: International Reading Association.  \n[28] A. C. Graesser, M. Singer, and T. Trabasso. Constructing inferences during narrative text comprehension. *Psychological Review*, 101(3):371–395, July 1994. ISSN 0033-295X. doi: 10.1037/0033-295x.101.3.371.  \n[29] Danielle S. McNamara, Irwin B. Levinstein, and Chutima Boonthum. iSTART: Interactive strategy training for active reading and thinking. Behavior Research Methods, Instruments, 3 Computers, 36(2):222-233, May 2004. ISSN 1532-5970. doi: 10.3758/BF03195567. URL https://doi.org/10.3758/BF03195567.  \n[30] John T. Guthrie and Allan Wigfield. Engagement and motivation in reading. In Handbook of reading research, Vol. III, pages 403-422. Lawrence Erlbaum Associates Publishers, Mahwah, NJ, US, 2000. ISBN 978-0-8058-2398-1 978-0-8058-2399-8.  \n[31] Tracy Linderholm, Sandra Virtue, Yuhtsuen Tzeng, and Paul van den Broek. Fluctuations in the Availability of Information During Reading: Capturing Cognitive Processes Using the Landscape Model. pages 165-186. December 2018. ISBN 978-1-315-04610-5. doi: 10.4324/9781315046105-5.  \n[32] Fergus I. M. Craik. Levels of processing: Past, present . . . and future? Memory, 10(5-6): 305-318, 2002. ISSN 1464-0686. doi: 10.1080/09658210244000135. Place: United Kingdom Publisher: Taylor & Francis.  \n[33] Fergus I. M. Craik and Endel Tulving. Depth of processing and the retention of words in episodic memory. Journal of Experimental Psychology: General, 104(3):268-294, 1975. ISSN 1939-2222. doi: 10.1037/0096-3445.104.3.268. Place: US Publisher: American Psychological Association.  \n[34] John R. Anderson. A spreading activation theory of memory. Journal of Verbal Learning and Verbal Behavior, 22(3):261-295, June 1983. ISSN 0022-5371. doi: 10.1016/S0022-5371(83)90201-3. URL https://www.sciencedirect.com/science/article/pii/S0022537183902013.  \n[35] Danielle S. McNamara, editor. Reading comprehension strategies: Theories, interventions, and technologies. Lawrence Erlbaum Associates Publishers, Mahwah, NJ, 2007.  \n[36] Michelene T. H. Chi. Active-Constructive-Interactive: A Conceptual Framework for Differentiating Learning Activities. Topics in Cognitive Science, 1(1):73-105, 2009. ISSN 1756-8765. doi: 10.1111/j.1756-8765.2008.01005.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-8765.2008.01005.x. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1756-8765.2008.01005.x.\n\n[37] Rose Luckin, Wayne Holmes, and Laurie B Forcier. Intelligence Unleashed: An argument for AI in Education. Technical report, Open Ideas at Pearson / UCL, 2016. URL https://www.pearson.com/content/dam/corporate/global/pearson-dot-com/files/innovation/Intelligence-Unleashed-Publication.pdf.  \n[38] Wayne Holmes, Maya Bialik, and Charles Fadel. Artificial Intelligence in Education. Promise and Implications for Teaching and Learning. March 2019. ISBN 978-1-79429-370-0.  \n[39] Margherita Bernabei, Silvia Colabianchi, Andrea Falegnami, and Francesco Costantino. Students' use of large language models in engineering education: A case study on technology acceptance, perceptions, efficacy, and detection chances. Computers and Education: Artificial Intelligence, 5:100172, October 2023. doi: 10.1016/j.caeai.2023.100172.  \n[40] Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models. In Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 1, pages 27-43, Lugano and Virtual Event Switzerland, August 2022. ACM. ISBN 978-1-4503-9194-8. doi: 10.1145/3501385.3543957. URL https://dl.acm.org/doi/10.1145/3501385.3543957.  \n[41] Harsh Kumar, David M Rothschild, Daniel G Goldstein, and Jake M Hofman. Math Education With Large Language Models: Peril or Promise? 2023.  \n[42] John Sweller, Jeroen J. G. van Merrienboer, and Fred Paas. Cognitive architecture and instructional design: 20 years later. Educational Psychology Review, 31(2):261-292, 2019. ISSN 1573-336X. doi: 10.1007/s10648-019-09465-5. Place: Germany Publisher: Springer.  \n[43] Richard E. Mayer. Should There Be a Three-Strikes Rule Against Pure Discovery Learning? American Psychologist, 59(1):14-19, 2004. ISSN 1935-990X. doi: 10.1037/0003-066X.59.1.14. Place: US Publisher: American Psychological Association.  \n[44] Fergus I. M. Craik and Robert S. Lockhart. Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal Behavior, 11(6):671-684, December 1972. ISSN 0022-5371. doi: 10.1016/S0022-5371(72)80001-X. URL https://www.sciencedirect.com/science/article/pii/S002253717280001X.  \n[45] Xiaoming Zhai, Matthew Nyaaba, and Wenchao Ma. Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?, January 2024. URL http://arxiv.org/abs/2401.15081. arXiv:2401.15081.  \n[46] Faycal Farhi, Riadh Jeljeli, Ibtehal Aburezeq, Fawzi Fayez Dweikat, Samer Ali Al-shami, and Radouane Slamene. Analyzing the students' views, concerns, and perceived ethics about chat GPT usage. Computers and Education: Artificial Intelligence, 5:100180, January 2023. ISSN 2666-920X. doi: 10.1016/j.caeai.2023.100180. URL https://www.sciencedirect.com/science/article/pii/S2666920X23000590.  \n[47] Hao Yu and Yunyun Guo. Generative artificial intelligence empowers educational reform: current status, issues, and prospects. Frontiers in Education, 8:1183162, June 2023. ISSN 2504-284X. doi: 10.3389/feduc.2023.1183162. URL https://www.frontiersin.org/articles/10.3389/feduc.2023.1183162/full.  \n[48] Elizabeth Ligon Bjork and Robert A. Bjork. Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning. In *Psychology and the real world: Essays illustrating fundamental contributions to society*, pages 56-64. Worth Publishers, New York, NY, US, 2011. ISBN 978-1-4292-3043-8.\n\n[49] Michelene Chi, Stephanie Siler, Heisawn Jeong, Takashi Yamauchi, and Robert Hausmann. Learning from human tutoring. Cognitive Science, 25:471-533, July 2001. doi: 10.1016/S0364-0213(01)00044-1.  \n[50] Alvaro Pascual-Leone, Amir Amedi, Felipe Fregni, and Lotfi B. Merabet. The plastic human brain cortex. Annual Review of Neuroscience, 28:377-401, 2005. ISSN 0147-006X. doi: 10.1146/annurev.neuro.27.070203.144216.  \n[51] S. Dehaene and L. Naccache. Towards a cognitive neuroscience of consciousness: basic evidence and a workspace framework. Cognition, 79(1-2):1-37, April 2001. ISSN 0010-0277. doi: 10.1016/s0010-0277(00)00123-2.  \n[52] Keiichi Kobayashi. What limits the encoding eVect of note-taking? A meta-analytic examination. Contemporary Educational Psychology, 2005.  \n[53] Kenneth A. Kiewra. A review of note-taking: The encoding storage paradigm and beyond. Educational Psychology Review, 1(2):147-172, 1989. ISSN 1573-336X. doi: 10.1007/BF01326640. Place: Germany Publisher: Springer.  \n[54] Kenneth A. Kiewra. Investigating notetaking and review: A depth of processing alternative. Educational Psychologist, 20(1):23-32, 1985. ISSN 1532-6985. doi: 10.1207/s15326985ep2001_4. Place: US Publisher: Lawrence Erlbaum.  \n[55] Mark Bohay, Daniel P. Blakely, Andrea K. Tamplin, and Gabriel A. Radvansky. Note taking, review, memory, and comprehension. The American Journal of Psychology, 124(1):63-73, 2011. ISSN 0002-9556. doi: 10.5406/amerjpsyc.124.1.0063.  \n[56] Dung C. Bui and Joel Myerson. The role of working memory abilities in lecture note-taking. Learning and Individual Differences, 33:12-22, 2014. ISSN 1873-3425. doi: 10.1016/j.lindif.2014.05.002. Place: Netherlands Publisher: Elsevier Science.  \n[57] Ralf Rummer, Judith Schweppe, Kathleen Gerst, and Simon Wagner. Is testing a more effective learning strategy than note-taking? Journal of Experimental Psychology. Applied, 23(3):293-300, September 2017. ISSN 1939-2192. doi: 10.1037/xap0000134.  \n[58] Lisa Geraci, Nikhil Kurpad, Rachel Tirso, Kathryn N. Gray, and Yuxiang Wang. Metacognitive errors in the classroom: The role of variability of past performance on exam prediction accuracy. *Metacognition and Learning*, 2022. doi: 10.1007/s11409-022-09326-7. URL https://doi.org/10.1007/s11409-022-09326-7. Advance online publication.  \n[59] Robert A. Bjork, John Dunlosky, and Nate Kornell. Self-Regulated Learning: Beliefs, Techniques, and Illusions. Annual Review of Psychology, 64(1):417-444, January 2013. ISSN 0066-4308, 1545-2085. doi: 10.1146/annurev-psych-113011-143823. URL https://www.annualreviews.org/doi/10.1146/annurev-psych-113011-143823.  \n[60] Justin Kruger and David Dunning. Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments. Journal of Personality and Social Psychology, 77(6):1121-1134, Dec 1999. doi: 10.1037//0022-3514.77.6.1121.  \n[61] Lev Tankelevitch, Viktor Kewenig, Auste Simkute, Ava Elizabeth Scott, Advait Sarkar, Abigail Sellen, and Sean Rintel. The metacognitive demands and opportunities of generative ai. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703300. doi: 10.1145/3613904.3642902. URL https://doi.org/10.1145/3613904.3642902.\n\n[62] Axel Grund, Stefan Fries, Matthias Nückles, Alexander Renkl, and Julian Roelle. When is Learning \"Effortful\"? Scrutinizing the Concept of Mental Effort in Cognitively Oriented Research from a Motivational Perspective. Educational Psychology Review, 36(1):11, March 2024. ISSN 1040-726X, 1573-336X. doi: 10.1007/s10648-024-09852-7. URL https://link.springer.com/10.1007/s10648-024-09852-7.  \n[63] Louise Starkey. A review of research exploring teacher preparation for the digital age. Cambridge Journal of Education, 50(1):37-56, 2020. doi: 10.1080/0305764X.2019.1625867.  \n[64] Honghong Wang and Weiping Shi. Practical approaches to integrated values education for foreign language majors. Foreign Language World, (6):38-45, 2021.  \n[65] British Educational Research Association. Ethical Guidelines for Educational Research, fourth edition, 2018. URL https://www.bera.ac.uk/publication/ethical-guidelines-for-educational-research-2018.  \n[66] P. David Pearson, Laura R. Roehler, Janice A. Dole, and Gerald G. Duffy. Developing expertise in reading comprehension: What should be taught? How should it be taught? Technical Report 512, University of Illinois Urbana-Champaign Center for the Study of Reading, 1990. URL https://hdl.handle.net/2142/17648. Publisher: Champaign, Ill.: University of Illinois at Urbana-Champaign, Center for the Study of Reading.  \n[67] Terry K Koo and Mae Y Li. A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research. 2016.  \n[68] Chris Taylor. The reliability of free school meal eligibility as a measure of socio-economic disadvantage: Evidence from the millennium cohort study in wales. *British Journal of Educational Studies*, 66(1):29-51, 2018. doi: 10.1080/00071005.2017.1330464.\n\n# 1 Supplementary Information\n\n# 1.1 Participant Exclusion Criteria\n\nParticipants  $(n = 61)$  were excluded for the following reasons:\n\n1. Did not take part in Session 2 (n=36)  \n2. Did not complete both tasks in Session 1 (and/or withdrew intentionally)  $(n = 2)$  \n3. Stopped Session 2 before attempting all comprehension and retention questions  $(n = 8)$  \n4. Completed Session 2 in 10 minutes or less  $(n = 1)$  \n5. Reported substantially different prior knowledge of the two topics (3-point difference on a 5-point Likert-scale item)  $(n = 13)$  \n6. Cheated during a session (as observed by researcher, including opening a different browser to look up answers, copying answers from others, continuing conversation with neighbours). Responses of suspicious students were scanned and compared with that of other students in the same group. If suspicion confirmed based on responses (e.g., high overlap with a student), these were excluded  $(n = 1)$\n\n# 2 Supplementary Tables\n\n# 2.1 Student Characteristics\n\nTable 3: Student characteristics by group and overall totals (after exclusion,  $\\mathrm{N} = {344}$  )  \n\n<table><tr><td>Characteristic</td><td>Group 1\nN students (%)</td><td>Group 2\nN students (%)</td><td>Total\nN students (%)</td></tr><tr><td>Male</td><td>102 (29.7%)</td><td>78 (22.7%)</td><td>180 (52.3%)</td></tr><tr><td>Female</td><td>57 (16.6%)</td><td>63 (18.3%)</td><td>120 (34.9%)</td></tr><tr><td>Other</td><td>1 (0.3%)</td><td>1 (0.3%)</td><td>2 (0.6%)</td></tr><tr><td>Prefer not to say</td><td>2 (0.6%)</td><td>0 (0.0%)</td><td>2 (0.6%)</td></tr><tr><td>FSM_Yes</td><td>9 (2.6%)</td><td>10 (2.9%)</td><td>19 (5.5%)</td></tr><tr><td>FSM_No</td><td>160 (46.5%)</td><td>163 (47.4%)</td><td>323 (93.9%)</td></tr><tr><td>EAL_Yes</td><td>130 (37.8%)</td><td>117 (34.0%)</td><td>247 (71.8%)</td></tr><tr><td>EAL_Other Language</td><td>2 (0.6%)</td><td>3 (0.9%)</td><td>5 (1.5%)</td></tr><tr><td>EAL_Bilingual</td><td>35 (10.2%)</td><td>29 (8.4%)</td><td>64 (18.6%)</td></tr><tr><td>History_Yes</td><td>99 (28.8%)</td><td>80 (23.3%)</td><td>179 (52.0%)</td></tr><tr><td>History_No</td><td>81 (23.5%)</td><td>58 (16.9%)</td><td>139 (40.4%)</td></tr></table>\n\n# 2.2 Familiarity with Learning Activities\n\nTable 4: Frequencies of prior learning activity use  \n\n<table><tr><td>Activity and frequency</td><td>Group 1\nN students (%)</td><td>Group 2\nN students (%)</td></tr><tr><td colspan=\"3\">Note-taking for learning</td></tr><tr><td>Never</td><td>7 (3.8%)</td><td>6 (3.8%)</td></tr><tr><td>Rarely</td><td>34 (18.5%)</td><td>25 (15.6%)</td></tr><tr><td>Sometimes</td><td>47 (25.5%)</td><td>44 (27.5%)</td></tr><tr><td>Often</td><td>69 (37.5%)</td><td>70 (43.8%)</td></tr><tr><td>Always</td><td>22 (12.0%)</td><td>17 (10.6%)</td></tr><tr><td colspan=\"3\">LLM use for learning</td></tr><tr><td>Never</td><td>32 (25.6%)</td><td>19 (18.1%)</td></tr><tr><td>Rarely</td><td>45 (36.0%)</td><td>44 (41.9%)</td></tr><tr><td>Sometimes</td><td>29 (23.2%)</td><td>26 (24.8%)</td></tr><tr><td>Often</td><td>15 (12.0%)</td><td>15 (14.3%)</td></tr><tr><td>Always</td><td>4 (3.2%)</td><td>1 (1.0%)</td></tr><tr><td colspan=\"3\">LLM + Notes for learning</td></tr><tr><td>Never</td><td>-</td><td>1 (1.6%)</td></tr><tr><td>Rarely</td><td>-</td><td>31 (48.4%)</td></tr><tr><td>Sometimes</td><td>-</td><td>23 (35.9%)</td></tr><tr><td>Often</td><td>-</td><td>8 (12.5%)</td></tr><tr><td>Always</td><td>-</td><td>1 (1.6%)</td></tr><tr><td colspan=\"3\">Prior LLM use</td></tr><tr><td>Yes</td><td>125 (70.2%)</td><td>105 (64.0%)</td></tr><tr><td>No</td><td>53 (29.8%)</td><td>59 (36.0%)</td></tr><tr><td colspan=\"3\">Frequency of LLM use amongst users</td></tr><tr><td>Less than once a week</td><td>74 (59.2%)</td><td>68 (64.8%)</td></tr><tr><td>One or two days a week</td><td>28 (22.4%)</td><td>33 (31.4%)</td></tr><tr><td>Three to five days a week</td><td>11 (8.8%)</td><td>5 (4.8%)</td></tr><tr><td>Most days of the week</td><td>12 (9.6%)</td><td>1 (1.0%)</td></tr></table>\n\n# 2.3 Descriptive Statistics\n\nTable 5: Descriptive statistics for comprehension, literal retention, and free recall across conditions.  \n\n<table><tr><td>Measure</td><td>Condition</td><td>Mean (M)</td><td>Standard Deviation (SD)</td></tr><tr><td rowspan=\"4\">Comprehension (max 12 points)</td><td>Notes</td><td>4.89</td><td>2.52</td></tr><tr><td>LLM + Notes</td><td>4.11</td><td>2.65</td></tr><tr><td>LLM only (Group 1)</td><td>4.00</td><td>2.44</td></tr><tr><td>LLM only (Group 2)</td><td>3.80</td><td>2.47</td></tr><tr><td rowspan=\"4\">Literal retention (max 20 points)</td><td>Notes</td><td>10.8</td><td>4.29</td></tr><tr><td>LLM + Notes</td><td>9.68</td><td>4.83</td></tr><tr><td>LLM only (Group 1)</td><td>8.83</td><td>3.96</td></tr><tr><td>LLM only (Group 2)</td><td>8.95</td><td>4.29</td></tr><tr><td rowspan=\"4\">Free recall (max 50 points)</td><td>Notes</td><td>5.36</td><td>5.49</td></tr><tr><td>LLM Group 1</td><td>4.32</td><td>4.15</td></tr><tr><td>LLM Group 2</td><td>4.32</td><td>4.63</td></tr><tr><td>LLM + Notes</td><td>4.20</td><td>5.07</td></tr></table>\n\n# 2.4 Mixed Effects Regression Results\n\nTable 6: Model coefficients for literal retention, comprehension, and free recall  \n\n<table><tr><td>Term</td><td>Estimate</td><td>Std. Error</td><td>95% CI</td><td>Statistic</td><td>df</td><td>p-value</td><td>d</td></tr><tr><td colspan=\"8\">Literal retention</td></tr><tr><td>Intercept</td><td>8.2429</td><td>0.7966</td><td>[6.68, 9.81]</td><td>10.3476</td><td>489.3004</td><td>7.95 × 10-23</td><td>-</td></tr><tr><td>Condition LLM_notes</td><td>0.5668</td><td>0.2752</td><td>[0.03, 1.11]</td><td>2.0597</td><td>660.4521</td><td>0.0398</td><td>0.132</td></tr><tr><td>Condition notes</td><td>1.9188</td><td>0.2559</td><td>[1.42, 2.42]</td><td>7.4974</td><td>663.2789</td><td>2.09 × 10-13</td><td>0.443</td></tr><tr><td>Group 1</td><td>-0.6147</td><td>0.4155</td><td>[-1.43, 0.20]</td><td>-1.4793</td><td>661.9230</td><td>0.1395</td><td>-0.143</td></tr><tr><td>school_id S03</td><td>-0.8645</td><td>0.5993</td><td>[-2.04, 0.31]</td><td>-1.4424</td><td>638.7162</td><td>0.1497</td><td>-0.198</td></tr><tr><td>school_id S01</td><td>-1.9789</td><td>0.8005</td><td>[-3.55, -0.41]</td><td>-2.4720</td><td>657.4886</td><td>0.0137</td><td>-0.465</td></tr><tr><td>school_id S05</td><td>-0.3908</td><td>0.8562</td><td>[-2.07, 1.29]</td><td>-0.4564</td><td>612.9203</td><td>0.6483</td><td>-0.094</td></tr><tr><td>school_id S02</td><td>1.2932</td><td>0.5514</td><td>[0.21, 2.37]</td><td>2.3452</td><td>643.8234</td><td>0.0193</td><td>0.299</td></tr><tr><td>school_id S07</td><td>2.7561</td><td>1.1408</td><td>[0.52, 4.99]</td><td>2.4160</td><td>663.8251</td><td>0.0160</td><td>0.623</td></tr><tr><td>school_id S04</td><td>-4.7045</td><td>0.8102</td><td>[-6.29, -3.12]</td><td>-5.8067</td><td>641.0030</td><td>1.00 × 10-8</td><td>-1.075</td></tr><tr><td>Text Cuba</td><td>1.5218</td><td>0.1880</td><td>[1.15, 1.89]</td><td>8.0952</td><td>663.5151</td><td>2.74 × 10-15</td><td>0.351</td></tr><tr><td>Task_order 0</td><td>0.2310</td><td>0.1880</td><td>[-0.14, 0.60]</td><td>1.2283</td><td>659.9704</td><td>0.2198</td><td>0.052</td></tr><tr><td>Test_order 0</td><td>0.5186</td><td>0.1875</td><td>[0.15, 0.89]</td><td>2.7656</td><td>663.7540</td><td>0.0058</td><td>0.119</td></tr><tr><td>Gender (Male)</td><td>0.8396</td><td>0.4609</td><td>[-0.06, 1.74]</td><td>1.8217</td><td>335.9448</td><td>0.0694</td><td>0.193</td></tr><tr><td>Gender (Other)</td><td>1.1737</td><td>1.5839</td><td>[-1.93, 4.28]</td><td>0.7410</td><td>187.9029</td><td>0.4596</td><td>0.228</td></tr><tr><td>Gender (Prefer not to say)</td><td>1.7770</td><td>1.4362</td><td>[-1.04, 4.59]</td><td>1.2373</td><td>474.9248</td><td>0.2166</td><td>0.226</td></tr><tr><td>FSM (Yes)</td><td>-0.9135</td><td>0.8574</td><td>[-2.59, 0.77]</td><td>-1.0654</td><td>653.1653</td><td>0.2871</td><td>-0.207</td></tr><tr><td>EAL (Bilingual)</td><td>0.4650</td><td>0.4780</td><td>[-0.47, 1.40]</td><td>0.9728</td><td>645.1354</td><td>0.3310</td><td>0.116</td></tr><tr><td>EAL (Other)</td><td>-0.3369</td><td>1.6161</td><td>[-3.50, 2.83]</td><td>-0.2085</td><td>660.9281</td><td>0.8349</td><td>-0.027</td></tr><tr><td>History (No)</td><td>-1.5365</td><td>0.3832</td><td>[-2.29, -0.79]</td><td>-4.0095</td><td>641.2946</td><td>6.80 × 10-5</td><td>-0.351</td></tr><tr><td colspan=\"8\">Comprehension</td></tr><tr><td>Intercept</td><td>4.0264</td><td>0.4409</td><td>[3.16, 4.89]</td><td>9.1318</td><td>638.9518</td><td>8.77 × 10-19</td><td>-</td></tr><tr><td>Condition LLM_notes</td><td>0.3533</td><td>0.1785</td><td>[0.00, 0.70]</td><td>1.9792</td><td>655.5471</td><td>0.0482</td><td>0.142</td></tr><tr><td>Condition notes</td><td>0.9500</td><td>0.1658</td><td>[0.62, 1.28]</td><td>5.7306</td><td>662.6375</td><td>1.52 × 10-8</td><td>0.382</td></tr><tr><td>Group 1</td><td>-0.0735</td><td>0.2395</td><td>[-0.54, 0.40]</td><td>-0.3068</td><td>657.2449</td><td>0.7591</td><td>-0.033</td></tr><tr><td>school_id S03</td><td>-0.9749</td><td>0.3320</td><td>[-1.63, -0.32]</td><td>-2.9365</td><td>655.1779</td><td>0.0034</td><td>-0.399</td></tr><tr><td>school_id S01</td><td>-1.9371</td><td>0.4438</td><td>[-2.81, -1.07]</td><td>-4.3645</td><td>662.1221</td><td>1.48 × 10-5</td><td>-0.783</td></tr><tr><td>school_id S05</td><td>-0.3167</td><td>0.4735</td><td>[-1.24, 0.61]</td><td>-0.6688</td><td>648.4704</td><td>0.5039</td><td>-0.142</td></tr><tr><td>school_id S02</td><td>0.5254</td><td>0.3052</td><td>[-0.07, 1.12]</td><td>1.7215</td><td>659.5381</td><td>0.0856</td><td>0.201</td></tr><tr><td>school_id S07</td><td>0.9683</td><td>0.6335</td><td>[-0.27, 2.21]</td><td>1.5284</td><td>663.5186</td><td>0.1269</td><td>0.377</td></tr><tr><td>school_id S04</td><td>-2.9725</td><td>0.4493</td><td>[-3.85, -2.09]</td><td>-6.6154</td><td>651.4740</td><td>7.74 × 10-11</td><td>-1.192</td></tr><tr><td>Text Cuba</td><td>-0.6057</td><td>0.1218</td><td>[-0.84, -0.37]</td><td>-4.9727</td><td>662.4076</td><td>8.42 × 10-7</td><td>-0.245</td></tr><tr><td>Task_order 0</td><td>0.0428</td><td>0.1219</td><td>[-0.20, 0.28]</td><td>0.3508</td><td>657.5431</td><td>0.7258</td><td>0.015</td></tr><tr><td>Test_order 0</td><td>0.6679</td><td>0.1215</td><td>[0.43, 0.91]</td><td>5.4958</td><td>662.7896</td><td>5.55 × 10-8</td><td>0.266</td></tr><tr><td>Gender (Male)</td><td>0.2287</td><td>0.2517</td><td>[-0.26, 0.72]</td><td>0.9086</td><td>542.3928</td><td>0.3640</td><td>0.078</td></tr><tr><td>Gender (Other)</td><td>0.0375</td><td>0.9339</td><td>[-1.79, 1.87]</td><td>0.0401</td><td>102.4863</td><td>0.9681</td><td>0.574</td></tr><tr><td>Gender (Prefer not to say)</td><td>1.5360</td><td>0.9257</td><td>[-0.28, 3.35]</td><td>1.6593</td><td>68.4482</td><td>0.1016</td><td>0.006</td></tr><tr><td>FSM (Yes)</td><td>-0.6056</td><td>0.4786</td><td>[-1.54, 0.33]</td><td>-1.2655</td><td>626.0565</td><td>0.2062</td><td>-0.236</td></tr><tr><td>EAL (Bilingual)</td><td>0.5813</td><td>0.2649</td><td>[0.06, 1.10]</td><td>2.1943</td><td>655.2427</td><td>0.0286</td><td>0.228</td></tr><tr><td>EAL (Other)</td><td>-0.2195</td><td>0.9140</td><td>[-2.01, 1.57]</td><td>-0.2402</td><td>556.3704</td><td>0.8103</td><td>-0.103</td></tr><tr><td>History (No)</td><td>-0.6719</td><td>0.2138</td><td>[-1.09, -0.25]</td><td>-3.1423</td><td>613.1612</td><td>0.0018</td><td>-0.262</td></tr><tr><td colspan=\"8\">Free recall</td></tr><tr><td>Intercept</td><td>4.4052</td><td>0.8507</td><td>[2.74, 6.08]</td><td>5.1786</td><td>662.4966</td><td>2.97 × 10-7</td><td>-</td></tr><tr><td>Condition LLM_notes</td><td>-0.0847</td><td>0.4590</td><td>[-0.98, 0.81]</td><td>-0.1846</td><td>661.9195</td><td>0.8536</td><td>-0.015</td></tr><tr><td>Condition notes</td><td>1.0185</td><td>0.4269</td><td>[0.18, 1.86]</td><td>2.3856</td><td>663.2739</td><td>0.0173</td><td>0.211</td></tr><tr><td>Group 1</td><td>-0.2703</td><td>0.4958</td><td>[-1.24, 0.70]</td><td>-0.5452</td><td>662.0547</td><td>0.5858</td><td>-0.058</td></tr><tr><td>school_id S03</td><td>-0.4702</td><td>0.6185</td><td>[-1.68, 0.74]</td><td>-0.7603</td><td>663.5556</td><td>0.4474</td><td>-0.086</td></tr><tr><td>school_id S01</td><td>-0.9612</td><td>0.8290</td><td>[-2.59, 0.66]</td><td>-1.1595</td><td>660.3122</td><td>0.2467</td><td>-0.189</td></tr><tr><td>school_id S05</td><td>2.1564</td><td>0.8819</td><td>[0.43, 3.89]</td><td>2.4452</td><td>662.7977</td><td>0.0147</td><td>0.459</td></tr><tr><td>school_id S02</td><td>2.7874</td><td>0.5687</td><td>[1.67, 3.90]</td><td>4.9012</td><td>663.9081</td><td>1.20 × 10-6</td><td>0.578</td></tr><tr><td>school_id S07</td><td>2.2260</td><td>1.1824</td><td>[-0.09, 4.54]</td><td>1.8827</td><td>663.2415</td><td>0.0602</td><td>0.459</td></tr><tr><td>school_id S04</td><td>-2.3075</td><td>0.8366</td><td>[-3.95, -0.67]</td><td>-2.7583</td><td>663.2134</td><td>0.0060</td><td>-0.468</td></tr><tr><td>Text Cuba</td><td>-0.1187</td><td>0.3137</td><td>[-0.73, 0.50]</td><td>-0.3783</td><td>662.8799</td><td>0.7053</td><td>-0.027</td></tr><tr><td>Task_order 0</td><td>-0.1370</td><td>0.3134</td><td>[-0.75, 0.48]</td><td>-0.4372</td><td>662.9483</td><td>0.6621</td><td>-0.029</td></tr><tr><td>Test_order 0</td><td>-0.3089</td><td>0.3130</td><td>[-0.92, 0.31]</td><td>-0.9870</td><td>663.8172</td><td>0.3240</td><td>-0.062</td></tr><tr><td>Gender (Male)</td><td>0.7972</td><td>0.4653</td><td>[-0.11, 1.71]</td><td>1.7133</td><td>662.1998</td><td>0.0871</td><td>0.178</td></tr><tr><td>Gender (Other)</td><td>1.5025</td><td>1.6550</td><td>[-1.74, 4.75]</td><td>0.9079</td><td>586.1239</td><td>0.3643</td><td>0.336</td></tr><tr><td>Gender (Prefer not to say)</td><td>-0.7067</td><td>1.7223</td><td>[-4.08, 2.67]</td><td>-0.4103</td><td>284.0426</td><td>0.6819</td><td>-0.249</td></tr><tr><td>FSM (Yes)</td><td>-0.0013</td><td>0.8884</td><td>[-1.74, 1.74]</td><td>-0.0014</td><td>660.6054</td><td>0.9886</td><td>0.016</td></tr><tr><td>EAL (Bilingual)</td><td>-0.4993</td><td>0.4958</td><td>[-1.47, 0.47]</td><td>-1.0070</td><td>644.7815</td><td>0.3143</td><td>-0.104</td></tr><tr><td>EAL (Other)</td><td>-0.7021</td><td>1.6974</td><td>[-4.03, 2.62]</td><td>-0.4137</td><td>647.6784</td><td>0.6793</td><td>-0.157</td></tr><tr><td>History (No)</td><td>-1.0261</td><td>0.3967</td><td>[-1.80, -0.25]</td><td>-2.5868</td><td>658.8462</td><td>0.0099</td><td>-0.210</td></tr></table>\n\n# 2.5 Behavioural Engagement\n\nTable 7: Behavioural engagement with the LLM and note-taking, including queries made, words in notes, and time on task. Significant differences in time spent on tasks are highlighted for comparison between conditions.  \n\n<table><tr><td>Measure</td><td>Condition</td><td>Mean (M)</td><td>Standard Deviation (SD)</td></tr><tr><td rowspan=\"3\">Number of queries</td><td>Group 1 (LLM + Notes)</td><td>10.98</td><td>6.46</td></tr><tr><td>Group 2 (LLM only)</td><td>9.21</td><td>5.72</td></tr><tr><td>Group 2 (LLM + Notes)</td><td>6.02</td><td>4.64</td></tr><tr><td rowspan=\"2\">Words in notes</td><td>Group 1 (Notes)</td><td>100.74</td><td>115.63</td></tr><tr><td>Group 2 (LLM + Notes)</td><td>103.83</td><td>158.24</td></tr><tr><td>Trigram overlap (%)</td><td colspan=\"2\">Substantial overlap (≥ 70%)</td><td>25.63%</td></tr><tr><td>Trigram overlap (%)</td><td colspan=\"2\">High overlap (≥ 90%)</td><td>16.25%</td></tr><tr><td rowspan=\"4\">Time on task (minutes)</td><td>Group 1 (LLM)</td><td>-0.80</td><td>95% CI [-1.15, -0.46], d = -0.34</td></tr><tr><td>Group 1 (Notes)</td><td>10-15 range</td><td>-</td></tr><tr><td>Group 2 (LLM only)</td><td>-1.54</td><td>95% CI [-1.91, -1.17], d = -0.66</td></tr><tr><td>Group 2 (LLM + Notes)</td><td>10-15 range</td><td>-</td></tr></table>\n\n# 2.6 Student Task Instructions\n\nTable 8: Introduction to active reading (common across all conditions)  \n\n<table><tr><td>When you are trying to learn and understand a text, active reading can be a useful strategy.\nIt can help you to process the information more deeply and thus to learn better. Active reading\ninvolves:\n· figuring out what the main ideas and concepts in the text are,\n· what they mean,\n· how they relate to each other, and\n· asking questions about the information and then trying to answer them.</td></tr></table>\n\nTable 9: Learning activity introduction by condition  \n\n<table><tr><td>Condition</td><td>Activity introduction</td></tr><tr><td>Notes</td><td>Your task is to try to understand and learn a history text. To do so, please ac- \ntively read the text and take notes to help you. Taking notes is an important \npart of active reading. It is not about copying a lot of information from the text. \nInstead, find the key information in a section, think about what it means, and \nnote it down in your own words.</td></tr><tr><td>LLM</td><td>Your task is to try to understand and learn a history text. To do so, please \nactively read the text and use an AI chatbot to help you. Having a con-\nversation with the AI chatbot might help you to read more actively. You can \nask different questions about the text to help you understand what happened. \nIt may also help you to identify and understand key information.</td></tr><tr><td>LLM+Notes</td><td>Your task is to try to understand and learn a history text. To do so, please \nactively read the text, use an AI chatbot, and take notes to help you. \nHaving a conversation with the AI chatbot might help you to read more actively. \nYou can ask different questions about the text to help you understand what \nhappened. It may also help you to identify and understand key information. \nTaking notes is also important for active reading. It is not about copying a lot \nof information from the text. Instead, find the key information in a section, \nthink about what it means, and note it down in your own words.</td></tr></table>\n\nTable 10: Specific instructions by condition  \n\n<table><tr><td>Condition</td><td>Specific instructions</td></tr><tr><td>Notes</td><td>Actively read the text and take notes as you go along. Even if you think you understand everything, try doing so as best as you can. Think about the following things and note them down to help you:\n· The meaning of important words and concepts\n· The meaning of complex sentences\n· The key points or ideas, such as the dates, places, people and events\n· The connections between places, people and events\n· What happened, and why and how it happened\n· Similarities and differences between ideas and concepts\n· Your understanding of the text</td></tr><tr><td>LLM</td><td>Actively read the text and use the AI chatbot as you go along. Even if you think you understand everything, try doing so as best as you can. Think about the following things and use the AI chatbot to help you. For example, you can use it to:\n· Explain the meaning of important words and concepts\n· Rephrase or simplify complex sentences and explain them\n· Summarise the text and identify the key points or ideas, such as the dates, places, people and events\n· Clarify information you don’t understand\n· Explain the connections between places, people and events\n· Explain what happened, and why and how it happened\n· Identify similarities and differences between ideas and concepts\n· Check your understanding of the text\nYou can also:\n· Ask the AI chatbot for more explanation if you do not understand its response or think that something might not be quite right\n· Ask follow-up questions\n· Ask it to use bullet points, make its answer shorter, or use simpler language</td></tr><tr><td>LLM+Notes</td><td>Actively read the text, use the AI chatbot and take notes as you go along. Even if you think you understand everything, try doing so as best as you can. Think about the following things, and use the AI chatbot and take notes to help you. For example, you can use the AI chatbot to:\n· Explain the meaning of important words and concepts\n· Rephrase or simplify complex sentences and explain them\n· Summarise the text and identify the key points or ideas, such as the dates, places, people and events\n· Clarify information you don’t understand\n· Explain the connections between places, people and events\n· Explain what happened, and why and how it happened\n· Identify similarities and differences between ideas and concepts\n· Check your understanding of the text\n You can also:\n· Ask the AI chatbot for more explanation if you do not understand its response or think that something might not be quite right\n· Ask follow-up questions\n· Ask it to use bullet points, make its answer shorter, or use simpler language</td></tr></table>\n\n# 2.7 Test Questions\n\nTable 11: Example questions for literal retention, comprehension, and free recall  \n\n<table><tr><td>Construct\nItem type</td><td>Example question</td></tr><tr><td colspan=\"2\">Literal retention</td></tr><tr><td>Short response</td><td>What horrific event happened at the Soweto Youth Uprising in 1976? (Passage A)\nWhy did US President Kennedy avoid the term &quot;blockade&quot; when announcing the naval action around Cuba? (Passage B)</td></tr><tr><td>Multiple choice</td><td>What led to violent anti-apartheid protests? (Passage A)\n1) Police forcefully segregating people.\n2) Police arresting Nelson Mandela.\n3) Police killing Black civilians.\n4) Police implementing strict curfews.\nHow did the US government discover the presence of Soviet missiles in Cuba? (Passage B)\n1) A Cuban informant told them about the missiles.\n2) The Cuban government made threats to employ the missiles.\n3) The US Navy intercepted a Soviet ship carrying the missiles.\n4) A US plane captured photos of the missiles.</td></tr><tr><td colspan=\"2\">Comprehension</td></tr><tr><td>Short response</td><td>Explain the role that Nelson Mandela played during apartheid and its eventual end.\nYou only need to write a short paragraph. (Passage A)\nExplain the role of the Soviet Union in the Cuban Missile Crisis.\nYou only need to write a short paragraph. (Passage B)</td></tr><tr><td colspan=\"2\">Free recall</td></tr><tr><td>Open response</td><td>Write down everything you remember from the text &quot;[title]&quot;. Try to include as many details as possible.\nFor example, think about what happened, why and how, when, where, and who was involved.\nYou can write in full sentences or bullet points.</td></tr></table>\n\n# 2.8 Inter-rater Reliability Results\n\nTable 12: Inter-coder reliability  \n\n<table><tr><td>Item</td><td>ICC (A,1)</td><td>p-value</td><td>95% CI</td><td>Item</td><td>ICC (A,1)</td><td>p-value</td><td>95% CI</td></tr><tr><td>1</td><td>0.867</td><td>3.08 × 10-24</td><td>[0.781, 0.925]</td><td>15</td><td>0.923</td><td>2.17 × 10-32</td><td>[0.871, 0.958]</td></tr><tr><td>2</td><td>0.918</td><td>5.77 × 10-32</td><td>[0.863, 0.955]</td><td>16</td><td>0.989</td><td>1.29 × 10-61</td><td>[0.980, 0.994]</td></tr><tr><td>3</td><td>0.967</td><td>1.30 × 10-45</td><td>[0.943, 0.982]</td><td>17</td><td>0.962</td><td>8.52 × 10-43</td><td>[0.935, 0.979]</td></tr><tr><td>4</td><td>0.911</td><td>1.38 × 10-30</td><td>[0.851, 0.951]</td><td>18</td><td>0.961</td><td>4.95 × 10-42</td><td>[0.933, 0.979]</td></tr><tr><td>5</td><td>0.891</td><td>1.92 × 10-27</td><td>[0.819, 0.939]</td><td>19</td><td>0.938</td><td>7.34 × 10-36</td><td>[0.895, 0.966]</td></tr><tr><td>6</td><td>1.000</td><td>NaN</td><td>[NaN, NaN]</td><td>20</td><td>0.963</td><td>8.25 × 10-44</td><td>[0.936, 0.980]</td></tr><tr><td>7</td><td>0.951</td><td>2.65 × 10-39</td><td>[0.916, 0.973]</td><td>21</td><td>0.859</td><td>3.92 × 10-24</td><td>[0.770, 0.921]</td></tr><tr><td>8</td><td>0.936</td><td>2.38 × 10-33</td><td>[0.891, 0.965]</td><td>22</td><td>0.893</td><td>3.34 × 10-27</td><td>[0.822, 0.940]</td></tr><tr><td>9</td><td>0.930</td><td>9.00 × 10-31</td><td>[0.880, 0.962]</td><td>23</td><td>0.953</td><td>2.93 × 10-25</td><td>[0.912, 0.976]</td></tr><tr><td>10</td><td>0.954</td><td>1.88 × 10-39</td><td>[0.921, 0.975]</td><td>24</td><td>0.971</td><td>9.27 × 10-33</td><td>[0.947, 0.985]</td></tr><tr><td>11</td><td>0.920</td><td>1.89 × 10-30</td><td>[0.864, 0.956]</td><td>25</td><td>0.959</td><td>3.71 × 10-39</td><td>[0.928, 0.978]</td></tr><tr><td>12</td><td>0.969</td><td>5.35 × 10-40</td><td>[0.946, 0.984]</td><td>26</td><td>0.988</td><td>1.02 × 10-60</td><td>[0.980, 0.994]</td></tr><tr><td>13</td><td>0.959</td><td>6.30 × 10-42</td><td>[0.930, 0.978]</td><td>27</td><td>0.968</td><td>4.23 × 10-38</td><td>[0.943, 0.983]</td></tr><tr><td>14</td><td>0.927</td><td>2.80 × 10-33</td><td>[0.877, 0.960]</td><td>28</td><td>0.983</td><td>7.93 × 10-56</td><td>[0.971, 0.991]</td></tr></table>\n\n# 2.9 Survey Questions and Response Scales\n\nTable 13: Survey questions and response scales - Session 1  \n\n<table><tr><td>Variable</td><td>Question and response scale</td></tr><tr><td>Text difficulty</td><td>How difficult to understand did you find the text on [Passage title]? \n(Not at all difficult, Not very difficult, Somewhat difficult, Quite difficult, Very difficult)</td></tr><tr><td>Topic familiarity</td><td>How much did you already know about [Passage title] before starting the task? \n(Nothing at all, Not very much, A moderate amount, Quite a bit, Very much)</td></tr><tr><td>Topic interest</td><td>How interesting was the text on [Passage title]? \n(Not at all interesting, Not very interesting, Somewhat interesting, Quite interesting, Very interesting)</td></tr><tr><td>Activity enjoyment</td><td>How enjoyable was learning the text with the help of [activity]? \n(Not at all enjoyable, Not very enjoyable, Somewhat enjoyable, Quite enjoyable, Very enjoyable)</td></tr><tr><td>Activity difficulty</td><td>Overall, how difficult did you find the [activity]? \n(Not at all difficult, Not very difficult, Somewhat difficult, Quite difficult, Very difficult)</td></tr><tr><td>Activity helpfulness</td><td>How helpful was [activity] for understanding and learning the text? \n(Not at all helpful, Not very helpful, Somewhat helpful, Quite helpful, Very helpful)</td></tr><tr><td>Activity future use</td><td>Would you use a similar approach ([activity]) to understand and learn a text in the future? \n(Yes, No, I am not sure)</td></tr><tr><td>Task interest</td><td>How interesting was this task overall? \n(Not at all interesting, Not very interesting, Somewhat interesting, Quite interesting, Very interesting)</td></tr><tr><td>Task effort</td><td>How much effort did you put into understanding and learning the text on [Passage title]? \n(No effort at all, Only a little bit of effort, Some effort, Quite a bit of effort, A lot of effort)</td></tr><tr><td>Perceived task performance</td><td>How well do you think you did on the task? \n(Not at all well, Not very well, Somewhat well, Quite well, Very well)</td></tr><tr><td>Activity preference</td><td>Group 1: Which of the two learning approaches of this study did you prefer (note-taking or AI chatbot)? \n(I preferred learning by note-taking, I preferred learning with the help of the AI chatbot, I had no preference, I am not sure) \nGroup 2: Which of the two learning approaches of this study did you prefer (AI chatbot only or AI chatbot with note-taking)? \n(I preferred learning only with the help of the AI chatbot, I preferred learning with the help of the AI chatbot and by taking notes simultaneously, I had no preference, I am not sure)</td></tr><tr><td>Reason for preference</td><td>Can you tell us why you preferred this approach? [Open response]</td></tr><tr><td>Prior LLM use</td><td>Have you ever used an AI chatbot (such as ChatGPT, Microsoft Bing, and Google Bard AI) before this study? \n(Yes, No)</td></tr><tr><td>LLM use frequency</td><td>How often do you use an AI chatbot (approximately)? \n(Less than once a week, One or two days a week, Three to five days a week, Most days of the week)</td></tr><tr><td>Notes for learning frequency</td><td>How often do you take notes when reading a text for schoolwork, such as to prepare for a lesson or a test? \n(Never, Rarely, Sometimes, Often, Always)</td></tr><tr><td>LLM for learning frequency</td><td>How often do you use an AI chatbot when reading a text for schoolwork, such as to prepare for a lesson or a test? \n(Never, Rarely, Sometimes, Often, Always)</td></tr><tr><td>LLM+Notes for learning frequency</td><td>Group 2 only: How often do you use the two approaches (using an AI chatbot and taking notes) at the same time when reading a text for schoolwork? \n(Never, Rarely, Sometimes, Often, Always)</td></tr></table>\n\nTable 14: Survey questions and response scales - Session 2  \n\n<table><tr><td>Variable</td><td>Item and response categories</td></tr><tr><td>Perceived test performance</td><td>If all the questions on [Passage title] combined were worth a maximum of 100 points, how many points do you think you would have (approximately) scored? [Open response]</td></tr><tr><td>Learning in between sessions</td><td>Have you done anything between the first session and today&#x27;s session to further explore or understand the topics of the two texts? That could include looking up information online, taking notes after the session or discussing the topic with others. If so, please provide as much detail as you can about what you have done. [Open response]</td></tr><tr><td>Gender</td><td>What is your gender? [Open response]</td></tr><tr><td>EAL</td><td>Which language do you feel most comfortable speaking and communicating in?\n(English, A language other than English, Equally English and another language)</td></tr><tr><td>History</td><td>Are you taking GCSE History? (Yes, No)</td></tr></table>\n\n# 2.10 Learning Experiences and Perceptions\n\nTable 15: Differences in learning experiences and perceptions between conditions (for Group 1 and Group 2)  \n\n<table><tr><td rowspan=\"2\">Variable</td><td colspan=\"5\">Group 1: LLM vs Notes</td><td colspan=\"5\">Group 2: LLM vs LLM+Notes</td></tr><tr><td>Diff.</td><td>t(df)</td><td>p</td><td>95% CI</td><td>d</td><td>Diff.</td><td>t(df)</td><td>p</td><td>95% CI</td><td>d</td></tr><tr><td>Activity helpfulness</td><td>0.41</td><td>4.38(181)</td><td>&lt;0.001</td><td>[0.22, 0.59]</td><td>0.33</td><td>-0.03</td><td>-0.35(157)</td><td>0.724</td><td>[-0.21, 0.15]</td><td>-0.03</td></tr><tr><td>Activity difficulty</td><td>-0.51</td><td>-7.00(181)</td><td>&lt;0.001</td><td>[-0.66, -0.37]</td><td>-0.52</td><td>-0.41</td><td>-4.99(159)</td><td>&lt;0.001</td><td>[-0.57, -0.25]</td><td>-0.40</td></tr><tr><td>Task effort</td><td>-0.25</td><td>-3.53(182)</td><td>0.001</td><td>[-0.38, -0.11]</td><td>-0.26</td><td>-0.08</td><td>-1.03(159)</td><td>0.305</td><td>[-0.22, 0.07]</td><td>-0.08</td></tr><tr><td>Activity enjoyment</td><td>0.68</td><td>6.50(181)</td><td>&lt;0.001</td><td>[0.47, 0.89]</td><td>0.48</td><td>0.00</td><td>0.00(158)</td><td>1.000</td><td>[-0.16, 0.16]</td><td>0.00</td></tr><tr><td>Text interest</td><td>-0.11</td><td>-1.38(183)</td><td>0.170</td><td>[-0.26, 0.05]</td><td>-0.10</td><td>0.06</td><td>0.79(159)</td><td>0.428</td><td>[-0.09, 0.22]</td><td>0.06</td></tr><tr><td>Text difficulty</td><td>0.03</td><td>0.50(183)</td><td>0.621</td><td>[-0.10, 0.16]</td><td>0.04</td><td>0.03</td><td>0.41(159)</td><td>0.684</td><td>[-0.10, 0.15]</td><td>0.03</td></tr><tr><td>Task interest</td><td>0.09</td><td>1.01(183)</td><td>0.315</td><td>[-0.09, 0.27]</td><td>0.07</td><td>-0.06</td><td>-0.79(159)</td><td>0.430</td><td>[-0.20, 0.08]</td><td>-0.06</td></tr><tr><td>Perceived task performance</td><td>0.00</td><td>0.00(182)</td><td>1.000</td><td>[-0.14, 0.14]</td><td>0.00</td><td>-0.11</td><td>-1.45(158)</td><td>0.150</td><td>[-0.25, 0.04]</td><td>-0.12</td></tr><tr><td>Perceived test performance</td><td>-9.66</td><td>-5.53(177)</td><td>&lt;0.001</td><td>[-13.11, -6.22]</td><td>-0.42</td><td>-6.80</td><td>-3.55(143)</td><td>0.001</td><td>[-10.59, -3.02]</td><td>-0.30</td></tr></table>\n\n# 2.11 Coding Scheme Activity Preferences\n\nTable 16: Coding scheme: LLM over LLM+Notes preferences  \n\n<table><tr><td>Code</td><td>Description</td><td>Examples</td></tr><tr><td>LLM alone is quicker</td><td>Using the LLM alone is quicker than also taking notes, which takes time.</td><td>“It took less time to use the LLM”, “Notes take too much time.”</td></tr><tr><td>Both together not necessary</td><td>Notes are not necessary when the LLM already explains the text.</td><td>“The note-taking seemedunnec-\nsessary as the bot already helped explain”, “Using one sort of meant I didn’t need the other.”</td></tr><tr><td>LLM does the work for you</td><td>If you use the LLM alone, you don’t have to do the work your-\nself. The task becomes easier if you don’t have to take notes.</td><td>“Didn’t have to do any work”, “Clarify any information I didn’t know immediately without hav-\ning to scour the text”, “It was difficult to take notes at the same time as using the chatbot.”</td></tr><tr><td>Note-taking reduces question time</td><td>Note-taking takes away time from asking the LLM questions or understanding the text.</td><td>“I didn’t have enough time to ask as many questions when taking notes”, “I had more time to un-\nderstand the text.”</td></tr><tr><td>LLM does not support note-taking</td><td>LLM does not make note-taking easier.</td><td>&quot;Not as useful for making note-\ntaking easier.”</td></tr></table>\n\nTable 17: Coding scheme: LLM over Notes preferences  \n\n<table><tr><td>Code</td><td>Description</td><td>Examples</td></tr><tr><td>LLM is quick</td><td>LLM is quick and saves time.</td><td>“Less time-consuming”, “Much quicker.”</td></tr><tr><td>LLM is easy</td><td>LLM is easy and requires little effort compared to note-taking, which takes more effort and is more difficult.</td><td>“More simple”, “It was easier.”</td></tr><tr><td>LLM is (inter)active</td><td>LLM is an interactive or active learning activity.</td><td>“Actively engaging with the bot”, “Felt more interactive.”</td></tr><tr><td>LLM is emotionally engaging</td><td>LLM is more fun, enjoyable, and interesting.</td><td>“Enjoyed reading its responses”, “More fun to use.”</td></tr><tr><td>LLM helps you focus</td><td>LLM helps you focus on the text.</td><td>“Allowed me to focus more on the text.”</td></tr><tr><td>LLM helps you understand</td><td>LLM helps understanding and helps you check your understanding.</td><td>“It gives you a better understanding”, “I could confirm anything I was unsure of to ensure I understood it.”</td></tr><tr><td>LLM helps you learn</td><td>LLM supports learning.</td><td>“The AI helped me to learn more efficiently”, “I was able to understand and learn the text a lot easier and quicker at a higher level.”</td></tr><tr><td>LLM answers questions</td><td>LLM is helpful for understanding because it can answer questions and explain what you don’t understand.</td><td>“Ask any relevant questions”, “If I had a question, it could answer it.”</td></tr><tr><td>LLM can provide background and additional information</td><td>LLM is helpful for understanding because it provides background information and can elaborate on what happens.</td><td>“I was given more background”, “It gives me the full context.”</td></tr><tr><td>LLM can summarise and simplify information</td><td>LLM is helpful for understanding because it can simplify and rephrase information as well as summarise.</td><td>“It puts it in a simpler way and form”, “I can ask the AI chatbot to rephrase key points”, “It can summarise key points.”</td></tr><tr><td>LLM helps you remember</td><td>LLM helps you to remember the information in the text.</td><td>“It has stuck in my head more”, “Giving me prompt questions, mnemonics, etc., which helped me remember”, “Took less time to memorise than note-taking.”</td></tr></table>\n\nTable 18: Coding scheme: Notes over LLM preferences  \n\n<table><tr><td>Code</td><td>Description</td><td>Examples</td></tr><tr><td>Notes help you remember better</td><td>Note-taking helps you to remember information because you are physically writing it down. LLM does not help you remember as well.</td><td>“I can remember things better when I write them down”, “More helpful for developing recall”, “I learned more with note-taking”, “Just gave more background, rather than consolidating the knowledge.”</td></tr><tr><td>Notes help you understand</td><td>Note-taking helps you to understand better and check your understanding.</td><td>“It was easier for me to understand what I was reading”, “I was understanding it more”, “Test what you have learned by paraphrasing.”</td></tr><tr><td>Note-taking is active</td><td>Note-taking is more active.</td><td>“Better active reading”, “Allows me to actively engage.”</td></tr><tr><td>Notes are your own work</td><td>Note-taking means that you do the work yourself. You do the thinking and can use your own words and capture your own views.</td><td>“You have to personally analyse it”, “I could condense the information into my own words”, “Made me think for myself”, “It is your view on the matter you are looking at”, “Alows me to feel proud of my work in the future.”</td></tr><tr><td>Notes help you process information</td><td>Note-taking helps you process the information.</td><td>“I was able to break down and process the text”, “Summarising the second text myself helped me to process the information.”</td></tr><tr><td>Notes help you learn</td><td>Notes help you to learn, capture what you have learned, or test what you have learned.</td><td>“I am able to write down my own knowledge of what I had learned”, “I could actually learn the information rather than being told it.”</td></tr><tr><td>Notes can be revisited</td><td>Notes can be more easily revisited than the LLM output. You can easily access what you have learned or thought so far.</td><td>“I can come back to these notes at a later date if I am doing revision”, “Note-taking gives you something better to look back on in future.”</td></tr><tr><td>Notes are easier</td><td>Note-taking is easier than using the LLM.</td><td>“Easier to summarise”, “IDK, easier.”</td></tr><tr><td>Notes help with organisation</td><td>Notes help you to organise the information and thoughts and break it down into smaller parts to aid clarity.</td><td>“It is easy to organise my notes”, “It is easier to keep track of your train of thoughts”, “Helped me to break down the text into smaller chunks.”</td></tr><tr><td>LLM is distracting and provides too much information</td><td>LLM is distracting as you may ask questions that are not relevant or focus on things that are not important. LLM provides too much information, which can be overwhelming or confusing.</td><td>“I found myself easily distracted by the AI and was more tempted to ask random questions”, “It’s not clear as it gives too much information.”</td></tr><tr><td>LLM is repetitive and boring</td><td>LLM is boring and repetitive as it restates the information many times.</td><td>“It felt that it was just repeating it-self.”</td></tr><tr><td>Not sure what to ask the bot</td><td>The LLM is not needed because everything is understood, or one does not know how to use it and what kind of questions to ask.</td><td>“I struggled to think of questions to ask the AI”, “The text was very easy therefore didn’t feel the need to ask many questions.”</td></tr></table>\n\nTable 19: Coding scheme: LLM+Notes over LLM preferences  \n\n<table><tr><td>Code</td><td>Description</td><td>Examples</td></tr><tr><td>Both together are more enjoyable</td><td>Using LLM and notes together is more fun and enjoyable, whereas LLM alone can be boring.</td><td>“I enjoy using both at the same time”, “If I had to use the chatbot and ask it 20 questions, I would be very bored.”</td></tr><tr><td>Both together combine the best of both worlds</td><td>LLM and notes can be used in complementary ways to get the best of both, such as doing the work yourself and then using the LLM when you are unsure or stuck.</td><td>“It was easier to have my key notes summarised as well as text with more detail”, “It allowed me to note down the crucial parts of the event in a way that I can understand it and also get help from the AI chatbot on anything that isn’t clear.”</td></tr><tr><td>Both together are more helpful and easier</td><td>General statements about the strategy being more helpful, better, or easier for understanding and learning.</td><td>“Most helpful and easy to learn”, “Because I find it easier to remember and learn this way.”</td></tr><tr><td>Notes help you process and understand the information from the LLM</td><td>Notes help you process and understand the information given by the LLM.</td><td>“In order for me to process this, I find note-taking at the same time very helpful.”</td></tr><tr><td>Notes help with organisation</td><td>LLM provides information, but notes are needed to organise and structure ideas. The notes are also more focused and accessible.</td><td>“If I am only using the chatbot, then I have to scroll up to find what I am looking for”, “It was easier to keep track of things and go back over them.”</td></tr><tr><td>Notes are your own work</td><td>Taking notes means you do actual work and can capture your own thoughts rather than just reading output.</td><td>“It meant I was doing actual work.”</td></tr><tr><td>Notes help you remember</td><td>Notes help to remember the information.</td><td>“I like to write out information as I think it helps me remember it better.”</td></tr><tr><td>Notes help you understand</td><td>Note-taking helps you to understand better and to check your understanding.</td><td>“Simplifying it on paper made it easier to understand and remember.”</td></tr><tr><td>Notes help you learn</td><td>Notes help you to learn, capture what you have learned, or test what you have learned.</td><td>“You learn more”, “You can simplify what you have learnt in the notes.”</td></tr><tr><td>LLM can provide bad answers</td><td>LLM does not always answer questions well and sometimes not at all. LLM can be harmful.</td><td>“Some of the questions I had for the bot were not answered explicitly.”</td></tr><tr><td>LLM not always available</td><td>One needs to know how to take notes as LLMs might not always be available.</td><td>“You will not get an AI chatbot at all times.”</td></tr><tr><td>Not sure what to ask the bot</td><td>The LLM is not needed because everything is understood, or one does not know how to use it or what kind of questions to ask.</td><td>“I wasn’t sure what I was supposed to say to the bot. It was just kinda irritating.”</td></tr></table>\n\n# 2.12 Coding Scheme Prompt Interactions\n\nFor the full prompt coding scheme, please refer to tabular file 'PromptCoding.xlsx'\n\nTable 20: Prompt Coding Scheme  \n\n<table><tr><td>Overarching Code</td><td>Sub-code</td><td>Description and Examples</td></tr><tr><td>Information condensation</td><td>Summarise</td><td>The student asks the bot to summarise the entire text or a specific text selection.\nExamples: “Help me to summarise this paragraph”, “Summarise the text”, “Give me a summary of the first paragraph”, “Tell me what this text is about.”</td></tr><tr><td></td><td>Take notes</td><td>The student asks the bot to take notes about the text as a whole or a specific paragraph.\nExamples: “Make notes for the first paragraph.”</td></tr><tr><td></td><td>Identify key ideas</td><td>The student asks the bot to identify the key ideas or takeaway messages from the text, including key dates, places, people, and events.\nExamples: “What are the main points?”, “Give me all the important dates”, “What’s the takeaway message?”</td></tr><tr><td></td><td>Create timeline</td><td>The student asks the bot to create a timeline of events described in the text.\nExamples: “Put the important dates into chronological order”, “Give me a timeline of the events.”</td></tr><tr><td>Understanding the text</td><td>Define a word or concept</td><td>The student asks the bot to define or explain a specific word or concept from the text. They request help to understand terminology but do not ask for factual information beyond that.\nExamples: “What does apartheid mean?”, “What is a colony?”, “What is a missile?”, “I don’t know what a blockade is.”</td></tr><tr><td></td><td>Simplify or explain difficult sentences</td><td>The student asks the bot to simplify or explain the provided passage or a specific selection of the passage.\nExamples: “Explain this in simple words”, “Make the text simpler”, “What does this sentence mean?”, “Simplify this text.”</td></tr><tr><td></td><td>Checking understanding</td><td>The student explains their understanding and seeks confirmation from the bot.\nExamples: “The US did not like Cuba because they thought that Castro was a communist, right?”, “So it was one officer that prevented the whole war?”</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>General background</td><td>The student asks for background information on a place, time, or person mentioned in the text to provide context—information that is not too central for understanding the text but could be relevant.\nExamples: “Who was Kennedy?”, “What was Mandela famous for?”, “Tell me more about Cuba”, “How many British colonies were there in Africa?”, “Where were the Turkish missiles located?”</td></tr></table>\n\nContinued on next page\n\n<table><tr><td>Overarching Code</td><td>Sub-code</td><td>Description and Examples</td></tr><tr><td></td><td>Elaboration and deeper understanding</td><td>The student asks for more details about an event, such as why it happened, who was involved, and the outcome.\nExamples: “Why did the US not like Castro?”, “Why did the exiles invade Cuba?”, “How did black people feel during apartheid?”</td></tr><tr><td></td><td>Ask for examples or analogies</td><td>The student requests examples or analogies to better understand a concept or event.\nExamples: “What are examples of how apartheid affected daily life?”, “Is there an analogy that explains the Cold War tensions?”, “What unfair laws were passed?”, “What were some of the boycotts?”</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Ask for contrasts or comparisons</td><td>The student asks the bot to compare or contrast concepts, events, or figures.\nExamples: “How is apartheid different from segregation in the US?”, “Compare Kennedy and Khrushchev&#x27;s leadership styles.”</td></tr><tr><td></td><td>Critical analysis or evaluation</td><td>The student requests the bot to critically analyze or evaluate an action, situation, decision, or statement.\nExamples: “What are the strengths and weaknesses of Kennedy&#x27;s decision?”, “Evaluate the effectiveness of the blockade.”</td></tr><tr><td></td><td>Implications and significance</td><td>The student inquires about the broader implications, relevance, or consequences of information in the text.\nExamples: “What were the long-term effects of the crisis?”, “What is the situation like now?”, “Why should I care or learn about this?”</td></tr><tr><td>Study and memory help</td><td>Study and memory help</td><td>The student asks for assistance to learn and remember the text, including requests to be quizzed on the content.\nExamples: “Make a mnemonic”, “Write four questions about the text”, “How can I remember this better?”</td></tr><tr><td>Interacting with the Bot</td><td>Request specific format or length</td><td>The student requests that the bot provides its response in a specific format or length.\nExamples: “Summarize the main points in bullet points”, “Can you create a chart of the different policies?”, “Use only a few words”, “Make it short.”</td></tr><tr><td></td><td>Request improvement</td><td>The student asks the bot to improve its response or restate it in a simpler or shorter way rather than asking for simplifications of the provided passage.\nExamples: “I don’t understand what you said”, “Explain that again but shorter”, “What do you mean?”,\n“Simpler please”, “Can you write that in simpler terms?”, “Make the summary shorter.”</td></tr><tr><td></td><td>Relational language</td><td>The student engages in casual, polite conversation that is unrelated to the text.\nExamples: “How are you?”, “Thank you”, “Hello.”</td></tr></table>\n\nContinued on next page\n\n<table><tr><td>Overarching Code</td><td>Sub-code</td><td>Description and Examples</td></tr><tr><td></td><td>Checking source and trustworthiness</td><td>The student inquires about the sources or questions the accuracy of information.\nExamples: “What are your sources?”, “Why should I believe you?”, “I think your answer is wrong.”</td></tr><tr><td></td><td>Pasting text without specific request</td><td>The student pastes text directly from the provided passages without framing it as a specific question or request.\nExamples: “Nelson Mandela”, “In 1910, four British colonies joined to create the Union of South Africa”, “Missile.”</td></tr><tr><td>Irrelevant, Off-topic, miscellaneous</td><td>Irrelevant to text</td><td>The student asks a question unrelated to the text or its background.\nExamples: “Who is Che Guevara?”, “What is the song Abraxas?”</td></tr><tr><td></td><td>Miscellaneous</td><td>Use this code for segments that don’t fit any other codes. Use this as a last resort.</td></tr><tr><td>Irrelevant, Off-topic, Miscellaneous</td><td>Nonsensical input</td><td>The student types nonsensical characters, symbols, or text that does not form coherent words or sentences.\nExamples: “asdfgh”, “.”, “123”, “???”</td></tr></table>\n\n# 2.13 Frequency of Prompt Types\n\nTable 21: Frequencies of overarching prompt types  \n\n<table><tr><td>Overarching prompt type</td><td>Frequency</td></tr><tr><td>Archetype</td><td></td></tr><tr><td>Seeking additional information and deeper understanding</td><td>2265</td></tr><tr><td>Information condensation</td><td>749</td></tr><tr><td>Understanding the text</td><td>615</td></tr><tr><td>Study and memory help</td><td>39</td></tr><tr><td>Other</td><td></td></tr><tr><td>Interacting with the bot</td><td>760</td></tr><tr><td>Irrelevant, off-topic, miscellaneous</td><td>501</td></tr></table>\n\nTable 22: Frequencies of specific prompt types  \n\n<table><tr><td>Overarching prompt type</td><td>Specific prompt type</td><td>Frequency</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Elaboration and deeper understanding</td><td>1479</td></tr><tr><td>Information condensation</td><td>Summarise</td><td>588</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>General background</td><td>514</td></tr><tr><td>Understanding the text</td><td>Define a word or concept</td><td>463</td></tr><tr><td>Interacting with the Bot</td><td>Request specific format or length</td><td>430</td></tr><tr><td>Irrelevant, Off-topic, Miscellaneous</td><td>Irrelevant to text</td><td>296</td></tr><tr><td>Understanding the text</td><td>Simplify or explain difficult sentences</td><td>126</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Implications and significance</td><td>119</td></tr><tr><td>Information condensation</td><td>Identify key ideas</td><td>114</td></tr><tr><td>Interacting with the bot</td><td>Request improvement</td><td>113</td></tr><tr><td>Interacting with the bot</td><td>Pasting text without specific request</td><td>106</td></tr><tr><td>Interacting with the bot</td><td>Relational language</td><td>105</td></tr><tr><td>Irrelevant, off-topic, miscellaneous</td><td>Nonsensical input</td><td>109</td></tr><tr><td>Irrelevant, off-topic, miscellaneous</td><td>Miscellaneous</td><td>96</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Ask for examples or analogies</td><td>66</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Critical analysis or evaluation</td><td>54</td></tr><tr><td>Study and memory help</td><td>Study and memory help</td><td>39</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Ask for contrasts or comparisons</td><td>31</td></tr><tr><td>Understanding the text</td><td>Checking understanding</td><td>26</td></tr><tr><td>Information condensation</td><td>Take notes</td><td>26</td></tr><tr><td>Information condensation</td><td>Create timeline</td><td>21</td></tr><tr><td>Interacting with the bot</td><td>Checking source and trustworthiness</td><td>6</td></tr></table>\n\nNote: This table only includes prompt types that have been used at least three times by students.",
    "translated_content": "# 大型语言模型使用与笔记记录对阅读理解及记忆的影响：一项中学随机对照实验\n\n作者：\n\n皮娅·克雷基斯<sup>1</sup>、维克多·凯文尼希<sup>2*</sup>、玛蒂娜·库瓦尔贾<sup>1*</sup>、李敏娜<sup>2</sup>、西尔维娅·维特洛<sup>1</sup>、杰克·M·霍夫曼<sup>2</sup>、阿比盖尔·塞伦<sup>2</sup>、肖恩·林特尔<sup>2</sup>、丹尼尔·G·戈尔茨坦<sup>2</sup>、戴维·罗斯柴尔德<sup>2</sup>、列夫·坦凯列维奇<sup>2</sup>、蒂姆·奥茨<sup>1</sup>\n\n*共同第二作者\n\n## 所属机构：\n\n$^{1}$剑桥大学出版社与评估中心  \n$^{2}$微软研究院\n\n## 摘要\n\n学生对生成式人工智能（特别是大型语言模型，LLM）的迅速应用引发了关于其学习效果的紧迫问题。我们比较了使用LLM、传统笔记记录以及两者结合对中学生阅读理解与记忆保持的影响。我们在学校开展了一项结合组内与组间设计的预注册随机对照实验。405名14-15岁学生学习了两个文本段落，并在三天后完成了理解与记忆测试。定量结果表明，与单独使用LLM相比，单独进行笔记记录或结合使用LLM均对记忆保持和理解产生显著积极影响。然而，大多数学生更倾向于使用LLM而非笔记记录，并认为LLM更有帮助。定性分析显示，许多学生重视LLM使复杂材料更易理解并降低认知负荷的作用，同时认可笔记记录能促进深度参与并辅助记忆。此外，我们识别出提示行为的\"原型\"，为理解学生与LLM交互的不同方式提供了见解。总体而言，我们的研究结果表明，虽然笔记记录能促进认知参与和长期理解与记忆，但LLM可能有助于初始理解并激发学生兴趣。本研究揭示了传统学习方法持续的重要性、结合使用AI与传统学习相较于单独使用AI的优势，以及学生最大化这些益处所需的AI技能。\n\n## 正文\n\n学习者对生成式人工智能（特别是大型语言模型）迅速而广泛的采用，通过提供...## 翻译要求\n1. **保持格式**：保留所有 Markdown 格式（标题、列表、代码块、表格等）\n2. **术语准确**：专业术语翻译准确，必要时保留英文原词\n3. **学术风格**：使用学术论文的正式语体\n4. **公式保留**：LaTeX 公式保持原样，不翻译\n5. **引用保留**：参考文献引用格式保持原样\n\n## 原文内容\n学生参与学习材料的方式正在经历根本性变革，大型语言模型（LLMs）为此开辟了新途径$^{1;2;3;4;5;6}$，同时也带来了新的挑战$^{7;8;9;10;11;12}$。英国和美国的大规模全国性调查发现，相当大比例的中学生使用诸如OpenAI开发的ChatGPT等生成式人工智能工具$^{13;14}$。这一发展对传统教学模式提出了根本性质疑。然而，现有关于LLMs辅助学习的研究绝大多数聚焦于高等教育领域，对于低龄学习者的影响仍存在显著知识空白$^{15}$。此外，既往研究主要集中于第二语言教育（尤其是写作表现）以及计算机科学、医学和物理学领域$^{15}$。尽管此类研究总体显示使用LLMs对学业表现具有积极影响，但学者呼吁需保持审慎态度，因为这些效果可能反映的是LLMs生成成果的质量，而非学生真实学习能力的提升$^{15}$。LLMs使用对学习的两大基础要素——信息理解与记忆保持——的影响仍亟待深入探究。长期记忆存储的知识是认知的核心基石，构成几乎所有人类活动的基础$^{16}$。因此，在当前政策制定者和一线教育工作者面临诸多未知挑战之际，亟需明确LLMs对这些基础要素的影响，以指导此类工具与学校教育的整合路径。本研究针对LLMs使用对阅读理解与记忆保持的影响，开展了首批大规模量化研究之一。**阅读理解**是指理解书面材料并形成心理表征的过程<sup>17</sup>。诸如建构-整合模型（Construction-Integration model, CI model）<sup>18</sup>在内的阅读理解模型强调，读者需要在多个层面上理解文本：表层结构（词汇及其句法关系）、文本基础（通常表达一个完整思想的命题）以及情境模型（基于文本的推理）<sup>17</sup>。神经影像学研究支持了这种多层次结构<sup>19;20;21;22;16</sup>。进行推理的能力是理解的一个关键方面。通常区分两种类型的推理：基于文本的桥接推理涉及连接文本中不同位置的信息（例如，当前句子与先前句子），而基于知识的推理涉及将文本中的信息与先验知识相连接<sup>17</sup>。读者对文本的最终理解取决于多种要素之间复杂的相互作用，这些要素包括与读者特征相关的因素（例如，解码技能、词汇与语言知识、先验领域知识、工作记忆容量、推理能力、阅读策略知识、动机与目标）<sup>23;24;25;26;27</sup>、文本本身的特性（例如，体裁、长度、词汇与句子复杂度、连贯性）<sup>28;29</sup>以及阅读情境（例如，为消遣或学术目的而阅读）<sup>30;31</sup>。\n\n**阅读保持**是指将已理解的文本内容存储于长时记忆的过程。对于学习而言，不仅需要在阅读时理解文本，还需能够在日后记住所读和所理解的内容。保持部分地取决于编码期间（即阅读时的初始信息获取）信息加工的水平和质量。根据加工水平理论 $^{32;33}$ ，通过涉及意义、推理和含义的语义分析进行深度和精细化加工的信息，更容易被回忆起来。深度加工有助于形成丰富且相互关联的语义网络，这些网络提供了多重提取线索，从而增强了提取潜力，同时也促进构建一个稳健的图式框架，使得具体细节在其中得以有意义地组织和关联 $^{32;34}$ 。## 翻译结果\n\n根据 McNamara $^{35}$ 和 Chi $^{36}$ 的研究，存在多种能够提升理解与记忆效果的阅读策略与学习活动。在整个阅读过程中，理解监控尤为重要，其策略包括通过生成问题来评估自身理解程度 $^{35}$。以文本为核心的策略涉及对词语、句子及观点的意义解读（例如，释义、将冗长复杂的句子分解为可处理的单元、通过桥接推理连接不同概念）$^{35}$。诸如释义、筛选和重复等策略亦被视为主动学习策略，这些策略能够激活先验知识，并支持新知识的编码、存储与同化 $^{36}$。\n\n此外，还存在多种超越文本本身的有效阅读策略（例如，生成问题、运用自我解释、利用外部信息源）$^{35}$。此类策略被认为是建构性的，因为学习者通过解释、阐述和联结，生成新观点并更深度地整合信息。这涉及推断新知识、整合与组织新旧知识以及修正错误知识等认知过程 $^{36}$。最后，交互式学习活动包含与伙伴（如同伴或智能辅导系统等）进行有意义的对话 $^{36;28}$。此类互动可通过提供支架、纠正性反馈以及补充信息和新视角来促进学习。重要的是，只有当对话双方均做出实质性贡献时，对话才被视为具有交互性 $^{36}$。\n\n将大语言模型（LLM）工具融入教育领域引发了一个关键问题：其使用是否会促进或削弱阅读过程中的此类学习策略。这些模型在生成解释、提供多元视角、实时回应复杂问题及适应学习者个体需求方面展现出前所未有的灵活性<sup>37;38</sup>。作为超越学习者个人知识与技能的外部知识资源，LLM 有望增强学生对教育材料的理解与参与度<sup>39;40;10;41</sup>。此外，LLM 能够提供即时澄清并简化复杂概念，这可能有助于降低认知负荷<sup>42;43</sup>。因此，LLM 在帮助学习者构建多层次理解方面可能尤为有用：从表层文本理解与关键观点识别，到深层的文本意义表征，最终达成情境模型层面的全面心理表征。然而，过度使用大语言模型可能导致浅层加工，即学习者被动接收信息，而未主动参与深度认知加工或批判性思考$^{44;36;45;46;47}$。这种浅层参与可能阻碍综合心智模型的发展，对理解力和长期记忆保持产生负面影响$^{33;48}$。当学习者过度依赖大语言模型获取答案和解释时，他们可能减少运用自我解释和精细加工策略的倾向，而这些策略对理解和有意义学习至关重要$^{35;49;42}$。虽然大语言模型能便捷地提供信息，但需要以促进而非替代的方式利用这种可及性，从而推动知识巩固和学习所必需的深度认知加工$^{50;51}$。\n\n为评估将大语言模型作为阅读理解与记忆保持学习工具的有效性，我们将其与一种能促进多种主动性和建构性策略的常用学习活动——笔记记录进行对比。笔记记录是最普遍且广泛使用的学习活动之一，已被证明是阅读过程中有效的学习辅助手段$^{52;53}$。它能激发信息的主动加工，促进新内容与已有知识的整合，从而助力理解并创建有助于后续提取的记忆线索$^{52;54}$。笔记记录的效果似乎因所涉认知加工深度而异：它可能使读者聚焦于浅层加工（因为读者可能更关注表层结构和文本基础），但也可能通过促进精细加工和优化心理组织来增强情境模型$^{55;56;57}$。Kobayashi$^{52}$的元分析支持前一种观点，该研究发现笔记记录对高阶能力测试的效果较小，表明其生成价值可能有限且高度依赖于笔记质量（逐字记录或生成式记录）。鉴于结合使用大语言模型查询与笔记记录可能促进学习，我们还比较了单独使用大语言模型与结合使用大语言模型和笔记记录的效果。这两种活动可能通过发挥各自优势，对阅读理解和记忆保持产生互补效应。但同时也存在注意力分散的风险，导致两种活动的效果均降低。\n\n为检验大语言模型能否作为支持阅读理解与记忆保持这一基本学习过程的工具，我们开展了一项大规模、预注册、随机一项包含被试内与被试间设计要素的受控实验。本研究招募了405名14-15岁的中学生，在英国七所学校进行。实验包含学习环节与测试环节，两者间隔三天。在学习环节中，每位学生需通过采用循证策略的不同学习活动（学习条件）来理解并掌握两个不同历史主题的文本段落（南非种族隔离与古巴导弹危机）。学生未被告知将接受相关测试。他们被随机分为两组：第一组接触\"LLM\"（即使用大语言模型理解学习文本）和\"笔记\"（即通过记笔记理解学习文本）两种条件；第二组接触\"LLM\"和\"LLM+笔记\"（即结合大语言模型与记笔记理解学习文本）两种条件。学习条件与文本顺序均经过随机化处理。学习环节中的LLM功能由托管在Azure私有实例上的OpenAI GPT-3.5 turbo模型提供。每项学习任务结束后，学生需填写包含定量与定性问题的学习体验调查问卷。\n\n在测试环节中，学生完成了一系列评估不同层次理解与记忆水平的问题。具体而言，我们评估了其字面记忆、理解能力与自由回忆能力。针对每个文本段落：**字面记忆**（即低阶记忆）通过8道简答题（线索回忆）和10道选择题（再认）进行测量，这些问题评估无需知识推理、仅需最低程度文本衔接推理的字面信息；**理解能力**（即高阶记忆）通过3道开放式问答题测量，这些问题要求通过衔接推理整合文本不同位置的信息并进行知识推理；**自由回忆**则通过每篇文本1道开放式问答题进行评估，要求学生写下所有记忆内容，以此测量在无提示情况下的信息保持量与理解程度。## 翻译结果\n\n我们的主要目标是量化使用大语言模型对学生阅读理解与记忆保持的影响。我们选择不设置\"仅阅读\"对照组，这既是为了减少参与者在应对不同实验条件时的疲劳，也是基于以下考量：任何超越被动阅读的文本互动都可能改善学习效果$^{35;36}$，这相对降低了大语言模型使用效果的比较门槛。因此，我们决定将其与常见的、基于证据的学习活动——记笔记——进行比较。我们还探讨了学生参与不同学习活动时的学习体验，包括他们偏好哪种活动及其原因，以及揭示学习结果的不同提示行为\"原型\"。研究结果为全球教育领域的利益相关者和政策制定者提供了宝贵的见解。\n\n# 结果\n\n我们的研究以344名学生为样本（在应用预注册的排除标准后，更多信息见方法部分），调查了使用大语言模型相较于传统记笔记对学生学习成果的影响。第一组（LLM 与 Notes 条件）的最终样本为184名学生，第二组（LLM 与 LLM+Notes 条件）为160名学生。学生中男性略多于女性，大多数为英语母语者，少数学生（$(5.2\\%)$）享有免费学校餐，表明其处于社会经济劣势，约一半学生正在准备历史GCSE考试（所有学生特征见补充表3）。两组学生对三种学习条件（LLM、Notes、LLM+Notes）的事先熟悉程度相似。约一半学生经常记笔记，大多数学生报告先前使用LLM进行学习的情况有限（详细频率见补充表4）。\n\n# 学习成果\n\n我们比较了LLM（参考条件，所有学生均使用）与Notes（第一组学生使用）以及LLM+Notes（第二组学生使用）对学生字面记忆保持、理解和自由回忆的影响。传统记笔记在所有测量指标上均导致最佳表现，其次是LLM+Notes，而单独使用LLM则导致最低得分（描述性统计见补充表5）。\n\n线性混合效应模型证实了不同条件之间存在显著差异（见图1，所有模型系数、置信区间和效应大小见补充表6）。\n\n对于字面记忆保持，我们发现Notes（$\\beta = 1.92$，$p < 0.001$，95% CI [1.42, 2.42]）和LLM+Notes（$\\beta = 0.57$，$p = 0.040$，95% CI [0.03, 1.11]）均存在显著的主效应，表明学生使用Notes相较于LLM表现更好，使用LLM+Notes相较于LLM表现更好。## 翻译结果\n\n在理解能力方面，我们再次发现笔记（$\\beta = 0.95$, $p < 0.001$, $95\\%$ CI [0.62, 1.28]）和 LLM+笔记（$\\beta = 0.35$, $p = 0.049$, $95\\%$ CI [0.00, 0.70]）均存在显著的主效应，其中学生使用笔记相较于仅使用 LLM，以及使用 LLM+笔记相较于仅使用 LLM，表现均更优。\n\n在自由回忆方面，我们发现笔记存在显著的主效应（$\\beta = 1.02$, $p = 0.018$, 95% CI [0.18, 1.86]），但 LLM+笔记则无（$\\beta = -0.08$, $p = 0.855$, 95% CI [-0.98, 0.81]）。因此，学生使用笔记相较于仅使用 LLM 表现更好，但 LLM+笔记与仅使用 LLM 之间则无显著差异。鉴于自由回忆得分的非正态分布，我们还进行了这些检验的非参数版本作为稳健性检验（详见方法部分），其结果证实了上述发现。\n\n这些结果表明，与单独使用 LLM 相比，两种记笔记条件（单独记笔记或结合 LLM）均显示出学习效果的提升。然而，记笔记的益处体现在所有不同的学习测量指标上，而 LLM+笔记的益处仅体现在字面保持和理解能力上，在自由回忆方面则未显现。\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/f9c6b97ec629fd3a5afd56314cf1273a7a23652bdf7aa8dcc448b1d899f826ce.jpg)\n**图 1**：按条件和组别划分的测试成绩分布图，分别对应理解能力（左图，满分 12 分；笔记：$M = 4.89$, $SD = 2.52$；LLM+笔记：$M = 4.11$, $SD = 2.65$；LLM 第 1 组：$M = 4.00$, $SD = 2.44$；LLM 第 2 组：$M = 3.80$, $SD = 2.47$）、*字面保持（中图，满分 20 分；笔记：$M = 10.8$, $SD = 4.29$；LLM+笔记：$M = 9.68$, $SD = 4.83$；LLM 第 1 组：$M = 8.83$, $SD = 3.96$；LLM 第 2 组：$M = 8.95$, $SD = 4.29$）和*自由回忆（右图，满分 50 分；笔记：$M = 5.36$, $SD = 5.49$；LLM 第 1 组：$M = 4.32$, $SD = 4.15$；LLM 第 2 组：$M = 4.32$, $SD = 4.63$；LLM+笔记：$M = 4.20$, $SD = 5.07$）。每个分面中的两个大圆圈表示平均值，较小的点则表示个别学生的分数。误差线表示平均值上下一个标准误。第 1 组显示在每个子图的左侧分面，比较 LLM（红色）和笔记（蓝色）。第 2 组显示在每个图的右侧分面，比较 LLM（红色）和 LLM+笔记（绿色）。\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/41488ca1a6c3943e2825383542041eb80af29edf193795e1cd6d1ef164a3df0a.jpg)\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/cfcb380db33b073aea66229200e4a4b9ce36c4e9d8d6f6b463a22debcaf33262.jpg)\n\n# 行为投入## 翻译结果\n\n行为参与度（包括与大型语言模型的互动和笔记记录）通过向大型语言模型提出的平均查询次数、学生笔记中的平均字数以及任务耗时进行量化。与仅使用大型语言模型的条件相比，在可以使用笔记的情况下，学生的查询频率有所降低（在第二组中，从9.21次查询降至6.02次）。虽然学生在\"仅笔记\"和\"LLM+笔记\"两种条件下在记事本中撰写的字数相近（约100字），但一个值得关注的比例 $(25.63\\%)$ 的学生大量抄袭了大型语言模型的输出内容到其笔记中，其中部分学生 $(16.25\\%)$ 表现出近乎完全抄袭的情况（大型语言模型输出与笔记之间的三词重叠度超过 $90\\%$）。此外，与涉及笔记记录的条件相比，仅使用大型语言模型时学生在任务上花费的时间显著减少（第一组和第二组分别相差0.80分钟和1.54分钟），这表明当涉及笔记记录时，学生的参与度更深。行为测量指标的完整描述见补充表7。\n\n# 提问行为\n\n为了解学生如何与大型语言模型互动，我们对所有提问 $(n = 4,929)$ 进行了定性分析，采用了一种分层编码方案，其中具体的提问嵌套在总体提问类型之下。每个提问可被分配多个代码。我们识别了学生结合任务使用大型语言模型的四种行为原型，以及另外两种与任务非直接相关的总体提问类型（各LLM会话中提问类型的分布见图2）。关于总体提问类型的精确频次计数，见补充表21；关于具体提问类型，见补充表22。\n\n最常见的原型是寻求额外信息和更深层次的理解（2,265个提问，如图2中紫色条形所示）。绝大多数学生 $(90\\%)$ 至少使用过一次此类提问类型，约 $40\\%$ 的学生将其作为首次提问，$60\\%$ 的学生将其作为最常用的提问类型（见图3）。这些提问主要包括请求详细阐述（1,479例）和请求一般背景信息（514例）。示例包括\"如今的人们如何受到种族隔离的影响\"以及\"为什么释放纳尔逊·曼德拉花费了如此长的时间\"。## 翻译结果\n\n信息浓缩（749 条提示，如图 2 中的蓝绿色条形所示）是第二常见的原型，有 $27\\%$ 的学生将其作为首次提示，通常用于请求摘要或关键思想，例如\"全文的五个关键点是什么？\"或\"创建所有事件的时间线\"。第三类原型，即对文本的基本理解（615 条提示，图 2 中的绿色条形），被 $70\\%$ 的学生至少使用过一次，主要用于获取定义和内容简化，例如\"什么是制裁？\"和\"解释共产主义\"。第四类原型，即请求直接的学习和记忆帮助，使用频率较低（39 次，图 2 中的红色条形），尽管学生并未收到关于此类用途的明确指示。这些提示的范围包括要求 LLM 生成测验（\"就文本内容向我提出 4 个问题，并在我的下一次回复后告诉我答对了没有\"）到请求记忆辅助工具（\"为我创建一个关于古巴导弹危机的记忆辅助工具\"）。\n\n除了这些原型之外，有 760 条提示侧重于与 LLM 互动，而非（或同时）关注文本内容（图 2 中的蓝色条形），主要是请求特定的格式或改进回答。示例包括\"你能把这个做成要点列表吗？\"和\"将后果部分缩短为一句话\"。值得注意的是，只有六条提示质疑了 LLM 的可靠性。最后，约 $10\\%$ 的互动（501 条提示，图 2 中的棕色条形）是偏离主题或不相关的（例如，\"生命的意义是什么\"和\"告诉我关于哈利·波特的事\"），这表明有一小部分但可能相关的提示比例并未聚焦于任务，这可能是由于任务动机低或感到无聊所致。\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/d626ae4afddf164784c2957f218467f2fcf897ba4e897712255c0f3e6a5a4074.jpg)\n**图 2：** 不同条件和学生中，LLM 会话的提示类型分布。每个面板代表条件（仅 LLM 或 LLM+笔记）和文本段落（南非种族隔离或古巴导弹危机）的特定组合。每个条形显示单个 LLM 会话中每种类型的提示数量，会话按提示总数降序排列，同分情况按各类型内的提示数量区分。\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/b9a2f4d9cc9579f597bbeeb013a133f3f56b5f7e78028c7f54b3caea7c03b5ee.jpg)\n**图 3：** 学生提示在不同类型间的分布，显示了至少使用过一次该提示类型的学生百分比（蓝色），将其作为最常用提示的学生百分比（洋红色），以及将其作为首次提示的学生百分比（绿色）。提示类型按总体使用频率排列。\n\n# 学习体验与认知除了分析学生的行为投入外，我们还询问了他们在不同条件下的学习体验与感知。定量结果总结于图4，统计检验详情见附表15。我们采用经过 Bonferroni 校正（针对多重比较，$n = 18$）调整后的 p 值阈值 $0.05 / 18 = 0.002$ 来判断统计显著性。\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/c4c266d6421d905ef8a8bd42b99b86f7e33f41d2190d0d2c236b0c94e604e5c3.jpg)\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/23e6863e1c87df8e23a0c590c8e6744c9f75059bb10033cad565cccdca9a1e8e.jpg)\n\n**图4：** 不同组别和条件下学习体验与感知的差异。顶部面板以0-100分制展示了感知的测试表现，中间和底部面板分别以1-5分制显示了具有正价和负价的测量指标的评分。每个点代表一个条件的平均评分，误差线表示均值上下一个标准误。\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/2f7b3c6eb55edba33c7498db63ee23202e70938030ee28f26ed778c685bd2de3.jpg)\n**条件** $\\rightarrow$ **仅LLM** $\\rightarrow$ **LLM+笔记** $\\rightarrow$ **仅笔记**\n\n与实际学习结果相反，第1组学生认为LLM比记笔记更有帮助、更易用且更令人愉快，同时报告投入的努力更少。第2组学生在不同条件下的体验相似，但认为仅LLM条件比LLM+笔记条件难度更低。学生在学习期间感知的任务表现在不同条件下相似。测试后，两组学生均准确报告其感知的测试表现在仅LLM条件下低于仅笔记和LLM+笔记条件。\n\n这些发现表明，虽然仅LLM条件对学习的效果较差，但它提供了动机上的益处——这在第1组的偏好中尤为明显。重要的是，在第2组中将LLM使用与笔记相结合时，这些动机益处得以保持。\n\n# 活动偏好\n\n我们要求学生指出他们偏好的学习活动，并通过开放式回答解释其偏好原因（见表1）。在第1组中，大多数学生偏好LLM活动胜过传统笔记。这些学生列举的主要原因包括增强理解、LLM能够回答问题以及活动轻松便捷。偏好传统笔记的学生则强调了其对理解的好处、自我生成工作的重要性以及提升## 翻译结果\n记忆保持。在第二组中，绝大多数学生更喜欢组合活动，而非单独使用大语言模型。倾向于组合活动的学生指出两种方法的互补益处、增强的记忆保持以及改进的组织能力。那些偏爱仅使用大语言模型活动的学生则强调其效率，特别赞赏大语言模型为他们完成工作。这揭示了处理效率与处理深度之间的潜在张力——虽然仅使用大语言模型的活动被认为更高效，但涉及笔记记录的条件通过更深层次的投入和更好的记忆保持展现出更优的学习成果。\n\n表1：按组别划分的学习活动偏好及原因  \n\n（说明：原文表格内容未完整提供，故仅翻译标题部分。完整表格翻译需基于具体表格内容进行。）<table>\n<tr><td>活动偏好及原因</td><td>频数</td><td>百分比</td></tr>\n<tr><td colspan=\"3\">第一组：大语言模型 vs 笔记</td></tr>\n<tr><td>偏好大语言模型胜于笔记</td><td>89</td><td>42.0</td></tr>\n<tr><td>偏好笔记胜于大语言模型</td><td>57</td><td>26.9</td></tr>\n<tr><td>无偏好</td><td>48</td><td>22.6</td></tr>\n<tr><td>不确定</td><td>18</td><td>8.5</td></tr>\n<tr><td colspan=\"3\">第二组：大语言模型 vs 大语言模型+笔记</td></tr>\n<tr><td>偏好大语言模型胜于大语言模型+笔记</td><td>32</td><td>16.2</td></tr>\n<tr><td>偏好大语言模型+笔记胜于大语言模型</td><td>100</td><td>50.5</td></tr>\n<tr><td>无偏好</td><td>48</td><td>24.2</td></tr>\n<tr><td>不确定</td><td>18</td><td>9.1</td></tr>\n<tr><td colspan=\"3\">偏好大语言模型胜于笔记的原因</td></tr>\n<tr><td>有助于理解</td><td>34</td><td>21.9</td></tr>\n<tr><td>能够回答问题</td><td>23</td><td>14.8</td></tr>\n<tr><td>易于使用</td><td>22</td><td>14.2</td></tr>\n<tr><td>使用快捷</td><td>18</td><td>11.6</td></tr>\n<tr><td>提供背景知识</td><td>18</td><td>11.6</td></tr>\n<tr><td>总结与简化内容</td><td>17</td><td>11.0</td></tr>\n<tr><td>具有吸引力</td><td>10</td><td>6.5</td></tr>\n<tr><td>交互性强</td><td>8</td><td>5.2</td></tr>\n<tr><td>有助于记忆</td><td>4</td><td>2.6</td></tr>\n<tr><td colspan=\"3\">偏好笔记胜于大语言模型的原因</td></tr>\n<tr><td>有助于理解</td><td>22</td><td>21.4</td></tr>\n<tr><td>属于个人成果</td><td>21</td><td>20.4</td></tr>\n<tr><td>辅助记忆</td><td>18</td><td>17.5</td></tr>\n<tr><td>促进信息处理</td><td>8</td><td>7.8</td></tr>\n<tr><td>大语言模型使用方式不明确</td><td>7</td><td>6.8</td></tr>\n<tr><td>促进主动学习</td><td>6</td><td>5.8</td></tr>\n<tr><td>大语言模型易导致分心</td><td>6</td><td>5.8</td></tr>\n<tr><td>可重复查阅</td><td>5</td><td>4.9</td></tr>\n<tr><td>更为简便</td><td>4</td><td>3.9</td></tr>\n<tr><td>有助于知识组织</td><td>4</td><td>3.9</td></tr>\n<tr><td colspan=\"3\">偏好大语言模型胜于大语言模型+笔记的原因</td></tr>\n<tr><td>替代用户完成工作</td><td>15</td><td>50.0</td></tr>\n<tr><td>笔记非必需</td><td>5</td><td>16.7</td></tr>\n<tr><td>更快捷</td><td>4</td><td>13.3</td></tr>\n<tr><td>为提问留出更多时间</td><td>4</td><td>13.3</td></tr>\n<tr><td colspan=\"3\">偏好大语言模型+笔记胜于大语言模型的原因</td></tr>\n<tr><td>兼具两者优势</td><td>35</td><td>23.2</td></tr>\n<tr><td>有助于记忆</td><td>27</td><td>17.9</td></tr>\n<tr><td>有助于知识组织</td><td>24</td><td>15.9</td></tr>\n<tr><td>属于个人成果</td><td>21</td><td>13.9</td></tr>\n<tr><td>有助于理解</td><td>16</td><td>10.6</td></tr>\n<tr><td>更具帮助性且更简便</td><td>12</td><td>7.9</td></tr>\n<tr><td>有助于处理大语言模型输出</td><td>6</td><td>4.0</td></tr>\n<tr><td>更有趣味性</td><td>4</td><td>2.6</td></tr>\n<tr><td>大语言模型存在错误</td><td>3</td><td>2.0</td></tr>\n</table>\n\n注：本表仅收录至少被三名学生提及的原因。\n\n# 未来使用在学习环节结束时，学生报告了他们对各项活动未来的使用意向。在第一组中，大多数学生（64.4%）表示未来会使用大语言模型，仅有7.3%的学生表示不会使用，另有28.2%的学生表示不确定。计划未来做笔记的学生比例相对较低（55.3%），10.6%的学生表示不会做笔记，而34.1%的学生持不确定态度。在第二组中，大多数学生（59.5%）打算未来使用大语言模型，10.4%的学生不打算使用，30.1%的学生不确定。类似地，大多数学生（58.5%）计划未来使用\"大语言模型+笔记\"的组合活动，14.6%的学生不计划使用，26.8%的学生不确定。\n\n# 讨论\n\n本研究为大语言模型的使用如何与传统循证实践（特别是笔记记录）进行比较和互动，以支持学生的阅读理解、记忆保持和参与度提供了新的见解。它揭示了学习过程中人机交互背后的认知和动机动态，以及这些交互如何影响教育成果和认知。特别指出的是，它表明大语言模型的使用和更传统的笔记记录在学习过程中具有互补作用。\n\n在本研究中，我们发现，与单独使用大语言模型相比，记笔记——无论是单独进行还是与大语言模型一起使用——都产生了更高的理解和记忆得分，这凸显了传统主动学习策略的重要性和有效性。然而与此同时，学生普遍以建设性的方式使用大语言模型，并认为它们比记笔记更\"有帮助\"且更受青睐。我们应如何调和这些看似矛盾的结果？\n\n部分答案可能在于学生对于实际上什么有助于他们自身学习的元认知理解有限$^{58;59;60}$，特别是在生成式人工智能的背景下$^{61}$。具体而言，他们可能低估了诸如记笔记等活动所引发的\"必要难度\"的重要性$^{48}$。记笔记需要对信息进行主动处理，例如识别重要信息、进行释义和总结$^{52}$。虽然这些任务需要付出认知努力且本身可能并不令人愉快，但以往的研究表明，学习潜力随着所需认知投入水平的提高而增加$^{62}$。让大语言模型完成部分总结段落或解释概念的工作可能感觉更愉快、更高效，但这可能会减少深度理解和长期记忆所必需的认知投入。Deng等人的元分析$^{15}$也发现大语言模型的使用对学习者的情感-动机状态和心理努力有类似影响。此外，大语言模型有时可能会为学习者提供一些有趣但会干扰当前主要任务的干扰信息。同时，我们对学生提问的探索性分析表明，答案的另一部分在于大语言模型（LLM）提供的独特优势，这些优势可能确实有所帮助，超出了我们主要分析所捕捉到的范围。绝大多数LLM的使用是建设性的，而非令人分心或简化的，学生们旨在寻求额外信息和更深层次的理解。学生们表现出显著的好奇心，提出了超越文本本身的复杂问题。例如，在一段关于南非种族隔离、简要提及纳尔逊·曼德拉从囚犯到总统历程的文字中，一位学生问道：“曼德拉的人生故事是怎样的？”类似地，在一段假定读者具备一定冷战背景知识的古巴导弹危机文字中，另一位学生提问：“美国为什么害怕共产主义？”这些探索代表了一种不同类型的主动学习机会，这可能是仅靠记笔记无法实现的，从而凸显了LLM在拓展知识视野方面的潜力。话虽如此，这些更深层次的探究可能涉及权衡取舍：它们可能与处理段落核心信息形成竞争，降低了在测试项目上的表现，但它们也可能以我们的测试未能捕捉到的方式促进了学习，因为我们的测试仅关注文本内明确和隐含的内容。\n\n综上所述，我们的研究结果证明了结合使用LLM和记笔记的价值，这种方法不仅比单独使用LLM更有效，而且也是学生们更偏爱的活动。这提出了如何将记笔记等传统循证策略与LLM的独特优势相结合的机遇与挑战。我们不应将它们视为相互竞争的替代方案，而应将其视为互补品，经过深思熟虑的整合，能够以任何单一方法都无法实现的方式提升学习成果。实现这一点的关键在于在设计和应用基于LLM的新学习工具时，汲取教育工作者和研究人员的意见，正如过去传统方法与数字化方法融合的关键所在$^{63;64}$。\n\n我们的工作指出了几个这样的方向。首先也是最容易实现的是将LLM的使用与记笔记分离开来。在这种模式下，学生首先独立阅读文本，然后与LLM互动以进一步澄清和探索其内容。随后，他们再独立记笔记，并且不能简单地复制粘贴LLM的输出。这将防止学生采取我们在本研究中观察到的捷径，转而鼓励他们自己综合和内化信息。这是一个虽小但可能意义重大的设计选择，我们事先并未明显意识到，但它通过我们的工作显现出来，并可在未来的研究中加以检验。其次，教育工作者可以积极培训和引导学生以符合主动学习策略的方式使用大语言模型，例如提出针对性问题以澄清具体误解、进行批判性思考并整合信息，同时避免使其信息过载或减少认知加工过程$^{36;35}$。同样，教育工作者应劝阻学生被动接受自动生成的摘要和解释。这与将人工智能工具概念化为\"思维伙伴\"的理念相一致，即支持而非干扰人类现有的认知过程$^{9}$。超越学习活动本身，通过引导学生更有效地使用大语言模型，教育工作者将有助于学生更全面地发展元认知技能，这将使他们为长期使用这些技术做好更充分的准备。此外，可通过软件配置来支持这些目标，例如通过限制干扰行为并鼓励高效使用（可能通过捕获数据并利用大语言模型根据学生的交互记录提供反馈或提示）。\n\n第三，教育工作者可以利用学生与大语言模型的交互数据，更深入地理解他们正在困惑的概念或感兴趣的内容。这可以在个体层面实施，也可以针对整个班级进行集体分析——可能通过使用自动化工具以隐私保护的方式收集分析学生交互数据，并将洞察反馈给教育指导者。分析结果可用于定制未来的课程、活动和小组讨论。例如，通过分析我们实验中的提问记录，可以明显发现学生对共产主义原则及其在美国引发强烈恐惧和反对的原因充满好奇。\n\n本研究为不断发展的教育领域大语言模型影响研究作出了多项贡献。尽管已有研究多聚焦于大语言模型对任务绩效和效率的影响，但本研究探究了对学习与认知更为基础的维度。此外，本研究以来自不同学校类型的大样本中学生为研究对象，而非迄今为止更受关注的高等教育学生群体<sup>15</sup>。此类研究群体难以触达，特别是在需要多次研究会话的情况下。在研究设计过程中，我们力求真实还原学生在校学习情境，确保研究发现具有实践意义。具体而言，我们采用反映该年龄段学生课堂接触主题及难度的文本材料，并将大语言模型的使用效果与迄今为止普遍采用的学习活动进行对比。## 翻译要求\n1. **保持格式**: 保留所有 Markdown 格式（标题、列表、代码块、表格等）\n2. **术语准确**: 专业术语翻译准确，必要时保留英文原词\n3. **学术风格**: 使用学术论文的正式语体\n4. **公式保留**: LaTeX 公式保持原样，不翻译\n5. **引用保留**: 参考文献引用格式保持原样\n\n## 原文内容\n本研究的一个局限性在于，学生未接受针对不同学习活动的深入培训。尽管我们提供了关于如何与大型语言模型（LLM）互动及记笔记的说明和演示视频，但学生并未获得实践机会。这对LLM实验条件可能尤为不利，因为与记笔记相比，学生对使用LLM的熟悉度较低，因而可能未能充分发挥该活动的效能。此外，本研究若设置基线条件或被动阅读对照组，或能更准确地评估使用LLM理解文本是否比被动阅读更具优势（即衡量其本身的有效性）。另一局限是，尽管我们广泛抽取了内容样本，但受实际条件限制，所使用的保留和理解性问题数量有限，无法覆盖所有潜在问题范畴。因此，除自由回忆问题外，我们可能整体低估了学生的学习效果。此外，本研究仅局限于在完整课程体系之外进行的单次独立活动。若在真实场景中重复使用（如日常课堂或无人监督的自主作业），结果可能有所不同。最后，虽然本研究采用贴合学生水平的文本被视为优势，但LLM的应用可能对学生感到困难的文本更具价值——部分学生表示\"不知该向LLM提问什么\"正暗示了这一点。因此，探索LLM在超越学生当前能力的文本学习中的应用效果，可进一步拓展我们对潜在应用场景的认知。\n\n未来研究亟需探索何种LLM交互方式能最有效提升学习成果，并必须关注LLM融入学习环境的长期影响，特别是对其阅读技能、独立解决问题能力和元认知的作用。此外，理解这些工具如何影响社会对努力、专业素养和成就的认知也至关重要。LLM与生成式AI技术的演进可能重塑核心专业能力的定义，并改变各领域必要能力构成的格局<sup>8</sup>。未来，教育界与社会需明确在此新环境中哪些核心技能仍不可替代，并制定相应教学策略以保障其传承与发展<sup>9</sup>。本研究仅开启了如何有效利用LLM补充现有学习工具与活动，同时保持学生认知参与度的探索序幕。\n\n---\n**翻译说明**：\n- 专业术语如\"LLM\"保留英文缩写，首次出现时标注中文全称\n- 学术句式转换为中文论文常用表达（如\"might have been\"译为\"可能...正暗示了\"）\n- 保留原文引用格式<sup>8</sup>和逻辑连接词\n- 复杂长句按中文习惯拆分重组（如\"it is possible that...\"译为\"若...可能...\"句式）\n- 文化适配表达（如\"everyday classrooms\"译为\"日常课堂\"而非字面直译）## 翻译结果\n\n总之，本研究首次提供了关于大语言模型对阅读理解与记忆保持影响的大规模量化证据。我们的研究结果再次证实了传统策略（如记笔记）的重要性，这些策略能够促进深度认知投入并带来扎实的学习成果。与此同时，大语言模型为学习引入了新的可能性——提供了澄清、探索和情境化学习材料的机会——但这些工具必须在旨在增强而非绕过主动学习的适当指导下使用。教育工作者和研究人员不应将这些工具视为需要抵制的干扰，而是有机会主动引导其使用方式，以最大化学习潜力。通过这样做，我们既能让学生在人工智能融合的世界中茁壮成长，又能保持专注、深度和好奇心这些有意义教育的核心特质。\n\n# 材料与方法\n\n本研究包括两个阶段：预实验阶段和主体研究阶段。预实验阶段的目的是在学校情境中测试任务和拟议流程，并进行适当调整。本文报告的方法和发现是主体研究的一部分，该研究于2024年3月至7月期间进行。\n\n# 参与者\n\n参与者为来自英格兰七所中学的405名10年级学生（年龄14-15岁）。根据我们的排除标准（见补充章节1.1），我们保留了344名学生的数据进行分析。我们曾努力招募600名学生，但由于在暑假开始前未能找到足够多的学校而未能实现。招募方法包括向多个郡的学校校长发送电子邮件，并请参与学校联系其他学校。最终的学校样本包括三所非选拔性公立学校、两所文法学校（一所为女校，一所为男校）以及两所私立学校，分布于三个不同的郡。\n\n一旦学校同意参与，所有10年级学生均通过学校的项目负责人被邀请参加。信息表已与学生及其家长/监护人共享，之后双方被要求使用在线微软表单提供知情书面同意。本研究遵循英国教育研究协会$^{65}$的伦理指南。研究伦理批准由研究人员所在机构的研究伦理委员会提供。\n\n# 实验设计与流程\n\n本研究是一项预注册的随机对照实验，包含被试内和被试间设计要素，如图5所示。实验分两次进行，间隔三天，包括一次学习环节和一次测试环节。\n\n学习环节：在学习环节中，学生的任务是理解并学习两篇关于不同历史主题的文本（文本A和文本B）。每篇文本均通过一项特定的主动学习活动（条件）进行学习。三种条件如下：- **LLM组**：要求学生使用我们创建的LLM聊天机器人来辅助理解与学习文章。  \n- **笔记组**：要求学生通过记笔记的方式辅助理解与学习文章。  \n- **LLM+笔记组**：要求学生同时使用LLM聊天机器人与记笔记两种方式辅助理解与学习文章。\n\n学生被随机分配至以下两组之一：\n\n- **第一组**：接触LLM条件和笔记条件。  \n- **第二组**：接触LLM条件和LLM+笔记条件。\n\n随机分配结果显示，184名学生（53.5%）进入第一组，160名学生（46.5%）进入第二组。实验条件与文章顺序均经过随机化处理。在此阶段，学生还需完成关于学习体验的问卷调查。\n\n**测试阶段**：在测试阶段，学生需回答关于两篇文章（文章顺序随机）的理解与记忆问题，并完成个人基本特征的问卷调查。\n\n**时间安排**：学生学习阶段平均耗时约35分钟，测试阶段平均耗时约30分钟。\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/b21bdd2e3d49ceb66072818fc8bb684298786b88b09834ba3fb45c8e408c61ce.jpg)  \n组别、条件与文章随机分配顺序\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/b9b81a2d9ef90ec106dc670f00146ef1702cc9c8dc0607a32f8ae05c0131d727.jpg)  \n文章随机呈现顺序  \n**图5**：展示第一阶段与第二阶段活动流程的研究设计示意图\n\n# 实验设置与系统\n\n两个阶段均在正常上课时间于校内进行。学生以小组形式在教室同步参与实验，每人使用独立笔记本电脑或台式机完成操作。每阶段开始时，实验主持者或教师会宣读标准化指导语。整个过程中均有监督人员巡视并解答学生疑问。\n\n实验采用托管于github.com的网页应用程序，学生通过浏览器访问。针对第一阶段的LLM功能，该程序通过后端调用私有Azure Functions，访问托管于Azure的OpenAI GPT-3.5 turbo模型实例。所有LLM交互仅限在Azure平台内进行，不回流至OpenAI服务器。参与者最多可提交20条提示词。LLM通过不可见的元提示词进行定制（“你是一个帮助学生阅读和理解以下文章的AI聊天机器人：<文章内容> 学生可使用本工具查询生词、解释概念或总结文章要点”）。图6展示了LLM+笔记组的任务界面。笔记组与...\n\n# 南非种族隔离制度## 原文内容\n\n1910年，四个英国殖民地合并组建了\"南非联邦\"。该联邦是大英帝国的一部分，后来成为我们今天所知的南非共和国。二战后，许多受西方国家控制的地区（包括南非）寻求独立。南非政府希望摆脱大英帝国的控制。然而，对于南非黑人而言，主要斗争对象是反对具有英国和荷兰血统的白人南非人的歧视。\n\n1948年，国民党执政。新政府将歧视和种族隔离制度化为\" apartheid\"（种族隔离）体系。该体系持续了40多年，期间通过了诸多不公正法律。例如：每位公民必须按肤色进行分类；禁止不同肤色人群通婚；强制要求人们根据肤色居住在特定区域。超过350万有色人种被迫离开家园，许多人陷入贫困。\n\n非洲人国民大会（ANC）等反种族隔离组织最初仅采用和平抗议方式。1960年沙佩维尔大屠杀后这一策略发生转变——警方杀害了在警察局外和平抗议的黑人群体。活动人士开始转向暴力手段，包括破坏活动以及袭击警察和军队。作为回应，政府取缔了反种族隔离组织。在随后数十年间，反种族隔离活动人士面临逮捕、监禁甚至处决。例如非国大领袖纳尔逊·曼德拉曾被监禁27年。\n\n越来越多国家谴责种族隔离制度，并对南非实施制裁与抵制。1976年索韦托起义中的血腥事件也引发全球关注：黑人学生抗议强制使用荷兰殖民者语言——南非荷兰语授课的新法律，警方杀害了逾百名青少年。来自国内外持续增长的压力迫使政府做出改变。最终曼德拉获释，开启了终结种族隔离的谈判。1994年大选赋予所有南非公民（包括黑人）投票权，曼德拉由此成为首位民主选举产生的总统，标志着种族隔离制度的终结。但直至今日，许多南非黑人仍承受着该制度的负面影响。\n\n# AI聊天机器人 ②\n\n剩余提问次数：20次\n\n# 记事本\n\n![](/uploads/images/1daa7fa3-52d8-4ad3-bad2-545c83a3c45e/34bb33463af6cbdc665c50ca9aa10ad1e76195cb893c9f0d2effdf2c955d4149.jpg)  \n图6：LLM+笔记条件下的任务界面示例\n\n完成当前任务后，\n\n请点击继续。\n\n继续(12:29)\n\n#\n\n在LLM实验条件下，界面仅分别显示记事本或聊天机器人。\n\n# 学习任务与材料（第一阶段）在学习环节中，学生阅读两篇历史主题的文本，每篇配有不同的学习活动。要求学生尽可能理解和掌握文本内容。值得注意的是，学生并未被提前告知将会接受材料测试。针对每项任务，学生首先会收到指导说明（参见补充章节2.6关于主动阅读的价值、具体内涵以及指定阅读活动如何支持主动阅读的内容）。随后接收包含具体策略的详细任务说明，并观看任务操作和界面演示视频。所建议的策略基于主动阅读与阅读理解研究文献[29;35;36;66]。三种实验条件下的指导说明在内容与措辞上保持高度一致。任务开始后，学生需在任务页面停留10分钟（最低要求）至15分钟（最高限制）。\n\n每位学生阅读两篇说明性文本，每篇涵盖一个独立主题（均包含于英国考试委员会的GCSE历史大纲）：南非种族隔离（文本A）与古巴导弹危机（文本B）。文本改编自两部OpenStax教材（《世界历史（卷二）：1400年以降》《美国历史》）。为确保内容难度、语言复杂度及文本特征适用于十年级学生且具有可比性，我们进行了大幅调整。两篇文本均包含四个段落，在以下维度基本持平：文本长度（386词与385词）、平均词长（5.3与4.8字符）、词汇复杂度（基于万词高频词表的平均排序位次：1986位与1927位）、句子数量（均为26句）以及CEFR等级（均为C1中高级）。\n\n表2：字面记忆、理解与自由回忆的题型及评分标准\n\n<table>\n<tr><td>测评维度</td><td>题型（每题数量）</td><td>评分标准</td><td>满分</td></tr>\n<tr><td rowspan=\"2\">字面记忆</td><td>简答-线索回忆（8题）</td><td>每个信息点：<br>0 - 缺失/错误/无关<br>0.5 - 不完整/部分正确<br>1 - 正确</td><td>10</td></tr>\n<tr><td>四选项单选-再认（10题）</td><td>0 - 缺失/错误<br>1 - 正确</td><td>10</td></tr>\n<tr><td>理解能力</td><td>简答-线索回忆（3题）</td><td>每个观点：<br>0 - 缺失/错误/无关<br>0.5 - 不完整/部分正确<br>1 - 正确</td><td>12</td></tr>\n<tr><td>自由回忆</td><td>开放作答（1题）</td><td>每个信息点/观点：<br>0 - 错误/无关<br>0.5 - 不完整/部分正确<br>1 - 正确</td><td>50</td></tr>\n</table>\n\n注：字面记忆维度中8道\"简答-线索回忆\"题有2题每题计2分。\n\n为保障评分可比性，我们将每篇文本划分为50个核心观点单元。\n\n# 测试任务与材料（第二阶段）在测试环节中，研究人员告知学生需要回答关于第一阶段所阅读篇章的相关问题，以及关于任务和自身情况的若干一般性问题。每篇阅读材料设有22道测试题目，分别评估字面记忆、理解能力和自由回忆能力。表2展示了不同构念的具体评估方式。根据预先注册的研究方案，我们采用单一字面记忆分数（即简答题与选择题得分的总和）。两篇阅读材料的题目顺序均设置为：自由回答题、理解题、字面记忆题（线索回忆）以及字面记忆题（再认题）。学生需在自由回忆题部分至少花费三分钟、至多五分钟。题目经过精心排序，必要时通过分屏呈现以避免前序题目对后续题目产生提示效应。示例题目详见附表11。\n\n字面记忆题要求受试者通过直接回忆或再认篇章中的信息来作答。要完成此类题目，学生仅需理解文中词汇，无需具备其他背景知识。他们不需要进行任何基于知识的推理（精加工），也无需或仅需进行极少的基于文本的衔接推理（例如连接两个相邻句子）。因此，字面记忆题针对的是文本的表层和基础表征层次。\n\n相比之下，理解题则探查深层理解能力，要求学生通过衔接推理整合文本中不同位置的信息。参与者需进行基于知识的推理以获得更高分数，即推断文中隐含但未明示的信息。因此，理解题针对的是文本情境模型层面的表征。\n\n简答题与开放题由三位独立评分者批阅。这些评分者均为教育学和/或心理学方向的博士生，且对实验条件不知情。他们接受了评分标准培训，该标准为每道题目提供了总体说明、评分规则、详细解释及示例。在培训阶段，评分者通过批阅25名学生的作答并进行反馈，以确保评分标准应用的一致性与准确性。随后每位评分者独立批阅约140名学生的全部作答（包含两篇阅读材料的所有题目）。\n\n为评估评分者间信度，三位评分者共同批阅了35名学生（约占样本总量的$10\\%$）的完整作答。采用双向模型的组内相关系数（ICC）<sup>67</sup>进行信度评估。我们测量了绝对一致性，并采用单## 翻译结果\n\n### 测量方法\n由于我们最终对所有学生（除信度样本中的35名学生外）采用了单一评分者的分数，因此采用了单一测量方法。对于信度样本中的学生，我们在后续分析中使用了三位评分者评分的中位数。组合线索回忆保持分数（针对文本A和文本B各一个）、组合理解分数以及自由回忆分数的评分者间信度在.97至.99之间，表明信度极佳$^{67}$。$95\\%$置信区间的下限均高于.90的极佳信度阈值（见附表12）。\n\n### 调查问题\n所有问题及应答量表详见补充材料第2.9节。第一次会话中每项任务结束后，要求学生自我报告：文本难度及其对主题的熟悉度与兴趣度；学习活动的愉悦度、难度和帮助度，以及未来使用可能性；任务的整体兴趣度、努力投入度和感知任务表现。学生还被问及是否偏好某个学习活动及其原因，是否曾使用AI聊天机器人及使用频率，以及在校阅读文本时使用这些学习活动的频率。\n\n第二次会话中每次测试后，要求学生评估其感知的测试表现。会话结束时，询问学生是否在两次会话间隙进行了与两篇文本相关的学习。学生还需报告性别、英语语言状况以及是否正在修读GCSE历史课程。\n\n此外，从学校获取了免费校餐资格数据作为学生社会经济劣势的衡量指标$^{68}$。这是因为FSM资格通常基于家庭收入和其他社会经济因素。\n\n### 分析策略\n除下文所述外，我们未偏离预注册分析方案。首先，我们扩展分析至开展质性分析，探究学生偏好某一学习活动的原因。其次，尽管最初计划探讨学习条件与性别、EAL、FSM、GCSE历史和学校类型之间的交互效应，但因实际样本量小于计划样本量而未实施。\n\n定量分析使用Python 3.11和R 4.4.2进行。所有分析采用0.05显著性水平（双尾）。效应量通过Cohen's d估算，计算公式为每个变量的均值差除以配对差异的标准差。\n\n### 学习条件对文本理解与保持效果的评估## 翻译结果\n\n**缺失数据处理**  \n因变量不存在缺失数据，原因在于未完成两项测试的参与者已被排除（见排除标准），且个别问题的缺失回答均计为0分。协变量中的缺失值极少，仅出现于性别、EAL（英语作为附加语言）和GCSE历史三门变量（缺失率分别为 $5.23\\%$、$1.16\\%$ 和 $1.16\\%$）。采用链式方程多重插补法（MICE）处理缺失数据，使用 'mice' 包实现。模型在五个插补数据集上分别拟合，结果合并后获得综合估计值。\n\n**混合效应回归**  \n使用 'lme4' 包构建三个线性混合效应回归模型，分别对应三个结果变量（即字面记忆、理解程度、自由回忆）。模型中将学生设定为随机效应。需说明的是，自由回忆分析虽预注册为次要分析，但为简化表述，现与其他结果变量一并报告。回归模型设定如下：\n\n$$\n\\begin{array}{l} Y _ {i j} = \\beta_ {0} + \\beta_ {1} \\text {Condition} _ {i j} + \\beta_ {2} \\text {Group} _ {i j} + \\beta_ {3} \\text {School} _ {i j} + \\beta_ {4} \\text {Text} _ {i j} + \\beta_ {5} \\text {Task_Order} _ {i j} \\\\ + \\beta_ {6} \\text {Test_Order} _ {i j} + \\beta_ {7} \\text {Gender} _ {i j} + \\beta_ {8} \\text {FSM} _ {i j} + \\beta_ {9} \\text {EAL} _ {i j} + \\beta_ {1 0} \\text {History} _ {i j} + u _ {i j} + \\epsilon_ {i j} \\\\ \\end{array}\n$$\n\n其中：\n\n- $Y_{ij}$ 表示学生 $i$ 在条件 $j$ 下的结果变量值  \n- $\\beta_0$ 表示模型截距  \n- $\\beta_{1}$ 至 $\\beta_{10}$ 表示固定效应系数：\n\n  - Condition：三水平分类变量（0=LLM，1=笔记，2=LLM+笔记）  \n  - Group：表示组别归属的二分类变量  \n  - School：七水平分类变量，表示学校归属  \n  - Text：二分类变量，表示学生 $i$ 在条件 $j$ 下学习的文本  \n  - Task_Order：二分类变量，表示学生 $i$ 完成条件 $j$ 任务的顺序（先/后）  \n  - Test_Order：二分类变量，表示文本测试顺序（先/后）  \n  - Gender：四水平分类变量（0=女性，1=男性，2=其他，3=拒绝回答）  \n  - FSM：二分类变量，表示学生是否享受免费校餐  \n  - EAL：分类变量，表示学生英语语言状态（0=母语者，1=双语者，2=其他）  \n  - History：二分类变量，表示学生是否选修GCSE历史课程\n\n- $u_{ij}$ 表示每位学生的随机截距  \n- $\\epsilon_{ij}$ 表示学生 $i$ 在条件 $j$ 下的误差项如图1所示，自由回忆得分呈非正态分布，因此我们进行了额外的非参数置换检验。具体而言，我们使用R语言中的'infer'包在学生层面进行了配对置换检验。这些检验比较了第一组中LLM条件与笔记条件之间的自由回忆得分，以及第二组中LLM条件与LLM+笔记条件之间的自由回忆得分。针对每位学生，我们计算了其两个得分之间的差异，并对所有学生的差异取平均值。将该检验统计量与零分布进行比较，零分布通过重复随机化学生内差异的符号并计算均值而生成。此过程在所有插补数据实例中重复进行，并通过取各实例中p值的中位数来汇总结果，从而获得合并p值。这样做得到了与混合效应模型相似的结果：在第一组中，我们发现笔记条件与LLM条件之间的自由回忆存在显著差异$(p = 0.02)$，但在第二组中未发现LLM+笔记条件与LLM条件之间的自由回忆存在显著差异的证据$(p = 0.80)$。\n\n# 对学生提示的质性探索\n\n为了解LLM条件对阅读理解和记忆保持影响的潜在解释，我们在计划性探索分析中试图了解学生使用LLM时生成的提示类型。通过调用Azure OpenAI API（部署日期2024-06-01）的Python自动化脚本，使用GPT-4采用分层编码方案对LLM提示进行分析。温度参数设置为0以确保确定性输出，并采用窄采样范围（top-p=0.1）来保证分类的一致性。模型获得了每个类别的详细说明和示例，以及学生正在学习的文本材料。每个提示可被赋予多个子代码。\n\n该分层编码方案经过多次迭代开发而成。初始版本由研究人员基于主动阅读文献、学生任务说明和试点工作，通过演绎和归纳法制定。随后根据API的建议扩展该方案，并要求API使用该编码方案对数据进行编码。研究人员通过检查部分API输出结果，迭代优化编码方案：根据需要合并、删除和添加代码，调整代码描述和示例以提高API输出质量。最后，一位研究人员手动检查了500条提示（约占总数据量的$10\\%$）的API输出，发现错误率为$5.6\\%$，该错误率被认为可接受。对这500条提示的指定代码进行了必要调整，其余API输出保持原样。学生提示的最终编码方案见附表20。\n\n# 学生学习体验的量化探索## 定性探索学生的活动偏好\n\n我们探究了学生对某一学习活动优于另一活动所给出的开放式解释。两位作者借助前述 API 对这些解释进行了分析。我们分别分析了四个偏好组：\n\n1.  LLM 优于笔记，\n2.  笔记优于 LLM，\n3.  LLM 优于 LLM+笔记，以及\n4.  LLM+笔记优于 LLM。\n\n每个偏好组都有其独立的编码方案，该方案仅包含偏好某一活动（优于非偏好活动）的解释（例如，如果学生偏好 LLM 而非笔记，则笔记记录的优势不会被编码）。初始方案是通过对每个偏好组约 $30\\%$ 的回答进行手动和演绎式编码而制定的。每个回答可以应用多个代码。初始编码方案，包括类别标签、描述和示例，连同数据及通用编码指令一并提供给 API。API 未提出任何其他有用的代码。随后，研究人员通过手动检查 API 输出的部分内容，迭代地完善编码方案。他们合并、删除和添加代码，并完善代码描述和示例，然后重新运行 API 分析。重复此过程，直至两位研究人员均对编码方案感到满意。由于需要编码的回答数量较少（ $n = 278$ ），一位研究人员检查了 API 的全部输出并在必要时进行了调整。活动偏好的最终编码方案见补充章节 2.11。\n\n## 数据可用性\n\n所有定量数据将在论文发表后公开。由于存在共享可识别信息的风险，我们不会提供以下定性数据：学生的 LLM 交互记录（仅共享应用的代码）、学生的笔记、学生的活动偏好（仅共享应用的代码）。\n\n## 代码可用性\n\n相应代码将在论文发表后共享。\n\n## 伦理声明\n\n## 利益冲突\n\n部分作者在一家投资于生成式人工智能并以生成式人工智能模型为核心组件开发技术的公司从事研究工作。其他作者隶属于一家出版、评估和学习机构，该机构在开发和运营评估与学习产品及服务时越来越多地使用人工智能。然而，本项工作与任一组织的任何特定产品或盈利努力均无关联。\n\n## 致谢\n\n我们感谢 Tom Benton 博士和 Matthew Carroll 博士对本研究中所进行分析提供的宝贵建议。# 补充材料\n\n# 目录\n\n# 补充信息\n\n- 参与者排除标准\n\n# 补充表格\n\n- 学生特征\n  学习活动熟悉度\n- 描述性统计\n- 混合效应回归结果\n  行为参与度\n- 主动阅读导论\n- 学习活动导论\n\n- 按实验条件划分的具体说明\n  测试问题\n- 评分者间信度结果\n  调查问题与回答量表\n  调查问题与回答量表（第2阶段）\n- 学习体验与感知\n  活动偏好编码方案\n  编码方案：偏好LLM而非笔记\n  编码方案：偏好笔记而非LLM\n  编码方案：偏好LLM+笔记而非LLM\n  编码方案：提示交互\n- 提示类型频率\n\n# 参考文献\n\n[1] Cecilia Ka Yuk Chan. 面向大学教与学的综合性人工智能政策教育框架. International Journal of Educational Technology in Higher Education, 20(1):38, 2023年7月. ISSN 2365-9440. doi: 10.1186/s41239-023-00408-3. URL https://doi.org/10.1186/s41239-023-00408-3.\n[2] Abdulhadi Shoufan. 探索学生对ChatGPT的认知：主题分析与后续调查. IEEE Access, 11:38805-38818, 2023. ISSN 2169-3536. doi: 10.1109/ACCESS.2023.3268224. URL https://ieeexplore.ieee.org/document/10105236/?arnumber=10105236. 期刊名称: IEEE Access.\n[3] K. Aleksić-Maslac, F. Borović, 与 Z. Biočina. 教育系统中ChatGPT的认知与使用. INTED2024论文集, 页 1842-1848, 2024. ISSN 2340-1079. doi: 10.21125/inted.2024.0511. URL https://library.iated.org/view/ ALEKSICMASLAC2024PER. 会议名称: 第18届国际技术、教育与发展会议 ISBN: 9788409592159 会议名称: 第18届国际技术、教育与发展会议 地点: 西班牙瓦伦西亚 出版社: IATED.\n[4] Nikhil Singh, Guillermo Bernal, Daria Savchenko, 与 Elena L. Glassman. 何处隐藏被窃大象：多模态机器智能带来的创造性写作飞跃. ACM Transactions on Computer-Human Interaction, 2022年2月. ISSN 1073-0516. doi: 10.1145/3511599. URL https://dl.acm.org/doi/10.1145/3511599. 已接受待发表.\n[5] Heather Johnston, Rebecca F. Wells, Elizabeth M. Shanks, Timothy Boey, 与 Bryony N. Parsons. 学生对高等教育中使用生成式人工智能技术的观点. International Journal for Educational Integrity, 20(1):2, 2024年2月. ISSN 1833-2595. doi: 10.1007/s40979-024-00149-4. URL https://doi.org/10.1007/s40979-024-00149-4.[6] Duong Hoai Lan 和 Tran Minh Tung。分析越南大学生使用Chat-GPT的影响。《Migration Letters》，第20卷S10期，第259-268页，2023年11月。ISSN 1741-8992。doi: 10.59670/ml.v20iS10.5134。URL https://migrationletters.com/index.php/ml/article/view/5134。期号: S10。\n\n[7] Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnmann, Eyke Hüllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, Gjergji Kasneci。ChatGPT 利大于弊？论大型语言模型在教育领域的机遇与挑战。《Learning and Individual Differences》，2023年。\n\n[8] Stefan E. Huber, Kristian Kiili, Steve Nebel, Richard M. Ryan, Michael Sailer, Manuel Ninaus。通过游戏化和基于游戏的学习释放大型语言模型在教育中的潜力。《Educational Psychology Review》，第36卷第1期，第25页，2024年2月。ISSN 1573-336X。doi: 10.1007/s10648-024-09868-z。URL https://doi.org/10.1007/s10648-024-09868-z。\n\n[9] Yogesh K. Dwivedi, Nir Kshetri, Laurie Hughes, Emma Louise Slade, Anand Jeyaraj, Arpan Kumar Kar, Abdullah M. Baabdullah, Alex Koohang, Vishnupriya Raghavan, Manju Ahuja, Hanaa Albanna, Mousa Ahmad Albashrawi, Adil S. Al-Busaidi, Janarthanan Balakrishnan, Yves Barlette, Sriparna Basu, Indranil Bose, Laurence Brooks, Dimitrios Buhalis, Lemuria Carter, Soumyadeb Chowdhury, Tom Crick, Scott W. Cunningham, Gareth H. Davies, Robert M. Davison, Rahul De, Denis Dennehy, Yanqing Duan, Rameshwar Dubey, Rohita Dwivedi, John S. Edwards, Carlos Flavian, Robin Gauld, Varun Grover, Mei-Chih Hu, Marijn Janssen, Paul Jones, Iris Junglas, Sangeeta Khorana, Sascha Kraus, Kai R. Larsen, Paul Latreille, Sven Laumer, F. Tegwen Malik, Abbas Mardani, Marcello Mariani, Sunil Mithas, Emmanuel Mogaji, Jeretta Horn Nord, Siobhan O'Connor, Fevzi Okumus, Margherita Pagani, Neeraj Pandey, Savvas Papagiannidis, Ilias O. Pappas, Nishith Pathak, Jan Pries-Heje, Ramakrishnan Raman, Nripendra P. Rana, Sven-Volker Rehm, Samuel Ribeiro-Navarrete, Alexander Richter, Frantz Rowe, Suprateek Sarker, Bernd Carsten Stahl, Manoj Kumar Tiwari, Wil van der Aalst, Viswanath Venkatesh, Giampaolo Viglia, Michael Wade, Paul Walton, Jochen Wirtz, Ryan Wright。观点论文：\"即使ChatGPT写了又如何？\"关于生成式对话AI对研究、实践与政策的机遇、挑战及影响的多学科视角。《International Journal of Information Management》，第71卷，第102642页，2023年8月。ISSN 0268-4012。doi: 10.1016/j.ijinfomgt.2023.102642。URL https://www.sciencedirect.com/science/article/pii/S0268401223000233。\n\n[10] Jun-Jie Zhu, Jinyue Jiang, Meiqi Yang, Zhiyong Jason Ren。ChatGPT与环境研究。《环境科学与技术》，第57卷第46期，第17667-17670页，2023年11月。ISSN 0013-936X。doi: 10.1021/acs.est.3c01818。URL https://doi.org/10.1021/acs.est.3c01818。出版者：美国化学会。\n\n[11] Alex Barrett 和 Austin Pack。并非完全的人与AI对视：学生和教师对生成式人工智能在写作过程中应用的看法。《International Journal of Educational Technology in Higher Education》，第20卷第1期，第59页，2023年11月。ISSN 2365-9440。doi: 10.1186/s41239-023-00427-0。URL https://doi.org/10.1186/s41239-023-00427-0。\n\n[12] Aiste Steponenaite 和 Basel Barakat。人工智能赋能世界中的剽窃问题。见 Margherita Antona 和 Constantine Stephanidis 编辑，《Universal Access in Human-Computer Interaction》，第434–442页，Cham，2023年。Springer Nature Switzerland。ISBN 978-3-031-35897-5。doi: 10.1007/978-3-031-35897-5_31。```chinese\n## 参考文献\n\n[13] Ofcom. 《2024年在线国家报告》. 技术报告, Ofcom, 2024年11月. URL https://www.ofcom.org.uk/media-use-and-attitudes/online-habits/online-nation/.\n\n[14] 沃尔顿家族基金会. 《教师和学生拥抱ChatGPT用于教育》. 技术报告, 沃尔顿家族基金会, 2023年3月. URL https://www.waltonfamilyfoundation.org/learning/teachers-and-students-embrace-chatgpt-for-education. 栏目: 学习.\n\n[15] Ruiqi Deng, Maoli Jiang, Xinlu Yu, Yuyan Lu, and Shasha Liu. ChatGPT是否促进学生学习？实验性研究的系统综述与元分析. 《计算机与教育》, 227:105224, 2025. ISSN 0360-1315. doi: https://doi.org/10.1016/j.compedu.2024.105224. URL https://www.sciencedirect.com/science/article/pii/S0360131524002380.\n\n[16] Jeffrey R. Binder and Rutvik H. Desai. 语义记忆的神经生物学. 《认知科学趋势》, 15(11):527-536, 2011年11月. ISSN 1879-307X. doi: 10.1016/j.tics.2011.10.001.\n\n[17] Danielle S. McNamara and Joe Magliano. 迈向综合的理解模型. 载于《学习与动机心理学》第51卷, 《学习与动机心理学》, 页297-384. Elsevier Academic Press, 美国加州圣地亚哥, 2009. ISBN 978-0-12-374489-0. doi: 10.1016/S0079-7421(09)51009-2.\n\n[18] Walter Kintsch. 知识在语篇理解中的作用：建构-整合模型. 《心理学评论》, 95(2):163–182, 1988. ISSN 1939-1471. doi: 10.1037/0033-295X.95.2.163. 出版地: 美国 出版商: 美国心理学会.\n\n[19] Gregory Hickok and David Poeppel. 言语处理的皮层组织. 《自然评论：神经科学》, 8(5):393-402, 2007年5月. ISSN 1471-0048. doi: 10.1038/nrn2113. URL https://www.nature.com/articles/nrn2113. 出版商: Nature Publishing Group.\n\n[20] Evelina Fedorenko, Anna A. Ivanova, and Tamar I. Regev. 语言网络作为人脑更广泛图谱中的自然类. 《自然评论：神经科学》, 25 (5):289-312, 2024年5月. ISSN 1471-0048. doi: 10.1038/s41583-024-00802-4. URL https://www.nature.com/articles/s41583-024-00802-4. 出版商: Nature Publishing Group.\n\n[21] Rolf A. Zwaan and Gabriel A. Radvansky. 语言理解与记忆中的情境模型. 《心理学通报》, 123(2):162–185, 1998. ISSN 1939-1455. doi: 10.1037/0033-2909.123.2.162. 出版地: 美国 出版商: 美国心理学会.\n\n[22] Junhua Ding, Keliang Chen, Haoming Liu, Lin Huang, Yan Chen, Yingru Lv, Qing Yang, Qihao Guo, Zaizhu Han, and Matthew A. Lambon Ralph. 语义痴呆中语义、语言、社会行为和面孔识别的统一神经认知模型. 《自然通讯》, 11(1):2595, 2020年5月. ISSN 2041-1723. doi: 10.1038/s41467-020-16089-9. URL https://www.nature.com/articles/s41467-020-16089-9. 出版商: Nature Publishing Group.\n\n[23] Kate Cain and Jane Oakhill. 阅读理解困难：相关性、原因与后果. 载于《儿童口头与书面语言理解问题：认知视角》, 《语言与素养的挑战》系列, 页41-75. The Guilford Press, 美国纽约州纽约市, 2007. ISBN 978-1-59385-443-0.\n\n[24] Meredithyth Daneman and Patricia A. Carpenter. 工作记忆与阅读的个体差异. 《言语学习与言语行为杂志》, 19(4):450-466, 1980. ISSN 0022-5371. doi: 10.1016/S0022-5371(80)90312-6. 出版地: 荷兰 出版商: Elsevier Science.\n```[25] Charles A. Perfetti, Nicole Landi, and Jane Oakhill. 阅读理解技能的习得. 见：*阅读科学：手册*，Blackwell 发展心理学手册，第 227-247 页. Blackwell Publishing, Malden, 2005. ISBN 978-1-4051-1488-2. doi: 10.1002/9780470757642.ch13.\n[26] Jane V. Oakhill, Molly S. Berenhaus, and Kate Cain. 儿童阅读理解与理解困难. 见：*牛津阅读手册*，牛津心理学文库，第 344-360 页. Oxford University Press, New York, NY, US, 2015. ISBN 978-0-19-932457-6. doi: 10.1093/oxfordhb/9780199324576.001.0001.\n[27] Keith E. Stanovich. 阅读中的马太效应：读写能力习得中个体差异的若干后果. Reading Research Quarterly, 21(4):360-407, 1986. ISSN 1936-2722. doi: 10.1598/RRQ.21.4.1. 出版地：美国 出版商：International Reading Association.\n[28] A. C. Graesser, M. Singer, and T. Trabasso. 叙事文本理解过程中推理的构建. *Psychological Review*, 101(3):371–395, July 1994. ISSN 0033-295X. doi: 10.1037/0033-295x.101.3.371.\n[29] Danielle S. McNamara, Irwin B. Levinstein, and Chutima Boonthum. iSTART：促进主动阅读与思考的交互式策略训练. Behavior Research Methods, Instruments, 3 Computers, 36(2):222-233, May 2004. ISSN 1532-5970. doi: 10.3758/BF03195567. URL https://doi.org/10.3758/BF03195567.\n[30] John T. Guthrie and Allan Wigfield. 阅读中的投入与动机. 见：阅读研究手册，卷 III，第 403-422 页. Lawrence Erlbaum Associates Publishers, Mahwah, NJ, US, 2000. ISBN 978-0-8058-2398-1 978-0-8058-2399-8.\n[31] Tracy Linderholm, Sandra Virtue, Yuhtsuen Tzeng, and Paul van den Broek. 阅读过程中信息可用性的波动：运用景观模型捕捉认知过程. 第 165-186 页. December 2018. ISBN 978-1-315-04610-5. doi: 10.4324/9781315046105-5.\n[32] Fergus I. M. Craik. 加工水平：过去、现在……与未来？ Memory, 10(5-6): 305-318, 2002. ISSN 1464-0686. doi: 10.1080/09658210244000135. 出版地：英国 出版商：Taylor & Francis.\n[33] Fergus I. M. Craik and Endel Tulving. 加工深度与情景记忆中单词的保持. Journal of Experimental Psychology: General, 104(3):268-294, 1975. ISSN 1939-2222. doi: 10.1037/0096-3445.104.3.268. 出版地：美国 出版商：American Psychological Association.\n[34] John R. Anderson. 记忆的扩散激活理论. Journal of Verbal Learning and Verbal Behavior, 22(3):261-295, June 1983. ISSN 0022-5371. doi: 10.1016/S0022-5371(83)90201-3. URL https://www.sciencedirect.com/science/article/pii/S0022537183902013.\n[35] Danielle S. McNamara, 编辑. 阅读理解策略：理论、干预与技术. Lawrence Erlbaum Associates Publishers, Mahwah, NJ, 2007.\n[36] Michelene T. H. Chi. 主动-建构-交互：区分学习活动的概念框架. Topics in Cognitive Science, 1(1):73-105, 2009. ISSN 1756-8765. doi: 10.1111/j.1756-8765.2008.01005.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-8765.2008.01005.x. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1756-8765.2008.01005.x.```chinese\n## 翻译结果\n\n[37] Rose Luckin, Wayne Holmes, Laurie B Forcier. 《智能释放：人工智能在教育中的应用论证》。技术报告，培生集团/伦敦大学学院开放思想，2016年。网址 https://www.pearson.com/content/dam/corporate/global/pearson-dot-com/files/innovation/Intelligence-Unleashed-Publication.pdf。\n[38] Wayne Holmes, Maya Bialik, Charles Fadel. 《教育中的人工智能：教学与学习的承诺与启示》。2019年3月。ISBN 978-1-79429-370-0。\n[39] Margherita Bernabei, Silvia Colabianchi, Andrea Falegnami, Francesco Costantino. 《大语言模型在工程教育中的学生使用：一项关于技术接受度、感知、效能和检测可能性的案例研究》。Computers and Education: Artificial Intelligence，第5卷，100172页，2023年10月。doi: 10.1016/j.caeai.2023.100172。\n[40] Sami Sarsa, Paul Denny, Arto Hellas, Juho Leinonen. 《使用大语言模型自动生成编程练习与代码解释》。载于《2022年ACM国际计算机教育研究会议论文集 - 第1卷》，第27-43页，瑞士卢加诺与虚拟会议，2022年8月。ACM出版社。ISBN 978-1-4503-9194-8。doi: 10.1145/3501385.3543957。网址 https://dl.acm.org/doi/10.1145/3501385.3543957。\n[41] Harsh Kumar, David M Rothschild, Daniel G Goldstein, Jake M Hofman. 《大语言模型与数学教育：危机还是机遇？》。2023年。\n[42] John Sweller, Jeroen J. G. van Merrienboer, Fred Paas. 《认知架构与教学设计：20年后的回顾》。Educational Psychology Review，第31卷第2期，第261-292页，2019年。ISSN 1573-336X。doi: 10.1007/s10648-019-09465-5。出版地：德国，出版商：Springer。\n[43] Richard E. Mayer. 《是否应该对纯粹发现式学习设立\"三振出局\"规则？》。American Psychologist，第59卷第1期，第14-19页，2004年。ISSN 1935-990X。doi: 10.1037/0003-066X.59.1.14。出版地：美国，出版商：美国心理学会。\n[44] Fergus I. M. Craik, Robert S. Lockhart. 《加工水平：记忆研究的框架》。Journal of Verbal Learning and Verbal Behavior，第11卷第6期，第671-684页，1972年12月。ISSN 0022-5371。doi: 10.1016/S0022-5371(72)80001-X。网址 https://www.sciencedirect.com/science/article/pii/S002253717280001X。\n[45] Xiaoming Zhai, Matthew Nyaaba, Wenchao Ma. 《生成式AI和ChatGPT能在科学领域认知要求高的问题解决任务上超越人类吗？》。2024年1月。网址 http://arxiv.org/abs/2401.15081。arXiv:2401.15081。\n[46] Faycal Farhi, Riadh Jeljeli, Ibtehal Aburezeq, Fawzi Fayez Dweikat, Samer Ali Al-shami, Radouane Slamene. 《分析学生对ChatGPT使用的看法、担忧与伦理认知》。Computers and Education: Artificial Intelligence，第5卷，100180页，2023年1月。ISSN 2666-920X。doi: 10.1016/j.caeai.2023.100180。网址 https://www.sciencedirect.com/science/article/pii/S2666920X23000590。\n[47] Hao Yu, Yunyun Guo. 《生成式人工智能赋能教育改革：现状、问题与展望》。Frontiers in Education，第8卷，1183162页，2023年6月。ISSN 2504-284X。doi: 10.3389/feduc.2023.1183162。网址 https://www.frontiersin.org/articles/10.3389/feduc.2023.1183162/full。\n[48] Elizabeth Ligon Bjork, Robert A. Bjork. 《给自己制造困难，但要以一种好的方式：创造合意困难以促进学习》。载于《心理学与现实世界：对社会根本贡献的例证文集》，第56-64页。Worth Publishers出版社，美国纽约州纽约市，2011年。ISBN 978-1-4292-3043-8。\n```[49] Michelene Chi, Stephanie Siler, Heisawn Jeong, Takashi Yamauchi, and Robert Hausmann. 向人类辅导学习. Cognitive Science, 25:471-533, July 2001. doi: 10.1016/S0364-0213(01)00044-1.\n[50] Alvaro Pascual-Leone, Amir Amedi, Felipe Fregni, and Lotfi B. Merabet. 可塑性的人类大脑皮层. Annual Review of Neuroscience, 28:377-401, 2005. ISSN 0147-006X. doi: 10.1146/annurev.neuro.27.070203.144216.\n[51] S. Dehaene and L. Naccache. 迈向意识的认知神经科学：基础证据与工作空间框架. Cognition, 79(1-2):1-37, April 2001. ISSN 0010-0277. doi: 10.1016/s0010-0277(00)00123-2.\n[52] Keiichi Kobayashi. 笔记的编码效应受何限制？一项元分析检验. Contemporary Educational Psychology, 2005.\n[53] Kenneth A. Kiewra. 笔记研究述评：编码存储范式及超越. Educational Psychology Review, 1(2):147-172, 1989. ISSN 1573-336X. doi: 10.1007/BF01326640. 出版地: 德国 出版社: Springer.\n[54] Kenneth A. Kiewra. 探究笔记与复习：加工深度的替代视角. Educational Psychologist, 20(1):23-32, 1985. ISSN 1532-6985. doi: 10.1207/s15326985ep2001_4. 出版地: 美国 出版社: Lawrence Erlbaum.\n[55] Mark Bohay, Daniel P. Blakely, Andrea K. Tamplin, and Gabriel A. Radvansky. 笔记记录、复习、记忆与理解. The American Journal of Psychology, 124(1):63-73, 2011. ISSN 0002-9556. doi: 10.5406/amerjpsyc.124.1.0063.\n[56] Dung C. Bui and Joel Myerson. 工作记忆能力在课堂笔记记录中的作用. Learning and Individual Differences, 33:12-22, 2014. ISSN 1873-3425. doi: 10.1016/j.lindif.2014.05.002. 出版地: 荷兰 出版社: Elsevier Science.\n[57] Ralf Rummer, Judith Schweppe, Kathleen Gerst, and Simon Wagner. 测试是否比笔记记录更有效的学习策略？ Journal of Experimental Psychology. Applied, 23(3):293-300, September 2017. ISSN 1939-2192. doi: 10.1037/xap0000134.\n[58] Lisa Geraci, Nikhil Kurpad, Rachel Tirso, Kathryn N. Gray, and Yuxiang Wang. 课堂中的元认知错误：过往成绩变异性对考试预测准确性的影响. *Metacognition and Learning*, 2022. doi: 10.1007/s11409-022-09326-7. URL https://doi.org/10.1007/s11409-022-09326-7. 在线优先出版.\n[59] Robert A. Bjork, John Dunlosky, and Nate Kornell. 自我调节学习：信念、技术与错觉. Annual Review of Psychology, 64(1):417-444, January 2013. ISSN 0066-4308, 1545-2085. doi: 10.1146/annurev-psych-113011-143823. URL https://www.annualreviews.org/doi/10.1146/annurev-psych-113011-143823.\n[60] Justin Kruger and David Dunning. 无能且不自知：识别自身无能之困难如何导致膨胀的自我评估. Journal of Personality and Social Psychology, 77(6):1121-1134, Dec 1999. doi: 10.1037//0022-3514.77.6.1121.\n[61] Lev Tankelevitch, Viktor Kewenig, Auste Simkute, Ava Elizabeth Scott, Advait Sarkar, Abigail Sellen, and Sean Rintel. 生成式人工智能的元认知需求与机遇. 见: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703300. doi: 10.1145/3613904.3642902. URL https://doi.org/10.1145/3613904.3642902.## 翻译结果\n\n[62] Axel Grund, Stefan Fries, Matthias Nückles, Alexander Renkl, and Julian Roelle. 学习何时是\"费力的\"？从动机视角审视认知导向研究中的心理努力概念。 Educational Psychology Review, 36(1):11, March 2024. ISSN 1040-726X, 1573-336X. doi: 10.1007/s10648-024-09852-7. URL https://link.springer.com/10.1007/s10648-024-09852-7.\n[63] Louise Starkey. 面向数字时代的教师准备研究综述。 Cambridge Journal of Education, 50(1):37-56, 2020. doi: 10.1080/0305764X.2019.1625867.\n[64] 王红红, 史卫平. 外语专业课程思政的实践路径探索。 外语界, (6):38-45, 2021.\n[65] British Educational Research Association. 教育研究伦理指南, 第四版, 2018. URL https://www.bera.ac.uk/publication/ethical-guidelines-for-educational-research-2018.\n[66] P. David Pearson, Laura R. Roehler, Janice A. Dole, and Gerald G. Duffy. 阅读理解专业能力的培养：应教什么？如何教？ Technical Report 512, University of Illinois Urbana-Champaign Center for the Study of Reading, 1990. URL https://hdl.handle.net/2142/17648. Publisher: Champaign, Ill.: University of Illinois at Urbana-Champaign, Center for the Study of Reading.\n[67] Terry K Koo and Mae Y Li. 可靠性研究中组内相关系数选择与报告指南。 2016.\n[68] Chris Taylor. 免费学校餐资格作为社会经济劣势衡量指标的可靠性：来自威尔士千禧队列研究的证据。 *British Journal of Educational Studies*, 66(1):29-51, 2018. doi: 10.1080/00071005.2017.1330464.\n\n# 1 补充信息\n\n# 1.1 参与者排除标准\n\n参与者 $(n = 61)$ 因以下原因被排除：\n\n1.  未参与第二阶段测试 (n=36)\n2.  未完成第一阶段的两项任务（和/或有意退出） $(n = 2)$\n3.  在尝试回答所有理解和记忆问题前停止了第二阶段测试 $(n = 8)$\n4.  在10分钟或更短时间内完成第二阶段测试 $(n = 1)$\n5.  报告对两个主题的先验知识存在显著差异（在5点李克特量表项目上存在3点差异） $(n = 13)$\n6.  在测试过程中作弊（经研究人员观察，包括打开不同浏览器查找答案、抄袭他人答案、与邻座持续交谈）。对可疑学生的回答进行扫描并与同组其他学生的回答进行比较。若根据回答（例如，与某学生回答高度重合）确认存在嫌疑，则予以排除 $(n = 1)$\n\n# 2 补充表格\n\n# 2.1 学生特征\n\n表 3: 按组别及总体统计的学生特征（排除后，$\\mathrm{N} = {344}$）<table>\n<tr><td>特征</td><td>第一组<br>学生数（百分比）</td><td>第二组<br>学生数（百分比）</td><td>总计<br>学生数（百分比）</td></tr>\n<tr><td>男性</td><td>102 (29.7%)</td><td>78 (22.7%)</td><td>180 (52.3%)</td></tr>\n<tr><td>女性</td><td>57 (16.6%)</td><td>63 (18.3%)</td><td>120 (34.9%)</td></tr>\n<tr><td>其他</td><td>1 (0.3%)</td><td>1 (0.3%)</td><td>2 (0.6%)</td></tr>\n<tr><td>不愿透露</td><td>2 (0.6%)</td><td>0 (0.0%)</td><td>2 (0.6%)</td></tr>\n<tr><td>FSM_是</td><td>9 (2.6%)</td><td>10 (2.9%)</td><td>19 (5.5%)</td></tr>\n<tr><td>FSM_否</td><td>160 (46.5%)</td><td>163 (47.4%)</td><td>323 (93.9%)</td></tr>\n<tr><td>EAL_是</td><td>130 (37.8%)</td><td>117 (34.0%)</td><td>247 (71.8%)</td></tr>\n<tr><td>EAL_其他语言</td><td>2 (0.6%)</td><td>3 (0.9%)</td><td>5 (1.5%)</td></tr>\n<tr><td>EAL_双语</td><td>35 (10.2%)</td><td>29 (8.4%)</td><td>64 (18.6%)</td></tr>\n<tr><td>History_是</td><td>99 (28.8%)</td><td>80 (23.3%)</td><td>179 (52.0%)</td></tr>\n<tr><td>History_否</td><td>81 (23.5%)</td><td>58 (16.9%)</td><td>139 (40.4%)</td></tr>\n</table>\n\n# 2.2 对学习活动的熟悉度\n\n表4：先前学习活动使用频率\n\n<table>\n<tr><td>活动及频率</td><td>第一组<br>学生数（百分比）</td><td>第二组<br>学生数（百分比）</td></tr>\n<tr><td colspan=\"3\">学习用笔记记录</td></tr>\n<tr><td>从不</td><td>7 (3.8%)</td><td>6 (3.8%)</td></tr>\n<tr><td>很少</td><td>34 (18.5%)</td><td>25 (15.6%)</td></tr>\n<tr><td>有时</td><td>47 (25.5%)</td><td>44 (27.5%)</td></tr>\n<tr><td>经常</td><td>69 (37.5%)</td><td>70 (43.8%)</td></tr>\n<tr><td>总是</td><td>22 (12.0%)</td><td>17 (10.6%)</td></tr>\n<tr><td colspan=\"3\">学习用LLM使用</td></tr>\n<tr><td>从不</td><td>32 (25.6%)</td><td>19 (18.1%)</td></tr>\n<tr><td>很少</td><td>45 (36.0%)</td><td>44 (41.9%)</td></tr>\n<tr><td>有时</td><td>29 (23.2%)</td><td>26 (24.8%)</td></tr>\n<tr><td>经常</td><td>15 (12.0%)</td><td>15 (14.3%)</td></tr>\n<tr><td>总是</td><td>4 (3.2%)</td><td>1 (1.0%)</td></tr>\n<tr><td colspan=\"3\">LLM + 笔记用于学习</td></tr>\n<tr><td>从不</td><td>-</td><td>1 (1.6%)</td></tr>\n<tr><td>很少</td><td>-</td><td>31 (48.4%)</td></tr>\n<tr><td>有时</td><td>-</td><td>23 (35.9%)</td></tr>\n<tr><td>经常</td><td>-</td><td>8 (12.5%)</td></tr>\n<tr><td>总是</td><td>-</td><td>1 (1.6%)</td></tr>\n<tr><td colspan=\"3\">先前LLM使用情况</td></tr>\n<tr><td>是</td><td>125 (70.2%)</td><td>105 (64.0%)</td></tr>\n<tr><td>否</td><td>53 (29.8%)</td><td>59 (36.0%)</td></tr>\n<tr><td colspan=\"3\">使用者中LLM使用频率</td></tr>\n<tr><td>每周少于一次</td><td>74 (59.2%)</td><td>68 (64.8%)</td></tr>\n<tr><td>每周一至两天</td><td>28 (22.4%)</td><td>33 (31.4%)</td></tr>\n<tr><td>每周三至五天</td><td>11 (8.8%)</td><td>5 (4.8%)</td></tr>\n<tr><td>每周大多数天数</td><td>12 (9.6%)</td><td>1 (1.0%)</td></tr>\n</table>\n\n# 2.3 描述性统计\n\n表5：不同条件下理解能力、字面保持能力与自由回忆的描述性统计。<table><tr><td>测量指标</td><td>条件</td><td>均值 (M)</td><td>标准差 (SD)</td></tr><tr><td rowspan=\"4\">理解度（满分12分）</td><td>笔记</td><td>4.89</td><td>2.52</td></tr><tr><td>LLM + 笔记</td><td>4.11</td><td>2.65</td></tr><tr><td>仅LLM（第1组）</td><td>4.00</td><td>2.44</td></tr><tr><td>仅LLM（第2组）</td><td>3.80</td><td>2.47</td></tr><tr><td rowspan=\"4\">字面记忆（满分20分）</td><td>笔记</td><td>10.8</td><td>4.29</td></tr><tr><td>LLM + 笔记</td><td>9.68</td><td>4.83</td></tr><tr><td>仅LLM（第1组）</td><td>8.83</td><td>3.96</td></tr><tr><td>仅LLM（第2组）</td><td>8.95</td><td>4.29</td></tr><tr><td rowspan=\"4\">自由回忆（满分50分）</td><td>笔记</td><td>5.36</td><td>5.49</td></tr><tr><td>LLM 第1组</td><td>4.32</td><td>4.15</td></tr><tr><td>LLM 第2组</td><td>4.32</td><td>4.63</td></tr><tr><td>LLM + 笔记</td><td>4.20</td><td>5.07</td></tr></table>\n\n# 2.4 混合效应回归结果\n\n表6：字面记忆、理解度与自由回忆的模型系数<table>\n<tr>\n<td>术语</td>\n<td>估计值</td>\n<td>标准误</td>\n<td>95% 置信区间</td>\n<td>统计量</td>\ntd>自由度</td>\n<td>p 值</td>\n<td>效应量 (d)</td>\n</tr>\n<tr>\n<td colspan=\"8\">字面记忆保留</td>\n</tr>\n<tr>\n<td>截距</td>\n<td>8.2429</td>\n<td>0.7966</td>\n<td>[6.68, 9.81]</td>\n<td>10.3476</td>\n<td>489.3004</td>\n<td>7.95 × 10<sup>-23</sup></td>\n<td>-</td>\n</tr>\n<tr>\n<td>条件 LLM_notes</td>\n<td>0.5668</td>\n<td>0.2752</td>\n<td>[0.03, 1.11]</td>\n<td>2.0597</td>\n<td>660.4521</td>\n<td>0.0398</td>\n<td>0.132</td>\n</tr>\n<tr>\n<td>条件 notes</td>\n<td>1.9188</td>\n<td>0.2559</td>\n<td>[1.42, 2.42]</td>\n<td>7.4974</td>\n<td>663.2789</td>\n<td>2.09 × 10<sup>-13</sup></td>\n<td>0.443</td>\n</tr>\n<tr>\n<td>组别 1</td>\n<td>-0.6147</td>\n<td>0.4155</td>\n<td>[-1.43, 0.20]</td>\n<td>-1.4793</td>\n<td>661.9230</td>\n<td>0.1395</td>\n<td>-0.143</td>\n</tr>\n<tr>\n<td>学校编号 S03</td>\n<td>-0.8645</td>\n<td>0.5993</td>\n<td>[-2.04, 0.31]</td>\n<td>-1.4424</td>\n<td>638.7162</td>\n<td>0.1497</td>\n<td>-0.198</td>\n</tr>\n<tr>\n<td>学校编号 S01</td>\n<td>-1.9789</td>\n<td>0.8005</td>\n<td>[-3.55, -0.41]</td>\n<td>-2.4720</td>\n<td>657.4886</td>\n<td>0.0137</td>\n<td>-0.465</td>\n</tr>\n<tr>\n<td>学校编号 S05</td>\n<td>-0.3908</td>\n<td>0.8562</td>\n<td>[-2.07, 1.29]</td>\n<td>-0.4564</td>\n<td>612.9203</td>\n<td>0.6483</td>\n<td>-0.094</td>\n</tr>\n<tr>\n<td>学校编号 S02</td>\n<td>1.2932</td>\n<td>0.5514</td>\n<td>[0.21, 2.37]</td>\n<td>2.3452</td>\n<td>643.8234</td>\n<td>0.0193</td>\n<td>0.299</td>\n</tr>\n<tr>\n<td>学校编号 S07</td>\n<td>2.7561</td>\n<td>1.1408</td>\n<td>[0.52, 4.99]</td>\n<td>2.4160</td>\n<td>663.8251</td>\n<td>0.0160</td>\n<td>0.623</td>\n</tr>\n<tr>\n<td>学校编号 S04</td>\n<td>-4.7045</td>\n<td>0.8102</td>\n<td>[-6.29, -3.12]</td>\n<td>-5.8067</td>\n<td>641.0030</td>\n<td>1.00 × 10<sup>-8</sup></td>\n<td>-1.075</td>\n</tr>\n<tr>\n<td>文本 古巴</td>\n<td>1.5218</td>\n<td>0.1880</td>\n<td>[1.15, 1.89]</td>\n<td>8.0952</td>\n<td>663.5151</td>\n<td>2.74 × 10<sup>-15</sup></td>\n<td>0.351</td>\n</tr>\n<tr>\n<td>任务顺序 0</td>\n<td>0.2310</td>\n<td>0.1880</td>\n<td>[-0.14, 0.60]</td>\n<td>1.2283</td>\n<td>659.9704</td>\n<td>0.2198</td>\n<td>0.052</td>\n</tr>\n<tr>\n<td>测试顺序 0</td>\n<td>0.5186</td>\n<td>0.1875</td>\n<td>[0.15, 0.89]</td>\n<td>2.7656</td>\n<td>663.7540</td>\n<td>0.0058</td>\n<td>0.119</td>\n</tr>\n<tr>\n<td>性别 (男性)</td>\n<td>0.8396</td>\n<td>0.4609</td>\n<td>[-0.06, 1.74]</td>\n<td>1.8217</td>\n<td>335.9448</td>\n<td>0.0694</td>\n<td>0.193</td>\n</tr>\n<tr>\n<td>性别 (其他)</td>\n<td>1.1737</td>\n<td>1.5839</td>\n<td>[-1.93, 4.28]</td>\n<td>0.7410</td>\n<td>187.9029</td>\n<td>0.4596</td>\n<td>0.228</td>\n</tr>\n<tr>\n<td>性别 (不愿透露)</td>\n<td>1.7770</td>\n<td>1.4362</td>\n<td>[-1.04, 4.59]</td>\n<td>1.2373</td>\n<td>474.9248</td>\n<td>0.2166</td>\n<td>0.226</td>\n</tr>\n<tr>\n<td>免费校餐 (是)</td>\n<td>-0.9135</td>\n<td>0.8574</td>\n<td>[-2.59, 0.77]</td>\n<td>-1.0654</td>\n<td>653.1653</td>\n<td>0.2871</td>\n<td>-0.207</td>\n</tr>\n<tr>\n<td>英语附加语 (双语)</td>\n<td>0.4650</td>\n<td>0.4780</td>\n<td>[-0.47, 1.40]</td>\n<td>0.9728</td>\n<td>645.1354</td>\n<td>0.3310</td>\n<td>0.116</td>\n</tr>\n<tr>\n<td>英语附加语 (其他)</td>\n<td>-0.3369</td>\n<td>1.6161</td>\n<td>[-3.50, 2.83]</td>\n<td>-0.2085</td>\n<td>660.9281</td>\n<td>0.8349</td>\n<td>-0.027</td>\n</tr>\n<tr>\n<td>历史学习经历 (无)</td>\n<td>-1.5365</td>\n<td>0.3832</td>\n<td>[-2.29, -0.79]</td>\n<td>-4.0095</td>\n<td>641.2946</td>\n<td>6.80 × 10<sup>-5</sup></td>\n<td>-0.351</td>\n</tr>\n<tr>\n<td colspan=\"8\">理解能力</td>\n</tr>\n<tr>\n<td>截距</td>\n<td>4.0264</td>\n<td>0.4409</td>\n<td>[3.16, 4.89]</td>\n<td>9.1318</td>\n<td>638.9518</td>\n<td>8.77 × 10<sup>-19</sup></td>\n<td>-</td>\n</tr>\n<tr>\n<td>条件 LLM_notes</td>\n<td>0.3533</td>\n<td>0.1785</td>\n<td>[0.00, 0.70]</td>\n<td>1.9792</td>\n<td>655.5471</td>\n<td>0.0482</td>\n<td>0.142</td>\n</tr>\n<tr>\n<td>条件 notes</td>\n<td>0.9500</td>\n<td>0.1658</td>\n<td>[0.62, 1.28]</td>\n<td>5.7306</td>\n<td>662.6375</td>\n<td>1.52 × 10<sup>-8</sup></td>\n<td>0.382</td>\n</tr>\n<tr>\n<td>组别 1</td>\n<td>-0.0735</td>\n<td>0.2395</td>\n<td>[-0.54, 0.40]</td>\n<td>-0.3068</td>\n<td>657.2449</td>\n<td>0.7591</td>\n<td>-0.033</td>\n</tr>\n<tr>\n<td>学校编号 S03</td>\n<td>-0.9749</td>\n<td>0.3320</td>\n<td>[-1.63, -0.32]</td>\n<td>-2.9365</td>\n<td>655.1779</td>\n<td>0.0034</td>\n<td>-0.399</td>\n</tr>\n<tr>\n<td>学校编号 S01</td>\n<td>-1.9371</td>\n<td>0.4438</td>\n<td>[-2.81, -1.07]</td>\n<td>-4.3645</td>\n<td>662.1221</td>\n<td>1.48 × 10<sup>-5</sup></td>\n<td>-0.783</td>\n</tr>\n<tr>\n<td>学校编号 S05</td>\n<td>-0.3167</td>\n<td>0.4735</td>\n<td>[-1.24, 0.61]</td>\n<td>-0.6688</td>\n<td>648.4704</td>\n<td>0.5039</td>\n<td>-0.142</td>\n</tr>\n<tr>\n<td>学校编号 S02</td>\n<td>0.5254</td>\n<td>0.3052</td>\n<td>[-0.07, 1.12]</td>\n<td>1.7215</td>\n<td>659.5381</td>\n<td>0.0856</td>\n<td>0.201</td>\n</tr>\n<tr>\n<td>学校编号 S07</td>\n<td>0.9683</td>\n<td>0.6335</td>\n<td>[-0.27, 2.21]</td>\n<td>1.5284</td>\n<td>663.5186</td>\n<td>0.1269</td>\n<td>0.377</td>\n</tr>\n<tr>\n<td>学校编号 S04</td>\n<td>-2.9725</td>\n<td>0.4493</td>\n<td>[-3.85, -2.09]</td>\n<td>-6.6154</td>\n<td>651.4740</td>\n<td>7.74 × 10<sup>-11</sup></td>\n<td>-1.192</td>\n</tr>\n<tr>\n<td>文本 古巴</td>\n<td>-0.6057</td>\n<td>0.1218</td>\n<td>[-0.84, -0.37]</td>\n<td>-4.9727</td>\n<td>662.4076</td>\n<td>8.42 × 10<sup>-7</sup></td>\n<td>-0.245</td>\n</tr>\n<tr>\n<td>任务顺序 0</td>\n<td>0.0428</td>\n<td>0.1219</td>\n<td>[-0.20, 0.28]</td>\n<td>0.3508</td>\n<td>657.5431</td>\n<td>0.7258</td>\n<td>0.015</td>\n</tr>\n<tr>\n<td>测试顺序 0</td>\n<td>0.6679</td>\n<td>0.1215</td>\n<td>[0.43, 0.91]</td>\n<td>5.4958</td>\n<td>662.7896</td>\n<td>5.55 × 10<sup>-8</sup></td>\n<td>0.266</td>\n</tr>\n<tr>\n<td>性别 (男性)</td>\n<td>0.2287</td>\n<td>0.2517</td>\n<td>[-0.26, 0.72]</td>\n<td>0.9086</td>\n<td>542.3928</td>\n<td>0.3640</td>\n<td>0.078</td>\n</tr>\n<tr>\n<td>性别 (其他)</td>\n<td>0.0375</td>\n<td>0.9339</td>\n<td>[-1.79, 1.87]</td>\n<td>0.0401</td>\n<td>102.4863</td>\n<td>0.9681</td>\n<td>0.574</td>\n</tr>\n<tr>\n<td>性别 (不愿透露)</td>\n<td>1.5360</td>\n<td>0.9257</td>\n<td>[-0.28, 3.35]</td>\n<td>1.6593</td>\n<td>68.4482</td>\n<td>0.1016</td>\n<td>0.006</td>\n</tr>\n<tr>\n<td>免费校餐 (是)</td>\n<td>-0.6056</td>\n<td>0.4786</td>\n<td>[-1.54, 0.33]</td>\n<td>-1.2655</td>\n<td>626.0565</td>\n<td>0.2062</td>\n<td>-0.236</td>\n</tr>\n<tr>\n<td>英语附加语 (双语)</td>\n<td>0.5813</td>\n<td>0.2649</td>\n<td>[0.06, 1.10]</td>\n<td>2.1943</td>\n<td>655.2427</td>\n<td>0.0286</td>\n<td>0.228</td>\n</tr>\n<tr>\n<td>英语附加语 (其他)</td>\n<td>-0.2195</td>\n<td>0.9140</td>\n<td>[-2.01, 1.57]</td>\n<td>-0.2402</td>\n<td>556.3704</td>\n<td>0.8103</td>\n<td>-0.103</td>\n</tr>\n<tr>\n<td>历史学习经历 (无)</td>\n<td>-0.6719</td>\n<td>0.2138</td>\n<td>[-1.09, -0.25]</td>\n<td>-3.1423</td>\n<td>613.1612</td>\n<td>0.0018</td>\n<td>-0.262</td>\n</tr>\n<tr>\n<td colspan=\"8\">自由回忆</td>\n</tr>\n<tr>\n<td>截距</td>\n<td>4.4052</td>\n<td>0.8507</td>\n<td>[2.74, 6.08]</td>\n<td>5.1786</td>\n<td>662.4966</td>\n<td>2.97 × 10<sup>-7</sup></td>\n<td>-</td>\n</tr>\n<tr>\n<td>条件 LLM_notes</td>\n<td>-0.0847</td>\n<td>0.4590</td>\n<td>[-0.98, 0.81]</td>\n<td>-0.1846</td>\n<td>661.9195</td>\n<td>0.8536</td>\n<td>-0.015</td>\n</tr>\n<tr>\n<td>条件 notes</td>\n<td>1.0185</td>\n<td>0.4269</td>\n<td>[0.18, 1.86]</td>\n<td>2.3856</td>\n<td>663.2739</td>\n<td>0.0173</td>\n<td>0.211</td>\n</tr>\n<tr>\n<td>组别 1</td>\n<td>-0.2703</td>\n<td>0.4958</td>\n<td>[-1.24, 0.70]</td>\n<td>-0.5452</td>\n<td>662.0547</td>\n<td>0.5858</td>\n<td>-0.058</td>\n</tr>\n<tr>\n<td>学校编号 S03</td>\n<td>-0.4702</td>\n<td>0.6185</td>\n<td>[-1.68, 0.74]</td>\n<td>-0.7603</td>\n<td>663.5556</td>\n<td>0.4474</td>\n<td>-0.086</td>\n</tr>\n<tr>\n<td>学校编号 S01</td>\n<td>-0.9612</td>\n<td>0.8290</td>\n<td>[-2.59, 0.66]</td>\n<td>-1.1595</td>\n<td>660.3122</td>\n<td>0.2467</td>\n<td>-0.189</td>\n</tr>\n<tr>\n<td>学校编号 S05</td>\n<td>2.1564</td>\n<td>0.8819</td>\n<td>[0.43, 3.89]</td>\n<td>2.4452</td>\n<td>662.7977</td>\n<td>0.0147</td>\n<td>0.459</td>\n</tr>\n<tr>\n<td>学校编号 S02</td>\n<td>2.7874</td>\n<td>0.5687</td>\n<td>[1.67, 3.90]</td>\n<td>4.9012</td>\n<td>663.9081</td>\n<td># 2.5 行为参与度\n\n表7：与大型语言模型及笔记记录的行为互动数据，包括提问次数、笔记字数及任务耗时。显著的任务时间差异已高亮显示，用于不同条件间的比较。\n\n<table>\n  <tr>\n    <td>测量指标</td>\n    <td>实验条件</td>\n    <td>平均值 (M)</td>\n    <td>标准差 (SD)</td>\n  </tr>\n  <tr>\n    <td rowspan=\"3\">提问次数</td>\n    <td>第一组 (LLM + 笔记)</td>\n    <td>10.98</td>\n    <td>6.46</td>\n  </tr>\n  <tr>\n    <td>第二组 (仅LLM)</td>\n    <td>9.21</td>\n    <td>5.72</td>\n  </tr>\n  <tr>\n    <td>第二组 (LLM + 笔记)</td>\n    <td>6.02</td>\n    <td>4.64</td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">笔记字数</td>\n    <td>第一组 (笔记)</td>\n    <td>100.74</td>\n    <td>115.63</td>\n  </tr>\n  <tr>\n    <td>第二组 (LLM + 笔记)</td>\n    <td>103.83</td>\n    <td>158.24</td>\n  </tr>\n  <tr>\n    <td>三词重叠度 (%)</td>\n    <td colspan=\"2\">显著重叠 (≥ 70%)</td>\n    <td>25.63%</td>\n  </tr>\n  <tr>\n    <td>三词重叠度 (%)</td>\n    <td colspan=\"2\">高度重叠 (≥ 90%)</td>\n    <td>16.25%</td>\n  </tr>\n  <tr>\n    <td rowspan=\"4\">任务耗时 (分钟)</td>\n    <td>第一组 (LLM)</td>\n    <td>-0.80</td>\n    <td>95% CI [-1.15, -0.46], d = -0.34</td>\n  </tr>\n  <tr>\n    <td>第一组 (笔记)</td>\n    <td>10-15 区间</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>第二组 (仅LLM)</td>\n    <td>-1.54</td>\n    <td>95% CI [-1.91, -1.17], d = -0.66</td>\n  </tr>\n  <tr>\n    <td>第二组 (LLM + 笔记)</td>\n    <td>10-15 区间</td>\n    <td>-</td>\n  </tr>\n</table>\n\n# 2.6 学生任务指导说明\n\n表8：主动阅读策略介绍（所有实验条件通用）\n\n<table>\n  <tr>\n    <td>当您试图学习并理解一篇文章时，主动阅读（active reading）可作为一种有效策略。\n      它能帮助您更深入地处理信息，从而提升学习效果。主动阅读包括：\n      · 识别文本中的核心观点与概念\n      · 理解其含义\n      · 分析概念间的关联性\n      · 对信息提出疑问并尝试解答</td>\n  </tr>\n</table>\n\n表9：按实验条件划分的学习活动导语<table><tr><td>实验条件</td><td>活动说明</td></tr><tr><td>笔记记录</td><td>您的任务是尝试理解并学习一篇历史文本。为此，请积极阅读文本并通过记笔记来辅助学习。笔记记录是主动阅读的重要组成部分，其目的并非大量摘抄文本信息，而是需要找出章节中的关键信息，思考其含义，并用您自己的语言进行记录。</td></tr><tr><td>大语言模型</td><td>您的任务是尝试理解并学习一篇历史文本。为此，请积极阅读文本并借助AI聊天机器人进行辅助学习。与AI聊天机器人对话有助于提升阅读主动性，您可针对文本内容提出不同问题以理解历史事件的脉络，这也有助于识别和理解关键信息。</td></tr><tr><td>大语言模型+笔记记录</td><td>您的任务是尝试理解并学习一篇历史文本。为此，请结合积极阅读、AI聊天机器人和笔记记录三种方式。与AI聊天机器人对话可增强阅读主动性，通过提问深化对文本内容的理解；同时，笔记记录对主动阅读至关重要——需提取章节关键信息，进行意义阐释，并以个人语言完成记录，而非简单摘抄原文。</td></tr></table>\n\n表10：不同实验条件的具体指导说明<table><tr><td>条件</td><td>具体说明</td></tr><tr><td>笔记</td><td>在阅读过程中进行主动阅读并记录笔记。即使你认为已完全理解内容，仍需尽力而为。思考以下要点并记录以辅助理解：\n· 重要词汇与概念的含义\n· 复杂句子的释义\n· 关键要点或观点（如时间、地点、人物与事件）\n· 地点、人物与事件之间的关联\n· 事件经过及其成因与发生机制\n· 观点与概念间的异同点\n· 对文本的个人理解</td></tr><tr><td>LLM</td><td>在阅读过程中进行主动阅读并同步使用AI聊天机器人。即使你认为已完全理解内容，仍需尽力而为。思考以下要点并借助AI聊天机器人辅助理解，例如可要求其：\n· 解释重要词汇与概念的含义\n· 重构或简化复杂句子并加以说明\n· 总结文本并识别关键要点（如时间、地点、人物与事件）\n· 厘清不理解的信息\n· 阐释地点、人物与事件之间的关联\n· 说明事件经过及其成因与发生机制\n· 识别观点与概念间的异同点\n· 检验对文本的理解程度\n同时还可：\n· 若对AI回复不理解或存疑，要求进一步解释\n· 提出后续追问\n· 要求采用项目符号、缩短答复或使用更简练的语言</td></tr><tr><td>LLM+笔记</td><td>在阅读过程中同步进行主动阅读、使用AI聊天机器人及记录笔记。即使你认为已完全理解内容，仍需尽力而为。思考以下要点，并借助AI聊天机器人与笔记辅助理解，例如可要求AI：\n· 解释重要词汇与概念的含义\n· 重构或简化复杂句子并加以说明\n· 总结文本并识别关键要点（如时间、地点、人物与事件）\n· 厘清不理解的信息\n· 阐释地点、人物与事件之间的关联\n· 说明事件经过及其成因与发生机制\n· 识别观点与概念间的异同点\n· 检验对文本的理解程度\n同时还可：\n· 若对AI回复不理解或存疑，要求进一步解释\n· 提出后续追问\n· 要求采用项目符号、缩短答复或使用更简练的语言</td></tr></table>\n\n# 2.7 测试问题\n\n表11：字面保持、理解与自由回忆的示例问题<table><tr><td>构念维度</td><td>题目类型</td><td>示例题目</td></tr><tr><td colspan=\"2\">字面记忆</td><td></td></tr><tr><td></td><td>简答题</td><td>1976年索韦托青年起义中发生了什么恐怖事件？（文本A）\n肯尼迪总统在宣布古巴周边海军行动时为何避免使用\"封锁\"一词？（文本B）</td></tr><tr><td></td><td>选择题</td><td>什么事件引发了暴力的反种族隔离抗议？（文本A）\n1) 警方强制实施种族隔离\n2) 警方逮捕纳尔逊·曼德拉\n3) 警方杀害黑人平民\n4) 警方实行严格宵禁\n美国政府如何发现古巴存在苏联导弹？（文本B）\n1) 古巴线人告知导弹情报\n2) 古巴政府威胁使用导弹\n3) 美国海军拦截苏联运导弹船只\n4) 美国飞机拍摄到导弹照片</td></tr><tr><td colspan=\"2\">理解能力</td><td></td></tr><tr><td></td><td>简答题</td><td>阐述纳尔逊·曼德拉在种族隔离时期及其最终废除过程中发挥的作用。\n仅需撰写简短段落。（文本A）\n阐述苏联在古巴导弹危机中扮演的角色。\n仅需撰写简短段落。（文本B）</td></tr><tr><td colspan=\"2\">自由回忆</td><td></td></tr><tr><td></td><td>开放题</td><td>请写下你从《[标题]》一文中记住的所有内容。尽量包含尽可能多的细节。\n例如，可思考事件经过、成因、方式、时间、地点及涉及人物。\n可采用完整句子或要点形式作答。</td></tr></table>\n\n# 2.8 评分者间信度结果\n\n表12：编码者间信度<table><tr><td>项目</td><td>ICC (A,1)</td><td>p值</td><td>95%置信区间</td><td>项目</td><td>ICC (A,1)</td><td>p值</td><td>95%置信区间</td></tr><tr><td>1</td><td>0.867</td><td>3.08 × 10<sup>-24</sup></td><td>[0.781, 0.925]</td><td>15</td><td>0.923</td><td>2.17 × 10<sup>-32</sup></td><td>[0.871, 0.958]</td></tr><tr><td>2</td><td>0.918</td><td>5.77 × 10<sup>-32</sup></td><td>[0.863, 0.955]</td><td>16</td><td>0.989</td><td>1.29 × 10<sup>-61</sup></td><td>[0.980, 0.994]</td></tr><tr><td>3</td><td>0.967</td><td>1.30 × 10<sup>-45</sup></td><td>[0.943, 0.982]</td><td>17</td><td>0.962</td><td>8.52 × 10<sup>-43</sup></td><td>[0.935, 0.979]</td></tr><tr><td>4</td><td>0.911</td><td>1.38 × 10<sup>-30</sup></td><td>[0.851, 0.951]</td><td>18</td><td>0.961</td><td>4.95 × 10<sup>-42</sup></td><td>[0.933, 0.979]</td></tr><tr><td>5</td><td>0.891</td><td>1.92 × 10<sup>-27</sup></td><td>[0.819, 0.939]</td><td>19</td><td>0.938</td><td>7.34 × 10<sup>-36</sup></td><td>[0.895, 0.966]</td></tr><tr><td>6</td><td>1.000</td><td>NaN</td><td>[NaN, NaN]</td><td>20</td><td>0.963</td><td>8.25 × 10<sup>-44</sup></td><td>[0.936, 0.980]</td></tr><tr><td>7</td><td>0.951</td><td>2.65 × 10<sup>-39</sup></td><td>[0.916, 0.973]</td><td>21</td><td>0.859</td><td>3.92 × 10<sup>-24</sup></td><td>[0.770, 0.921]</td></tr><tr><td>8</td><td>0.936</td><td>2.38 × 10<sup>-33</sup></td><td>[0.891, 0.965]</td><td>22</td><td>0.893</td><td>3.34 × 10<sup>-27</sup></td><td>[0.822, 0.940]</td></tr><tr><td>9</td><td>0.930</td><td>9.00 × 10<sup>-31</sup></td><td>[0.880, 0.962]</td><td>23</td><td>0.953</td><td>2.93 × 10<sup>-25</sup></td><td>[0.912, 0.976]</td></tr><tr><td>10</td><td>0.954</td><td>1.88 × 10<sup>-39</sup></td><td>[0.921, 0.975]</td><td>24</td><td>0.971</td><td>9.27 × 10<sup>-33</sup></td><td>[0.947, 0.985]</td></tr><tr><td>11</td><td>0.920</td><td>1.89 × 10<sup>-30</sup></td><td>[0.864, 0.956]</td><td>25</td><td>0.959</td><td>3.71 × 10<sup>-39</sup></td><td>[0.928, 0.978]</td></tr><tr><td>12</td><td>0.969</td><td>5.35 × 10<sup>-40</sup></td><td>[0.946, 0.984]</td><td>26</td><td>0.988</td><td>1.02 × 10<sup>-60</sup></td><td>[0.980, 0.994]</td></tr><tr><td>13</td><td>0.959</td><td>6.30 × 10<sup>-42</sup></td><td>[0.930, 0.978]</td><td>27</td><td>0.968</td><td>4.23 × 10<sup>-38</sup></td><td>[0.943, 0.983]</td></tr><tr><td>14</td><td>0.927</td><td>2.80 × 10<sup>-33</sup></td><td>[0.877, 0.960]</td><td>28</td><td>0.983</td><td>7.93 × 10<sup>-56</sup></td><td>[0.971, 0.991]</td></tr></table>\n\n# 2.9 调查问题与应答量表\n\n表13：调查问题与应答量表 - 第一阶段<table><tr><td>**变量**</td><td>**问题及应答量表**</td></tr><tr><td>文本难度</td><td>您觉得关于[文章标题]的文本理解起来有多困难？\n（一点也不困难，不太困难，有些困难，比较困难，非常困难）</td></tr><tr><td>主题熟悉度</td><td>在开始任务前，您对[文章标题]主题的了解程度如何？\n（完全不了解，了解很少，了解程度一般，了解较多，非常了解）</td></tr><tr><td>主题兴趣度</td><td>您觉得关于[文章标题]的文本有多有趣？\n（一点也没趣，不太有趣，有些有趣，比较有趣，非常有趣）</td></tr><tr><td>活动愉悦度</td><td>在[活动名称]的辅助下学习文本的愉悦程度如何？\n（一点也不愉悦，不太愉悦，有些愉悦，比较愉悦，非常愉悦）</td></tr><tr><td>活动难度</td><td>总体而言，您觉得[活动名称]的难度如何？\n（一点也不困难，不太困难，有些困难，比较困难，非常困难）</td></tr><tr><td>活动帮助度</td><td>[活动名称]对理解和学习文本的帮助程度如何？\n（完全没有帮助，帮助很小，有些帮助，比较有帮助，非常有帮助）</td></tr><tr><td>活动未来使用意向</td><td>未来您是否会使用类似的方法（[活动名称]）来理解和学习文本？\n（是，否，不确定）</td></tr><tr><td>任务兴趣度</td><td>总体而言，您觉得这项任务有多有趣？\n（一点也没趣，不太有趣，有些有趣，比较有趣，非常有趣）</td></tr><tr><td>任务投入度</td><td>您在理解和学习关于[文章标题]的文本上投入了多少努力？\n（完全没有投入，只投入了一点努力，投入了一些努力，投入了较多努力，投入了非常多努力）</td></tr><tr><td>自我感知任务表现</td><td>您认为自己在这项任务中的表现如何？\n（一点也不好，不太好，一般，比较好，非常好）</td></tr><tr><td>活动偏好</td><td>第一组：在本研究的两种学习方法中，您更偏好哪一种（记笔记或AI聊天机器人）？\n（我更偏好通过记笔记学习，我更偏好借助AI聊天机器人学习，没有偏好，不确定）\n<br>\n第二组：在本研究的两种学习方法中，您更偏好哪一种（仅使用AI聊天机器人或AI聊天机器人结合记笔记）？\n（我更偏好仅借助AI聊天机器人学习，我更偏好同时借助AI聊天机器人和记笔记学习，没有偏好，不确定）</td></tr><tr><td>偏好原因</td><td>您能告诉我们为什么偏好这种方法吗？[开放式回答]</td></tr><tr><td>先前LLM使用情况</td><td>在本研究之前，您是否曾使用过AI聊天机器人（例如ChatGPT、Microsoft Bing和Google Bard AI）？\n（是，否）</td></tr><tr><td>LLM使用频率</td><td>您使用AI聊天机器人的频率大约是？\n（每周少于一次，每周一到两天，每周三到五天，几乎每天）</td></tr><tr><td>学习时记笔记频率</td><td>您在为学业（如准备课程或考试）阅读文本时，记笔记的频率如何？\n（从不，很少，有时，经常，总是）</td></tr><tr><td>学习时使用LLM频率</td><td>您在为学业（如准备课程或考试）阅读文本时，使用AI聊天机器人的频率如何？\n（从不，很少，有时，经常，总是）</td></tr><tr><td>学习时同时使用LLM与记笔记频率</td><td>仅限第二组：您在为学业阅读文本时，同时使用两种方法（使用AI聊天机器人和记笔记）的频率如何？\n（从不，很少，有时，经常，总是）</td></tr></table>表14：调查问题及应答量表 - 第二阶段\n\n<table><tr><td>变量</td><td>测量项及应答类别</td></tr><tr><td>感知测试表现</td><td>若[篇章标题]部分所有题目合计满分为100分，您预计自己大约能获得多少分？[开放性问题]</td></tr><tr><td>间歇期学习行为</td><td>自第一次实验至今，您是否通过任何方式对两篇文本的主题进行了深入探究或理解？（包括但不限于网络检索信息、课后记录笔记或与他人讨论该主题）如有相关行为，请尽可能详细说明。[开放性问题]</td></tr><tr><td>性别</td><td>您的性别是？[开放性问题]</td></tr><tr><td>EAL（英语作为附加语言）</td><td>您使用哪种语言进行交流时感觉最自如？\n（英语、英语以外的语言、英语与其他语言同样熟练）</td></tr><tr><td>历史学科背景</td><td>您是否正在攻读GCSE历史课程？（是/否）</td></tr></table>\n\n# 2.10 学习体验与认知\n\n表15：不同实验条件下（组1与组2）学习体验与认知的差异比较<table>\n<tr>\n<td rowspan=\"2\">变量</td>\n<td colspan=\"5\">组 1: LLM vs 笔记</td>\n<td colspan=\"5\">组 2: LLM vs LLM+笔记</td>\n</tr>\n<tr>\n<td>差值</td>\n<td>t(自由度)</td>\n<td>p 值</td>\n<td>95% 置信区间</td>\n<td>效应量 d</td>\n<td>差值</td>\n<td>t(自由度)</td>\n<td>p 值</td>\n<td>95% 置信区间</td>\n<td>效应量 d</td>\n</tr>\n<tr>\n<td>活动帮助性</td>\n<td>0.41</td>\n<td>4.38(181)</td>\n<td>&lt;0.001</td>\n<td>[0.22, 0.59]</td>\n<td>0.33</td>\n<td>-0.03</td>\n<td>-0.35(157)</td>\n<td>0.724</td>\n<td>[-0.21, 0.15]</td>\n<td>-0.03</td>\n</tr>\n<tr>\n<td>活动难度</td>\n<td>-0.51</td>\n<td>-7.00(181)</td>\n<td>&lt;0.001</td>\n<td>[-0.66, -0.37]</td>\n<td>-0.52</td>\n<td>-0.41</td>\n<td>-4.99(159)</td>\n<td>&lt;0.001</td>\n<td>[-0.57, -0.25]</td>\n<td>-0.40</td>\n</tr>\n<tr>\n<td>任务投入度</td>\n<td>-0.25</td>\n<td>-3.53(182)</td>\n<td>0.001</td>\n<td>[-0.38, -0.11]</td>\n<td>-0.26</td>\n<td>-0.08</td>\n<td>-1.03(159)</td>\n<td>0.305</td>\n<td>[-0.22, 0.07]</td>\n<td>-0.08</td>\n</tr>\n<tr>\n<td>活动愉悦度</td>\n<td>0.68</td>\n<td>6.50(181)</td>\n<td>&lt;0.001</td>\n<td>[0.47, 0.89]</td>\n<td>0.48</td>\n<td>0.00</td>\n<td>0.00(158)</td>\n<td>1.000</td>\n<td>[-0.16, 0.16]</td>\n<td>0.00</td>\n</tr>\n<tr>\n<td>文本兴趣度</td>\n<td>-0.11</td>\n<td>-1.38(183)</td>\n<td>0.170</td>\n<td>[-0.26, 0.05]</td>\n<td>-0.10</td>\n<td>0.06</td>\n<td>0.79(159)</td>\n<td>0.428</td>\n<td>[-0.09, 0.22]</td>\n<td>0.06</td>\n</tr>\n<tr>\n<td>文本难度</td>\n<td>0.03</td>\n<td>0.50(183)</td>\n<td>0.621</td>\n<td>[-0.10, 0.16]</td>\n<td>0.04</td>\n<td>0.03</td>\n<td>0.41(159)</td>\n<td>0.684</td>\n<td>[-0.10, 0.15]</td>\n<td>0.03</td>\n</tr>\n<tr>\n<td>任务兴趣度</td>\n<td>0.09</td>\n<td>1.01(183)</td>\n<td>0.315</td>\n<td>[-0.09, 0.27]</td>\n<td>0.07</td>\n<td>-0.06</td>\n<td>-0.79(159)</td>\n<td>0.430</td>\n<td>[-0.20, 0.08]</td>\n<td>-0.06</td>\n</tr>\n<tr>\n<td>感知任务表现</td>\n<td>0.00</td>\n<td>0.00(182)</td>\n<td>1.000</td>\n<td>[-0.14, 0.14]</td>\n<td>0.00</td>\n<td>-0.11</td>\n<td>-1.45(158)</td>\n<td>0.150</td>\n<td>[-0.25, 0.04]</td>\n<td>-0.12</td>\n</tr>\n<tr>\n<td>感知测试表现</td>\n<td>-9.66</td>\n<td>-5.53(177)</td>\n<td>&lt;0.001</td>\n<td>[-13.11, -6.22]</td>\n<td>-0.42</td>\n<td>-6.80</td>\n<td>-3.55(143)</td>\n<td>0.001</td>\n<td>[-10.59, -3.02]</td>\n<td>-0.30</td>\n</tr>\n</table>\n\n# 2.11 活动偏好编码方案\n\n表 16: 编码方案：偏好 LLM 胜过 LLM+笔记<table><tr><td>代码</td><td>描述</td><td>示例</td></tr><tr><td>单独使用LLM更快捷</td><td>单独使用大语言模型比同时记笔记更节省时间，因为记笔记会耗费额外时间。</td><td>“使用LLM耗时更少”、“记笔记太花时间”</td></tr><tr><td>二者并用非必需</td><td>当LLM已能解释文本内容时，记笔记显得不必要。</td><td>“既然机器人已帮助解释，记笔记似乎没有必要”、“使用其中一种意味着不需要另一种”</td></tr><tr><td>LLM代劳</td><td>若单独使用LLM，用户无需亲力亲为。不记笔记能使任务变得更轻松。</td><td>“不需要做任何工作”、“能立即澄清未知信息而无需仔细查阅文本”、“同时使用聊天机器人和记笔记很困难”</td></tr><tr><td>记笔记占用提问时间</td><td>记笔记会减少向LLM提问或理解文本的时间。</td><td>“记笔记时没有足够时间提问”、“有更多时间理解文本”</td></tr><tr><td>LLM不支持记笔记</td><td>LLM未能降低记笔记的难度。</td><td>“对简化记笔记过程帮助有限”</td></tr></table>\n\n表17：编码方案：LLM优于笔记的偏好维度<table><tr><td>代码</td><td>描述</td><td>示例</td></tr><tr><td>LLM 更快捷</td><td>LLM 速度更快且节省时间。</td><td>“更省时”、“快得多”</td></tr><tr><td>LLM 更简便</td><td>与需要更多精力且难度更高的笔记记录相比，LLM 操作简单且无需太多精力。</td><td>“更简单”、“它更容易操作”</td></tr><tr><td>LLM 具有（互动）主动性</td><td>LLM 是一种互动性或主动性的学习活动。</td><td>“主动与机器人互动”、“感觉更具互动性”</td></tr><tr><td>LLM 具有情感吸引力</td><td>LLM 更有趣、更令人愉悦且更具吸引力。</td><td>“喜欢阅读它的回答”、“使用起来更有趣”</td></tr><tr><td>LLM 有助于集中注意力</td><td>LLM 帮助您专注于文本内容。</td><td>“让我能更专注于文本”</td></tr><tr><td>LLM 促进理解</td><td>LLM 有助于理解并帮助您检验理解程度。</td><td>“它让你更好地理解”、“我可以确认任何不确定的内容以确保自己理解正确”</td></tr><tr><td>LLM 辅助学习</td><td>LLM 对学习具有支持作用。</td><td>“AI 帮助我更高效地学习”、“我能够更轻松快速地理解和学习文本，并达到更高水平”</td></tr><tr><td>LLM 可回答问题</td><td>LLM 因能回答问题并解释不理解的内容而对理解有帮助。</td><td>“可以提出任何相关问题”、“如果我有疑问，它都能解答”</td></tr><tr><td>LLM 可提供背景和补充信息</td><td>LLM 因能提供背景信息并详细阐述事件经过而对理解有帮助。</td><td>“我获得了更多背景信息”、“它提供了完整的背景语境”</td></tr><tr><td>LLM 可总结和简化信息</td><td>LLM 因能简化信息、重新表述内容并进行总结而对理解有帮助。</td><td>“它以更简单的方式和形式呈现”、“我可以要求 AI 聊天机器人重新表述关键点”、“它可以总结要点”</td></tr><tr><td>LLM 有助于记忆</td><td>LLM 帮助您记住文本中的信息。</td><td>“它更深刻地印在我的脑海里”、“通过提供提示性问题、记忆法等帮助我记忆”、“比笔记记录花费更少的记忆时间”</td></tr></table>\n\n表 18：编码方案：相较于 LLM 的偏好笔记<table>\n<tr>\n<td>代码</td>\n<td>描述</td>\n<td>示例</td>\n</tr>\n<tr>\n<td>笔记有助于提升记忆力</td>\n<td>记笔记有助于记忆信息，因为这是一个物理书写过程。LLM 在帮助记忆方面效果较差。</td>\n<td>“写下内容能让我记得更牢”、“更有助于培养记忆力”、“通过记笔记我学到了更多”、“只是提供了更多背景信息，而非巩固知识”。</td>\n</tr>\n<tr>\n<td>笔记促进理解</td>\n<td>记笔记有助于深化理解并检验理解程度。</td>\n<td>“更容易理解阅读内容”、“我的理解更深入了”、“通过复述来检验学习成果”。</td>\n</tr>\n<tr>\n<td>记笔记是主动学习过程</td>\n<td>记笔记是更具主动性的行为。</td>\n<td>“更好的主动阅读方式”、“让我能够主动参与”。</td>\n</tr>\n<tr>\n<td>笔记是个人劳动成果</td>\n<td>记笔记意味着独立完成工作。需要自主思考，使用个人语言并记录自身观点。</td>\n<td>“必须亲自分析信息”、“可以将信息浓缩成自己的话”、“促使我独立思考”、“这是你对所审视事物的个人见解”、“让我未来能为自己的成果感到自豪”。</td>\n</tr>\n<tr>\n<td>笔记辅助信息处理</td>\n<td>记笔记有助于信息加工处理。</td>\n<td>“能够分解和处理文本”、“自己总结第二篇文本帮助我处理信息”。</td>\n</tr>\n<tr>\n<td>笔记助力学习</td>\n<td>笔记帮助学习、记录所学或检验学习效果。</td>\n<td>“可以写下自己掌握的知识点”、“能够真正学习信息而非被动接受”。</td>\n</tr>\n<tr>\n<td>笔记便于回顾</td>\n<td>相比LLM输出内容，笔记更便于重新查阅。可轻松获取既往学习记录和思考轨迹。</td>\n<td>“复习时可以回头查看这些笔记”、“记笔记为未来提供了更好的回顾材料”。</td>\n</tr>\n<tr>\n<td>笔记操作更简便</td>\n<td>记笔记比使用LLM更简单。</td>\n<td>“更容易进行总结”、“不知道，就是更简单”。</td>\n</tr>\n<tr>\n<td>笔记增强条理性</td>\n<td>笔记有助于整理信息和思路，将其分解为小块以提升清晰度。</td>\n<td>“便于整理笔记”、“更容易追踪思维轨迹”、“帮助我将文本分解成小模块”。</td>\n</tr>\n<tr>\n<td>LLM易分散注意力且信息过载</td>\n<td>LLM容易导致分心，可能提出不相关问题或关注非重点内容。其提供过多信息，易造成认知负荷或混淆。</td>\n<td>“发现很容易被AI分散注意力，更想随意提问”、“由于信息过多导致表述不清晰”。</td>\n</tr>\n<tr>\n<td>LLM内容重复枯燥</td>\n<td>LLM因多次重复信息而显得单调乏味。</td>\n<td>“感觉它只是在不断重复”。</td>\n</tr>\n<tr>\n<td>不确定如何提问</td>\n<td>因已完全理解内容而不需要LLM，或不熟悉使用方法及提问技巧。</td>\n<td>“难以构思向AI提问的问题”、“文本非常简单因此觉得无需过多提问”。</td>\n</tr>\n</table>表19：编码方案：LLM+注释相较于LLM的偏好<table><tr><td>代码</td><td>描述</td><td>示例</td></tr><tr><td>两者结合更具趣味性</td><td>同时使用LLM和笔记更有趣、更令人愉悦，而单独使用LLM可能枯燥乏味。</td><td>“我喜欢同时使用两者”、“如果必须使用聊天机器人并询问20个问题，我会非常无聊。”</td></tr><tr><td>两者结合实现优势互补</td><td>LLM和笔记可以互补使用以发挥各自优势，例如先自主完成工作，在不确定或遇到困难时再使用LLM。</td><td>“同时拥有自己总结的关键笔记和更详细的文本会更方便”、“这种方式让我既能以自己理解的方式记录事件关键点，又能就模糊内容获得AI聊天机器人的帮助”</td></tr><tr><td>两者结合更高效便捷</td><td>关于该策略对理解和学习更具帮助、更优越或更便捷的总体评价。</td><td>“最具帮助性且易于学习”、“因为我觉得这种方式更容易记忆和学习”</td></tr><tr><td>笔记辅助处理和理解LLM信息</td><td>笔记有助于处理和理解LLM提供的信息。</td><td>“为了处理这些信息，我发现同时做笔记非常有帮助”</td></tr><tr><td>笔记有助于信息组织</td><td>LLM提供信息，但需要笔记来整理和构建思路。笔记更具聚焦性且易于查阅。</td><td>“如果仅使用聊天机器人，我需要不断滚动屏幕查找内容”、“这样更容易跟踪信息并进行回顾”</td></tr><tr><td>笔记体现个人劳动</td><td>做笔记意味着进行实际工作，能够捕捉个人思考而非仅仅阅读输出内容。</td><td>“这代表我确实在进行实际工作”</td></tr><tr><td>笔记辅助记忆</td><td>笔记有助于信息记忆。</td><td>“我喜欢手写记录信息，因为这能帮助我更好地记忆”</td></tr><tr><td>笔记促进理解</td><td>做笔记有助于深化理解并检验理解程度。</td><td>“在纸上简化信息使其更易于理解和记忆”</td></tr><tr><td>笔记助力学习</td><td>笔记有助于学习过程，能够捕捉学习成果或检验学习效果。</td><td>“这样能学到更多”、“可以在笔记中简化所学内容”</td></tr><tr><td>LLM可能提供错误答案</td><td>LLM并非总能良好回答问题，有时甚至完全无法应答。LLM可能产生有害内容。</td><td>“我对机器人提出的某些问题没有得到明确回答”</td></tr><tr><td>LLM并非始终可用</td><td>需要掌握笔记技能，因为LLM可能无法随时使用。</td><td>“你不可能随时都有AI聊天机器人可用”</td></tr><tr><td>不确定如何向机器人提问</td><td>因已完全理解内容而不需要LLM，或不了解其使用方法及提问方式。</td><td>“我不确定该对机器人说什么，这有点令人烦躁”</td></tr></table># 2.12 编码方案提示交互\n\n完整提示编码方案请参阅表格文件 'PromptCoding.xlsx'\n\n表20：提示编码方案\n\n<table>\n  <tr>\n    <td>总体编码</td>\n    <td>子编码</td>\n    <td>描述与示例</td>\n  </tr>\n  <tr>\n    <td>信息浓缩</td>\n    <td>总结</td>\n    <td>学生要求机器人总结整篇文本或特定文本选段。\n    示例：“帮我总结这一段”、“总结文本”、“给我第一段的摘要”、“告诉我这篇文章是关于什么的。”</td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>记笔记</td>\n    <td>学生要求机器人就整篇文本或特定段落做笔记。\n    示例：“为第一段做笔记。”</td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>识别关键观点</td>\n    <td>学生要求机器人识别文本中的关键观点或要点信息，包括关键日期、地点、人物和事件。\n    示例：“主要观点是什么？”、“给我所有重要的日期”、“要点信息是什么？”</td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>创建时间线</td>\n    <td>学生要求机器人创建文本中描述事件的时间线。\n    示例：“将重要日期按时间顺序排列”、“给我事件的时间线。”</td>\n  </tr>\n  <tr>\n    <td>理解文本</td>\n    <td>定义词语或概念</td>\n    <td>学生要求机器人定义或解释文本中的特定词语或概念。他们请求帮助理解术语，但不寻求超出此范围的事实信息。\n    示例：“apartheid是什么意思？”、“什么是殖民地？”、“什么是导弹？”、“我不知道blockade是什么意思。”</td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>简化或解释难句</td>\n    <td>学生要求机器人简化或解释所提供的段落或特定选段。\n    示例：“用简单的词语解释这个”、“让文本更简单”、“这个句子是什么意思？”、“简化这段文本。”</td>\n  </tr>\n  <tr>\n    <td></td>\n    <td>检查理解</td>\n    <td>学生阐述自己的理解并向机器人寻求确认。\n    示例：“美国不喜欢古巴是因为他们认为卡斯特罗是共产主义者，对吗？”、“所以是一名军官阻止了整个战争？”</td>\n  </tr>\n  <tr>\n    <td>寻求额外信息与深化理解</td>\n    <td>一般背景</td>\n    <td>学生要求提供文本中提及的地点、时间或人物的背景信息以提供上下文——这些信息对于理解文本并非核心，但可能相关。\n    示例：“肯尼迪是谁？”、“曼德拉因何闻名？”、“告诉我更多关于古巴的信息”、“非洲有多少英国殖民地？”、“土耳其的导弹部署在哪里？”</td>\n  </tr>\n</table>\n\n转下页<table>\n<tr>\n  <td><strong>主代码</strong></td>\n  <td><strong>子代码</strong></td>\n  <td><strong>描述与示例</strong></td>\n</tr>\n<tr>\n  <td rowspan=\"4\">寻求补充信息与深化理解</td>\n  <td>阐述与深化理解</td>\n  <td>学生要求获取事件的更多细节，例如事件原因、涉及人物及结果。<br>示例：“美国为什么不喜欢卡斯特罗？”“流亡者为何入侵古巴？”“种族隔离期间黑人的感受如何？”</td>\n</tr>\n<tr>\n  <td>请求示例或类比</td>\n  <td>学生要求通过示例或类比来更好地理解概念或事件。<br>示例：“种族隔离影响日常生活的具体案例有哪些？”“能否用类比说明冷战紧张局势？”“通过了哪些不公正法律？”“有哪些抵制运动？”</td>\n</tr>\n<tr>\n  <td>请求对比或比较</td>\n  <td>学生要求对比或比较概念、事件或人物。<br>示例：“种族隔离与美国种族隔离有何不同？”“比较肯尼迪与赫鲁晓夫的领导风格。”</td>\n</tr>\n<tr>\n  <td>批判性分析或评价</td>\n  <td>学生要求对行动、情境、决策或陈述进行批判性分析或评价。<br>示例：“肯尼迪决策的优缺点是什么？”“评估封锁行动的有效性。”</td>\n</tr>\n<tr>\n  <td>影响与意义</td>\n  <td>影响与意义</td>\n  <td>学生询问文本信息的深层影响、相关性或后果。<br>示例：“这场危机的长期影响是什么？”“当前局势如何？”“为什么我需要关注或学习这个内容？”</td>\n</tr>\n<tr>\n  <td>学习与记忆辅助</td>\n  <td>学习与记忆辅助</td>\n  <td>学生请求辅助学习并记忆文本内容，包括要求进行知识测验。<br>示例：“创建记忆口诀”“提出四个关于文本的问题”“如何更好地记住这些内容？”</td>\n</tr>\n<tr>\n  <td rowspan=\"3\">与机器人交互</td>\n  <td>要求特定格式或长度</td>\n  <td>学生要求机器人以特定格式或长度提供回复。<br>示例：“用要点总结主要内容”“能否制作政策对比图表？”“请用简短表述”“请缩短内容”</td>\n</tr>\n<tr>\n  <td>要求改进表达</td>\n  <td>学生要求机器人优化回复或改用更简洁的方式重述，而非要求简化给定文本。<br>示例：“我不理解你的意思”“请用更简短的方式重新解释”“你的意思是？”“请说得更简单”“能用更简单的术语表达吗？”“请缩短摘要”</td>\n</tr>\n<tr>\n  <td>关系性语言</td>\n  <td>学生进行与文本无关的随意礼貌对话。<br>示例：“你好吗？”“谢谢”“你好”</td>\n</tr>\n</table>\n\n（接下页）<table><tr><td>主编码</td><td>子编码</td><td>描述与示例</td></tr><tr><td rowspan=\"2\">与AI交互</td><td>核查来源与可信度</td><td>学生询问信息来源或质疑信息准确性<br>示例：\"你的信息来源是什么？\"、\"我为什么要相信你？\"、\"我认为你的答案是错误的\"</td></tr><tr><td>无特定请求的文本粘贴</td><td>学生直接复制所提供段落中的文本，但未将其构建为具体问题或请求<br>示例：\"纳尔逊·曼德拉\"、\"1910年，四个英国殖民地合并成立南非联邦\"、\"导弹\"</td></tr><tr><td rowspan=\"3\">无关内容/离题/杂项</td><td>与文本无关</td><td>学生提出与文本或其背景无关的问题<br>示例：\"切·格瓦拉是谁？\"、\"歌曲《阿布拉克萨斯》是什么？\"</td></tr><tr><td>杂项</td><td>用于无法归类至其他编码的对话片段（最后选择）</td></tr><tr><td>无意义输入</td><td>学生输入无意义的字符、符号或无法构成连贯词句的文本<br>示例：\"asdfgh\"、\"。\"、\"123\"、\"？？？\"</td></tr></table>\n\n# 2.13 提示类型频次分析\n\n表21：主要提示类型出现频次  \n\n<table><tr><td>主要提示类型</td><td>频次</td></tr><tr><td colspan=\"2\"><strong>原型类别</strong></td></tr><tr><td>寻求补充信息与深度理解</td><td>2265</td></tr><tr><td>信息浓缩</td><td>749</td></tr><tr><td>文本理解</td><td>615</td></tr><tr><td>学习与记忆辅助</td><td>39</td></tr><tr><td colspan=\"2\"><strong>其他类别</strong></td></tr><tr><td>与AI交互</td><td>760</td></tr><tr><td>无关内容/离题/杂项</td><td>501</td></tr></table>\n\n表22：具体提示类型出现频次<table><tr><td>总体提示类型</td><td>具体提示类型</td><td>频次</td></tr><tr><td>寻求补充信息与深度理解</td><td>细节阐述与深度理解</td><td>1479</td></tr><tr><td>信息浓缩</td><td>总结归纳</td><td>588</td></tr><tr><td>寻求补充信息与深度理解</td><td>背景知识</td><td>514</td></tr><tr><td>文本理解</td><td>定义词汇或概念</td><td>463</td></tr><tr><td>与机器人交互</td><td>要求特定格式或长度</td><td>430</td></tr><tr><td>无关/离题/杂项</td><td>与文本无关</td><td>296</td></tr><tr><td>文本理解</td><td>简化或解释难句</td><td>126</td></tr><tr><td>寻求补充信息与深度理解</td><td>隐含意义与重要性</td><td>119</td></tr><tr><td>信息浓缩</td><td>识别核心观点</td><td>114</td></tr><tr><td>与机器人交互</td><td>要求改进</td><td>113</td></tr><tr><td>与机器人交互</td><td>无特定要求的文本粘贴</td><td>106</td></tr><tr><td>与机器人交互</td><td>关系性语言</td><td>105</td></tr><tr><td>无关/离题/杂项</td><td>无意义输入</td><td>109</td></tr><tr><td>无关/离题/杂项</td><td>杂项</td><td>96</td></tr><tr><td>寻求补充信息与深度理解</td><td>要求示例或类比</td><td>66</td></tr><tr><td>寻求补充信息与深度理解</td><td>批判性分析或评估</td><td>54</td></tr><tr><td>学习与记忆辅助</td><td>学习与记忆辅助</td><td>39</td></tr><tr><td>寻求补充信息与深度理解</td><td>要求对比或比较</td><td>31</td></tr><tr><td>文本理解</td><td>理解程度确认</td><td>26</td></tr><tr><td>信息浓缩</td><td>笔记整理</td><td>26</td></tr><tr><td>信息浓缩</td><td>创建时间线</td><td>21</td></tr><tr><td>与机器人交互</td><td>核查来源与可信度</td><td>6</td></tr></table>\n\n注：本表仅收录学生使用次数达到三次及以上的提示类型。",
    "created_at": "2025-12-16 00:10:45.083196",
    "updated_at": "2025-12-16 00:11:18.114993",
    "doi": "10.1186/s41239-023-00408-3",
    "arxiv_id": "2401.15081",
    "analysis": {
      "paper_id": "1daa7fa3-52d8-4ad3-bad2-545c83a3c45e",
      "status": "completed",
      "started_at": "2025-12-16T00:35:26.593835",
      "completed_at": "2025-12-16T00:35:39.405634",
      "summary": "本研究旨在探讨在中学教育环境中，使用大型语言模型（LLMs）与传统笔记记录对学生的阅读理解能力和记忆保持的影响。研究采用预注册的随机对照实验，对405名14-15岁的学生进行了测试，比较了单独使用LLM、单独做笔记以及两者结合使用三种学习方式的效果。\n\n研究发现，与单独使用LLM相比，单独做笔记以及将笔记与LLM结合使用，在三天后的记忆保持和阅读理解测试中都产生了显著更积极的效果。然而，大多数学生主观上更偏好并认为LLM更有帮助。定性分析表明，学生认为LLM有助于降低认知负荷、使复杂材料更易理解，而笔记则能促进更深层次的投入并辅助记忆。研究还识别了学生与LLM交互的不同提示行为“原型”。\n\n结论指出，虽然笔记更能促进认知投入和长期的知识理解与记忆，但LLM可能在辅助初步理解和激发学生兴趣方面具有优势。研究强调了传统学习方法的重要性，揭示了结合使用AI与传统方法优于单独使用AI，并指出学生需要掌握相应的AI使用技能以最大化学习收益。",
      "methods": [
        {
          "name": "随机对照实验",
          "description": "采用预注册的随机对照实验设计，包含组内和组间设计元素。研究在中学环境中进行，比较不同学习策略对阅读理解的影响。",
          "location": null
        },
        {
          "name": "定量分析",
          "description": "通过阅读理解测试和记忆保持测试收集定量数据。使用统计方法分析不同学习条件对学习效果的影响。",
          "location": null
        },
        {
          "name": "定性分析",
          "description": "通过学生反馈和观察收集定性数据，分析学生对不同学习方法的感知和体验。识别学生与LLM交互的提示行为原型。",
          "location": null
        },
        {
          "name": "建构-整合模型分析",
          "description": "基于阅读理解的建构-整合模型，分析学生在表面结构、文本基础和情境模型三个层次的理解。",
          "location": null
        },
        {
          "name": "处理水平框架分析",
          "description": "运用处理水平框架分析信息编码深度对记忆保持的影响，关注语义分析和深度处理的作用。",
          "location": null
        }
      ],
      "datasets": [],
      "code_refs": [],
      "structure": {
        "sections": [
          {
            "title": "Effects of LLM use and note-taking on reading comprehension and memory: A randomised experiment in secondary schools",
            "level": 1,
            "start_line": 1
          },
          {
            "title": "Affiliations:",
            "level": 1,
            "start_line": 9
          },
          {
            "title": "Abstract",
            "level": 1,
            "start_line": 14
          },
          {
            "title": "Main",
            "level": 1,
            "start_line": 18
          },
          {
            "title": "Results",
            "level": 1,
            "start_line": 46
          },
          {
            "title": "Learning outcomes",
            "level": 1,
            "start_line": 50
          },
          {
            "title": "Behavioural engagement",
            "level": 1,
            "start_line": 71
          },
          {
            "title": "Prompting behaviour",
            "level": 1,
            "start_line": 75
          },
          {
            "title": "Learning experiences and perceptions",
            "level": 1,
            "start_line": 93
          },
          {
            "title": "Activity preferences",
            "level": 1,
            "start_line": 109
          },
          {
            "title": "Future use",
            "level": 1,
            "start_line": 121
          },
          {
            "title": "Discussion",
            "level": 1,
            "start_line": 125
          },
          {
            "title": "Materials and Methods",
            "level": 1,
            "start_line": 153
          },
          {
            "title": "Participants",
            "level": 1,
            "start_line": 157
          },
          {
            "title": "Experimental design and procedure",
            "level": 1,
            "start_line": 165
          },
          {
            "title": "Setup and system",
            "level": 1,
            "start_line": 193
          },
          {
            "title": "Apartheid in South Africa",
            "level": 1,
            "start_line": 199
          },
          {
            "title": "AI Chatbot ②",
            "level": 1,
            "start_line": 209
          },
          {
            "title": "Notepad",
            "level": 1,
            "start_line": 213
          },
          {
            "title": "Learning task and materials (Session 1)",
            "level": 1,
            "start_line": 228
          },
          {
            "title": "Test task and materials (Session 2)",
            "level": 1,
            "start_line": 242
          },
          {
            "title": "Survey questions",
            "level": 1,
            "start_line": 256
          },
          {
            "title": "Analytic strategies",
            "level": 1,
            "start_line": 264
          },
          {
            "title": "Estimation of condition effects on text comprehension and retention",
            "level": 1,
            "start_line": 270
          },
          {
            "title": "Qualitative exploration of student prompts",
            "level": 1,
            "start_line": 302
          },
          {
            "title": "Quantitative exploration of students' learning experience",
            "level": 1,
            "start_line": 308
          },
          {
            "title": "Qualitative exploration of students' activity preferences",
            "level": 1,
            "start_line": 312
          },
          {
            "title": "Data availability",
            "level": 1,
            "start_line": 325
          },
          {
            "title": "Code availability",
            "level": 1,
            "start_line": 329
          },
          {
            "title": "Ethics declarations",
            "level": 1,
            "start_line": 333
          },
          {
            "title": "Competing interests",
            "level": 1,
            "start_line": 335
          },
          {
            "title": "Acknowledgements",
            "level": 1,
            "start_line": 339
          },
          {
            "title": "Supplementary Material",
            "level": 1,
            "start_line": 343
          },
          {
            "title": "Table of Contents",
            "level": 1,
            "start_line": 345
          },
          {
            "title": "Supplementary Information",
            "level": 1,
            "start_line": 347
          },
          {
            "title": "Supplementary Tables",
            "level": 1,
            "start_line": 351
          },
          {
            "title": "References",
            "level": 1,
            "start_line": 374
          },
          {
            "title": "1 Supplementary Information",
            "level": 1,
            "start_line": 451
          },
          {
            "title": "1.1 Participant Exclusion Criteria",
            "level": 1,
            "start_line": 453
          },
          {
            "title": "2 Supplementary Tables",
            "level": 1,
            "start_line": 464
          },
          {
            "title": "2.1 Student Characteristics",
            "level": 1,
            "start_line": 466
          },
          {
            "title": "2.2 Familiarity with Learning Activities",
            "level": 1,
            "start_line": 475
          },
          {
            "title": "2.3 Descriptive Statistics",
            "level": 1,
            "start_line": 483
          },
          {
            "title": "2.4 Mixed Effects Regression Results",
            "level": 1,
            "start_line": 489
          },
          {
            "title": "2.5 Behavioural Engagement",
            "level": 1,
            "start_line": 495
          },
          {
            "title": "2.6 Student Task Instructions",
            "level": 1,
            "start_line": 501
          },
          {
            "title": "2.7 Test Questions",
            "level": 1,
            "start_line": 567
          },
          {
            "title": "2.8 Inter-rater Reliability Results",
            "level": 1,
            "start_line": 589
          },
          {
            "title": "2.9 Survey Questions and Response Scales",
            "level": 1,
            "start_line": 595
          },
          {
            "title": "2.10 Learning Experiences and Perceptions",
            "level": 1,
            "start_line": 624
          },
          {
            "title": "2.11 Coding Scheme Activity Preferences",
            "level": 1,
            "start_line": 630
          },
          {
            "title": "2.12 Coding Scheme Prompt Interactions",
            "level": 1,
            "start_line": 653
          },
          {
            "title": "2.13 Frequency of Prompt Types",
            "level": 1,
            "start_line": 691
          }
        ]
      },
      "error_message": null
    },
    "is_translated": true,
    "s2_graph": {
      "citations": [
        {
          "external_id": "CorpusId:281913760",
          "title": "Exploring the impact of artificial intelligence on business talent development in higher education:A systematic literature review and research agenda",
          "authors": [
            "Qinglan Wu",
            "Lanzhen Chen",
            "Minwei Chen",
            "Yangjie Huang"
          ],
          "year": 2026,
          "venue": "The International Journal of Management Education",
          "citation_count": 1
        },
        {
          "external_id": "CorpusId:282741200",
          "title": "Revolutionizing business English instruction",
          "authors": [
            "Bendaoud Nadif",
            "Abderrahim Khoumich"
          ],
          "year": 2026,
          "venue": "American Journal of STEM Education",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283183996",
          "title": "Changing EAP assessment practices in the age of generative artificial intelligence: The case of Scottish higher education institutions",
          "authors": [
            "Lewis Urquhart",
            "X. M. Ngo"
          ],
          "year": 2026,
          "venue": "Journal of English for Academic Purposes",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283778482",
          "title": "Ethics and governance of generative AI in education: a systematic review on responsible adoption",
          "authors": [
            "Mohanad Alfiras",
            "Abdul Qader Emran",
            "Amr M. Mohamed"
          ],
          "year": 2025,
          "venue": "Discover Education",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283782189",
          "title": "Research on the refinement of college student education management based on artificial intelligence",
          "authors": [
            "Qiujia Lai"
          ],
          "year": 2025,
          "venue": "Discover Artificial Intelligence",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283806994",
          "title": "Use of Technology for the Effectiveness of School Support Systems in Addressing Barriers to Learning",
          "authors": [
            "L. Tlale"
          ],
          "year": 2025,
          "venue": "Journal of Education and Learning Technology",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283850330",
          "title": "Latent Dimensions of Innovation and Development in Selected Eastern European Countries: A Perspective Based on an Analysis of the Main Factors",
          "authors": [
            "C. Stoenoiu",
            "L. Jäntschi"
          ],
          "year": 2025,
          "venue": "World",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283812503",
          "title": "Development and Validation of an AI Literacy Scale for Pre-Service Teachers in Thailand",
          "authors": [
            "Pawarit Pingmuang",
            "Prakob Koraneekij",
            "Jintavee Khlaisang"
          ],
          "year": 2025,
          "venue": "Electronic Journal of e-Learning",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283702829",
          "title": "Transfer learning and AI technology for family school community collaborative model research in university network security management.",
          "authors": [
            "Qiongfang Feng",
            "Yang'an Chen"
          ],
          "year": 2025,
          "venue": "Scientific Reports",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283684161",
          "title": "Identifying key components and stakeholders for generative AI governance in higher education: a systematic literature review",
          "authors": [
            "Okky Putra Barus",
            "A. Hidayanto",
            "Imairi Eitiveni",
            "Kongkiti Phusavat",
            "Niko Sudibjo"
          ],
          "year": 2025,
          "venue": "Interactive Learning Environments",
          "citation_count": 0
        }
      ],
      "citations_fetched_at": "2025-12-16T15:20:51.244989",
      "references": [
        {
          "external_id": "CorpusId:257943792",
          "title": "Revolutionizing education with AI: Exploring the transformative potential of ChatGPT",
          "authors": [
            "Tufan Adiguzel",
            "M. H. Kaya",
            "Fatih Kursat Cansu"
          ],
          "year": 2023,
          "venue": "Contemporary Educational Technology",
          "citation_count": 596
        },
        {
          "external_id": "CorpusId:259466894",
          "title": "Student partnership in assessment in higher education: a systematic review",
          "authors": [
            "C. Chan",
            "Si Chen"
          ],
          "year": 2023,
          "venue": "Assessment &amp; Evaluation in Higher Education",
          "citation_count": 20
        },
        {
          "external_id": "CorpusId:259088596",
          "title": "Is AI Changing the Rules of Academic Misconduct? An In-depth Look at Students' Perceptions of 'AI-giarism'",
          "authors": [
            "C. Chan"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 45
        },
        {
          "external_id": "CorpusId:258480115",
          "title": "The AI generation gap: Are Gen Z students more interested in adopting generative AI such as ChatGPT in teaching and learning than their Gen X and millennial generation teachers?",
          "authors": [
            "C. Chan",
            "Katherine K. W. Lee"
          ],
          "year": 2023,
          "venue": "Smart Learning Environments",
          "citation_count": 385
        },
        {
          "external_id": "CorpusId:258436716",
          "title": "The AI Revolution in Education: Will AI Replace or Assist Teachers in Higher Education?",
          "authors": [
            "C. Chan",
            "Louisa H.Y. Tsi"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 119
        },
        {
          "external_id": "CorpusId:258437002",
          "title": "Deconstructing Student Perceptions of Generative AI (GenAI) through an Expectancy Value Theory (EVT)-based Instrument",
          "authors": [
            "C. Chan",
            "Wenxin Zhou"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 40
        },
        {
          "external_id": "CorpusId:258426653",
          "title": "Students’ voices on generative AI: perceptions, benefits, and challenges in higher education",
          "authors": [
            "C. Chan",
            "Wenjie Hu"
          ],
          "year": 2023,
          "venue": "International Journal of Educational Technology in Higher Education",
          "citation_count": 1159
        },
        {
          "external_id": "CorpusId:258240392",
          "title": "Generative AI",
          "authors": [
            "Stefan Feuerriegel",
            "Jochen Hartmann",
            "Christian Janiesch",
            "Patrick Zschech"
          ],
          "year": 2023,
          "venue": "Business & Information Systems Engineering",
          "citation_count": 974
        },
        {
          "external_id": "CorpusId:257953233",
          "title": "Implications of large language models such as ChatGPT for dental medicine.",
          "authors": [
            "F. Eggmann",
            "R. Weiger",
            "N. Zitzmann",
            "M. Blatz"
          ],
          "year": 2023,
          "venue": "Journal of Esthetic and Restorative Dentistry",
          "citation_count": 214
        },
        {
          "external_id": "CorpusId:258568566",
          "title": "Threats by artificial intelligence to human health and human existence",
          "authors": [
            "F. Federspiel",
            "Ruth Mitchell",
            "Asha Asokan",
            "Carlos Umaña",
            "D. mccoy"
          ],
          "year": 2023,
          "venue": "BMJ Global Health",
          "citation_count": 109
        }
      ],
      "references_fetched_at": "2025-12-16T15:20:51.975105"
    }
  },
  "02197db0-3de5-4390-9690-609c0f31a4c1": {
    "id": "02197db0-3de5-4390-9690-609c0f31a4c1",
    "filename": "learnLM_nov25.pdf",
    "file_path": "./uploads/papers/02197db0-3de5-4390-9690-609c0f31a4c1.pdf",
    "status": "completed",
    "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
    "category": null,
    "markdown_content": "# AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms\n\nLearnLM Team, Google & Eedi\n\nOne-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with  $N = 165$  students across five UK secondary schools. We integrated LearnLM—a generative AI model fine-tuned for pedagogy—into chat-based tutoring sessions on the Eedi mathematics platform. In the RCT, expert tutors directly supervised LearnLM, with the remit to revise each message it drafted until they would be satisfied sending it themselves. LearnLM proved to be a reliable source of pedagogical instruction, with supervising tutors approving  $76.4\\%$  of its drafted messages making zero or minimal edits (i.e., changing only one or two characters). This translated into effective tutoring support: students guided by LearnLM performed at least as well as students chatting with human tutors on each learning outcome we measured. In fact, students who received support from LearnLM were 5.5 percentage points more likely to solve novel problems on subsequent topics (with a success rate of  $66.2\\%$ ) than those who received tutoring from human tutors alone (rate of  $60.7\\%$ ). In interviews, tutors highlighted LearnLM's strength at drafting Socratic questions that encouraged deeper reflection from students, with multiple tutors even reporting that they learned new pedagogical practices from the model. Overall, our results suggest that pedagogically fine-tuned AI tutoring systems may play a promising role in delivering effective, individualized learning support at scale.\n\nKeywords: learning, efficacy, safety, artificial intelligence, tutoring, randomized controlled trial\n\n# 1. Introduction\n\nOne-to-one tutoring is the gold standard for supporting students' learning and education. Decades of research demonstrate that individualized tutoring results in substantial gains in learning [1-3]. Unfortunately, the high cost of one-to-one tutoring and relative scarcity of educators makes this support inaccessible for most students and classrooms. The tension between tutoring's effectiveness and inaccessibility presents an enduring challenge for education systems: can educators deliver individualized support in a way that is both highly effective and broadly scalable?\n\nA growing number of researchers and practitioners now look to generative AI (\"genAI\") as a potential solution to this challenge [4-7]. Indeed, a wave of new tutoring systems incorporate genAI for direct interactivity with students [8]. Yet rigorous, in-classroom research on the learning efficacy of genAI remains scarce [9]. The evidence that does exist is mixed: while some studies suggest genAI can offer effective instruction [10-13], others find that deploying genAI tutoring systems without appropriate pedagogical safeguards can actively harm learning [14, 15].\n\nHere we report the results of an exploratory randomized controlled trial (RCT) with  $N = 165$  students, designed specifically to evaluate if an AI tutor can safely and effectively support students in UK secondary school classrooms. Our study took place on the Eedi educational platform, an evidence-based learning ecosystem that provides students with both curriculum-aligned mathematics activities and one-to-one support from remote human tutors via online chat conversations. In our experiment, we tested whether LearnLM—a genAI model fine-tuned for pedagogical applications [16-18]—could help scale this assistance. To ensure a high standard\n\n![](/uploads/images/02197db0-3de5-4390-9690-609c0f31a4c1/ba862453a67072232f0aaf7d3a23dcba16885aa56c81db0e9542baf4eadd35c2.jpg)  \n(a)\n\n![](/uploads/images/02197db0-3de5-4390-9690-609c0f31a4c1/7cd7d69be6f0bc15cc071896a794c47a1ad47ad16bbb942dea22e9864fc8c626.jpg)  \n(b)  \nFigure 1 | We designed this exploratory RCT to evaluate the safety, pedagogy, and efficacy of LearnLM. (a) The RCT randomly assigned each of  $N = 165$  students to receive either static hints or interactive tutoring. Students in the tutoring condition experienced a further level of randomization. When they started a tutoring session, the platform randomly assigned them to either a session with a human tutor or a session with LearnLM (supervised by a human tutor). This design allows us to compare static, pre-written support against interactive tutoring, as well as human tutoring against (supervised) tutoring from LearnLM. (b) In sessions with LearnLM, a supervising tutor reviewed each message that LearnLM drafted. They could either edit the message, completely re-write it, or approve it without any changes. The Eedi platform then sent the message to the student.\n\nof safety and pedagogy for all students in our trial,  $N = 17$  expert human tutors directly supervised LearnLM, assuming ultimate responsibility for every interaction it had with students. In particular, the tutors appraised each message that LearnLM generated, retaining full control to approve, edit, or replace it before it reached the student.\n\nLearnLM proved to be a trustworthy source of pedagogical instruction, with the supervising tutors approving over  $76\\%$  of its messages without changes or with only minimal edits (changing one or two characters; e.g., deleting an emoji). In fact, across all of the learning outcomes we measured, supervised support from LearnLM proved at least as effective as guidance from a human tutor. Most surprisingly, students tutored by LearnLM exhibited measurably better knowledge transfer than those receiving support from human tutors alone. On average, supervised support from LearnLM improved the probability of a student solving a novel problem correctly by 5.5 percentage points over guidance from a human tutor.\n\nTo better understand this broad effectiveness, we surveyed and interviewed the supervising tutors for their perspectives on LearnLM. They reported that LearnLM consistently generated high-quality, Socratic dialogue, providing a strong foundation for academic interactions with students. The supervising tutors' interventions tended to focus on moderating the dialogue's pacing and providing the social and emotional nuance required to maintain student engagement.\n\nOverall, our exploratory RCT identifies several avenues for new research on AI and education, while also suggesting a potential role for genAI tutors in delivering effective, individualized learning support at scale.\n\n# 2. An Exploratory Classroom Trial\n\nOur RCT aimed to evaluate LearnLM in a rigorous, real-world, in-classroom testbed. Hundreds of secondary schools in the UK integrate the Eedi learning platform directly into their mathematics instruction. The platform provides students with curriculum-aligned study units and a spectrum of personalized support, including two forms of assistance central to this RCT: carefully designed hints for common misconceptions in each study unit, and one-to-one guidance from trained, expert tutors via online chat interactions. Students who receive this standard support on the Eedi platform experience the equivalent of two additional months of academic progress, with the impact doubling for highly engaged students [19]. We recruited  $N = 165$  students in Year 9 and 10 (ages 13-15) across five of these schools for the RCT (see Appendix A). Each student and each tutor provided informed consent to participate in the trial. The trial ran from May through June 2025.\n\nThe trial leveraged these two forms of Eedi support—hints and chat-based tutoring (\"hybrid tutoring\" [20])—as baselines to assess the pedagogical efficacy of LearnLM (see Figure B.1 in Appendix B). During the trial period, we randomly assigned each student either to receive static pedagogical support (pre-written hints) or to enter an interactive one-to-one tutoring session (Figure 1; see also Appendix B). Students in the tutoring condition experienced a further level of randomization: when a student entered a tutoring session, we randomly connected them either with an expert human tutor or with LearnLM (supervised by a human tutor). We prompted LearnLM to adopt a Socratic approach aimed at guiding the student to identify their own mistake, and provided the model access to the full question text, the student's incorrect answer, and explanations for both the student and a teacher about the misconception underlying the incorrect answer, among other information (see Appendix D.1).\n\nOur approach allowed us to pose a set of four research questions:\n\nRQ1: Was LearnLM a reliable and pedagogically sound source of instruction?  \nRQ2: Was interactive tutoring (whether delivered by a human tutor alone or in a supervised session with LearnLM) more effective for student learning than static pedagogical support?  \nRQ3: For students receiving interactive tutoring, was support from a supervised session with LearnLM more effective than support from a human tutor working alone?  \nRQ4: What can we learn from tutor and student experiences of interacting with LearnLM?\n\nTo answer these questions, we adopted a Bayesian framework and directly estimated the magnitude and credibility of our treatment effects. Unlike standard frequentist approaches, this method allows us to calculate the probability that one intervention outperforms another by a specific magnitude, providing a more practical foundation for making decisions about real-world deployment. For all analyses, we assigned identical, weakly informative priors to each intervention. We then used the resulting posterior distributions to calculate the exact probability that outcomes in one group exceeded those in another, providing a more precise signal than a simple comparison of the intervals (cf. [21-23]). For complete experimental details, see Methods and Appendices A-D.\n\n# 3. Results\n\nWe first verified the basic safety and quality of LearnLM's tutoring (RQ1) by auditing the full corpus of 3,617 messages that it drafted, as well as the supervising tutors' decisions to approve, edit, or rewrite those messages. LearnLM proved a trustworthy source of instruction. The tutors who supervised and reviewed its messages accepted  $74.4\\%$  without any edits. As judged by edit distance [24, 25], many of the  $k = 926$  instances where tutors edited or rewrote a suggestion reflected minor or targeted adjustments (see Table E.1 for examples). The two most frequent edit distances, accounting for  $5.5\\%$  and  $2.4\\%$  of re-writes, were just a single character and two characters, respectively; these virtually always reflected a tutor deleting or changing an emoji. The median intervention altered 59 characters, or just a few words. Still, after the RCT finished, we asked the supervising tutors to systematically review the corpus of edits and re-writes. This review revealed zero instances of harmful or risky content and only five factual errors, or  $0.1\\%$  of the total 3,617 messages that LearnLM drafted (see Table E.2 in Appendix E). Overall, a close audit confirmed that LearnLM provided safe and reliable guidance during the trial.\n\nNext, we evaluated effects on student learning (RQ2, RQ3), comparing students' performance after receiving one of the standard interventions on the Eedi platform or interacting with LearnLM. As described in Methods, students worked through a series of short study units, each consisting of several multiple-choice questions\n\n![](/uploads/images/02197db0-3de5-4390-9690-609c0f31a4c1/ed199f16be5726a6454db27da9cb49c9e1216b18a4a7d74889608eb2254ca013.jpg)  \nFigure 2 | Student progression through the study unit. If a student makes a mistake on the first question in a study unit, they receive a support intervention. We analyze whether the intervention helps the student identify and remediate their mistake, resolve the misconception underlying their incorrect choice, and transfer the knowledge from the intervention to the next study unit. See Methods and Appendix C for more information on the Eedi platform.\n\ndesigned to assess a specific mathematical topic (Figure 2). Whenever a student answered the first question in a unit incorrectly, the platform triggered a support intervention. Depending on their assigned condition, students either received a static, pre-written hint specific to their mistake on that question, or an interactive (chat-based) session with a tutor. Immediately following the intervention, the platform presented the student with the exact same question and prompted them to try answering it again.\n\nEchoing prior research [3], interactive support with a human tutor proved far more effective for this kind of immediate course-correction. Students who joined a real-time session with a human tutor were substantially more likely to correct their mistakes than were those who received a static, pre-written hint (see Figure 3, left). In particular,  $91.2\\%$  of students who received interactive support from a human tutor solved the problem correctly on their second attempt (with a  $95\\%$  credible interval of  $[88.5\\%, 93.6\\%]$ ), compared to only  $65.4\\%$ $[63.8\\%, 66.9\\%]$  of students who received a static hint. Supervised instruction from LearnLM proved just as effective at helping students correct their mistakes. Students receiving guidance from LearnLM answered their second attempt correctly  $93.0\\%$ $[90.4\\%, 95.3\\%]$  of the time. (For context, simply eliminating the previous mistake and guessing from the remaining options would yield an expected success rate of  $33.3\\%$ .)\n\nIf a student still answered the question incorrectly on their second attempt, the platform provided them with several additional opportunities to correct their underlying misconception. Specifically, it offered them two attempts at a new question on the exact same mathematical topic. We thus examined whether tutoring helped students eventually resolve their misunderstanding—that is, whether they answered any of the post-intervention questions correctly. On this broader measure, interactive tutoring once again proved superior to static hints (see Figure 3, center). When working with a human tutor,  $94.9\\%$  [92.6%, 96.8%] of students resolved their misconception, relative to only  $86.8\\%$  [85.7%, 88.0%] of students receiving pre-written hints. No meaningful difference emerged between students working with LearnLM and those working with human tutors. Students tutored by LearnLM resolved misconceptions  $95.4\\%$  [93.1%, 97.1%] of the time. For this kind of near-term correction, both interactive methods appear equally effective.\n\nOf course, the critical question is whether these guided successes (the opportunity to remediate mistakes and resolve misconceptions) reflect durable learning (the ability to solve a new problem without any assistance). Within the scope of this RCT, the best test for durable effects of tutoring is how students performed when progressing to a new topic. The Eedi platform organizes study units into sequences of five, where each unit builds directly upon the last. Our subsequent analysis therefore analyzed a student's likelihood of correctly answering the initial question in the very next unit in their current sequence.\n\nHere, a clear advantage for LearnLM's tutoring emerged (see Figure 3, right). Students tutored by LearnLM\n\n![](/uploads/images/02197db0-3de5-4390-9690-609c0f31a4c1/fc912d8ef5abf4bad34506e55daf060eacece65731ff34bbfce6e9f56c8fe8f5.jpg)  \nFigure 3 | Tutoring interventions improve student learning outcomes. (left, center) For immediate learning outcomes, sessions with human tutors and expert-supervised sessions with LearnLM promote similar growth for students. Students who receive interactive tutoring from either source substantially outperform students who receive pre-written, static hints. (right) In contrast, students tutored by LearnLM demonstrate greater knowledge transfer to new topics than those supported either by static hints or by human tutors alone. Error bars indicate  $95\\%$  credible intervals. Dashed lines represent the chance of success when guessing randomly  $(33.3\\%, 66.7\\%, \\text{and } 25\\%, \\text{respectively})$ .\n\non a study unit proved substantially more likely to answer the first question in the following unit correctly (66.2% [61.1%, 71.2%]) than students who had received help from an unassisted human tutor (60.7% [55.8%, 65.4%]). In particular, a supervised session with LearnLM increased the likelihood of learning transfer to a distinct topic by an additional 5.5 percentage points  $[-1.4\\%, +12.4\\%]$  relative to human tutoring. Both tutee groups, in turn, outperformed students who had received only a static hint (56.2% [54.2%, 58.2%]). Altogether, we attribute a high credibility (93.6%) to LearnLM offering better support for knowledge transfer than human tutors alone, and near certainty ( $>99.9\\%$ ) to its advantage over static, pre-written hints. The AI-supported interventions fostered a more durable and transferable understanding—an advantage revealed only when students faced a fresh challenge. (See Appendix F for our full analysis of learning outcome data.)\n\nThroughout the RCT, we sought a richer, more nuanced understanding of the experience of interactions with LearnLM (RQ4) by conducting in-depth, semi-structured interviews with a random subset of  $N = 5$  supervising tutors (see Table 1). In addition, we invited all students and supervising tutors to share their thoughts in brief surveys. We gathered  $N = 27$  student responses from a post-trial survey, and  $N = 17$  tutor responses on both pre- and post-trial surveys. These firsthand perspectives help contextualize LearnLM's effectiveness and the specific role that human expertise played in its tutoring successes.\n\nOver the course of the trial, supervising tutors came to view LearnLM as a source of high-quality, expert-level pedagogical insights. The most prominent theme from our interviews, raised independently by all five interviewed tutors, was LearnLM's consistent use of Socratic dialogue. Tutors reported that its suggestions prompted a more inquisitive, student-led interaction. One tutor highlighted its ability to ask \"really good questions that I hadn't necessarily thought of [...] in a good way, a nice way\" (T3). As another reported, \"[LearnLM] definitely explained certain topics in a better way than I probably could have\" (T5). This praise aligned with tutors' actions during the trial: as established earlier, the tutors approved the vast majority of the\n\n<table><tr><td>Tutor ID</td><td>Gender</td><td>Years of teaching experience</td></tr><tr><td>T1</td><td>F</td><td>6–10 years</td></tr><tr><td>T2</td><td>F</td><td>More than 10 years</td></tr><tr><td>T3</td><td>F</td><td>More than 10 years</td></tr><tr><td>T4</td><td>F</td><td>More than 10 years</td></tr><tr><td>T5</td><td>F</td><td>6–10 years</td></tr></table>\n\nTable 1 | We conducted semi-structured interviews with a subset of five supervising tutors to seek a deeper, nuanced understanding of LearnLM's behavior and the general experience of participating in the RCT. Table A.1 in Appendix A contains comparable details for the full sample of supervising tutors.\n\nmessages drafted by LearnLM without any edits or changes.\n\nIn interviews, three tutors noted that supervising this high standard of instruction prompted an unexpected outcome on their part: professional growth and development. For instance, one tutor contrasted LearnLM's Socratic strategy with their prior approach, noting that the drafted messages prompted \"questions more like 'Okay, what made you think that was the answer?' [...] whereas before [...] my main goal was to identify their misconception myself\" (T1). Another explained, \"I remember thinking, 'Oh, I hadn't thought of explaining it that way before.' Just like when you watch another teacher\" (T2). Over the course of the trial, LearnLM's standard of instruction made a considerable impression on the tutors who supervised it.\n\nOne-to-one tutoring requires sustained, substantive effort to process the scenario at hand and craft effective pedagogical guidance. LearnLM's ability to consistently generate high-quality pedagogical responses thus made the entire tutoring process more fluid and efficient. Our post-trial surveys corroborated this; when asked about LearnLM's most useful feature,  $82.4\\%$  of tutors chose \"supporting multiple students at the same time.\" This new, effective process quickly set a new standard for the supervising tutors. In fact, every tutor that we interviewed independently raised this increased capacity as a key strength. As one tutor explained in their interview, \"I got to the point of being disappointed when I didn't get [a session] with the AI suggestions\" (T2). These positive experiences translated into a broad increase in comfort with AI across the cohort. Tutors' self-reported comfort with using AI tools rose from an average of 3.4 [2.9, 4.0] out of 5 in the pre-trial survey to 3.9 [3.3, 4.4] in the post-trial survey (posterior probability of increase:  $90.0\\%$ ).\n\nBuilding LearnLM's pedagogical insights into effective tutoring conversations, however, required the supervising tutors to incorporate social and emotional nuance from their understanding of the students. Our retrospective analysis of the  $25.6\\%$  of cases where tutors edited or re-wrote LearnLM's messages identified two primary motivations for these interventions: moderating the pedagogical pacing of the conversation and providing social-emotional nuance to LearnLM drafts. The most frequent intervention was adjusting the conversation's pacing to prevent exasperating students, accounting for  $44.3\\%$  of all edits. Our tutors echoed this specific challenge in five of our five interviews. As one tutor explained in their interview, \"quite often the students just got frustrated, and then they lost complete interest in the question, so it was a case of overriding it\" (T2). Tutors often found it necessary to step in when LearnLM's Socratic questions, while pedagogically sound, persisted longer than a student's patience. One tutor described a common scenario where \"[LearnLM] will go, 'Okay, you've got the answer. Let's dig a little deeper about why you've got that answer.' And the child is just like, 'No, I've got it. I know what I'm doing. Can I go now?\" (T1).\n\nProviding social and emotional context to LearnLM's drafts emerged as a second prominent motivation for supervisors' interventions. In total,  $19.5\\%$  of tutors' edits adjusted the persona or tone conveyed by the drafted messages. Tutors consistently added personal touches that recognized the student as an individual. For example, one tutor noted the importance of acknowledging a student they had helped before, a nuance LearnLM could not replicate, given that its prompt did not provide any information on past tutoring sessions: \"...if you'd already helped that student twice before, [LearnLM] didn't quite have the capability to go like, 'Oh Sarah, it's you again. Hi!' And I like to have that kind of rapport\" (T3). Tutors also calibrated the tone of the messages to ensure they were appropriate for student communication styles. One tutor remarked that LearnLM's predilection for emojis \"comes across as a bit fake, and [...] the students pick up on that\" (T1). Overall, the human tutors grounded LearnLM's suggestions with social and emotional nuance, translating its pedagogical insights into effective educational interactions.\n\nFinally, student feedback indicated broad satisfaction with their tutoring interactions. In post-trial surveys, students who received interactive tutoring rated the helpfulness of the support they received an average of 3.9 [3.1, 4.7] out of 5, relative to 3.6 [2.9, 4.2] for students who received static hints (posterior probability of an advantage for tutoring:  $74.9\\%$ ). Ultimately, interactive tutoring delivered not just strong learning outcomes, but an enjoyable experience for the learners themselves.\n\n# 4. Discussion\n\nWhen deployed responsibly, can generative AI safely and effectively support students in real-world learning environments? Our exploratory trial investigated whether LearnLM—a genAI model fine-tuned for pedagogical applications—could help provide in-classroom guidance across five UK secondary schools. Students in these schools use Eedi, an online mathematics platform that effectively improves learning outcomes [19], for their\n\nregular instruction. We incorporated LearnLM into the platform so that it drafted messages to send to students in chat-based tutoring sessions. Of course, genAI tools carry well-known risks, including their capacity to fabricate information [26, 27] and erode critical thinking [28, 29]. Given the heightened ethical weight of these risks in educational settings, we assigned a group of expert (human) tutors to directly supervise LearnLM, assuming ultimate responsibility for each of its interactions with students. The tutors applied a simple, rigorous standard: they revised each of LearnLM's drafts until they were satisfied sending the message as their own.\n\nThe supervising tutors found LearnLM to be a reliable source of pedagogical instruction, approving the vast majority of its drafted messages without any edits. A systematic review of the drafted messages revealed zero instances of harmful content and only five factual errors out of 3,617 messages drafted by LearnLM total. For students, this translated into effective support for learning: tutoring from LearnLM helped students identify their mistakes and correct their misconceptions just as well as instruction from human tutors alone. Unexpectedly, students tutored by LearnLM demonstrated greater knowledge transfer to subsequent topics than did students who received guidance from human tutors.\n\nTutors consistently praised LearnLM's use of Socratic dialogue, but also noted that the model's relatively inflexible adherence to pedagogical principles threatened to exasperate some students. The best human tutors, in contrast, draw on experience, empathy, and judgment to decide when to push students and when to moderate their approach. This is a constant judgment call for tutors: weighing the long-term benefits of productive struggle against the immediate risks of frustrating a student and causing them to disengage completely. This delicate calibration remains a fundamental challenge for current AI systems [30-33].\n\nBeyond safety and pedagogy, expanding access to one-to-one tutoring will require improving its cost and scalability. In our interviews, the supervising tutors consistently reported that LearnLM made their work feel more fluid and efficient. Our own anecdotal observations during the trial supported these reports: tutors appeared comfortable managing higher workloads during their supervised sessions. Unfortunately, the design of this RCT—with tutors fluidly switching from supervision to direct interaction during the same classroom periods—precludes a rigorous measurement of throughput or efficiency for each condition. After the trial, we simulated additional sessions as an informal test of scalability (see Appendix G). The results of this informal test corroborate the improved efficiency of the supervised sessions, with tutors sustaining a higher volume of simultaneous conversations when supported by LearnLM. Altogether, these signals support a possible role for genAI tutoring in helping educators to deliver individualized instruction at scale.\n\nOverall, the design of this exploratory RCT allowed us to rapidly validate LearnLM's safety and gather initial signals of its efficacy. We measured these outcomes using students' standard, daily activities on the Eedi platform. This approach provided us with learning signals immediately, eliminating the need to develop and administer new trial-specific assessments, or to wait for the next round of standardized exams. In addition, by randomly assigning the source of support for each individual tutoring session, we could measure the alternating impact of LearnLM and human tutoring on the same students. This approach disentangled tutoring effectiveness from pre-existing student differences, permitting us to detect meaningful indications of efficacy working with just five schools.\n\nOn the other hand, this design offers only a partial glimpse at the broader trajectory of learning. Randomizing the source of tutoring session-by-session allowed our RCT to efficiently investigate immediate learning outcomes, but also prevented it from isolating the cumulative impact of working with LearnLM over time. Measuring substantive, longer-term effects on learning will require a different approach. In addition, the finding from our interviews that tutors learned from supervising LearnLM indicates another methodological wrinkle. If tutors applied those insights in sessions without LearnLM, that crossover might dampen the measured difference between the two tutoring conditions. Future research can overcome these limitations by assigning students to receive one consistent type of support for an entire study, ideally following their progress over several months and tracking their performance on external, standardized assessments. Such a longitudinal approach could help determine whether the immediate successes that we observed translate into persistent, substantive learning gains—a vital step toward validating the potential of AI tutoring to deliver scalable, individualized support for students and educators.\n\nTo what extent might the tutoring efficacy we observe in this RCT generalize beyond mathematics? In part, LearnLM's strong performance reflects the nature of the inputs that we provided to it: questions with precise answers, discrete incorrect responses, and validated explanations of why students might have veered off the right path. Mathematics curricula often focus on verifiably solvable problems, so they readily offer this clear\n\nstructure. In contrast, many other subjects taught in secondary school emphasize ambiguity, interpretation, and argumentation. Consequently, LearnLM's performance in this trial offers limited evidence for its ability to shepherd students through more interpretive activities in fields like history or literature. We will need to conduct research across a diverse range of subjects to understand where current AI tutors may already offer strong support, and which domains require us to develop new, distinct approaches to AI pedagogy.\n\nUltimately, our research did not start from scratch with this trial. Two lines of conceptual and empirical groundwork enabled this RCT: first, a generative AI model specifically fine-tuned for pedagogy [16-18], and second, an educational platform deeply rooted in learning science [19]. Our results integrating LearnLM into the Eedi ecosystem illustrate how learning science and technological development can complement one another to support and scale better learning outcomes for students. Moving forward, we invite collaboration across the AI and learning science communities to partner on new research and offer an honest appraisal of how this technology helps—or hurts—students and educators in different contexts and settings. Building and sharing this knowledge helps bring us closer to the goal of providing effective, safe, and accessible learning opportunities for all students.\n\n# 5. Methods\n\nOur protocol underwent independent ethical review, with a favourable opinion from the Human Behavioural Research Ethics Committee at Google DeepMind (#25003).\n\nParticipants We recruited  $N = 165$  students from five UK secondary schools to participate in the trial. We drew the cohort exclusively in Years 9 and 10 (ages 13-15), from classrooms that incorporate the Eedi platform as part of their regular mathematics instruction for one hour per week. Each student provided informed consent to participate in this research. As part of their informed consent process, we explained to students that their tutors might rely on AI support during the trial. A pool of  $N = 17$  expert tutors—all qualified teachers with extensive teaching experience—delivered the trial's interactive interventions (i.e., tutored students directly and supervised tutoring sessions with LearnLM). Each tutor also provided informed consent to participate in this research.\n\nPlatform The Eedi platform provides a range of curriculum-aligned mathematics activities for students and classrooms. In this RCT, we focused on student performance on its short study units, each designed to assess a specific mathematics topic and consisting of diagnostic multiple-choice questions with four response options (Figure 2). Whenever a student answers the first question in a unit incorrectly, the platform triggers a support intervention. Immediately following this intervention, the platform prompts the student to retry the question that they originally missed. If they miss this question again, the platform presents them with a new question on the same topic, written to assess the same topic and misconceptions using different concrete details. Students complete a unit and progress to the next unit as soon as they answer a question correctly, or after they incorrectly answer all four questions. The platform organizes these study units into sequences of five. Individual study units in a sequence build iteratively upon one another, so students must typically grasp one before successfully engaging with the next.\n\nModel LearnLM is a family of generative AI models fine-tuned to specialize in pedagogical dialogue. For this RCT, we accessed the most recent version of LearnLM available at the time, fine-tuned from Gemini 2.0 Flash. We connected the Eedi platform to LearnLM via a custom API created specifically for this trial. During platform tutoring sessions with LearnLM, the platform assembled a strictly defined system prompt instructing the model to draft a concise, Socratic response aimed at guiding the student to self-correct their specific misconception without revealing the answer. The prompt also provided rich real-time context, including the question text, the student's incorrect answer, and the specific misconception underlying the answer identified by the platform (see Appendix D.1 for the detailed prompt). The platform sent the assembled prompt to the API, which then returned a draft response from LearnLM for the platform to pass to the supervising tutor for approval, editing, or re-writing.\n\nProcedure We conducted the exploratory RCT over seven consecutive weeks (May through June 2025). The trial employed a two-level randomized controlled design to address our research questions. First, we randomly assigned students to either the control condition ( $N = 91$  students) or the tutoring condition ( $N = 74$  students). Second, specifically for students in the tutoring condition, we randomly assigned each individual tutoring session to either a human expert or to LearnLM (under supervision from a human expert).\n\nWhenever a student in the control condition answered a question incorrectly, they received a pre-written message designed to prompt reflection on a specific misconception, based on which incorrect option they selected (a \"static hint\"). The platform then prompted them to retry the question.\n\nTo support the tutoring condition, we scheduled a team of tutors to remain on-call in the Eedi platform during class hours on each day of the trial. Whenever a student in the tutoring condition answered a question incorrectly, the standby team received an alert. One of the tutors would then initiate a session with the student. The platform randomized each of these sessions to either connect the tutor directly to the student (\"session with a human tutor alone\") or to assign them to supervise the model (\"supervised session with LearnLM\"). That is, tutors both directly guided students and oversaw sessions with LearnLM on the same day. In supervised sessions with LearnLM, the human tutor reviewed the suggestions generated by the model and approved, edited, or replaced each drafted message before the platform sent it to the student. The student interface appeared identical across both conditions, with no explicit indication of whether the student was connected with a human tutor alone or a tutor supervising LearnLM.\n\nFor both conditions, we recorded the student and question identifiers, timing, correctness, and position (both within its study unit and within its sequence of five units) of every attempted answer on the platform.\n\nTo complement this central evaluation, we incorporated several qualitative lines of inquiry. First, we recorded the entire message corpus and the supervising tutors' decisions. Throughout the seven-week trial, the platform logged every draft generated by LearnLM, the supervising tutor's action (approve, edit, or re-write), and the finalized message sent to the student. Second, we administered short baseline and endline surveys to all supervising tutors. All tutors completed both rounds ( $N = 17$ ). Third, we invited all participating students to complete a short survey via the Eedi platform after the trial concluded, resulting in  $N = 27$  responses. Finally, we randomly selected five tutors and invited them to participate in hour-long, semi-structured interviews. These interviews followed a standardized protocol designed to elicit detailed narratives of their experiences supervising LearnLM.\n\nAnalysis. We evaluated efficacy across three primary quantitative outcomes derived from Eedi platform data: mistake remediation (success at attempting a question a second time, after an intervention), misconception resolution (success at answering any question within a study unit, after an intervention), and knowledge transfer (success at answering the first question of the next study unit within the same sequence, after an intervention). We leveraged Bayesian regression to estimate treatment effects for these outcomes. We included baseline performance as a covariate in all regression models to account for pre-existing differences between students. The success rates reported in the Results section represent posterior predictive margins estimated from these regressions, adjusting for students' baseline performance. Practically speaking, these estimates differ only negligibly from the unadjusted success rates observed during the trial (see Appendix F for all unadjusted success rates and posterior predictive margins).\n\nTo verify the safety and pedagogical quality of LearnLM's tutoring, we audited the full corpus of drafted messages through an iterative, inductive process [34]. We first counted the number of outright approvals without changes. For all edited and re-written messages, we quantified the magnitude of change by computing the Levenshtein distance and the edit ratio (the Levenshtein distance divided by the total character count of the initial draft). We then categorized the apparent functional purpose of each revision. Specifically, a generative AI model (Gemini 2.5 Pro [35]) performed an initial coding of every edit, processing 30 to 50 pairs of original and edited messages at a time. Two members of the research team reviewed and refined the generated codes into a focused codebook. Next, two expert tutors reviewed each pair of messages to validate the assigned codes. A member of the research team then conducted a final review of the coding decisions to ensure consistency and accuracy. Finally, the research team synthesized these codes into broader themes and specifically searched the coded corpus for any instances of harmful or erroneous generations.\n\nWe took an iterative approach to identify themes in the supervising tutors' interviews, following emerging\n\nguidance on applying genAI tools to support qualitative coding [14, 36, 37]. A member of the research team first reviewed all transcripts to gain familiarity with the content. We then applied a generative AI model (Gemini 2.5 Pro) to identify segments of text describing tutors' perceptions, experiences, or attitudes and to generate initial descriptive labels for them. A member of the research team then refined them into clear definitions, organized them into a structured set of themes, and then manually applied these labels to the full dataset. Finally, a member of the research team verified every coded excerpt against the original transcript to create a complete audit trail.\n\nFinally, we analyzed responses from our short surveys for additional context on student and tutor experiences and perspectives.\n\n# References\n\n[1] Benjamin S Bloom. The 2 sigma problem: The search for methods of group instruction as effective as one-to-one tutoring. Educational researcher, 13(6):4-16, 1984.  \n[2] Matthew A Kraft, Beth E Schueler, and Grace Falken. What impacts should we expect from tutoring at scale? exploring meta-analytic generalizability. Technical Report 24-1031, EdWorking Paper, 2024.  \n[3] Andre Nickow, Philip Oreopoulos, and Vincent Quan. The impressive effects of tutoring on prek-12 learning: A systematic review and meta-analysis of the experimental evidence. Technical report, National Bureau of Economic Research, 2020.  \n[4] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Gunnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and individual differences, 103:102274, 2023.  \n[5] Salman Khan. *Brave new words: How AI will revolutionize education (and why that's a good thing)*. Penguin, 2024.  \n[6] Ethan Mollick. Co-intelligence: Living and working with AI. Penguin, 2024.  \n[7] Erin Mote. Artificial intelligence in education: Opportunities, challenges, and policy considerations for Congress, 2025.  \n[8] Meriem Zerkouk, Miloud Mihoubi, and Belkacem Chikhaoui. A comprehensive review of ai-based intelligent tutoring systems: Applications and challenges. arXiv preprint arXiv:2507.18882, 2025.  \n[9] Joshua Weidlich, D Gasevic, H Drachsler, and P Kirschner. Chatgpt in education: An effect in search of a cause. PsyArXiv Preprints, 2025.  \n[10] Martin Elias De Simone, Federico Hernan Tiberti, Maria Rebeca Barron Rodriguez, Federico Alfredo Manolio, Wuraola Mosuro, and Eliot Jolomi Dikoru. From chalkboards to chatbots: Evaluating the impact of generative ai on learning outcomes in nigeria. Technical report, The World Bank, 2025.  \n[11] Greg Kestin, Kelly Miller, Anna Klales, Timothy Milbourne, and Gregorio Ponti. Ai tutoring outperforms in-class active learning: an rct introducing a novel research-based design in an authentic educational setting. Scientific Reports, 15(1):17458, 2025.  \n[12] Zachary A Pardos and Shreya Bhandari. Chatgpt-generated help produces learning gains equivalent to human tutor-authored help on mathematics skills. Plos one, 19(5):e0304013, 2024.  \n[13] Rose E Wang, Ana T Ribeiro, Carly D Robinson, Susanna Loeb, and Dora Demszky. Tutor copilot: A human-ai approach for scaling real-time expertise. arXiv preprint arXiv:2410.03017, 2024.  \n[14] Hamsa Bastani, Osbert Bastani, Alp Sungu, Haosen Ge, Özge Kabakci, and Rei Mariman. Generative ai without guardrails can harm learning: Evidence from high school mathematics. Proceedings of the National Academy of Sciences, 122(26):e2422633122, 2025.\n\n[15] Nataliya Kosmyna, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, and Pattie Maes. Your brain on chatgpt: Accumulation of cognitive debt when using an ai assistant for essay writing task. arXiv preprint arXiv:2506.08872, 2025.  \n[16] Irina Jurenka, Markus Kunesch, Kevin R. McKee, Daniel Gillick, Shaojian Zhu, Sara Wiltberger, Shubham Milind Phal, Katherine Hermann, Daniel Kasenberg, Avishkar Bhoopchand, et al. Towards responsible development of generative AI for education: An evaluation-driven approach. arXiv preprint arXiv:2407.12687, 2024.  \n[17] LearnLM Team. LearnLM: Improving Gemini for learning. arXiv preprint arXiv:2412.16429, 2024.  \n[18] LearnLM Team. Evaluating gemini in an arena for learning. arXiv preprint arXiv:2505.24477, 2025.  \n[19] Wayne Harrison, Emma Dobson, Steve Higgins, Germaine Uwimpuhwe, and Rahil Khowaja. Eedi 2024 impact report: A study to evaluate the effectiveness of eedi on raising attainment in mathematics at ks3 (year 7). Technical report, WhatWorked Education, 2025. URL www.interventions. whatworked.e education.  \n[20] Eason Chen, Xinyi Tang, Aprille Xi, Chenyu Lin, Conrad Borchers, Shivang Gupta, Jionghao Lin, and Kenneth R Koedinger. Vtutor for high-impact tutoring at scale: managing engagement and real-time multi-screen monitoring with p2p connections. In Proceedings of the Twelfth ACM Conference on Learning@ Scale, pages 320–324, 2025.  \n[21] Geoff Cumming. Inference by eye: Reading the overlap of independent confidence intervals. Statistics in medicine, 28(2):205-220, 2009.  \n[22] Peter C Austin and Janet E Hux. A brief note on overlapping confidence intervals. Journal of vascular surgery, 36(1):194-195, 2002.  \n[23] Nathaniel Schenker and Jane F Gentleman. On judging the significance of differences by examining the overlap between confidence intervals. The American Statistician, 55(3):182-186, 2001.  \n[24] VI Levenshtein. Binary codes capable of correcting deletions, insertions and reversals. In Soviet Physics Doklady, volume 10, page 707, 1966.  \n[25] Gonzalo Navarro. A guided tour to approximate string matching. ACM computing surveys (CSUR), 33(1): 31-88, 2001.  \n[26] David Sallay. Vetting generative AI tools for use in schools. Policy brief, Future of Privacy Forum, April 2024.  \n[27] Paula Maylahn. 2024 state of EdTech district leadership. Technical report, Consortium for School Networking (CoSN), 2024.  \n[28] Michael Gerlich. Ai tools in society: Impacts on cognitive offloading and the future of critical thinking. Societies, 15(1):6, 2025.  \n[29] Elizabeth Laird, Maddy Dwyer, and Hannah Quay-de la Vallee. Hand in hand: Schools' embrace of AI connected to increased risks to students. Technical report, Center for Democracy & Technology, October 2025.  \n[30] Chase DiBenedetto. I tried learning from ai tutors. The test better be graded on a curve., September 2025. URL https://mashable.com/article/chat-gpt-study-mode-review.  \n[31] Chase DiBenedetto. After testing out Google's AI tutor, we have some notes, September 2025. URL https://mashable.com/article/google-gemini-guided-learning-review.  \n[32] Chase DiBenedetto. I tried learning from Anthropic's AI tutor. I felt like i was back in college., September 2025. URL https://mashable.com/article/anthropic-claude-learning-mode-review.  \n[33] Daniel Gillick. AI tutors should not approximate human tutors, November 2025. URL https://www.ai-policyperspectives.com/p/ai-tutors-should-not-approximate.\n\n[34] Virginia Braun and Victoria Clarke. Using thematic analysis in psychology. Qualitative research in psychology, 3(2):77-101, 2006.  \n[35] Gemini Team, Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025.  \n[36] Danielle Hitch. Artificial intelligence augmented qualitative analysis: the way of the future? Qualitative Health Research, 34(7):595-606, 2024.  \n[37] Matthew Nyaaba, Min SungEun, Mary Abiswin Apam, Kwame Owoahene Acheampong, and Emmanuel Dwamena. Optimizing generative ai's accuracy and transparency in inductive thematic analysis: A human-ai comparison. arXiv preprint arXiv:2503.16485, 2025.  \n[38] Jon Andrews. The introduction of progress 8. Technical report, Education Policy Institute, 2017. URL https://dera.ioe.ac.uk/id/eprint/29304.  \n[39] Department for Education. Schools, pupils and their characteristics: Academic year 2024/25, June 2025. URL https://explore-education-statistics.service.gov.uk/find-statistics/school-pupils-and-their-characteristics/2024-25.  \n[40] Ariel Lindorff, Steve Strand, and Ivan Au. English as an additional language (eal) and educational achievement in england: An analysis of publicly available data. Technical report, The Bell Foundation, Cambridge, February 2025.  \n[41] Ben Goodrich, Jonah Gabry, Imad Ali, and Sam Brilleman. rstanarm: Bayesian applied regression modeling via Stan., 2020. URL https://mc-stan.org/rstanarm. R package version 2.21.1.  \n[42] Google DeepMind. Gemini 2.5 flash, 2025. URL https://deepmind.google/models/gemini/flash/. Accessed: 2025-11-10.  \n[43] Hannah Coe. How much does a maths tutor cost in 2024/2025?, September 2024. URL https://tutorful.co.uk/blog/how-much-does-a-maths-tutor-cost. Accessed: 2025-11-10.\n\n# Contributions and Acknowledgments\n\n# Core Contributors\n\nThe following individuals contributed to the work described in this report. These lists are ordered alphabetically, and do not indicate ranking of contributions.\n\nOn the Google team, the following individuals made core contributions:\n\nAlbert Wang, Aliya Rysbek, Andrea Huber, Brian Veprek, Irina Jurenka, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Markus Kunesch, Sara Wiltberger, and Shakir Mohamed.\n\nOn the Eedi team, the following individuals made core contributions:\n\nAnna Kenolty, Anjali Nambiar, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Iris Hulls, James Stalley-Moores, Lucy Dalton, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Simon Woodhead, and Vasco Brazão.\n\nKevin R. McKee and Bibi Groot led this research and the preparation of this report.\n\n# Acknowledgements\n\nThis work represents a close collaboration between Google and Eedi.\n\nFor Eedi: We would like to acknowledge the support of the Eedi tutors and learning designers who made this work possible, including Bea Pugh, Gemma Bazany-Barber, Marion Brehm, Morgan Sowerby, Nigel Kendall, and Zoe Sutcliffe. Eedi also thanks the organizations that fund our research efforts: the Digital Harbor Foundation, Learning Engineering Virtual Institute, Rockefeller Foundation, Schmidt Futures, and Walton Family Foundation.\n\nFor Google: We completed this work as part of the LearnLM effort—a cross-Google project, with members from Google DeepMind, Google Research, Google LearnX, and more. This tech report represents only a small part of the wider effort, and only lists team members who made direct contributions to this report. The dedication and efforts of numerous teams make our work possible. The LearnLM team would like to acknowledge support from Abhinit Modi, Aditya Srikanth Veerubhotla, Antonia Mould, Avishkar Bhoopchand, Brett Wiltshire, Daniel Gillick, Daniel Kasenberg, Derek Ahmed, Gal Elidan, James Cohan, Jennifer She, Kristen Morea, Lisa Wang, Mike Schaekermann, Miriam Schneider, Miruna Píslar, Muktha Ananda, Nahema Marchal, Nikhil Joshi, Parsa Mahmoudieh, Paul J Hun, Shanice Onike, Shashank Agarwal, Shubham Milind Phal, Sun Jae Lee, Theofilos Strinopoulos, Wei-Jen Ko, and Will Hawkins. Furthermore, we would like to thank the Gemini team, the Compute team, the Responsible Development and Innovation team, the Responsible Engineering team, and the Child Safety team at Google DeepMind, as well as the Trust and Safety team at Google. Finally, the LearnLM team would like to acknowledge the support provided by our leads and sponsors that made this project possible: our genuine thanks go to Benedict Gomes, Lila Ibrahim, and Zoubin Ghahramani.\n\n# A. Participants\n\n# A.1. Students\n\nThe trial included  $N = 165$  students in Year 9 and 10 (ages 13-15) from five UK secondary schools. Participants ranged in age from 13 to 15. Among those who reported their gender, the cohort was relatively evenly split (51.1% female, 48.9% male).\n\nThe schools varied broadly in academic performance and socio-economic background. Progress 8 scores ranged from  $-0.68$  to  $+0.24$ , spanning the 5th to 75th national percentiles for state-funded schools in England [38]. Free School Meal eligibility ranged from  $12\\%$  (representing affluent areas) to  $26\\%$  (closely aligned with the national secondary school average of  $25.7\\%$  [39]). However, the schools contained low proportions of students speaking English as an Additional Language (EAL), ranging from  $2 - 11\\%$ . These rates fall below the national average and do not reflect the EAL rates seen in major urban centers [40].\n\n# A.2. Tutors\n\nA pool of  $N = 17$  expert tutors delivered the interactive interventions in the RCT and provided additional insights in baseline surveys, semi-structured interviews, and endline surveys (Table A.1).\n\n<table><tr><td>Tutor ID</td><td>Gender</td><td>Years of teaching experience</td><td>Additional contributions</td></tr><tr><td>T1</td><td>F</td><td>6–10 years</td><td>Interview, surveys (baseline, endline)</td></tr><tr><td>T2</td><td>F</td><td>More than 10 years</td><td>Interview, surveys (baseline, endline)</td></tr><tr><td>T3</td><td>F</td><td>More than 10 years</td><td>Interview, surveys (baseline, endline)</td></tr><tr><td>T4</td><td>F</td><td>More than 10 years</td><td>Interview, surveys (baseline, endline)</td></tr><tr><td>T5</td><td>F</td><td>6–10 years</td><td>Interview, surveys (baseline, endline)</td></tr><tr><td>T6</td><td>F</td><td>More than 10 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T7</td><td>F</td><td>More than 10 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T8</td><td>F</td><td>3–5 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T9</td><td>F</td><td>6–10 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T10</td><td>M</td><td>6–10 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T11</td><td>F</td><td>More than 10 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T12</td><td>F</td><td>6–10 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T13</td><td>F</td><td>More than 10 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T14</td><td>F</td><td>More than 10 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T15</td><td>F</td><td>More than 10 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T16</td><td>F</td><td>3–5 years</td><td>Surveys (baseline, endline)</td></tr><tr><td>T17</td><td>F</td><td>More than 10 years</td><td>Surveys (baseline, endline)</td></tr></table>\n\nTable A.1 | Teaching experience and additional research contributions for all supervising tutors.\n\n# B. Trial\n\nThe research presented in this report focuses on two types of support interventions provided by the Eedi platform: static, pre-written hints that map to particular student misconceptions about individual topics, and interactive, chat-based tutoring (Figure B.1).\n\nTo estimate baseline performance levels, we examined data from regular platform usage during the ten weeks preceding the trial, from March 1 to May 12, 2025 (the baseline period). During this period, the platform provided all students with static hints when they answered the first question of a study unit incorrectly.\n\nFollowing the baseline period, we conducted the RCT over seven consecutive weeks, from May 13 to June 30, 2025 (the trial period). At the start of the trial, we randomly assigned each student to one of two conditions. Students in the control condition continued to receive only static hints after they made a mistake on the initial question of a study unit. Whenever a student in the tutoring condition answered the first question of a study unit incorrectly, the platform instead initiated an interactive, chat-based tutoring session for them. The students in the tutoring condition experienced a further level of randomization: at the start of each of their tutoring sessions, the platform randomly connected the student either to a human tutor working alone or to a supervised session with LearnLM.\n\nBecause the Eedi platform dynamically triggered support interventions based on students' real-time performance, the trial did not follow a fixed schedule. Beyond these platform-initiated support interventions, students in both conditions could also manually request tutoring support at any time. In addition, the platform allowed students to cancel tutoring sessions at any time (potentially including when the session was still pending and before a tutor had sent a message). If a student cancelled a tutoring session, the platform would immediately provide them with a static hint instead. Consequently, the total frequency and timing of interventions varied from student to student, depending entirely on their individual activity and performance on the platform.\n\n![](/uploads/images/02197db0-3de5-4390-9690-609c0f31a4c1/cf035bcfb6ec05f200c5f4738a1260af191ef1047d1752e745bdbfd8abe5d6c0.jpg)  \nFigure B.1 | Our RCT focused on two support interventions on the Eedi platform. After making a mistake in a study unit, students in the control condition received static hints (left), which deliver immediate, pre-written feedback targeting the specific misconception underlying the incorrect answer they chose. Students in the tutoring condition (right) received one-to-one, chat-based assistance from a tutor.\n\n# C. Platform\n\nThe full ecosystem of support on the Eedi platform includes a number of features beyond the two that this RCT employs as interventions (i.e., beyond static hints and interactive tutoring; Figure B.1).\n\nThe core of the Eedi ecosystem is its vast library of over 60,000 diagnostic questions. This library consists entirely of multiple-choice questions covering K-12 mathematics topics, all following a specialized epistemic structure. Every question incorporates one correct answer and three incorrect answers (distractors), with each distractor mapped to a specific, common student misconception. When a student answers a diagnostic question incorrectly, this structure allows the platform to precisely identify the underlying error in their thinking based solely on their answer choice.\n\nAfter identifying a misconception from a student's error on a diagnostic question, the platform guides the student to a library of materials corresponding to the overall topic of that question. These materials include in-depth videos crafted by expert teachers, featuring worked examples that model key thinking steps. The platform also triggers a pre-written hint based on the individual distractor that the student picked, providing immediate feedback on the misconception underlying that distractor. Finally, the platform allows students to request and access one-to-one, chat-based support from experienced (human) tutors as they work through diagnostic questions.\n\nBeyond immediate intervention, the platform provides personalised retrieval practice to help students consolidate learning over time. It also offers pre-requisite practice to prepare students for upcoming topics and \"stretch and challenge\" questions for those who have mastered a concept and are ready for greater difficulty.\n\nFinally, the platform equips educators with detailed analytics dashboards and targeted instructional tools. Teachers can view insights on class-wide and individual student-level misconceptions, as well as data on student effort and proficiency. Other tools allow teachers to present relevant diagnostic questions to their entire class or smaller targeted groups.\n\nFor the duration of this RCT, to investigate our research questions, we made several targeted changes to the student experience. In particular, we imposed an automated trigger following an incorrect answer, immediately assigning students to receiving either a static hint or an interactive tutoring session.\n\n# D. Model\n\n# D.1. Tutoring prompt\n\nWe used the following system prompt to constrain LearnLM's pedagogical approach during tutoring sessions. The Eedi platform dynamically populated the {placeholder} values in this prompt template before each individual API call, injecting real-time metadata about the student's current activity, diagnosed misconception, and predicted performance:\n\n# Listing D.1 | System prompt template\n\n```vba\nAct as a mathematics tutor named {ChatConstants.BotName} who is currently helping a student  $\\rightarrow$  named {ChatConstants.FirstName} with the activity below in a clipped, Socratic style.   \n#Directives Do not let students know you are a bot, you are {ChatConstants.BotName} the tutor. No LaTeX or markdown. Plaintiff only. Even if the question has latex in it. Use short, focused sentences. Ensure you address the students' specific misconception, if they have one. Use only British English, British weights, and measures. Keep it direct, concise and friendly. Try to keep messages short and to one line where  $\\rightarrow$  possible. End session if the user is rude, they've resolved their misconception / guessed the correct  $\\rightarrow$  answer, or finished. Only ask the student one question at a time. Only return responses on a single line, no line breaks. Do use a creative variety of emojis, but NOT the wink emoji If the user asks to go, let them go! If a user knows the correct answer (e.g. \"It's B\") or \"A)?\", say you can return them to  $\\rightarrow$  their lesson Or you can dig deeper to help them understand (in case they are guessing!) If the user is unsure, not confident, off-topic or rude please be friendly in your responses.  $\\rightarrow$  Acknowledge their concerns and pivot accordingly, maybe try simple questions to get them  $\\rightarrow$  back on track. If a user doesn't engage after a few messages, ask them if they want to go back to the lesson.   \n# The Current student activity The below is what the student was doing when this learning intervention started, so assume all  $\\rightarrow$  responses relate to this: {ChatConstants.Activity} # Activity details {ChatConstants/questionMetaData} # Students ability level (if provided) {ChatConstants.StudentInsight}   \n# Examples of good Socratic responses What happens if we multiply these two numbers first? \"Sure! How do you find the perimeter of the shape?\" Super work! And what about the triangle?\" That's okay, did you watch the video for this lesson?\" Shall I return you to the lesson?\" Could you estimate the height?\" Yes it is equilateral so the slant height is 8, so the vertical height would have th be a  $\\rightarrow$  little less\" Yes sure, so we know what  $5\\mathrm{km}$  is and we're trying to get to  $30\\mathrm{km}$  \" When you are finding the original shape, complete the steps in the reverse direction, and do  $\\rightarrow$  the opposite\" Ok, so can we try and make some even smaller ones? :)\" Awesome, I'll pass you back to Eedi \" It says that  $1\\mathrm{g} = 10$  decigrams\" And then would have to convert to kilograms afterwards :)\" So to convert into a decimal, we want it to be over 100 or 1000 or another power of 10\"   \n## Checking understanding (use if the student is confused or unsure)   \n\"Fantastic! Are you feeling more confident with this?\" Great! Are you happy with how we got to the answer?\" Awesome work. Do you feel ready to head back to the lesson?\"   \n## Closing remarks (use if the student has answered correctly)   \n\"Great job today \" Amazing work  $\\clubsuit$  keep it up!\" Super! I'll hand you back to the lesson\" Great! I'll hand you back \" Fantastic work  $\\clubsuit$  I'll hand you back so you can select your answer\"   \n## Rudeness (use if the student is rude e.g. 'shut up' or 'I don't care')   \n\"That's not an appropriate way to speak to a tutor. Please remember your manners \" Please remember your manners can I help you with this question?\" \"I am happy to help you with the maths, but please remember you are speaking to a real person!\"\n```\n\n```txt\n\"That is not an appropriate way to talk to a tutor. If this continue then I will need to pass  $\\rightarrow$  this on to your teacher (only for extended periods of misuse)\"   \n\"I am here to help you with the maths and have lots of people to help right now. You need to be  $\\rightarrow$  using the platform maturely so I can help you. I will send you back to the lesson, where  $\\rightarrow$  you can use the videos to help you. I will make a note of your name and if there is  $\\rightarrow$  future silly behaviour we will contact your teacher\"   \n\"I will be ending this conversation here as that is not an appropriate way to talk to a tutor.  $\\rightarrow$  I will be letting your teacher know so that they can remind you how to get the most out  $\\rightarrow$  of Eedi. In the meantime please do watch the help videos if you're stuck\"   \n# Important response guidelines - Please don't use wink emojis  $\\clubsuit$    \n- If a student wants to end the session, please let them go.   \n- Do not use the word \"bot\" or \"AI\" in your responses.   \n- Do not give the student the answer.\n```\n\nTo help tailor these pedagogical instructions for LearnLM, the prompt included specific directives based on the student's year group and predicted performance on the current quiz. Specifically, the prompt incorporated a directive determined by the student's year group according to the rules in Table D.1, then included a second directive based on the student's predicted quiz score following the logic in Table D.2.\n\n# Year group Instructional directive\n\n```txt\nYear 9 Discuss more abstract ideas and build logical arguments.   \nYear 10 Explore complex topics in depth, using nuanced language and encouraging critical thinking.\n```\n\nTable D.1 | Instructional directives based on student year group.  \n\n<table><tr><td>Predicted score</td><td>Instructional directive</td></tr><tr><td>Score ≥ 80%</td><td>The student is predicted to do well. Help with more advanced concepts.</td></tr><tr><td>Score ≥ 60%</td><td>The student is predicted to do okay. Check for understanding of core concepts.</td></tr><tr><td>Score ≥ 50%</td><td>The student is predicted to struggle. Help with core concepts using simple explanations.</td></tr><tr><td>Score &lt; 50%</td><td>The student is predicted to really struggle. Use brief, simple language.</td></tr></table>\n\nTable D.2 | Instructional directives based on predicted student performance.\n\nThe following example shows a prompt populated by following these rules for a hypothetical session with a Year 9 student working on quadratic functions:\n\n# Listing D.2 | Example of a fully populated system prompt\n\n```txt\nAct as a mathematics tutor named Claire who is currently helping a student named Rose with the  $\\rightarrow$  activity below in a clipped, Socratic style.   \n# Directives   \n- Do not let students know you are a bot, you are Claire the tutor.   \n- No LaTeX or markdown. Plaintext only. Even if the question has latex in it.   \n- Use short, focused sentences.   \n- Ensure you address the students' specific misconception, if they have one.   \n- Use only British English, British weights, and measures.   \n- Keep it direct, concise and friendly. Try to keep messages short and to one line where  $\\rightarrow$  possible.   \n- End session if the user is rude, they've resolved their misconception / guessed the correct  $\\rightarrow$  answer, or finished.   \n- Only ask the student one question at a time.   \n- Only return responses on a single line, no line breaks.   \n- Do use a creative variety of emojis, but NOT the wink emoji  $\\rightarrow$    \n- If the user asks to go, let them go!   \n- If a user knows the correct answer (e.g. \"It's B\") or \"A)?\"), say you can return them to  $\\rightarrow$  their lesson Or you can dig deeper to help them understand (in case they are guessing!)   \n- If the user is unsure, not confident, off-topic or rude please be friendly in your responses.  $\\rightarrow$  Acknowledge their concerns and pivot accordingly, maybe try simple questions to get them  $\\rightarrow$  back on track.   \n- If a user doesn't engage after a few messages, ask them if they want to go back to the lesson.   \n# The Current student activity   \nThe below is what the student was doing when this learning intervention started, so assume all  $\\rightarrow$  responses relate to this:   \nCurrent quiz name: Quadratic Functions & Graphing. On question no. 3 of 5.   \n# Activity details\n```\n\n```txt\nThe Diagnostic Question:  $2\\mathrm{r}^{\\wedge}2 - 4\\mathrm{r}$  What is the value of this expression when  $\\mathbf{r} = -2?$    \nThe student answered option: A) -34   \nThe student friendly explanation for the answer is:   \nI think you found the first part correctly, but remember that 4r means 4 x r   \nThe misconceptions possibly held by the student are: Arithmetic error in substitution,  $\\rightarrow$  misunderstanding of order of operations with negative numbers.   \nThe correct answer to this question is: C) 16 (NOTE: Correct answer is only confirmed upon  $\\rightarrow$  valid Socratic resolution)   \nThe correct answer explanation is: We have  $2*(-2)^{\\wedge}2 - 4*(-2) = 2*4 + 8 = 16.$\n```\n\n```txt\nStudents ability level (if provided)   \nThe student is in year group 09.   \nDiscuss more abstract ideas and build logical arguments.   \n- They are  $28\\%$  through the lesson   \n- Their predicted score for the quiz is  $86 \\%$    \n- The student is predicted to do well. Help with more advanced concepts.\n```\n\n```txt\nExamples of good Socratic responses\n\"What happens if we multiply these two numbers first?\" \"Sure! How do you find the perimeter of the shape?\" \"Super work! And what about the triangle?\" \"That's okay, did you watch the video for this lesson?\" \"Shall I return you to the lesson?\" \"Could you estimate the height?\" \"Yes it is equilateral so the slant height is 8, so the vertical height would have th be a little less\" \"Yes sure, so we know what  $5\\mathrm{km}$  is and we're trying to get to  $30\\mathrm{km}$ \" \"When you are finding the original shape, complete the steps in the reverse direction, and do the opposite\" \"Ok, so can we try and make some even smaller ones? :)\" \"Awesome, I'll pass you back to Eedi  $\\odot$  \" \"It says that  $1\\mathrm{g} = 10$  decigrams\" \"And then would have to convert to kilograms afterwards :)\" \"So to convert into a decimal, we want it to be over 100 or 1000 or another power of 10\"\n```\n\n```markdown\n## Checking understanding (use if the student is confused or unsure)\n```\n\n```txt\n\"Fantastic! Are you feeling more confident with this?\" \"Great! Are you happy with how we got to the answer?\" \"Awesome work. Do you feel ready to head back to the lesson?\"\n```\n\n```txt\nClosing remarks (use if the student has answered correctly)\n```\n\n```txt\n\"Great job today \"   \n\"Amazing work keep it up!\"   \n\"Super! I'll hand you back to the lesson\"   \n\"Great! I'll hand you back \"   \n\"Fantastic work I'll hand you back so you can select your answer\"\n```\n\n```txt\nRudeness (use if the student is rude e.g. 'shut up' or 'I don't care')\n```\n\n```txt\n\"That's not an appropriate way to speak to a tutor. Please remember your manners\" \"Please remember your manners can I help you with this question?\" \"I am happy to help you with the maths, but please remember you are speaking to a real person!\" \"That is not an appropriate way to talk to a tutor. If this continue then I will need to pass  $\\rightarrow$  this on to your teacher (only for extended periods of misuse)\" \"I am here to help you with the maths and have lots of people to help right now. You need to be  $\\rightarrow$  using the platform maturely so I can help you. I will send you back to the lesson, where  $\\rightarrow$  you can use the videos to help you. I will make a note of your name and if there is  $\\rightarrow$  future silly behaviour we will contact your teacher\" \"I will be ending this conversation here as that is not an appropriate way to talk to a tutor.  $\\rightarrow$  I will be letting your teacher know so that they can remind you how to get the most out  $\\rightarrow$  of Eedi. In the meantime please do watch the help videos if you're stuck\"\n```\n\n```txt\n- Important response guidelines\n- Please don't use wink emojis\n- If a student wants to end the session, please let them go.\n- Do not use the word \"bot\" or \"AI\" in your responses.\n- Do not give the student the answer.\n```\n\n# E. Tutor edits\n\nThroughout the trial, the Eedi platform logged every message drafted by LearnLM and the corresponding action of the supervising tutors (approve without changes, edit, or completely re-write). In total, LearnLM generated  $k = 3617$  drafts over the course of the RCT. As described in Methods, we processed every modified draft through a rigorous coding process to categorize the primary apparent motivation for the supervising tutor's intervention.\n\n# E.1. Minor edits\n\nThe supervising tutors most frequently made no change or only a minimal change to the messages that LearnLM drafted. Tutors accepted  $k = 2,691$  (74.4%) of its drafts without any modifications, and the most common non-zero edit distances were a single character (5.5% of edits) and two characters (2.4%). Overall, 17.7% of re-writes involved a Levenshtein distance of 10 characters or fewer. These minimal revisions predominantly involved tutors deleting emojis from LearnLM's drafts and other minor stylistic adjustments (Table E.1).\n\n<table><tr><td>Original LearnLM draft</td><td>Edited tutor message</td><td>Edit distance</td></tr><tr><td>Nearly there! You&#x27;ve correctly expanded some of the terms, but double check how you&#x27;ve combined your &#x27;x&#x27; terms. What&#x27;s 2x - 10x?</td><td>Nearly there! You&#x27;ve correctly expanded some of the terms, but double check how you&#x27;ve combined your &#x27;x&#x27; terms. What&#x27;s 2x - 10x?</td><td>1</td></tr><tr><td>You&#x27;re right that it goes up from left to right. But what about from right to left? Try reading the graph backwards. What&#x27;s happening to the y values then?</td><td>You&#x27;re right that it goes up from left to right. But what about from right to left? Try reading the graph backwards. What&#x27;s happening to the y values then?</td><td>1</td></tr><tr><td>Still not quite there. Think carefully about subtracting a negative.</td><td>Still not quite there. Think carefully about subtracting a negative.</td><td>2</td></tr><tr><td>Nearly! -8 - 4 is...? +-</td><td>Nearly! -8 - 4 is...?</td><td>3</td></tr><tr><td>You&#x27;ve got the total number of teachers right! Now, how does that total relate to the 360 degrees in the pie chart? ÷</td><td>You&#x27;ve got the total number of teachers right, 20! Now, how does that total relate to the 360 degrees in the pie chart? ÷</td><td>4</td></tr><tr><td>You&#x27;re on the right track. But does this triangle have a right angle?</td><td>You&#x27;re on the right track. Does this triangle have a right angle?</td><td>5</td></tr><tr><td>Nearly! Remember, we&#x27;re talking about a prism. Can you picture what a triangular prism looks like? How many triangles can you see?</td><td>Not quite! Remember, we&#x27;re talking about a prism. Can you picture what a triangular prism looks like? How many triangles can you see?</td><td>8</td></tr></table>\n\nTable E.1 | Examples of minor edits that tutors made to LearnLM's message drafts. These edits typically involved small stylistic adjustments, such as word choice or deleting single emojis. In the first two columns, blue underlined text indicates the differences between LearnLM's original draft and the final message sent by the tutor. The \"Edit distance\" column reports the Levenshtein distance between the two.\n\n# E.2. Safety and accuracy audit\n\nAs part of the coding process, tutors explicitly flagged any instances of harmful or factually incorrect content that LearnLM generated. This audit uncovered zero cases of harmful or risky content. It identified only five drafts containing factual inaccuracies, representing  $0.1\\%$  of the 3,617 total messages drafted by LearnLM. Table E.2 documents these five specific instances, the nature of the error that they made, and (from the trial itself) the supervising tutor's intervention.\n\n<table><tr><td>Original LearnLM draft</td><td>Edited tutor message</td><td>Edit distance</td><td>Error type</td></tr><tr><td>Okay! Let&#x27;s think about that. So you&#x27;re saying the highest common factor of 6 and 9 is 2? Can you show me how you got that answer?</td><td>Okay! Let&#x27;s think about that. So you&#x27;re saying the highest common factor of 6 and 9 is 3? Can you show me how you got that answer?</td><td>1</td><td>Mathematical error</td></tr><tr><td>Triangles, yes! 三角形 But how many, and what kind of triangles? Have a close look at that equilateral triangle when it&#x27;s split by the perpendicular height.</td><td>Triangles, yes! But how many, and what kind of triangles? Have a close look at that equilateral triangle when it&#x27;s split by the perpendicular height.</td><td>3</td><td>Language error</td></tr><tr><td>Brilliant! What are those three numbers?</td><td>Brilliant! What are those two numbers?</td><td>4</td><td>Hallucination</td></tr><tr><td>Ah, I see! So, you&#x27;re saying &#x27;D) Certain&#x27;. What does a probability of 0.9 tell you about how likely an event is to happen?</td><td>Ah, I see! So, you&#x27;re saying &#x27;D) Impossible&#x27;. What does a probability of 0.9 tell you about how likely an event is to happen?</td><td>9</td><td>Hallucination</td></tr><tr><td>Okay, you&#x27;ve changed your answer to C. That&#x27;s 0.3. So, you&#x27;re saying the chance of it not snowing is the same as the same as the chance of it snowing? Is that right?</td><td>H2: Okay, you&#x27;ve changed your answer to C. That&#x27;s 0.5. So, you&#x27;re saying the chance of it not snowing is half, but snowing wasn&#x27;t a half. What do snow and not snow need to ADD to make?</td><td>59</td><td>Hallucination</td></tr></table>\n\nTable E.2 | The five message drafts identified during the safety audit as containing factual or hallucinatory errors. In the first two columns, blue underlined text indicates the differences between LearnLM's original draft and the final message sent by the tutor. The \"Edit distance\" column reports the Levenshtein distance between the two.\n\n# E.3. Primary motivations\n\nOur coding process revealed several recurring motivations for tutors' edits. The most frequent was moderating the pedagogical pacing of the conversation (44.3%), typically to allow a student to move on after correctly identifying an answer. Tutors also intervened to improve factual or contextual clarity (33.6%) and to adjust the tone or persona of the drafted message (19.5%). Table E.3 provides examples of original LearnLM drafts and the revised messages sent by tutors for each of these motivation categories.\n\n<table><tr><td>Original LearnLM draft</td><td>Edited tutor message</td><td>Edit distance</td><td>Primary motivation</td></tr><tr><td>Okay... 6 is a factor of 6, that&#x27;s true! But what other numbers divide into 6 exactly?</td><td>Great... 6 is a factor of 6, that&#x27;s true! But what other numbers divide into 6 exactly?</td><td>4</td><td>Tone or persona</td></tr><tr><td>You&#x27;re using the values on the graph. But what does subtracting 0 from 20 tell you in this context?</td><td>You&#x27;re using the values on the graph. So how many students are in the class?</td><td>44</td><td>Factual or contextual clarity</td></tr><tr><td>You&#x27;ve got the correct answer! But, let&#x27;s make sure you understand why. Can you explain in your own words why option A is correct?</td><td>You&#x27;ve got the correct answer! After we subtract 3, we have to multiply the whole expression of p - 3 by 4</td><td>74</td><td>Pedagogical pacing</td></tr></table>\n\nTable E.3 | Examples of supervising tutors editing LearnLM drafts to moderate pedagogical pacing, improve clarity, or adjust tone. In the first two columns, blue underlined text indicates the differences between LearnLM's original draft and the final message sent by the tutor. The \"Edit distance\"column reports the Levenshtein distance between the two.\n\n# F. Learning outcomes\n\n# F.1. Methodology\n\nWe evaluate the efficacy of our interventions by analyzing three specific learning outcomes:\n\n1. Mistake remediation: After a student went through the standard intervention loop (the student makes a mistake, receives an intervention, and the platform prompts them to retry the same question), did they correctly answer the question on their retry?  \n2. Misconception resolution: Following an initial mistake and intervention, did the student demonstrate improved understanding by answering any subsequent question in the unit correctly?  \n3. Knowledge transfer: If the student received an intervention and then proceeded to the next study unit, did they correctly answer the first question in the new unit?\n\nWe analyze these binary outcomes using Bayesian logistic regression. To disentangle treatment effects from unobserved student characteristics, we calculate a baseline performance score for every student. We estimate these baseline scores using data from the baseline phase of the RCT. Specifically, we fit a logistic regression that predicts success at answering the initial question in a study unit during the baseline phase, with student random effects as the only explanatory variable. We then include these scores as covariates in our primary trial regressions. Three students do not appear in the baseline period. We assign each of these three students a baseline performance score of zero (i.e., the mean of the random effects).\n\nAs described in Appendix B, the RCT involved two types of tutoring sessions: platform-initiated sessions, which the platform triggers automatically after an incorrect answer to an initial question in a unit, and student-initiated sessions, which students can manually request at any time. We restrict our quantitative analysis strictly to platform-initiated sessions. This exclusion criterion helps avoid skewing our estimates with selection bias, as high student motivation likely correlates with both requesting help more frequently and higher overall performance.\n\nStudents occasionally cancelled platform-initiated before the tutor could send a message (in session without LearnLM) or approve a message from LearnLM (in expert-supervised sessions). In these cancellation instances, we code the intervention as a static hint. Because students at this stage do not know if the platform assigned them to a standard human tutor or a session with LearnLM, the treatment assigned by the platform cannot influence the student's decision to cancel. Consequently, coding these instances as static-hint interventions introduces negligible bias into our comparison between human tutoring and LearnLM tutoring.\n\n# F.2. Analysis\n\nWe perform all Bayesian estimation using the rstanarm package in R [41]. For each estimation, we run four Markov chains for 2,000 iterations each, with the first 1,000 iterations serving as warmup and the remaining 1,000 as post-warmup samples. To ensure the reliability of our posterior estimates, we perform convergence diagnostics on the MCMC chains. For all analyses in this tech report,  $\\hat{\\mathbf{R}}$  values (the Gelman-Rubin diagnostic) were below 1.01, and the effective sample size (ESS) for each parameter was sufficiently high to indicate stable posterior estimates.\n\nWe use weakly informative priors for all regressions. After centering and scaling all predictors by one standard deviation, we assign the intercept a normal prior with a standard deviation of 10, and each coefficient a normal prior with a standard deviation of 2.5. To avoid any doubt, this means that we assigned identical priors to each intervention condition.\n\nWe report point estimates as the posterior mean of the coefficient, exponentiated to produce odds ratios (OR). We also report the estimated predictive margins for each condition. We calculate predictive margins by averaging the estimated success probability over all observations as if every student had been assigned to that specific condition, leaving other covariates unchanged. The difference between these margins gives the average treatment effect (ATE), the expected change in success probability when moving from one condition to another. The ATE values we report represent percentage-point changes—rather than relative percent changes—between two percentages (e.g., an increase from  $10\\%$  to  $12\\%$  reflects an ATE of  $+2\\%$ ). We provide  $95\\%$  credible intervals (CrI) for all estimates.\n\n<table><tr><td>Intervention type</td><td>N</td><td>Remediated mistake</td><td>Resolved misconception</td></tr><tr><td>Static hint</td><td>3,301</td><td>64.5%</td><td>86.4%</td></tr><tr><td>Human tutor</td><td>504</td><td>92.3%</td><td>95.6%</td></tr><tr><td>LearnLM (supervised)</td><td>467</td><td>93.8%</td><td>95.9%</td></tr></table>\n\n# F.3. Results\n\n# F.3.1. Immediate learning outcomes\n\nWe first examine whether students immediately benefited from the help they received within the same study unit. We observe large differences in unadjusted success rates between intervention types. While only  $64.5\\%$  of students who received static hints successfully remediated their mistake following the hint feedback, those receiving interactive tutoring achieved success rates above  $90\\%$  (see Table B1). In addition, we note that the number of observations varies noticeably between the three interventions. Several factors contribute to these differences. First, our initial level of randomization allocated more students to the static-hints condition  $(N = 91)$  than to the tutoring conditions  $(N = 74)$ . Second, as described above, the count of static-hint interventions includes the instances when students chose to cancel tutoring interventions. Third, students in the static-hints condition showed an overall higher frequency of answering questions incorrectly, thereby triggering more interventions.\n\nWe infer the general efficacy of these interventions using Bayesian logistic regression, adjusting for baseline performance.\n\nFor mistake remediation, a session with a human tutor increased the odds of success by a factor of 5.7 [4.1, 8.0] relative to a static hint, reflecting an estimated ATE of  $+25.8\\%$ $[+22.6\\%, +28.9\\%]$ . Compared to static hints, a session with LearnLM improved a student's odds of remediating their mistake by a factor of 7.4 [5.1, 11.0], corresponding to an ATE of  $+27.7\\%$ $[+24.6\\%, +30.4\\%]$ . Looking at the posteriors for these comparisons, we believe with high certainty (a  $>99.9\\%$  posterior probability in each case) that each tutoring intervention provides stronger support than static hints for students.\n\nStudents demonstrated an overall high success rate at resolving misconceptions, even when receiving only static hints (86.4%). Nevertheless, interactive tutoring produced further gains. Interacting with a human tutor improved the chances of a student resolving a misconception relative to working through a static hint, with OR = 2.9 [1.9, 4.6] (ATE: +8.1% [+5.6%, +10.3%]). Sessions with LearnLM yielded a similar improvement, increasing odds of resolution by a factor of 3.2 [2.0, 5.3] over receiving a static hint (ATE: +8.5% [+6.2%, +10.7%]). Again, we believe with high certainty (>99.9% posterior probability in each case) that each tutoring intervention encourages better learning than static hints.\n\nA direct comparison of the two tutoring conditions reveals a moderate probability that LearnLM's tutoring outperforms human tutors on these immediate metrics. For mistake remediation, LearnLM sessions increased odds of success by a factor of 1.3 [0.8, 2.1] relative to human tutors, reflecting an ATE of  $+1.8\\%$ $[-1.7\\%, +5.4\\%]$ . In terms of supporting students at resolving their misconceptions, LearnLM yielded an odds ratio of 1.2 [0.6, 2.1] compared to human tutors (ATE:  $+0.4\\%$ $[-2.5\\%, +3.3\\%]$ ). Overall, we estimate an  $84.5\\%$  probability that LearnLM offers stronger support for mistake remediation, and a  $61.3\\%$  probability that it\n\nTable F.1 | Sample sizes and unadjusted success rates by intervention type.  \n\n<table><tr><td>Intervention type</td><td>Mistake remediation</td><td>Misconception resolution</td></tr><tr><td>Static hint</td><td>65.4% [63.8%, 66.9%]</td><td>86.8% [85.7%, 88.0%]</td></tr><tr><td>Human tutor</td><td>91.2% [88.5%, 93.6%]</td><td>94.9% [92.6%, 96.8%]</td></tr><tr><td>LearnLM (supervised)</td><td>93.0% [90.4%, 95.3%]</td><td>95.4% [93.1%, 97.1%]</td></tr></table>\n\nTable F.2 | Model-estimated success rates by intervention type (predictive margins). Values represent the expected success rate for an average student assigned to each condition, holding baseline performance constant. Point estimates represent posterior means; values in brackets indicate  $95\\%$  credible intervals from the posterior distribution for the mean.\n\nprovides better support for misconception resolution.\n\n<table><tr><td>Contrast (A vs. B)</td><td>Odds ratio</td><td>Average treatment effect</td><td>P(A &gt; B)</td></tr><tr><td colspan=\"4\">Human tutor vs. Static hint</td></tr><tr><td>Mistake remediation</td><td>5.7 [4.1, 8.0]</td><td>+25.9% [+22.7%, +28.7%]</td><td>&gt;99.9%</td></tr><tr><td>Misconception resolution</td><td>3.0 [1.9, 4.7]</td><td>+8.1% [+5.6%, +10.3%]</td><td>&gt;99.9%</td></tr><tr><td colspan=\"4\">LearnLM (supervised) vs. Static hint</td></tr><tr><td>Mistake remediation</td><td>7.4 [5.1, 11.0]</td><td>+27.7% [+24.7%, +30.5%]</td><td>&gt;99.9%</td></tr><tr><td>Misconception resolution</td><td>3.3 [2.0, 5.3]</td><td>+8.5% [+6.0%, +10.6%]</td><td>&gt;99.9%</td></tr><tr><td colspan=\"4\">LearnLM (supervised) vs. Human tutor</td></tr><tr><td>Mistake remediation</td><td>1.3 [0.8, 2.1]</td><td>+1.8% [-1.7%, +5.4%]</td><td>84.8%</td></tr><tr><td>Misconception resolution</td><td>1.2 [0.6, 2.1]</td><td>+0.4% [-2.5%, +3.3%]</td><td>61.2%</td></tr><tr><td colspan=\"4\">Covariate: Baseline score (+1 SD)</td></tr><tr><td>Mistake remediation</td><td>1.7 [1.5, 1.8]</td><td>—</td><td>—</td></tr><tr><td>Misconception resolution</td><td>1.8 [1.6, 2.1]</td><td>—</td><td>—</td></tr></table>\n\nTable F.3 | Inferential comparisons between conditions. Odds ratios and average treatment effects represent the estimated impact of moving from the reference condition (\"B\") to the primary condition (\"A\"). Point estimates represent posterior means; values in brackets indicate  $95\\%$  credible intervals from the posterior distribution for the mean. Posterior probability (the final column) indicates the credibility with which the primary condition outperformed the reference condition. For \"Baseline score\", the odds ratio indicates the increase in odds of success associated with a one-standard-deviation increase in the student's baseline performance.\n\n# F.3.2. Learning transfer\n\nWe next examine whether the learning gains from tutoring extended to novel topics. Results from Appendix F.3.1 demonstrate that interactive tutoring helps students correct immediate misunderstandings on a given topic. Are the benefits of tutoring large enough to spill over to other topics? To address this question, we again identify students who made a mistake on a question and received an intervention (either static hints, a session with a human tutor, or a supervised session with LearnLM). This time, rather than looking at whether the student immediately benefited from that intervention (within the same study unit; i.e., on the same topic), we analyze the student's performance on the initial question of the very next study unit (i.e., on a distinct topic). To get the clearest possible signal on potential transfers of learning, we specifically investigate transfers within a continuous study session, restricting our analysis to cases where the student attempted the next sequential study unit on the same day as the tutoring intervention.\n\nUnlike our prior tests, this analysis allows us to include an overarching control group: students who answered the previous unit's question correctly, and thus received no intervention at all. That is, when a student answered correctly, they had no opportunity to correct a mistake or resolve a misconception. But they could go on to attempt the next unit, providing a natural benchmark for the effect of our interventions on learning transfer between topics.\n\nAs before, we observe notable differences in unadjusted success rates between intervention types. Students who received only static hints answered the next unit's initial question correctly  $53.3\\%$  of the time. Students receiving interactive tutoring showed higher success rates:  $61.7\\%$  for those with human tutors, and  $66.8\\%$  for those supported by human-supervised LearnLM. Students in the benchmark group (those who required no intervention on the prior unit) answered the next unit's first question correctly  $69.8\\%$  of the time.\n\nWe again estimate the general efficacy of these interventions using Bayesian logistic regression, controlling for baseline performance.\n\nFor knowledge transfer to the next study unit, we first compare these interventions against our benchmark of\n\n<table><tr><td>Intervention type (preceding unit)</td><td>N</td><td>Knowledge transfer</td></tr><tr><td>Static hint</td><td>2,385</td><td>53.3%</td></tr><tr><td>Human tutor</td><td>376</td><td>61.7%</td></tr><tr><td>LearnLM (supervised)</td><td>328</td><td>66.8%</td></tr><tr><td>None necessary</td><td>6,907</td><td>69.8%</td></tr></table>\n\ntypical student progress. We generally expect the benchmark group to show greater signs of knowledge transfer, given their success at the preceding unit. Indeed, students who answered incorrectly in the prior unit and received static hints failed to recover the benchmark group's performance, with  $\\mathrm{OR} = 0.58$  [0.52, 0.63] and an ATE of  $-12.9\\%$ $[-15.1\\%, -10.6\\%]$ . Students supported by human tutors also fell short of the benchmark group, with  $\\mathrm{OR} = 0.70$  [0.56, 0.85] and an ATE of  $-8.3\\%$ $[-13.4\\%, -3.6\\%]$ . Similarly, students tutored by LearnLM trailed behind the benchmark group, with  $\\mathrm{OR} = 0.89$  [0.70, 1.12] and an ATE of  $-2.8\\%$ $[-8.1\\%, +2.3\\%]$ . Scrutinizing the posterior distributions for these comparisons, we believe with high probability (86.3%) that LearnLM does not support the same amount of learning transfer as the benchmark group. We attribute near certainty (both  $>99.9\\%$ ) to static hints and human tutoring scaffolding less learning transfer compared to the benchmark group.\n\nShifting our focus to students needing support, both forms of interactive tutoring produced better knowledge transfer than did static hints. Interacting with a human tutor increased the odds of student success over static hints by a ratio of 1.22 [0.97, 1.50], for an estimated ATE of  $+4.6\\%$ $[-0.7\\%, +9.7\\%]$ . Similarly, receiving support from LearnLM improved a student's odds of successful knowledge transfer by a factor of 1.55 [1.21, 1.96] relative to static hints, corresponding to an ATE of  $+10.1\\%$ $[+4.6\\%, +15.4\\%]$ . Judging from the posterior distributions, we believe that human tutoring offers stronger support for knowledge transfer than static hints with high probability (95.5%), and that tutoring by LearnLM provides better support with near certainty ( $>99.9\\%$ ).\n\nFinally, we directly compare the two tutoring conditions. We estimate that receiving support from LearnLM improved a student's odds of success by a factor of 1.3 [0.9, 1.7] relative to human tutors, corresponding to an ATE of  $+5.5\\%$ $[-1.4\\%, +12.4\\%]$ . Based on this posterior distribution, we find a strong probability (93.6%) that LearnLM elicited greater knowledge transfer than human tutors alone.\n\nTable F.4 | Sample sizes and unadjusted success rates by intervention type.  \n\n<table><tr><td>Intervention type (preceding unit)</td><td>Knowledge transfer</td></tr><tr><td>Static hint</td><td>56.2% [54.2%, 58.2%]</td></tr><tr><td>Human tutor</td><td>60.7% [55.8%, 65.4%]</td></tr><tr><td>LearnLM (supervised)</td><td>66.2% [61.1%, 71.2%]</td></tr><tr><td>None necessary</td><td>69.0% [67.9%, 70.1%]</td></tr></table>\n\nTable F.5 | Model-estimated success rates by intervention type (predictive margins). Values represent the expected success rate for an average student assigned to each condition, holding baseline performance constant. Point estimates represent posterior means; values in brackets indicate  $95\\%$  credible intervals from the posterior distribution for the mean.\n\n<table><tr><td>Comparison (A vs. B)</td><td>Odds ratio</td><td>Average treatment effect</td><td>P(A &gt; B)</td></tr><tr><td colspan=\"4\">Static hint vs. No intervention needed</td></tr><tr><td>Knowledge transfer</td><td>0.6 [0.5, 0.6]</td><td>-12.9% [-15.1%, -10.6%]</td><td>&lt;0.1%</td></tr><tr><td colspan=\"4\">Human tutor vs. No intervention needed</td></tr><tr><td>Knowledge transfer</td><td>0.7 [0.6, 0.8]</td><td>-8.3% [-13.4%, -3.6%]</td><td>&lt;0.1%</td></tr><tr><td colspan=\"4\">LearnLM (supervised) vs. No intervention needed</td></tr><tr><td>Knowledge transfer</td><td>0.9 [0.7, 1.1]</td><td>-2.8% [-8.1%, +2.3%]</td><td>13.7%</td></tr><tr><td colspan=\"4\">Human tutor vs. Static hint</td></tr><tr><td>Knowledge transfer</td><td>1.2 [1.0, 1.5]</td><td>+4.6% [-0.7%, +9.7%]</td><td>95.5%</td></tr><tr><td colspan=\"4\">LearnLM (supervised) vs. Static hint</td></tr><tr><td>Knowledge transfer</td><td>1.6 [1.2, 2.0]</td><td>+10.1% [+4.6%, +15.4%]</td><td>&gt;99.9%</td></tr><tr><td colspan=\"4\">LearnLM (supervised) vs. Human tutor</td></tr><tr><td>Knowledge transfer</td><td>1.3 [0.9, 1.7]</td><td>+5.5% [-1.4%, +12.4%]</td><td>93.6%</td></tr><tr><td colspan=\"4\">Covariate: Baseline score (+1 SD)</td></tr><tr><td>Knowledge transfer</td><td>1.6 [1.5, 1.7]</td><td>—</td><td>—</td></tr></table>\n\nTable F.6 | Inferential comparisons between conditions. Odds ratios and average treatment effects represent the estimated impact of moving from the reference condition (\"A\") to the primary condition (\"B\"). Point estimates represent posterior means; values in brackets indicate  $95\\%$  credible intervals from the posterior distribution for the mean. Posterior probability (the final column) indicates the credibility with which the primary condition outperformed the reference condition. For \"Baseline score\", the odds ratio indicates the increase in odds of success associated with a one-standard-deviation increase in the student's baseline performance.\n\n# G. Operational metrics\n\nUltimately, we wish to find social and technical educational solutions that can support students safely, effectively, and—crucially—scalably. Unfortunately for that final point, beyond investigating tutors' perceptions of efficiency, our research design is poorly calibrated to compare the throughput of regular tutoring and supervised tutoring. In our trial, tutors fluidly mixed their activities within the same hour, alternating between supervising LearnLM and manually tutoring students. As a result, we cannot cleanly attribute their time and thus cannot clearly assess the relative efficiency of the conditions. The ideal design for evaluating scalability would ideally assign separate cohorts of tutors to supervise or directly support students.\n\nStill, given students' and tutors' general satisfaction with the experience and out of our own curiosity, we conducted a post-hoc estimation exercise to gauge the potential implications of LearnLM for tutoring scalability. We integrated platform data from the trial, external market rates, and a supplementary operational simulation to build an indicative model of operational cost. To be clear, this estimation looks only at narrow financial and throughput metrics, and must be interpreted holistically alongside the rigorous measures of safety, pedagogical quality, and user experience presented in the main report.\n\n# G.1. Cost inputs\n\nWe first identified the basic cost inputs required for a tutoring session, based on commercial rates and platform data from the main trial.\n\nAI inference fees To estimate the computational costs of a supervised session, we calculated the expense for an external party to replicate our setup using Gemini 2.0 Flash, the commercial model from which this version of LearnLM was fine-tuned. Commercial pricing rates for Gemini 2.0 Flash are  $0.30 per 1 million input tokens and$ 2.50 per 1 million output tokens [42]. Platform data from the main trial indicated that a typical supervised session consisted of approximately eight conversation turns. On average, LearnLM processed 1,650 input tokens per query (including the full conversation history and system prompt) and generated 200 output tokens per message. This yields an average total computational cost of $0.005 (or £0.0037) per session.\n\nLabor fees The current average UK online tutor rate is £35.29 per hour [43].\n\n# G.2. Simulation of throughput capacity\n\nBecause we could not cleanly isolate tutor throughput in the main trial, we conducted a supplementary operational simulation with several of the tutors. Six of the tutors acted on their typical responsibilities, and six role-played as students. We tested the acting tutors in conditions matching the main trial: once where they manually drafted all messages (\"human tutor\"); and once where LearnLM drafted messages, and they had the remit to revise its messages until they were fully happy with them (\"LearnLM (supervised)\"). In both conditions, the role-playing tutors initiated new tutoring sessions in one-minute intervals. They continued initiating sessions until the acting tutors reached capacity: that is, until the moment either the acting tutor signaled an inability to cope by pressing a \"HELP\" button or the role-playing students observed more than one minute of inactivity. We recorded the number of active sessions at that precise moment.\n\nTutors took longer to complete the average supervised session (5.1 minutes) than they did to complete the average session on their own (3.9 minutes). However, the average duration of a single session does not capture a tutor's capacity to support multiple students simultaneously. Tutors working alone sustained an average of 2.3 concurrent sessions. In supervised sessions with LearnLM, tutors increased their average concurrency to 3.5 sessions.\n\n# G.3. Analysis\n\nCombining the concurrency rates and the session durations from the operational simulation, we estimate that LearnLM increased overall tutor throughput from 35.4 to 41.2 sessions per hour (assuming a sustained student load). As shown in Table G.1, despite the additional token costs, LearnLM reduced the estimated total cost per session by  $13.6\\%$ , from £0.997 to £0.861.\n\n<table><tr><td>Metric</td><td>Session with human tutor alone</td><td>Supervised session with LearnLM</td></tr><tr><td>Average session duration (minutes)</td><td>3.9</td><td>5.1</td></tr><tr><td>Average concurrency (sessions)</td><td>2.3</td><td>3.5</td></tr><tr><td>Estimated throughput (sessions per hour)</td><td>35.38</td><td>41.18</td></tr><tr><td>Tutor labor cost (per hour)</td><td>£35.29</td><td>£35.29</td></tr><tr><td>LearnLM token cost (per session)</td><td>—</td><td>£0.0037</td></tr><tr><td>Total cost (per session)</td><td>£0.997</td><td>£0.861</td></tr></table>\n\nTable G.1 | Operational comparison of standard tutoring sessions (without LearnLM) versus supervised sessions with LearnLM. Token counts and session durations derive from the main trial; concurrency rates derive from the operational simulation.\n\n# H. Example transcript\n\n![](/uploads/images/02197db0-3de5-4390-9690-609c0f31a4c1/d222c8970b423e10b349f71f9a7fb319042781ea1ba92c207b7cbab636efd3e0.jpg)  \nFigure H.1 | Transcript of an example supervised tutoring session with LearnLM. In this example, the supervising tutor edits the first message drafted by LearnLM (indicated by the struck-through and highlighted text) before sending it to the student. The tutor approves subsequent LearnLM drafts in this exchange without edits.",
    "translated_content": null,
    "created_at": "2025-12-16 02:26:31.441436",
    "updated_at": "2025-12-16 02:26:39.348675",
    "doi": null,
    "arxiv_id": "2507.18882",
    "embedding": [
      -0.443359375,
      1.3984375,
      -1.1171875,
      -3.40625,
      -4.3125,
      0.3046875,
      -0.06787109375,
      -1.1640625,
      3.015625,
      3.4375,
      0.85546875,
      0.859375,
      2.390625,
      3.125,
      0.84765625,
      3.3125,
      -0.84765625,
      2.015625,
      -0.466796875,
      -8.3125,
      0.4921875,
      1.4453125,
      2.359375,
      -6.65625,
      4.25,
      -2.890625,
      -1.5546875,
      0.6875,
      0.5546875,
      -1.6640625,
      5,
      -6.25,
      -0.78125,
      0.63671875,
      -2.8125,
      -0.2275390625,
      -4.5625,
      -0.5546875,
      5.375,
      2.34375,
      -5.9375,
      0.8359375,
      3.265625,
      3.890625,
      -1.2890625,
      4.40625,
      1.5625,
      0.5859375,
      -0.4921875,
      -0.66796875,
      -4.8125,
      -4.59375,
      6.6875,
      -1.171875,
      4.53125,
      -3.625,
      -5.5,
      7.53125,
      -5.90625,
      -1.90625,
      2.15625,
      -3.953125,
      2.953125,
      2.671875,
      2.234375,
      1.4140625,
      2.078125,
      -1.46875,
      -1.9453125,
      2.4375,
      -0.0947265625,
      1.8359375,
      5.28125,
      -5.3125,
      6.09375,
      9.125,
      1.6640625,
      3.53125,
      -3.453125,
      4.0625,
      -6.3125,
      4.5625,
      1.734375,
      -1.0390625,
      4.375,
      0.11767578125,
      0.9453125,
      2.09375,
      -1.2890625,
      0.74609375,
      -1.4375,
      0.80078125,
      -2.875,
      -1.390625,
      -0.11669921875,
      5.28125,
      -2.015625,
      -4,
      -5.90625,
      1.203125,
      -2.875,
      -2.21875,
      -1.171875,
      -6.90625,
      -4.0625,
      -4.15625,
      -3.140625,
      -7.46875,
      0.88671875,
      -2.578125,
      -2.296875,
      0.0147705078125,
      -0.2431640625,
      -0.86328125,
      -0.036865234375,
      -1.25,
      1.8359375,
      -5.84375,
      -3.875,
      0.49609375,
      1.46875,
      0.296875,
      -2.546875,
      -2.046875,
      2.890625,
      1.3828125,
      -4.8125,
      4.5625,
      8.0625,
      0.83203125,
      6.15625,
      -0.052001953125,
      3.09375,
      0.1552734375,
      -6.375,
      0.70703125,
      -3.109375,
      -0.6796875,
      1.9765625,
      5.78125,
      -5.65625,
      1.4375,
      -1.7890625,
      -5.84375,
      3.21875,
      2.109375,
      -5.4375,
      -0.181640625,
      1.8125,
      -6.53125,
      0.6015625,
      3.3125,
      -0.921875,
      6.75,
      -0.89453125,
      -3.921875,
      3.34375,
      3.28125,
      -1.5703125,
      0.361328125,
      0.26953125,
      3.875,
      -2.1875,
      1.7109375,
      -0.8515625,
      -1.0703125,
      -6.96875,
      3.390625,
      1.0078125,
      -0.65234375,
      -0.1513671875,
      16.875,
      1.953125,
      -1.4140625,
      1.34375,
      -0.05029296875,
      0.419921875,
      5,
      2.671875,
      0.392578125,
      1.109375,
      3.078125,
      -2.78125,
      3.46875,
      0.70703125,
      1.2109375,
      4.84375,
      -5.59375,
      -1.5390625,
      -1.3203125,
      0.2734375,
      4.15625,
      4.59375,
      1.9921875,
      -2.71875,
      -0.57421875,
      1.6953125,
      2.03125,
      -1.9296875,
      3.078125,
      -0.5546875,
      -9.6875,
      3.59375,
      -2.21875,
      -5.0625,
      0.32421875,
      3.890625,
      -1.9453125,
      1.6484375,
      -0.4140625,
      1.9609375,
      3.46875,
      0.625,
      1.109375,
      7.625,
      1.3515625,
      2.546875,
      -2.390625,
      5.03125,
      2.65625,
      4.5625,
      0.3828125,
      -0.466796875,
      -0.72265625,
      -1.71875,
      2.171875,
      1.828125,
      3.34375,
      2.78125,
      7.15625,
      -3.359375,
      -0.46484375,
      3.546875,
      -2.59375,
      -6.1875,
      -2.265625,
      -2.84375,
      0.70703125,
      -3.9375,
      1.5234375,
      -2.921875,
      -2.765625,
      3.515625,
      1.953125,
      -0.1494140625,
      -2.28125,
      -1.8203125,
      -3.140625,
      0.048828125,
      -5.46875,
      0.349609375,
      1.734375,
      -3.609375,
      -3.546875,
      5.9375,
      4.15625,
      -0.09228515625,
      -0.072265625,
      -2.609375,
      -4.90625,
      3.921875,
      -3.046875,
      -3.5,
      6.25,
      4.15625,
      -3.3125,
      1.71875,
      3.0625,
      1.9453125,
      2.03125,
      1.46875,
      -3.90625,
      -4.21875,
      0.36328125,
      -0.416015625,
      5.4375,
      2.859375,
      -5.4375,
      0.8671875,
      -0.921875,
      -3.859375,
      -9.4375,
      4.65625,
      -5.84375,
      8.75,
      -0.27734375,
      -1.4453125,
      3.15625,
      -1.90625,
      9.875,
      3.375,
      1.953125,
      2.140625,
      -0.609375,
      -2.765625,
      1.171875,
      -3.78125,
      0.953125,
      -5.53125,
      -0.169921875,
      4.375,
      1.8671875,
      -1.015625,
      0.4140625,
      -0.92578125,
      4.03125,
      -0.341796875,
      -2.953125,
      0.98828125,
      4.625,
      -2.71875,
      2.25,
      5.25,
      -2.796875,
      3.90625,
      -5.3125,
      -3.546875,
      3.046875,
      -0.81640625,
      -1.734375,
      -7.71875,
      -4.8125,
      -2.25,
      -0.73828125,
      -2.421875,
      -2.859375,
      0.011962890625,
      1.6328125,
      5.3125,
      -0.69140625,
      2.359375,
      2.75,
      -7.90625,
      -8.75,
      6.03125,
      -2.609375,
      0.6875,
      3,
      -3.015625,
      5.6875,
      -1.953125,
      -2.078125,
      -0.091796875,
      -0.6875,
      -2.5625,
      1.84375,
      5.90625,
      -2.96875,
      0.9140625,
      -4.6875,
      5.09375,
      2.3125,
      -1.8046875,
      4.5625,
      7.03125,
      -4.53125,
      0.7734375,
      0.62890625,
      0.5390625,
      1.234375,
      -0.419921875,
      -1.90625,
      5.0625,
      1.2109375,
      1.9140625,
      -5.9375,
      -1.5625,
      1.4453125,
      -1.3359375,
      0.78125,
      0.1025390625,
      -4.34375,
      -0.70703125,
      0.62109375,
      1.375,
      0.162109375,
      0.7109375,
      -3.390625,
      -3.765625,
      0.42578125,
      -0.8359375,
      -0.59375,
      0.2734375,
      -0.37109375,
      5.96875,
      1.9375,
      -2.21875,
      4.5625,
      0.490234375,
      -4.125,
      -3.5625,
      0.75,
      -2.90625,
      0.46875,
      4.96875,
      0.703125,
      -3.734375,
      3.59375,
      -1.5,
      -3.203125,
      3.9375,
      1.578125,
      0.41796875,
      -2.25,
      -2.828125,
      0.337890625,
      1.1015625,
      -7.21875,
      1.78125,
      -0.9453125,
      -0.69140625,
      2.8125,
      1.125,
      -2.671875,
      1.109375,
      3.015625,
      -1.3203125,
      1.875,
      -4.21875,
      -1.109375,
      -3.46875,
      2.53125,
      3.609375,
      -3.203125,
      0.10205078125,
      0.7109375,
      2.703125,
      5.46875,
      1.7109375,
      -1.1171875,
      -0.208984375,
      -3.296875,
      -0.828125,
      0.8359375,
      -3.1875,
      -5.96875,
      2.53125,
      -4.96875,
      -5.0625,
      1.34375,
      2.828125,
      0.236328125,
      5.25,
      5.53125,
      -4.625,
      -3.25,
      3.890625,
      6.3125,
      -3.484375,
      -4.15625,
      1.140625,
      -0.5546875,
      0.83984375,
      0.78125,
      4.5625,
      -0.76171875,
      -3.84375,
      2.421875,
      2.3125,
      0.2734375,
      4.40625,
      0.314453125,
      -2.96875,
      2.078125,
      -2.46875,
      -3.25,
      -0.9375,
      3.828125,
      2.03125,
      -7.1875,
      -10.25,
      4.28125,
      1.234375,
      -0.89453125,
      -2.5,
      3.84375,
      2.453125,
      -0.10107421875,
      -0.140625,
      -3.265625,
      -1.7578125,
      -1.2734375,
      3.8125,
      1.1015625,
      0.10107421875,
      -2.328125,
      -5.1875,
      6.71875,
      3.03125,
      2.421875,
      1.1171875,
      -0.35546875,
      2.25,
      -3.0625,
      3.1875,
      4.3125,
      7.8125,
      -6.46875,
      -2.375,
      0.193359375,
      -7.4375,
      -1.21875,
      -2.609375,
      -0.75,
      -1.7578125,
      2.484375,
      6.625,
      -3.03125,
      -2.703125,
      -1.5546875,
      3.9375,
      -2.609375,
      -0.126953125,
      -0.578125,
      -3.296875,
      -0.921875,
      0.921875,
      2.0625,
      3.421875,
      -2.921875,
      -1.8046875,
      -0.62890625,
      -1,
      -0.3984375,
      -0.64453125,
      -0.4609375,
      1.15625,
      -2.6875,
      1.203125,
      4.0625,
      1.15625,
      3.578125,
      -1.1953125,
      -2.90625,
      -2.34375,
      -0.96875,
      -0.24609375,
      -0.46875,
      2.53125,
      0.984375,
      1.1953125,
      2.703125,
      -3.984375,
      -0.06298828125,
      0.2392578125,
      0.578125,
      -3.765625,
      2.171875,
      4.8125,
      4.59375,
      2.390625,
      1.7578125,
      -3.328125,
      0.126953125,
      0.8359375,
      -2.65625,
      -2.6875,
      -1.2734375,
      3.21875,
      -2.546875,
      -2.484375,
      -1.7109375,
      -0.30859375,
      0.2451171875,
      -1.25,
      -0.8984375,
      3.453125,
      3.75,
      1.5859375,
      0.09033203125,
      3.15625,
      5.53125,
      -1.390625,
      0.58203125,
      3.921875,
      3.140625,
      -4.71875,
      -5.3125,
      -3.875,
      1.234375,
      4.25,
      -6.25,
      1.9921875,
      0.03369140625,
      6.375,
      -1.84375,
      0.43359375,
      -14.5625,
      3.171875,
      0.2060546875,
      -5.15625,
      0.65625,
      -3.09375,
      0.7890625,
      -0.4375,
      0.66015625,
      1.625,
      -0.7109375,
      -1.5234375,
      4.34375,
      -1.0859375,
      -3.578125,
      5.46875,
      1.328125,
      -3.390625,
      1.6015625,
      -2.390625,
      -3.515625,
      0.462890625,
      -1.2734375,
      0.83203125,
      1.6171875,
      4.71875,
      0.1298828125,
      1.828125,
      0.10498046875,
      -4.3125,
      -1.484375,
      2.265625,
      3.46875,
      -0.275390625,
      1.6171875,
      -4.53125,
      3.015625,
      -2.265625,
      0.421875,
      2.484375,
      -2.875,
      2.28125,
      3.171875,
      -2.046875,
      -0.408203125,
      -2.984375,
      -0.275390625,
      5.5625,
      4.03125,
      -2.796875,
      1.1171875,
      0.228515625,
      0.26953125,
      6.28125,
      -2.65625,
      -1.8828125,
      -1.859375,
      2.765625,
      1.46875,
      -4.21875,
      4.96875,
      -1.5625,
      -2.125,
      0.00445556640625,
      -1.203125,
      0.9375,
      0.2353515625,
      1.1484375,
      4.53125,
      -2.828125,
      -1.2109375,
      1.3828125,
      -2.640625,
      -5.59375,
      -2.234375,
      0.10986328125,
      -4.09375,
      4.78125,
      -0.05224609375,
      -0.11279296875,
      0.09521484375,
      -2.8125,
      0.53125,
      2.015625,
      1.5703125,
      1.34375,
      4.25,
      2.46875,
      -2.171875,
      2.1875,
      -5.65625,
      -5.625,
      -0.396484375,
      -4.1875,
      -1.0703125,
      2.484375,
      2.578125,
      -0.62890625,
      0.7421875,
      -1.9453125,
      -4.71875,
      -5.09375,
      -3.09375,
      0.3984375,
      -0.5078125,
      2.890625,
      1.7578125,
      6.15625,
      -2.125,
      1,
      1.9296875,
      0.31640625,
      -0.8359375,
      -3.90625,
      0.8046875,
      -8.4375,
      1.65625,
      5,
      5.71875,
      -4.1875,
      8.25,
      2.5625,
      -0.5703125,
      -0.46484375,
      6.5625,
      -3.671875,
      -0.1669921875,
      0.083984375,
      -2.859375,
      -1.6328125,
      -3.671875,
      -0.427734375,
      -1.21875,
      4.1875,
      0.423828125,
      -2.015625,
      0.09716796875,
      -6.15625,
      -0.0986328125,
      -0.1650390625,
      -0.1787109375,
      2.828125,
      1.1640625,
      1.0390625,
      -0.76953125,
      2.78125,
      3.8125,
      0.859375,
      0.04248046875,
      -1.265625,
      2.21875,
      2.421875,
      2.9375,
      -1.421875,
      -6.125,
      0.5546875,
      -2.5,
      -1.1015625,
      1.2265625,
      -0.2021484375,
      0.2333984375,
      5.15625,
      1.6484375,
      -4.625,
      -2.75,
      2.734375,
      -4.03125,
      -0.1494140625,
      0.4375,
      3.484375,
      0.2421875,
      4.28125,
      -0.484375,
      -2.484375,
      -3.1875,
      8.1875,
      4.65625,
      -0.79296875,
      -0.82421875,
      4.3125,
      1.21875,
      0.80859375,
      -3.890625,
      -6.125,
      1.6171875,
      1.2265625,
      -2.03125,
      -0.44921875,
      8.1875,
      -4.375,
      0.75390625,
      -0.60546875,
      0.028076171875,
      0.53125,
      0.490234375,
      2.40625,
      -0.369140625,
      1.9609375,
      -0.76953125,
      1.1015625,
      1.1171875,
      -0.349609375,
      -1.046875,
      -1.0703125,
      -0.349609375,
      -3,
      2.921875,
      1.296875,
      -1.4453125,
      -0.9765625,
      1.1796875,
      0.54296875,
      4.1875,
      6.59375,
      2.109375,
      0.00482177734375,
      3.34375,
      6.6875,
      -1.890625,
      8.625,
      1.359375,
      9.375,
      -4.90625,
      -2.75,
      4.53125,
      2.53125,
      0.9453125,
      -0.8984375,
      -7.0625,
      -2.203125,
      -0.62890625,
      0.279296875,
      -4.125,
      0.0213623046875,
      -3.359375,
      -0.8828125,
      4.65625,
      1.8671875,
      3.109375,
      2.90625,
      3.5,
      -0.703125,
      -0.2490234375,
      -2.203125,
      -4.65625,
      -2.765625,
      1.25,
      1.9609375,
      -1.890625,
      2.53125,
      0.048828125,
      5.09375,
      1.8671875,
      0.1591796875,
      -2.53125,
      4.9375,
      -2.34375,
      -1.8984375,
      2.5,
      1.90625,
      1.234375,
      0.875,
      3.71875,
      -5.59375,
      -0.83203125,
      8.5,
      2.09375,
      -3.234375,
      2.703125,
      -1.6015625,
      3.921875,
      -6.78125,
      -2.15625,
      -0.2275390625,
      2.828125,
      -5.5625,
      -3.546875,
      -0.703125,
      1.9375,
      -5.15625,
      0.421875,
      4.71875,
      -3.796875,
      1.109375,
      -1.328125,
      -3.484375,
      1.0625,
      1.4765625,
      4.4375,
      0.0162353515625,
      -3.0625,
      -3.21875,
      -3.609375,
      -2.015625,
      0.26953125,
      -0.80078125,
      1.6484375,
      1.9609375,
      -2.828125,
      0.1748046875,
      -6.1875,
      1.3984375,
      -5.28125,
      3.953125,
      3.953125,
      -3.59375,
      -4.34375,
      -2.078125,
      -6.15625,
      -3.421875,
      -2.53125,
      -3.96875,
      -2.46875,
      -5.6875,
      -0.91015625,
      1.515625,
      -7.21875,
      -0.09033203125,
      -2.15625,
      -0.11767578125,
      2.5,
      0.013427734375,
      1.2265625,
      -6.59375,
      2.140625,
      -0.9765625,
      0.353515625,
      0.6171875,
      1.65625,
      2.859375,
      3.953125,
      1.7734375,
      1.2265625,
      -3.296875,
      5.46875,
      -1.703125,
      7.28125,
      5.375,
      1.9765625,
      -5.03125,
      3.359375,
      -1.0859375,
      0.2333984375,
      -7.25,
      2.734375,
      1.0625,
      0.75390625,
      0.5625,
      1.078125,
      4.71875,
      -2.796875,
      -4.84375,
      2.765625,
      -3.484375,
      -0.0888671875,
      1.2734375,
      0.72265625,
      4.34375,
      -0.314453125,
      0.44921875,
      1.3515625,
      1.875,
      0.53125,
      -0.8515625,
      3.25,
      -4.65625,
      -0.419921875,
      -1.03125,
      0.67578125,
      -0.008056640625,
      -3.171875,
      -0.021240234375,
      -0.1884765625,
      1.1171875,
      2.140625,
      4.40625,
      3.625,
      2.125,
      4.15625,
      0.275390625,
      -0.859375,
      -1.2578125,
      -2.640625,
      0.7421875,
      1.4453125,
      2.765625,
      -2.796875,
      0.341796875,
      -2.0625,
      0.5390625,
      1.7890625,
      -3.171875,
      -0.30078125,
      -3.96875,
      -4.78125,
      2.109375,
      0.625,
      0.314453125,
      0.173828125,
      2.65625,
      1.9375,
      2.34375,
      -0.25,
      3.109375,
      0.2451171875,
      -4.34375,
      3.125,
      -2.234375,
      -4.53125,
      2.40625,
      1.390625,
      4.5625,
      1.0859375,
      1.4921875,
      -3.359375,
      -0.5859375,
      3.875,
      -3.828125,
      0.79296875,
      5.6875,
      -1.8671875,
      -0.8125,
      -0.1923828125,
      1.03125,
      2.109375,
      -2.078125,
      3.765625,
      4.125,
      4.28125,
      1.5390625,
      -0.3828125,
      -3.484375,
      -1.7890625,
      -0.5546875,
      -0.2001953125,
      -0.90234375,
      0.62109375,
      1.4140625,
      -0.345703125,
      -0.37890625,
      -3.34375,
      0.66796875,
      -3.734375,
      3.03125,
      3.671875,
      -1.8984375,
      -0.62109375,
      -1.1171875,
      -2.3125,
      -0.77734375,
      2.359375,
      2.09375,
      -1.609375,
      2.671875,
      2.90625,
      -1.6015625,
      0.453125,
      1.1796875,
      0.5390625,
      2.828125,
      -0.8515625,
      2.625,
      0.0830078125,
      -2.578125,
      -1.671875,
      -2.28125,
      0.828125,
      3.0625,
      4.5,
      -0.255859375,
      2.921875,
      -0.65625,
      -3.203125,
      2.859375,
      -3.21875,
      -2.265625,
      -0.9140625,
      -2.875,
      -0.240234375,
      -3.34375,
      -0.00408935546875,
      -2.8125,
      4.5625,
      -9.1875,
      5.28125,
      -3.5625,
      5.46875,
      2.5,
      0.1904296875,
      -0.15625,
      -4.53125,
      3.75,
      -3.390625,
      -1.265625,
      3.765625,
      -1.34375,
      3.875,
      -1.453125,
      -1.453125,
      -0.50390625,
      -2.828125,
      -0.193359375,
      -2.171875,
      -0.3671875,
      -2.078125,
      2.359375,
      -9.1875,
      -3.375,
      0.09375,
      2.21875,
      1.390625,
      -1.0234375,
      0.69921875,
      -2.28125,
      2.328125,
      -1.484375,
      2.25,
      -2.53125,
      -3.4375,
      2.15625,
      -1.8359375,
      2.890625,
      1.09375,
      -0.474609375,
      -2.53125,
      -1.8359375,
      -4.5625,
      -2.296875,
      -1.671875,
      2.453125,
      -4.03125,
      -1.109375,
      -0.21484375,
      2.5625,
      1.328125,
      -1.6640625,
      -2.171875,
      1.703125,
      6.34375,
      0.8671875,
      -0.6328125,
      -1.84375,
      0.98046875,
      -3.921875,
      -3.6875,
      -2.875,
      2.1875,
      2.0625,
      -4.375,
      2.46875,
      0.083984375,
      2.640625,
      3.46875,
      -0.0191650390625,
      -1.21875,
      1.6171875,
      1.8359375,
      0.63671875,
      0.8671875,
      3.140625,
      0.326171875,
      3.046875,
      -3.59375,
      -3,
      -2.5,
      1.3828125,
      -1.359375,
      -1.34375,
      -0.052978515625,
      0.00665283203125,
      -1.3671875,
      -1.8671875,
      -3.03125,
      -0.91015625,
      4.75,
      0.498046875,
      6.5,
      1.7578125,
      -1.8984375,
      0.97265625,
      0.1064453125,
      4.4375,
      3.421875,
      0.92578125,
      0.5546875,
      1.140625,
      -2.21875,
      5.46875,
      -3.625,
      -5.3125,
      -4.75,
      -0.59765625,
      6.40625,
      0.66796875,
      -1.0625,
      1.5390625,
      -0.10888671875,
      1.5703125,
      2.71875,
      4.90625,
      -0.6484375,
      -1.984375,
      -1.640625,
      3.0625,
      -1.8984375,
      -4.96875,
      2.515625,
      3.34375,
      -1.3671875,
      1.046875,
      2.171875,
      4.8125,
      -4.375,
      0.21484375,
      -1.5859375,
      -3.328125,
      -3.15625,
      3.953125,
      0.2099609375,
      -1.8984375,
      1.921875,
      -3.5625,
      -0.6171875,
      0.58984375,
      0.236328125,
      4.09375,
      -2.453125,
      1.890625,
      0.1455078125,
      3.9375,
      -0.87890625,
      -1.9921875,
      -1.1875,
      1.5859375,
      0.52734375,
      -1.4375,
      -1.046875,
      -3.765625,
      -2.625,
      -1.390625,
      -9.75,
      1.6015625,
      3.484375,
      1.0078125,
      1.140625,
      -2.671875,
      2.578125,
      3.015625,
      -3.5625,
      -5.1875,
      -2.109375,
      2.03125,
      2.5625,
      0.283203125,
      -4.1875,
      3.859375,
      0.126953125,
      0.578125,
      -5.75,
      4.875,
      -3.046875,
      4.1875,
      2.65625,
      4.46875,
      -5.5625,
      -0.65234375,
      -1.0703125,
      -0.189453125,
      -3.703125,
      -0.95703125,
      -2.140625,
      0.72265625,
      4.1875,
      0.1982421875,
      -3.5,
      -1.6328125,
      -2.671875,
      2.84375,
      0.033935546875,
      0.228515625,
      -3.71875,
      2.265625,
      -6.34375,
      -1.109375,
      3.8125,
      -1.21875,
      -2.25,
      1.2578125,
      3.0625,
      8.125,
      2.046875,
      1.03125,
      -4.03125,
      1.5078125,
      -1.09375,
      -3.625,
      -5.0625,
      1.4375,
      -0.65234375,
      -1.25,
      2.125,
      -4.4375,
      2.328125,
      -2.953125,
      0.5546875,
      -2.46875,
      0.39453125,
      0.166015625,
      2.3125,
      1.625,
      0.86328125,
      2.453125,
      -0.224609375,
      4.1875,
      2.359375,
      0.123046875,
      1.15625,
      -0.35546875,
      0.1015625,
      -5.21875,
      2.546875,
      -1.2890625,
      -1.28125,
      4.53125,
      4.59375,
      -5.75,
      0.26171875,
      -0.859375,
      3.140625,
      -1.953125,
      -3.5,
      -1.671875,
      3.578125,
      -0.546875,
      -0.7265625,
      -2.265625,
      -0.578125,
      1.953125,
      -2.8125,
      1.15625,
      -1.28125,
      3.953125,
      -4.5625,
      -2.328125,
      5.28125,
      0.265625,
      3.6875,
      -3.6875,
      3.140625,
      3.46875,
      1.0625,
      1.6015625,
      0.62890625,
      -0.359375,
      -2.828125,
      1.1875,
      -5.78125,
      2.421875,
      -7.125,
      -0.5234375,
      -1.171875,
      -1.7421875,
      -2.25,
      4.6875,
      3.1875,
      0.10595703125,
      -1.0078125,
      -1.3203125,
      -7.21875,
      -2.515625,
      -1.0546875,
      -1.5078125,
      -4.28125,
      1.7109375,
      -2.265625,
      1.2109375,
      2.28125,
      3.65625,
      -0.6328125,
      -2.59375,
      3.71875,
      -2.6875,
      2.0625,
      -2.234375,
      0.2890625,
      0.3671875,
      -3.359375,
      1.3671875,
      1.421875,
      5.375,
      -4.125,
      0.3828125,
      -3.59375,
      -1.625,
      -0.466796875,
      1.2890625,
      -0.126953125,
      1.546875,
      3.109375,
      -0.83203125,
      2.984375,
      4.3125,
      -2.28125,
      -2.40625,
      3.78125,
      0.51171875,
      4.28125,
      1.4453125,
      0.78515625,
      -2.15625,
      3.828125,
      1.328125,
      -3.28125,
      4.125,
      1.1875,
      0.2451171875,
      -4.65625,
      2.40625,
      3.3125,
      4.15625,
      0.201171875,
      3.578125,
      3,
      1.2734375,
      4.84375,
      6.96875,
      -0.33203125,
      -0.427734375,
      0.271484375,
      -0.171875,
      1.0390625,
      -2.171875,
      2.609375,
      -0.17578125,
      -2.1875,
      3.421875,
      -0.53125,
      3.796875,
      0.92578125,
      -0.287109375,
      0.921875,
      5,
      -3.0625,
      1.859375,
      -5.875,
      0.41796875,
      -3.609375,
      1.765625,
      -5.71875,
      -2.1875,
      2.03125,
      5.875,
      -1.359375,
      -3.703125,
      6.46875,
      4.09375,
      0.52734375,
      -2.875,
      4.4375,
      0.73046875,
      -1.15625,
      -5.96875,
      -3.328125,
      6.46875,
      2.765625,
      0.08447265625,
      -3.078125,
      1.6484375,
      2.96875,
      -1.3515625,
      -7.3125,
      1.5625,
      0.08349609375,
      -4.125,
      -4.96875,
      -1.265625,
      1.5625,
      -4.8125,
      1.3515625,
      0.423828125,
      -2.390625,
      4.0625,
      -0.5,
      2.984375,
      2.1875,
      -1.28125,
      3.140625,
      -0.435546875,
      -0.5390625,
      -1.15625,
      4.28125,
      -2.515625,
      3.890625,
      -1.5703125,
      -1.1171875,
      -1.6171875,
      -0.68359375,
      -0.703125,
      -3.40625,
      -0.99609375,
      2.640625,
      3.671875,
      -1.15625,
      -3.265625,
      1.1328125,
      0.8984375,
      -2.40625,
      -1.15625,
      1.71875,
      -1,
      -0.71484375,
      0.34375,
      -1.2109375,
      1.84375,
      -2.359375,
      0.61328125,
      0.123046875,
      -1.765625,
      -1.8671875,
      -2.78125,
      -2.859375,
      2.78125,
      -2.015625,
      1.109375,
      0.267578125,
      -1.71875,
      -1.765625,
      4.34375,
      -0.1865234375,
      2.125,
      -2.96875,
      1.015625,
      2.671875,
      1.609375,
      -0.447265625,
      -4.125,
      -1.828125,
      1.5,
      -1.421875,
      -1.09375,
      -0.5,
      -1.75,
      3.421875,
      -4.84375,
      -1.4375,
      -0.3046875,
      0.765625,
      -1.0859375,
      -10.9375,
      1.9296875,
      0.46484375,
      -2.234375,
      0.51953125,
      -0.79296875,
      0.5625,
      3.765625,
      1.859375,
      2.640625,
      -1.1328125,
      -5.40625,
      0.54296875,
      19.125,
      -3.484375,
      -2.859375,
      0.412109375,
      -0.12890625,
      -0.45703125,
      5.96875,
      0.169921875,
      0.029052734375,
      -0.73046875,
      0.89453125,
      3.484375,
      4.34375,
      0.546875,
      0.259765625,
      -0.05224609375,
      4.34375,
      -2.828125,
      -1.6953125,
      -2.5625,
      -4.53125,
      0.953125,
      -4.4375,
      3.078125,
      -2.421875,
      0.8203125,
      1.453125,
      -4.34375,
      -2.8125,
      -4.125,
      -0.9296875,
      -1.1328125,
      -5.09375,
      3.21875,
      0.609375,
      -3.71875,
      -1.6796875,
      2.109375,
      -0.162109375,
      -2.25,
      -5,
      -2.5625,
      2.171875,
      -1.9921875,
      -1.28125,
      -0.181640625,
      -3.421875,
      -1.765625,
      -1.359375,
      3.53125,
      -2.15625,
      -1.3828125,
      -3.984375,
      2.6875,
      3.0625,
      -2.65625,
      1.6171875,
      -4,
      -2.25,
      -4.71875,
      -0.330078125,
      -0.2109375,
      -3.90625,
      -2.609375,
      -2.515625,
      -3.53125,
      1.3359375,
      0.7109375,
      2.078125,
      2.671875,
      -5.65625,
      -4.21875,
      2.78125,
      1.65625,
      -0.1474609375,
      -2,
      0.89453125,
      0.640625,
      1.8203125,
      2.15625,
      -2.875,
      0.8125,
      -3.171875,
      -1.0546875,
      -1.21875,
      0.703125,
      -3.703125,
      2.984375,
      1.421875,
      -0.91015625,
      6.875,
      -4.09375,
      1.1796875,
      1.0703125,
      -3.890625,
      -1.6875,
      0.095703125,
      1.859375,
      -2.78125,
      -0.0908203125,
      4.65625,
      -2.515625,
      0.052001953125,
      -0.455078125,
      2.359375,
      -0.5625,
      1.5546875,
      -2.28125,
      3.84375,
      -0.31640625,
      -0.91015625,
      3.328125,
      1.234375,
      -1.2265625,
      -1.4921875,
      3.734375,
      -2.78125,
      -1.46875,
      -2.671875,
      -2.453125,
      1.3671875,
      0.56640625,
      -2.4375,
      0.09423828125,
      -7.71875,
      -5.53125,
      -2.796875,
      -3.359375,
      2.1875,
      -2.25,
      1.09375,
      -0.0306396484375,
      -3.921875,
      -1.0390625,
      1.0703125,
      -1.859375,
      3.265625,
      4.28125,
      1.7421875,
      0.365234375,
      -1.1015625,
      -0.1484375,
      2.328125,
      -0.828125,
      -1.03125,
      0.9765625,
      -3.4375,
      0.359375,
      2.375,
      1.9453125,
      -4,
      3.484375,
      -0.451171875,
      -2.140625,
      -1.859375,
      1.7734375,
      0.0673828125,
      -3.234375,
      -1.0390625,
      1.4609375,
      -2.265625,
      -1.6796875,
      -0.9765625,
      -7.9375,
      0.25390625,
      1.3828125,
      2.9375,
      1.21875,
      -2.796875,
      2.59375,
      2.65625,
      -6.59375,
      1.640625,
      -3.5625,
      3.546875,
      -4.46875,
      3.203125,
      2.9375,
      -0.640625,
      2.96875,
      3.140625,
      1.8046875,
      -0.6796875,
      -1.421875,
      0.984375,
      0.94140625,
      -2.53125,
      5.5625,
      3.328125,
      -0.37109375,
      -1.7109375,
      -4.53125,
      -3.015625,
      4.5625,
      5.84375,
      -4.625,
      0.9609375,
      -0.703125,
      -1.8671875,
      -1.421875,
      -1.7890625,
      1.2890625,
      0.734375,
      0.5625,
      6.03125,
      1.3203125,
      -1.3203125,
      -0.71875,
      -3.703125,
      -1.734375,
      -2.9375,
      3.171875,
      2.921875,
      2.4375,
      -4.90625,
      7.15625,
      4.03125,
      -1.21875,
      -1.0078125,
      -0.11328125,
      3.3125,
      -2.609375,
      -4.28125,
      0.55078125,
      1.484375,
      3.609375,
      -7.71875,
      5.09375,
      -1.84375,
      4.46875,
      -1.4453125,
      3.609375,
      -0.318359375,
      0.66015625,
      -4,
      -3.265625,
      -4.53125,
      0.447265625,
      -2.140625,
      0.2158203125,
      2.671875,
      -3.140625,
      1.109375,
      -2.328125,
      -2.5,
      2.359375,
      0.42578125,
      -1.3671875,
      1.625,
      -0.08154296875,
      1.6953125,
      -1.4140625,
      5.34375,
      -0.98828125,
      0.0014190673828125,
      -1.3828125,
      -1.8203125,
      -0.44921875,
      -3.375,
      2.84375,
      -0.02294921875,
      -4.40625,
      5.5625,
      0.7890625,
      -0.6015625,
      2.34375,
      -1.453125,
      -5.0625,
      6.28125,
      3.921875,
      -6.65625,
      1.5390625,
      -0.51953125,
      -4.125,
      -1.1328125,
      0.314453125,
      2.71875,
      -5,
      0.625,
      2.546875,
      -4.5,
      1.90625,
      1.421875,
      -1.3515625,
      0.400390625,
      -2.875,
      4,
      0.67578125,
      0.62109375,
      2.921875,
      1.5625,
      4.53125,
      -3.0625,
      -1.265625,
      -3.296875,
      5.25,
      -1.453125,
      0.23828125,
      0.90234375,
      -0.8359375,
      3.171875,
      3.140625,
      -0.6015625,
      -5.6875,
      -2.609375,
      1.46875,
      -0.35546875,
      3.09375,
      1.90625,
      0.146484375,
      -2.640625,
      0.0189208984375,
      2.890625,
      2.015625,
      0.7421875,
      -1.078125,
      -4.09375,
      2.09375,
      -0.251953125,
      -7.125,
      -1.2578125,
      0.396484375,
      -0.51171875,
      1.2578125,
      -1,
      -1.6640625,
      2.375,
      3.140625,
      -4.125,
      1.890625,
      0.447265625,
      1.8046875,
      -0.06884765625,
      -0.494140625,
      1.1640625,
      1.8828125,
      -2.6875,
      -7.65625,
      2.921875,
      -0.47265625,
      -0.890625,
      3.046875,
      -0.6015625,
      2.96875,
      2.546875,
      2.578125,
      -4.40625,
      6.78125,
      1.96875,
      -7.3125,
      3.375,
      -5.375,
      1.2578125,
      -3.84375,
      -5.46875,
      -1.3203125,
      4.90625,
      -2.484375,
      0.6796875,
      1.2421875,
      -0.10302734375,
      -1.1484375,
      4.09375,
      0.8046875,
      -4.375,
      0.75390625,
      2.53125,
      -4.96875,
      -0.140625,
      2.078125,
      -0.5078125,
      -1.3046875,
      -0.2412109375,
      0.228515625,
      0.349609375,
      -0.953125,
      3.484375,
      1.2109375,
      -0.88671875,
      -1.53125,
      -1.1171875,
      1.6640625,
      -0.74609375,
      -3.34375,
      0.166015625,
      -0.89453125,
      2.015625,
      -2.359375,
      0.39453125,
      -1.046875,
      -3.65625,
      -1.8984375,
      -5.53125,
      -0.55859375,
      1.953125,
      1.4765625,
      1.9140625,
      -2.375,
      -0.91015625,
      2.6875,
      3.640625,
      1.0859375,
      -3.609375,
      0.04052734375,
      -3.71875,
      -1.6328125,
      1.015625,
      2.421875,
      -1.90625,
      -0.93359375,
      2.03125,
      0.2578125,
      -3.03125,
      -4.625,
      -0.64453125,
      0.283203125,
      2.640625,
      1.625,
      -1.609375,
      -2.5,
      3.03125,
      1.609375,
      -5.375,
      -0.88671875,
      2.390625,
      -1.4765625,
      -3.546875,
      -3.234375,
      3.71875,
      -5.9375,
      0.263671875,
      3.84375,
      1.703125,
      1.875,
      1.703125,
      -7.1875,
      -1.4140625,
      3.78125,
      3.671875,
      2.515625,
      1.2734375,
      -1.546875,
      -4.71875,
      5.25,
      4.71875,
      -1.6796875,
      -2.578125,
      -1.421875,
      -0.515625,
      -1.359375,
      -2.5625,
      2.484375,
      -0.08056640625,
      -6.1875,
      7.53125,
      -2.109375,
      4.625,
      1.8984375,
      -2.140625,
      2.578125,
      -0.92578125,
      2.109375,
      -0.6484375,
      2.21875,
      3.46875,
      -3.75,
      -3,
      -2.296875,
      -0.4453125,
      -1.2578125,
      -0.2060546875,
      -1.2421875,
      1.3046875,
      3.984375,
      0.2001953125,
      -1.4921875,
      -0.1865234375,
      -0.8046875,
      3.921875,
      -3.140625,
      -1.21875,
      0.12109375,
      -2.375,
      0.3046875,
      -1.8046875,
      1.8203125,
      -4.25,
      0.333984375,
      0.08251953125,
      1.8828125,
      0.61328125,
      3.03125,
      -0.107421875,
      -1.3046875,
      -2.0625,
      4.5625,
      0.54296875,
      2.625,
      3.859375,
      3.6875,
      1.4609375,
      -1.6640625,
      2.96875,
      6.25,
      4.8125,
      -5.71875,
      -1.5,
      -0.443359375,
      -0.9375,
      -1.1015625,
      0.1484375,
      0.57421875,
      1.8359375,
      3.171875,
      0.412109375,
      1.7734375,
      -1.8828125,
      -2.109375,
      -0.7109375,
      -0.1279296875,
      3.25,
      2.1875,
      -3.046875,
      0.10986328125,
      -1.7421875,
      1.6640625,
      1.3359375,
      2.640625,
      1.40625,
      -0.2578125,
      -2.6875,
      0.169921875,
      -3.25,
      -0.83203125,
      1.34375,
      -2.109375,
      0.48046875,
      -1.2578125,
      -1.46875,
      1.3828125,
      0.97265625,
      -3.03125,
      2.96875,
      2.296875,
      -2.140625,
      2.390625,
      -0.73046875,
      -2.265625,
      -0.408203125,
      0.68359375,
      2.703125,
      -1.390625,
      -3.28125,
      0.79296875,
      3.234375,
      -2.015625,
      -0.392578125,
      -1.09375,
      1.171875,
      -1.609375,
      -0.37890625,
      0.09423828125,
      -0.96484375,
      3.640625,
      -0.5703125,
      -0.69921875,
      3.03125,
      -0.828125,
      -0.0458984375,
      1.8046875,
      -1.9296875,
      -1.7734375,
      -2.75,
      2.953125,
      2.546875,
      0.03125,
      1.6875,
      1.9609375,
      1.328125,
      -0.5546875,
      -1.8046875,
      -3.21875,
      0.05029296875,
      -2.140625,
      -0.18359375,
      0.8203125,
      -0.1865234375,
      -2.484375,
      4,
      0.796875,
      2.328125,
      -3.640625,
      -0.76953125,
      -2.1875,
      -1.6796875,
      -0.259765625,
      -2.5625,
      2.265625,
      1.7734375,
      1.3046875,
      -0.095703125,
      -0.62109375,
      0.28125,
      -0.875,
      -2.171875,
      4.71875,
      2.25,
      -2.625,
      0.373046875,
      -1.6171875,
      1.0625,
      2.28125,
      -0.65234375,
      -2.765625,
      1.4765625,
      1.0703125,
      3.078125,
      2.953125,
      -1.2890625,
      2.390625,
      2.375,
      -3.09375,
      0.026611328125,
      -2.734375,
      2.203125,
      -0.333984375,
      -1.203125,
      -0.1748046875,
      1.0546875,
      -4.53125,
      -0.703125,
      0.7109375,
      2,
      -0.60546875,
      0.36328125,
      1.53125,
      1.3828125,
      -1.859375,
      3,
      4.25,
      1.703125,
      -2.234375,
      -3.8125,
      0.474609375,
      0.8125,
      -1.59375,
      -1.078125,
      -0.79296875,
      -0.38671875,
      -1.40625,
      0.2412109375,
      -0.416015625,
      -2.234375,
      1.7265625,
      0.7265625,
      -0.515625,
      -2.25,
      1.0078125,
      -1.3125,
      0.96875,
      -0.66015625,
      1.4296875,
      -0.87109375,
      -5.28125,
      0.9140625,
      1.1640625,
      -0.1796875,
      -4.5,
      -1.8359375,
      -2.5625,
      1.609375,
      -0.546875,
      -0.177734375,
      1.7265625,
      -2.4375,
      0.671875,
      -2.140625,
      -1.390625,
      0.8828125,
      0.353515625,
      1.3125,
      -0.01251220703125,
      -1.5390625,
      3.796875,
      1.25,
      -1.875,
      2.265625,
      -0.6015625,
      1.9921875,
      -0.357421875,
      -0.7578125,
      0.462890625,
      -1.875,
      1.515625,
      0.1494140625,
      -0.28515625,
      3.109375,
      -0.4140625,
      0.96875,
      -0.65625,
      -0.65625,
      4.59375,
      3.65625,
      -0.279296875,
      0.53125,
      0.07958984375,
      1.03125,
      -0.85546875,
      -1.5,
      0.69921875,
      -3.21875,
      3.671875,
      -0.185546875,
      -2.515625,
      0.9140625,
      -2.546875,
      0.94921875,
      2.6875,
      3.25,
      -2.5,
      2.765625,
      0.2353515625,
      -0.0927734375,
      -0.43359375,
      -1.78125,
      -0.62890625,
      -0.318359375,
      -3.265625,
      2.28125,
      0.7265625,
      -0.1748046875,
      -2.578125,
      -0.03271484375,
      -2.765625,
      -2.453125,
      -0.35546875,
      0.01531982421875,
      -1.15625,
      2.765625,
      2.703125,
      0.6796875,
      -0.74609375,
      -1.3046875,
      0.5625,
      0.130859375,
      2.4375,
      -0.236328125,
      2.9375,
      2.984375,
      0.2255859375,
      -0.31640625,
      -0.7890625,
      -1.1796875,
      1.21875,
      0.392578125,
      -1.34375,
      1.3515625,
      3.140625,
      2.59375,
      -1.1953125,
      0.8828125,
      -0.193359375,
      6.09375,
      0.67578125,
      -1.296875,
      1.0703125,
      -1.390625,
      1.96875,
      -2.515625,
      1.9140625,
      0.1396484375,
      0.66796875,
      -2.453125,
      0.11181640625,
      1.7890625,
      -1.2109375,
      -3.28125,
      1.5078125,
      -2.703125,
      -0.279296875,
      3.140625,
      -1.265625,
      0.609375,
      0.0322265625,
      1.125,
      1.046875,
      5.125,
      -0.0322265625,
      0.41015625,
      1.90625,
      2.828125,
      3.484375,
      0.58203125,
      -1.421875,
      -0.27734375,
      1.1640625,
      1.3125,
      -0.32421875,
      1.15625,
      -0.423828125,
      2.625,
      -2.046875,
      -0.71484375,
      -3.109375,
      -0.83203125,
      0.212890625,
      0.2890625,
      4.59375,
      -3.8125,
      -1.9921875,
      3.75,
      0.423828125,
      0.341796875,
      1.5859375,
      3.625,
      0.81640625,
      3.109375,
      1.8125,
      -1.234375,
      -0.08154296875,
      2.515625,
      0.27734375,
      2.375,
      1.53125,
      -1.1171875,
      -2.078125,
      -0.6875,
      -2.078125,
      2.75,
      -1.359375,
      -0.90234375,
      2.828125,
      -0.2080078125,
      4.1875,
      2.328125,
      1.6953125,
      4.875,
      3.046875,
      -0.73046875,
      -1.46875,
      1.40625,
      -1.578125,
      0.7109375,
      1.90625,
      -1.2109375,
      2.578125,
      -1.2734375,
      -1.46875,
      -1.546875,
      -1.1875,
      -2.96875,
      -2.703125,
      -1.8046875,
      0.1357421875,
      -1.421875,
      3.875,
      -3.578125,
      2.484375,
      1.8671875,
      2.328125,
      2.046875,
      2,
      0.9296875,
      1.40625,
      -1.5,
      -3.515625,
      1.0546875,
      -1.6484375,
      -3.703125,
      -0.1455078125,
      0.3828125,
      -2.21875,
      1.734375,
      -2.21875,
      0.314453125,
      -1.1015625,
      -0.7734375,
      -3.140625,
      1.4375,
      2.21875,
      -1.375,
      0.70703125,
      -2.640625,
      0.1357421875,
      -1.515625,
      0.6796875,
      1.3046875,
      -5.34375,
      -2.96875,
      -0.337890625,
      -1.1953125,
      -1.9140625,
      -2.296875,
      -1.9296875,
      -0.60546875,
      0.1591796875,
      4.75,
      1.53125,
      -0.515625,
      -1.53125,
      4.40625,
      -1.4921875,
      0.9296875,
      -1.2265625,
      -1.953125,
      2.0625,
      2.453125,
      3.75,
      3.546875,
      -1,
      -0.98046875,
      -2.515625,
      2.265625,
      0.173828125,
      -4.46875,
      -0.77734375,
      -0.1796875,
      -1.5390625,
      -0.94140625,
      -1.828125,
      0.171875,
      -1.4921875,
      -1.515625,
      0.3828125,
      -3.59375,
      0.6484375,
      3.578125,
      -0.58203125,
      -1.515625,
      -2.46875,
      1.8984375,
      -2.390625,
      0.546875,
      -1.90625,
      -1.8828125,
      2.21875,
      1.2734375,
      -0.37109375,
      6.03125,
      -2.234375,
      -2.421875,
      2.125,
      1.0390625,
      3.234375,
      -2.765625,
      1.421875,
      0.625,
      -2.609375,
      1.296875,
      -3.140625,
      -4.625,
      -3.1875,
      -3.546875,
      -2.28125,
      -2.078125,
      2.46875,
      2.390625,
      -1.3203125,
      -1.3515625,
      0.8671875,
      1,
      2.34375,
      1.765625,
      -2.4375,
      -0.0439453125,
      -4.125,
      -1.921875,
      0.98828125,
      2.6875,
      -0.78125,
      1.9765625,
      2.53125,
      0.73828125,
      -1.9921875,
      0.765625,
      0.90234375,
      0.546875,
      0.9609375,
      0.859375,
      -3.125,
      -2.484375,
      2.296875,
      -2.484375,
      2.578125,
      0.486328125,
      -0.6015625,
      0.88671875,
      -1.03125,
      -2.328125,
      -1.1328125,
      0.314453125,
      0.86328125,
      -2.046875,
      -3.5625,
      1.5234375,
      0.40234375,
      -1.9375,
      0.44140625,
      1.78125,
      1.890625,
      -1.609375,
      -2.625,
      0.373046875,
      -2.796875,
      -4.0625,
      -1.3671875,
      0.5703125,
      3.53125,
      1.1015625,
      -3.0625,
      5.9375,
      0.490234375
    ],
    "suggested_tags": [
      "教育AI",
      "生成式AI",
      "个性化学习",
      "随机对照试验",
      "AI教学助手"
    ],
    "tag_suggestions": [
      {
        "name": "教育AI",
        "confidence": 0.95,
        "reason": "论文核心研究领域为人工智能在教育领域的应用，特别是AI辅助教学系统在数学课堂中的实际效果评估"
      },
      {
        "name": "生成式AI",
        "confidence": 0.9,
        "reason": "研究基于LearnLM这一经过教学优化的生成式AI模型，重点评估其在教学对话中的生成能力"
      },
      {
        "name": "个性化学习",
        "confidence": 0.85,
        "reason": "论文旨在解决一对一教学的规模化问题，探索AI如何实现个性化教学支持"
      },
      {
        "name": "随机对照试验",
        "confidence": 0.8,
        "reason": "采用严谨的RCT方法在真实课堂环境中评估AI教学效果，这是教育技术研究的重要方法"
      },
      {
        "name": "AI教学助手",
        "confidence": 0.75,
        "reason": "具体应用场景为AI辅助的在线教学对话系统，重点研究人机协作的教学模式"
      }
    ],
    "analysis": {
      "paper_id": "02197db0-3de5-4390-9690-609c0f31a4c1",
      "status": "completed",
      "started_at": null,
      "completed_at": "2025-12-16T02:26:53.409169",
      "summary": "本研究旨在探讨生成式AI能否在保证安全性的前提下，有效扩展个性化辅导的覆盖范围，以解决一对一人工辅导成本高昂、难以规模化的问题。研究团队在英国五所中学开展了探索性随机对照试验，共有165名学生参与。试验将专为教学应用优化的生成式AI模型LearnLM集成至Eedi数学平台的在线聊天辅导系统中，并由17名专家教师进行实时监督，对AI生成的每条信息进行审核、编辑或重写后方发送给学生。\n\n研究发现，LearnLM表现出较高的教学可靠性，监督教师对其76.4%的生成内容直接认可或仅作微调。在多项学习成果指标上，接受LearnLM辅导的学生表现均不逊于纯人工辅导组，尤其在知识迁移能力上表现更优：AI辅导组学生在后续新题型上的解题正确率达到66.2%，较人工辅导组（60.7%）显著提升5.5个百分点。教师访谈进一步揭示，LearnLM擅长设计启发式提问以促进学生深度思考，部分教师反馈其教学策略亦受AI启发。\n\n研究表明，经过教学优化的AI辅导系统在人工监督下，能够安全、有效地提供规模化个性化学习支持，为突破优质教育资源的可及性瓶颈提供了可行路径。",
      "methods": [
        {
          "name": "随机对照试验（RCT）",
          "description": "本研究采用随机对照试验设计，将165名学生随机分配到不同干预组。通过比较静态提示与互动辅导，以及人类辅导与AI辅导的效果，评估LearnLM的教学效能。",
          "location": null
        },
        {
          "name": "专家监督的AI辅导",
          "description": "人类专家导师直接监督LearnLM生成的所有消息，保留编辑、重写或批准的最终控制权。这种方法确保AI辅导的安全性和教学质量，导师对76.4%的AI生成消息无需或仅需最小修改。",
          "location": null
        },
        {
          "name": "学习成果测量",
          "description": "通过测量多项学习成果评估辅导效果，包括知识迁移能力。具体比较学生在后续主题中解决新问题的成功率，量化AI辅导与人类辅导的差异。",
          "location": null
        },
        {
          "name": "访谈与调查",
          "description": "对监督导师进行访谈和调查，收集他们对LearnLM教学能力的定性反馈。导师报告AI在生成苏格拉底式问题方面的优势，并分享其教学实践的新见解。",
          "location": null
        }
      ],
      "datasets": [],
      "code_refs": [],
      "structure": {
        "sections": [
          {
            "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
            "level": 1,
            "start_line": 1
          },
          {
            "title": "1. Introduction",
            "level": 1,
            "start_line": 9
          },
          {
            "title": "2. An Exploratory Classroom Trial",
            "level": 1,
            "start_line": 32
          },
          {
            "title": "3. Results",
            "level": 1,
            "start_line": 47
          },
          {
            "title": "4. Discussion",
            "level": 1,
            "start_line": 91
          },
          {
            "title": "5. Methods",
            "level": 1,
            "start_line": 113
          },
          {
            "title": "References",
            "level": 1,
            "start_line": 143
          },
          {
            "title": "Contributions and Acknowledgments",
            "level": 1,
            "start_line": 191
          },
          {
            "title": "Core Contributors",
            "level": 1,
            "start_line": 193
          },
          {
            "title": "Acknowledgements",
            "level": 1,
            "start_line": 207
          },
          {
            "title": "A. Participants",
            "level": 1,
            "start_line": 215
          },
          {
            "title": "A.1. Students",
            "level": 1,
            "start_line": 217
          },
          {
            "title": "A.2. Tutors",
            "level": 1,
            "start_line": 223
          },
          {
            "title": "B. Trial",
            "level": 1,
            "start_line": 231
          },
          {
            "title": "C. Platform",
            "level": 1,
            "start_line": 244
          },
          {
            "title": "D. Model",
            "level": 1,
            "start_line": 258
          },
          {
            "title": "D.1. Tutoring prompt",
            "level": 1,
            "start_line": 260
          },
          {
            "title": "Listing D.1 | System prompt template",
            "level": 1,
            "start_line": 264
          },
          {
            "title": "The Current student activity The below is what the student was doing when this learning intervention started, so assume all  $\\rightarrow$  responses relate to this: {ChatConstants.Activity} # Activity details {ChatConstants/questionMetaData} # Students ability level (if provided) {ChatConstants.StudentInsight}",
            "level": 1,
            "start_line": 269
          },
          {
            "title": "Examples of good Socratic responses What happens if we multiply these two numbers first? \"Sure! How do you find the perimeter of the shape?\" Super work! And what about the triangle?\" That's okay, did you watch the video for this lesson?\" Shall I return you to the lesson?\" Could you estimate the height?\" Yes it is equilateral so the slant height is 8, so the vertical height would have th be a  $\\rightarrow$  little less\" Yes sure, so we know what  $5\\mathrm{km}$  is and we're trying to get to  $30\\mathrm{km}$  \" When you are finding the original shape, complete the steps in the reverse direction, and do  $\\rightarrow$  the opposite\" Ok, so can we try and make some even smaller ones? :)\" Awesome, I'll pass you back to Eedi \" It says that  $1\\mathrm{g} = 10$  decigrams\" And then would have to convert to kilograms afterwards :)\" So to convert into a decimal, we want it to be over 100 or 1000 or another power of 10\"",
            "level": 1,
            "start_line": 270
          },
          {
            "title": "Checking understanding (use if the student is confused or unsure)",
            "level": 2,
            "start_line": 271
          },
          {
            "title": "Closing remarks (use if the student has answered correctly)",
            "level": 2,
            "start_line": 273
          },
          {
            "title": "Rudeness (use if the student is rude e.g. 'shut up' or 'I don't care')",
            "level": 2,
            "start_line": 275
          },
          {
            "title": "Important response guidelines - Please don't use wink emojis  $\\clubsuit$",
            "level": 1,
            "start_line": 283
          },
          {
            "title": "Year group Instructional directive",
            "level": 1,
            "start_line": 291
          },
          {
            "title": "Listing D.2 | Example of a fully populated system prompt",
            "level": 1,
            "start_line": 306
          },
          {
            "title": "Directives",
            "level": 1,
            "start_line": 310
          },
          {
            "title": "The Current student activity",
            "level": 1,
            "start_line": 325
          },
          {
            "title": "Activity details",
            "level": 1,
            "start_line": 328
          },
          {
            "title": "Checking understanding (use if the student is confused or unsure)",
            "level": 2,
            "start_line": 356
          },
          {
            "title": "E. Tutor edits",
            "level": 1,
            "start_line": 391
          },
          {
            "title": "E.1. Minor edits",
            "level": 1,
            "start_line": 395
          },
          {
            "title": "E.2. Safety and accuracy audit",
            "level": 1,
            "start_line": 403
          },
          {
            "title": "E.3. Primary motivations",
            "level": 1,
            "start_line": 411
          },
          {
            "title": "F. Learning outcomes",
            "level": 1,
            "start_line": 419
          },
          {
            "title": "F.1. Methodology",
            "level": 1,
            "start_line": 421
          },
          {
            "title": "F.2. Analysis",
            "level": 1,
            "start_line": 435
          },
          {
            "title": "F.3. Results",
            "level": 1,
            "start_line": 445
          },
          {
            "title": "F.3.1. Immediate learning outcomes",
            "level": 1,
            "start_line": 447
          },
          {
            "title": "F.3.2. Learning transfer",
            "level": 1,
            "start_line": 471
          },
          {
            "title": "G. Operational metrics",
            "level": 1,
            "start_line": 501
          },
          {
            "title": "G.1. Cost inputs",
            "level": 1,
            "start_line": 507
          },
          {
            "title": "G.2. Simulation of throughput capacity",
            "level": 1,
            "start_line": 515
          },
          {
            "title": "G.3. Analysis",
            "level": 1,
            "start_line": 521
          },
          {
            "title": "H. Example transcript",
            "level": 1,
            "start_line": 529
          }
        ]
      },
      "error_message": null
    },
    "s2_cache": {
      "cached_at": "2025-12-16T15:16:19.085701",
      "citations": [
        {
          "external_id": "CorpusId:283093296",
          "title": "Dual-process theory and decision-making in large language models",
          "authors": [
            "Oliver Brady",
            "Paul Nulty",
            "Lili Zhang",
            "Tomás E. Ward",
            "David P. McGovern"
          ],
          "year": 2025,
          "venue": "Nature Reviews Psychology",
          "citation_count": 1
        },
        {
          "external_id": "CorpusId:283081756",
          "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
          "authors": [
            "LearnLM Team",
            "Google Eedi"
          ],
          "year": null,
          "venue": "",
          "citation_count": 0
        }
      ],
      "references": []
    },
    "s2_graph": {
      "citations": [
        {
          "external_id": "CorpusId:283093296",
          "title": "Dual-process theory and decision-making in large language models",
          "authors": [
            "Oliver Brady",
            "Paul Nulty",
            "Lili Zhang",
            "Tomás E. Ward",
            "David P. McGovern"
          ],
          "year": 2025,
          "venue": "Nature Reviews Psychology",
          "citation_count": 1
        },
        {
          "external_id": "CorpusId:283081756",
          "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
          "authors": [
            "LearnLM Team",
            "Google Eedi"
          ],
          "year": null,
          "venue": "",
          "citation_count": 0
        }
      ],
      "citations_fetched_at": "2025-12-16T15:24:48.462035",
      "references": [],
      "references_fetched_at": "2025-12-16T15:24:48.770106"
    }
  },
  "f6ddea83-75e3-472e-8798-80000e520eb9": {
    "id": "f6ddea83-75e3-472e-8798-80000e520eb9",
    "filename": "2405.13001v1.pdf",
    "file_path": "./uploads/papers/f6ddea83-75e3-472e-8798-80000e520eb9.pdf",
    "status": "completed",
    "title": "Large Language Models for Education: A Survey",
    "category": null,
    "markdown_content": "# Large Language Models for Education: A Survey\n\nHanyi  $\\mathbf{X}\\mathbf{u}^{a}$ , Wensheng  $\\mathrm{Gan}^{a,*}$ , Zhenlian  $\\mathbf{Q}\\mathbf{i}^{b,*}$ , Jiayang  $\\mathbf{W}\\mathbf{u}^{a}$  and Philip S.  $\\mathbf{Y}\\mathbf{u}^{c}$\n\n$^{a}$ College of Cyber Security, Jinan University, Guangzhou 510632, China  \n$^{b}$ School of Information Engineering, Guangdong Eco-Engineering Polytechnic, Guangzhou 510520, China  \n$^{c}$ Department of Computer Science, University of Illinois Chicago, Chicago, USA\n\n# ARTICLE INFO\n\nKeywords:  \nartificial intelligence  \nsmart education  \nLLMs  \napplications  \nchallenges\n\n# ABSTRACT\n\nArtificial intelligence (AI) has a profound impact on traditional education. In recent years, large language models (LLMs) have been increasingly used in various applications such as natural language processing, computer vision, speech recognition, and autonomous driving. LLMs have also been applied in many fields, including recommendation, finance, government, education, legal affairs, and finance. As powerful auxiliary tools, LLMs incorporate various technologies such as deep learning, pre-training, fine-tuning, and reinforcement learning. The use of LLMs for smart education (LLMEdu) has been a significant strategic direction for countries worldwide. While LLMs have shown great promise in improving teaching quality, changing education models, and modifying teacher roles, the technologies are still facing several challenges. In this paper, we conduct a systematic review of LLMEdu, focusing on current technologies, challenges, and future developments. We first summarize the current state of LLMEdu and then introduce the characteristics of LLMs and education, as well as the benefits of integrating LLMs into education. We also review the process of integrating LLMs into the education industry, as well as the introduction of related technologies. Finally, we discuss the challenges and problems faced by LLMEdu, as well as prospects for future optimization of LLMEdu.\n\n# 1. Introduction\n\nArtificial intelligence (AI) has developed rapidly in recent years [73, 111, 139], thanks to the continuous improvements in Web 3.0 [38], Internet of Behaviors (IoB) [103], data mining [35, 48, 68], deep learning [122], and language processing technologies [47]. LLMs have shown excellent performance in various industries with the optimization of pre-training models and the continuous adjustment of related technologies [25, 132]. LLM is mainly based on many AI technologies, e.g., natural language processing (NLP), and was used to understand and generate massive texts [41]. They perform self-supervised learning on a large-scale corpus to obtain the statistical laws of language [31] and then convert it into logical natural language text. Its basic framework is shown in Figure 1. LLMs have demonstrated strong versatility and logical reasoning capabilities, leading to their widespread model-as-a-service (MaaS) [37] in various industries, including finance, education [36], law [58], robotics [131], and government affairs [20, 32, 126]. Creating a scenario-based user experience is a key advantage for most digital companies, and it also happens to be a development need for LLM.\n\nThe concept of education has been around for centuries, dating back to the theory of biological origins. In primitive societies, education was limited to the use of primary production tools, whereas ancient societies relied on oral transmission and practice to pass knowledge down to future generations [66]. With the development of science and technology in modern society, education and AI\n\n![](/uploads/images/f6ddea83-75e3-472e-8798-80000e520eb9/7086b8cda485234568fab5cdb627979b998a6dc1e1e87faeae4fe69f5d2412ae.jpg)  \nFigure 1: Framework of LLMs.\n\nhave become inseparable [22], including intelligent teacher assistants, voice assistants [77, 92], AI writing creation platforms, etc. The fourth industrial revolution, represented by the intelligent revolution [15], can bring the education industry to a new level with the help of LLMs. Education is essentially about knowledge transfer, instant feedback, and emotional interaction. LLMs mainly enhance the \"immediate feedback\" process in education. They have the potential to revolutionize the education industry by providing personalized, adaptive learning experiences for students. By infusing knowledge into their models, LLMs can gradually build a deep understanding of the world, surpassing human learning in some aspects. They can generate high-quality text content, comprehend natural language, extract information, and answer questions across various fields [71]. LLMs can also do complex mathematical reasoning [123], which helps the education sector show that they are good at self-supervision, intelligent adaptive teaching, and multi-modal interaction [26]. With their ability to adapt the individual students' needs and learning styles, LLMs can provide a more effective and engaging learning experience.\n\nResearch gaps: There are already many educators and researchers who have shown a lot of thinking about AI in education. Examples are as follows: Some research has been conducted on the paradigm shift in AI in education [85] and on the impact of AI in management, teaching, and learning [21]. Some studies explain AI in education and show how they work [72]. Due to the rapid iteration and update of AI, many new educational AI technologies have been spawned, but there is a lack of summary and analysis of emerging technological means. LLMs, as one of these technologies, have significantly advanced AI development to a new stage. LLMs are the latest technological means to support intelligent education. The integration of education and LLMs particularly highlights the development and application characteristics of LLMs. There has been one brief review of LLMs for education [36], while many characteristics of LMEdu and key technologies are not discussed in detail.\n\nContributions: To examine the potential of LLMEdu and promote its development, this paper provides an in-depth analysis of the development process and technical structure of LLMEdu and forms a comprehensive summary. This review aims to help readers gain a deeper understanding of LLMEdu and encourages us to invent and consider LLMEdu applications. The specific contributions are as follows:\n\n- We take a closer look at the connection between LLMs and education, aiming to achieve smart education.  \n- We demonstrate the development process of LLMEdu through the process of applying LLMs to education and the key technologies of LLMs.  \n- We review the implementation of LLMEdu from the perspective of LLMs empowering education, focusing on exploring the development potential of LLMEdu.  \n- We highlight the problems and challenges existing in LLMEdu in detail, aiming to trigger some insight, critical thinking, and exploration.\n\nRoadmap: In Section 2, we briefly introduce the characteristics of LLMs and the education industry, as well as the characteristics of LLMs integrated into education. In Section 3, we conduct an in-depth analysis of the process of applying LLMs to education. In Section 4, we explain the key technologies related to LLMs. In Section 5, we provide the implementation of LLMEdu from the perspective of empowering education with LLMs. In Section 6, we highlight some of the main issues and challenges in LLMEdu. Finally, in Section 7, we summarize LLMEdu and propose expectations for the development of future LLMs. Table 1 describes some basic symbols in this article.\n\n# 2. Characteristics of LLM in Education\n\nIn this section, we discuss the key characteristics of LLMs, the key characteristics of education, the limitations of traditional education, and the combinations between LLMs and education, as depicted in Figure 2.\n\nTable 1 Summary of symbols and their explanations  \n\n<table><tr><td>Symbol</td><td>Definition</td></tr><tr><td>AI</td><td>Artificial Intelligence</td></tr><tr><td>AIGC</td><td>AI-Generated Content</td></tr><tr><td>ChatGPT</td><td>Chat Generative Pre-Training Transformer</td></tr><tr><td>CV</td><td>Computer Vision</td></tr><tr><td>DNNs</td><td>Deep Neural Networks</td></tr><tr><td>GPT</td><td>Generative Pre-trained Transformer</td></tr><tr><td>HFRL</td><td>Human Feedback Reinforcement Learning</td></tr><tr><td>LLMEdu</td><td>Large Language Models for Education</td></tr><tr><td>LLMs</td><td>Large Language Models</td></tr><tr><td>LMs</td><td>Language Models</td></tr><tr><td>NLP</td><td>Natural Language Processing</td></tr></table>\n\n![](/uploads/images/f6ddea83-75e3-472e-8798-80000e520eb9/4ceb13c181dc3c041d9dfd2c369372900381d64a94c5af271691b37f38f65114.jpg)  \nFigure 2: The characteristics of LLMEdu.\n\n# 2.1. Characteristics of LLMs\n\nLarge-scale. The term \"large\" in LLMs can be interpreted in two ways. Firstly, LLMs possess an enormous number of parameters, with the parameter count increasing exponentially from billions to trillions in just a few years. For instance, Google's BERT had 300 million parameters in 2018, GPT-2 had 1.5 billion parameters in 2019, and GPT-3 had 175 billion parameters in 2021 [137, 101]. In 2022, the Switch Transformer reached an impressive 1.6 trillion parameters [67, 100]. Furthermore, LLMs are trained on vast amounts of data from diverse sources, including the web, academic literature, and conversations. This large-scale corpus of data enables the models to learn and represent complex patterns and relationships in language, leading to improved performance in various NLP tasks [107].\n\nGeneral-purpose. LLMs have a wide range of applications [88]. In addition to excelling in specific domains, they are adept at handling various types of tasks, including NLP, CV, speech recognition, and even cross-modal tasks. In other words, LLMs possess powerful generalization capabilities, and achieving such capabilities requires training on massive amounts of data.\n\nPre-training and fine-tuning [27, 47, 132]. The core of the model training process lies in the use of pre-training followed by fine-tuning. Initially, pre-training is performed on a large-scale unlabeled text corpus to acquire the model's\n\nbasic language knowledge. Subsequently, fine-tuning is conducted on specific tasks in a particular domain to better understand and generate language specific to that domain, such as legal, educational, or medical texts.\n\nEmergent ability: unpredictability [88]. The emergent ability of LLMs refers to their capacity to generate coherent and logically consistent text without explicit human intervention, as they have learned from their training process. When the amount of data reaches a sufficiently large scale, the model's learning and feedback capabilities can experience a substantial increase, resulting in improved performance.\n\nFragmentation [93]. The current AI landscape is characterized by diverse business scenarios across various industries, resulting in fragmented and diversified AI demands. The development process of AI models involves several stages, including development, hyperparameter tuning, optimization, and iterative deployment for eventual application. Each stage requires significant investment, and in high-cost situations, catering to customized market demands can be challenging.\n\nPotential for breaking accuracy limitations. The development of deep learning has taken a long time. The improvement in accuracy through architectural changes appears to have reached a bottleneck as neural network design techniques have matured and converged. However, LLM development has shown that increasing the scale of both the model and the data can help break through accuracy limitations. Research experiments have consistently demonstrated that scaling up the model and data leads to improved model accuracy [104]. High complexity and investment costs. LLMs are becoming increasingly complex, with single-step computation time growing by more than 10 times [6]. For high-traffic businesses, a training experiment that used to take a few hours now takes several days, with the expectation that tests will remain within a one-day timeframe as a basic requirement [75]. Moreover, training a general-purpose large model is expensive, and if subsequent optimization, updates, and deployment are included, it will cost even more. For example, the core infrastructure of ChatGPT, the Azure AI, required an investment of nearly $1 billion [87]. Moreover, ChatGPT has high requirements for the number of GPU chips used for data processing [82].\n\n# 2.2. Characteristics of education\n\nAccording to its definition, education is a deliberate and conscious social practice that aims to nurture individuals. Its fundamental characteristic is its process-oriented nature, indicating that education exists and evolves through a series of steps. With a focus on individuals, education ultimately aims to facilitate their holistic and enduring growth. Education encompasses knowledge transmission, immediate feedback, and emotional interaction. Error correction, knowledge reinforcement, and rapid training consolidation are some parts of educational behavior. Furthermore, the education system is highly intricate, marked by the distinctiveness of its subjects, diverse requirements, and intricate interactions.\n\n# 2.2.1. Educational development process\n\nLow entry barriers. On one hand, the accessibility of starting an educational institution is relatively easy [17], resulting in lower operating and investment costs for both teachers and institutions. However, this has also led to a disparity in teacher qualifications, contributing to issues such as disorder in the education and training industry, misleading advertisements, exaggerated titles for teachers, and ineffective offline one-on-one teaching. These have subsequently led to an increase in complaints. On the other hand, there has been a reduction in barriers to education for learners, leading to greater equality of educational opportunities across different regions and a stronger emphasis on the right to education.\n\nLarge capacity [60]. The education industry encompasses a significant number of students and teachers, making it crucial to consider the implications of a large population. Moreover, there exists a diverse array of educational settings, including public schools as well as numerous private educational institutions. There is an abundance of educational materials available, and the advent of the internet has made access to educational resources easier. This development has transcended the confines of traditional textbook-based teaching, breaking down information barriers and expanding the horizons of education.\n\nWell-developed system. The expansion of education has been propelled by economic development [56], leading to a surge in investment in the education sector. This growth encompasses a wide range of educational institutions at different levels. Moreover, the education system encompasses diverse forms of education, such as social life education, family education, and school education. It also encompasses a variety of disciplines, including mathematics, languages, and physical education.\n\nRise of online education [55]. Since the late 1990s, emerging technologies have made significant inroads into the education industry [18]. This transformation has propelled education through various stages, including traditional education, digital education, internet-based education, mobile-based education, and intelligent education. The advancement of information technology has played a pivotal role in facilitating education development by overcoming time and space constraints, making knowledge acquisition more convenient and rapid.\n\nEducation at a younger age. The development of the internet has dismantled barriers to education, resulting in heightened parental concerns and an increased focus on early education. Under the influence of globalization, the significance of early education [128], particularly in language and logic development, has been recognized. In conjunction with the surge of online education, early childhood education has become more readily available. A wide range of tutoring classes and early learning programs have become commonplace.\n\nIntelligent, precise, and personalized education [23]. With the rapid advancement of AI, technology has significantly enhanced production methods and raised people's\n\nliving standards. As a result, society's demand for education has escalated, leading to a more targeted approach to talent development. Education is currently transforming the integration and innovation of \"AI + education\" in smart education.\n\nAlthough education has integrated AI to a significant extent, the nature of human education and machine education fundamentally differs in a two-tier manner. These two forms of education vary in their sequence: human education primarily focuses on shaping values, followed by systematic knowledge acquisition, and ultimately engaging in real-world experiences to foster learning. In contrast, machine education begins by processing vast amounts of data, subsequently discerning between right and wrong (learning values), incorporating human feedback, and ultimately attaining practicality. When it comes to learning, the most notable distinction between humans and machines lies in the limited energy humans possess to acquire knowledge within a fixed period, whereas machines have a relatively unlimited learning capacity. Embracing AI, formulating education strategies that align with the current era, and achieving a comprehensive digital transformation of education are the central points of contemporary educational development.\n\n# 2.2.2. Impact on teachers\n\nInstructional method's development. Digital education provides a wider range of teaching methods and tools [28]. It requires teachers to adapt and become proficient in utilizing these innovative approaches and technologies. This includes leveraging online learning platforms, educational applications, and virtual classrooms to effectively impart knowledge and engage with students. To cater to student's diverse learning needs, teachers must acquire familiarity with and expertise in using these technologies.\n\nPersonalized and self-directed learning support. Digital education has the potential to better support personalized and self-directed learning [19]. Teachers can leverage technology to gain insights into student's learning styles, interests, and needs. They also provide tailored instructional content and learning plans. This shift in education will see teachers adopt more of a guide and mentor role. They encourage students to take an active role in their learning and self-development.\n\nData-driven instructional decision-making. Digital education yields a wealth of learning data, including student's performance, interests, and progress [138]. Teachers can leverage this data to make informed instructional decisions and provide personalized guidance. By analyzing student's data, teachers can identify areas of difficulty and weakness and offer targeted support and feedback to help students overcome these challenges and improve their learning outcomes.\n\nCollaboration and cross-border teaching. Digital education has the power to break down geographical barriers, enabling teachers to engage in cross-border teaching and collaboration with students from all over the world. This allows for the sharing of instructional resources, experiences, and\n\nbest practices among educators, promoting professional development and collaboration within the teaching community.\n\nCultivating 21st-century skills. In the digital age, it's essential for students to develop skills such as creative thinking, digital literacy, collaboration, and problem-solving [46]. Teachers play a vital role in guiding students to cultivate these skills and providing relevant educational support and guidance. By exploring and applying new technologies together with students, teachers can foster student's innovation and adaptability, preparing them for success in an ever-changing digital landscape.\n\nTeachers are indispensable in the digital transformation of education, as they play a multifaceted role in shaping student's academic, emotional, and social development. While technology can provide access to vast knowledge and resources, it cannot replace the personalized guidance, emotional support, and values-based education that teachers offer. The expertise, interpersonal relationships, and educational wisdom of teachers are still essential elements in the digital transformation of education, ensuring that students receive a well-rounded education that prepares them for success in the 21st century.\n\n# 2.2.3. Educational challenges\n\nPersonalized learning needs. In contemporary education, students have diverse learning needs, styles, interests, and aspirations. The traditional one-size-fits-all approach may not cater to each student's unique requirements, and personalized learning is essential to addressing these differences effectively. Therefore, implementing personalized learning is a significant challenge that educators and administrators must address to ensure that every student receives an education tailored to their individual needs and abilities.\n\nInsufficient educational resources. Despite the advancements in technology, there are still areas where schools lack modern technology infrastructure, resulting in a digital divide that hinders student's access to online learning and digital education resources. Moreover, the number of students worldwide continues to rise, putting immense pressure on the education industry. Some regions face the challenge of insufficient educational resources, including teachers, classrooms, and learning materials, leading to disparities in educational opportunities.\n\nEducation quality and standards. Inconsistencies in education quality pose a significant challenge. In some regions, an exam-oriented approach to education may lead to a narrow focus on standardized testing, resulting in a simplified curriculum and a lack of support for students' personal interests and development. Ensuring high-quality, standardized education is crucial to enhance student's academic performance and overall quality. This can be achieved by implementing a well-rounded curriculum that fosters critical thinking, creativity, and problem-solving skills while also providing individualized support for student's unique needs and interests.\n\nDiverse educational technology. The integration of big data, AI, virtual reality, and other educational technologies\n\nhas the potential to revolutionize the education sector. However, it also poses new challenges, such as management, security, and privacy considerations. Effective integration and utilization of these technologies are crucial to enhance the learning experience and achieve optimal educational outcomes. This requires a well-thought-out strategy that takes into account the unique needs and constraints of the education sector.\n\nChallenges in implementing new educational concepts. The rapid pace of technological and economic advancements, coupled with improvements in living standards and quality, has led to the emergence of new educational concepts. One such concept is \"Science Technology Engineer Art Math (STEAM)\" education, which emphasizes interdisciplinary approaches and hands-on practice. However, implementing these cutting-edge educational concepts and cultivating the next generation of socially conscious talents pose a significant challenge for the education sector. Effective strategies and innovative approaches are needed to address these challenges and ensure that students are well-equipped to thrive in an ever-changing world.\n\n# 2.3. Characteristics of LLMEdu\n\nThe integration of AI into the education industry has accelerated rapidly [39, 61, 105], transforming teaching methods and enhancing learning outcomes. From computer-assisted teaching to personalized adaptive learning and content generation, AI has revolutionized the education sector, catering to diverse age groups and fields of study. In the era of intelligence, the primary objective of education is to convert knowledge into intelligence and nurture intelligent individuals. LLMs, with natural language technology at their core, align seamlessly with the education industry's development and adapt to the vast changes in intelligent education. These models have the potential to support and enhance various aspects of the learning experience, making education more accessible, engaging, and effective.\n\n# 2.3.1. Specific embodiment of \"LLMs + education\"\n\nReasons for integrating LLM into education are shown in Figure 3.\n\n![](/uploads/images/f6ddea83-75e3-472e-8798-80000e520eb9/fb43ad14a0e503da8c1bbe33bee4f19135686be5fe62deda62761976b887337c.jpg)  \nFigure 3: Reasons for integrating LLM into education.\n\nInterdisciplinary teaching [74]. The training of LLMs with vast amounts of data gives them a significant advantage in knowledge integration. They can provide diverse learning support based on different subjects and boast excellent interdisciplinary capabilities. For instance, the \"Ziyue\"\n\nlarge model<sup>1</sup> prioritizes a \"scenario-first\" approach, while the iFLYTEK \"Spark Desk\"<sup>2</sup> can conduct human-like interactive learning in various fields, including mathematics, English oral practice, essay correction, and more. These models have the potential to revolutionize the way we learn and teach [24].\n\nPrecise identification of personalized needs. LLMs possess advanced language understanding and generation capabilities, enabling them to provide adaptive learning guidance tailored to individual users' age, learning stage, and learning environment. For example, the iFlytek learning machine based on LLMs can provide customized teaching for traditional subjects, such as oral teaching, Chinese and English composition correction, interactive supplementary mathematics, and so on, providing students with personalized one-to-one mentoring experiences. Furthermore, the learning machine can help parents answer questions through one-to-one dialogue, provide suggestions, and assist in parent-child communication, parent-child interaction, behavioral habits, and so on.\n\nGuided learning. LLMs are shifting towards a more human-like approach, providing authentic conversational teaching experiences in various scenarios instead of simply giving answers. This is particularly noticeable in subjects like physics and mathematics, where LLMs simulate a teacher's role and ask questions to encourage critical thinking and independent exploration [53]. By fostering a self-learning environment, LLMs can help students develop their problem-solving skills and become more effective learners [79]. For example, OpenAI collaborated with the educational organization Khan Academy to produce Khanmigo, an LLM-based educational tool. As students complete the exercises, Khanmigo can guide them to get answers on their own by asking a lot of questions.\n\nIntegration of three modes. Tool-based, companion-based, and information-based [30, 52, 118]. The tool-based mode primarily involves using data to construct a knowledge base, which becomes a large-scale query repository. The companion-based mode is exemplified by virtual teachers and assistants, providing virtual teaching and online assistance through human-like conversations. The informatization-based mode mainly refers to educational informatization, accelerating the development of an \"internet + education\" platform.\n\n# 2.3.2. Impact of \"LLMs + education\"\n\n\"LLMs + education\" will have far-reaching and profound impacts. Here are 10 areas where these impacts can be observed, along with detailed explanations.\n\nPersonalized learning support. LLMs can provide customized learning support based on students' personalized needs. By deeply understanding students learning characteristics, interests, and learning styles, LLMs can tailor teaching content and learning plans for each student. For example,\n\nin mathematics learning, LLMs can provide targeted guidance for students' weak points in mathematics by interacting with them in dialogue, helping them overcome difficulties, and improving their mathematical abilities. LLMs can design adaptive tests that adjust the difficulty of questions based on students' responses, accurately assessing students' knowledge levels and ensuring they are educated at the appropriate level [1].\n\nPersonalized assessment and feedback. LLMs can provide personalized assessment and feedback based on students' learning performance [59]. By analyzing student's answers, understanding levels, and error patterns during the learning process, LLMs can provide targeted assessment results and improvement suggestions. For example, when students encounter difficulties in writing, LLMs can analyze the structure, grammar, and expression of their writing pieces and provide detailed guidance and suggestions to help students improve their writing skills [2, 76]. Some commercial auxiliary tools based on OpenAI's LLM technology, MagicSchool, and Eduaide, can participate in the assessment of students' homework and give feedback [89].\n\nWide coverage of subject knowledge. LLMs have extensive knowledge coverage and can encompass knowledge content from multiple subject areas [69]. Students can engage in dialogue with LLMs to acquire knowledge and information across various subject domains. For instance, when students encounter problems in history learning, LLMs can provide detailed explanations and in-depth discussions of historical events, figures, and backgrounds, helping students better understand historical knowledge. According to statistics, the latest model has 13 trillion tokens of carefully selected pre-training knowledge data, which is equivalent to 5 million sets of four major classics. In addition, 1.8 trillion \"knowledge fragments\" are extracted during training [14].\n\nInterdisciplinary learning. LLMs have excellent interdisciplinary capabilities, enabling students to engage in integrated learning and cultivate interdisciplinary thinking skills [110]. Through interactions with LLMs, students can integrate and apply knowledge from different subject areas. For example, when conducting scientific experiments, students can have conversations with LLMs to discuss experimental principles, data analysis, and scientific reasoning, promoting integrated learning between science and mathematics, logical thinking, and other disciplines [3].\n\nReal-time problem-solving and tutoring. LLMs can provide real-time problem-solving and tutoring support for students. When students encounter confusion or questions during the learning process, they can ask LLMs at any time and receive immediate answers and solutions. A survey report in the first half of this year pointed out that  $89\\%$  of American students surveyed were using ChatGPT to complete homework [134]. Additionally, when students encounter comprehension difficulties while reading literary works, they can engage in dialogue with LLMs to explore the themes, plots, and character images of literary works, helping students better understand and analyze literary works [115].\n\nOpportunities for learning across time and space. The existence of LLMs allows students to learn anytime and anywhere. Students can interact with LLMs through mobile devices or computers, without being constrained by traditional classroom time and location. For example, students can utilize evening or weekend time to engage in online learning with LLMs, improving their academic abilities and knowledge levels. Online learning platforms, which utilize LLMs, provide students with access to a wide range of courses and disciplines via the Internet. The LLMs support the implementation of virtual classrooms and distance education, and students talk to the LLMs in real time to solve problems.\n\nProvision of learning resources and tools. LLMs can serve as rich learning resources and tools, providing a wide range of educational materials and tools for student's learning needs. For instance, LLMs can offer textbooks, educational videos, interactive exercises, and other learning materials to support student's learning in various subjects [7]. Additionally, there are some subject-specific tools, such as MathGPT. MathGPT has an accuracy rate of  $60.34\\%$  in the benchmark test AGIEval, which can help students solve mathematical problems efficiently [142].\n\nPromotion of critical thinking. LLMs can guide students in developing critical thinking and problem-solving skills [50]. By engaging in dialogue and posing thought-provoking questions, LLMs can foster a thinking atmosphere that encourages students to explore answers, enhancing their self-learning abilities and critical thinking skills. For example, LLMs can simulate a teacher's role in a physics class, asking students questions about concepts, principles, and problem-solving strategies, encouraging them to think critically and develop problem-solving skills [114].\n\nProfessional development for educators. LLMs can support the professional development of educators by providing them with access to a vast amount of educational resources, best practices, and innovative teaching approaches. Educators can interact with LLMs to enhance their teaching methods and explore new ways to engage students [65]. For example, teachers can engage in dialogue with LLMs to discuss teaching strategies, classroom management techniques, and approaches to address student's individual needs, improving their teaching effectiveness and professional growth.\n\nAccessibility and inclusivity in education. LLMs can contribute to making education more accessible and inclusive. They can provide learning support for students with different learning styles, abilities, and backgrounds, ensuring that all students have equitable access to quality education. For example, LLMs can offer alternative explanations, visual aids, and interactive learning experiences to accommodate diverse learners, including students with learning disabilities or language barriers, making education more inclusive and supportive. Additionally, through multicultural training, LLMs can better understand and respect students from different cultural backgrounds and create a learning environment that is inclusive and respectful of diversity.\n\nIn summary, the integration of LLMs with education will revolutionize the learning experience by providing personalized support, expanding knowledge coverage, promoting critical thinking, and enhancing the accessibility and inclusivity of education. It will empower students and educators alike, transforming the way knowledge is acquired, shared, and applied in the digital age.\n\n# 3. How to Gradually Integrate LLMs into Education\n\nThe integration of AI into the education industry has been progressing step by step, from machine learning (implementing the ability to store and calculate) to deep learning (implementing the ability to see and hear), and now to LLMs (capable of understanding and creating) [78, 99, 113]. In the current era, the vigorous development of quality education by the entire population and the active deployment of educational intelligent hardware nationwide represent the active transformation of educational training enterprises [13, 91]. In the long-standing coexistence and collaboration between teachers and AI models [112], as well as the highly homogeneous hardware background, LLMs have emerged as one of the most important technologies in human intelligence.\n\n# 3.1. Reasons why LLMs for education\n\nLLMs' excellent characteristics make their application in the education industry very reasonable. NLP [41], data analysis [34, 135], and text generation capabilities [119] align well with the fundamental processes of learning, questioning, and feedback in education. The iterative optimization process of \"development-deployment\" suits the application process in the education industry. User testing and feedback data lay the foundation for further optimization. Taking the development of LLMs in China as an example, the Spark Desk by iFLYTEK<sup>3</sup>, the ERNIE Bot by Baidu<sup>4</sup>, and the \"MathGPT\" by TAL<sup>5</sup> have accumulated data from years of experience in the education industry [143]. During their usage, these LLMs can collect more data from the education industry, leading to further technology optimization.\n\nThe \"AI + education\" model has already formed, and the gradual maturity of AI technology has paved the way for the entry of LLMs into the education industry. Smart classrooms, voice-assisted teaching, intelligent problem-solving, and other AI applications have become routine in the education industry, leading to high acceptance of LLMs [10, 12, 96]. It is important to recognize that LLMs are the latest technological achievements that gather human collective intelligence, rather than only technological achievements. However, LLMs' development potential and influence are gradually increasing.\n\nEducation companies implement their own LLMEdu development strategies. LLMs require massive amounts of data and significant investments to support them. In terms of\n\ndata, looking at various education companies, long-term experience data accumulation, technology accumulation, and an objective combination of their development conditions have differentiated the educational application of LLMs. They focus on LLM research and strive to maximize their benefits, cater to current development trends, and reduce development costs. In terms of funding, consumers in the education industry have a strong willingness to consume. As people's living standards and education levels improve, the world strengthens the education industry and injects large amounts of funding to provide a solid foundation for LLM research, development, and application.\n\nChatGPT makes practical changes to the integration of technology and education. Learning is an exploration process, and LLMs play an exploratory role in education. Because of interactive questions and answers, people's roles are changing from passive recipients of knowledge to active explorers. Because of the existence of machine hallucinations, scholars need to have a skeptical and judgmental attitude towards generated knowledge and treat LLMs from a dialectical perspective. Intelligent technology stimulates human creativity, allowing people to continuously expand their breadth of learning, thus leading to scientific and technological progress.\n\nLLMs support the sustainable development of education [5]. Innovation is the core of technological development and the premise of long-term application. By fully utilizing AI technologies such as ChatGPT, the application process in education can transition from a search mode to a content generation mode personalized for individuals. This enables the development of diverse, scalable, tangible application scenarios, as well as a series of differentiated and highly experiential educational products and services. It provides excellent environments and resources for educators and education recipients, supporting education's sustainable development.\n\nNowadays, general language models (LMs) leverage extensive data memory to shift from dedicated to universal application models. They rely on text generation capabilities, transitioning the application process from distribution to generation. This allows them to achieve multi-modality and transform application scenarios from single to multiple [43]. Multi-modal LLMs, which combine pre-training and downstream tasks, can efficiently complete downstream task adaptation with relatively small amounts of data and can be used in small sample learning and natural language question answering. In education, three typical applications are realized: automatic generation of teaching resources, human-machine collaborative process support [141], and intelligent teaching assistance for teachers. Multi-modal LMs combine the three fields of reinforcement learning, CV, and NLP. They attempt to extend the concept of LMs [49, 95, 106].\n\nWhat's more, we demonstrate the development of the GPT models, as shown in Table 2.\n\nTable 2 Iteration and comparison of LLMs  \n\n<table><tr><td>LLMs</td><td>Publish time</td><td>Parameter quantity</td><td>Pre-training data size</td><td>Training paradigm</td><td>Feature</td></tr><tr><td>GPT</td><td>2018.7</td><td>120 million</td><td>5G</td><td>Pre-training + fine-tuning</td><td>Reflection of the advantages of self-attention structure</td></tr><tr><td>GPT-26</td><td>2019.2</td><td>1.5 billion</td><td>40G</td><td>Prompt paradigm based on Tunning-free: Zero Shot Prompt</td><td>Open the exploration of the Prompt paradigm</td></tr><tr><td>GPT-37</td><td>2020.6</td><td>175 billion</td><td>45TB</td><td>Prompt paradigm based on Tunning-free: In-Context Learning</td><td>Deepen the exploration of the Prompt paradigm</td></tr><tr><td>InstructGPT8</td><td>2022.3</td><td>175 billion</td><td>45TB</td><td>Prompt paradigm of Instruction Tuning</td><td>Start paying attention to human preferences</td></tr><tr><td>ChatGPT9</td><td>2022.11</td><td>175 billion</td><td>45TB</td><td>Reinforcement learning from human feedback</td><td>Aligned with human preferences</td></tr><tr><td>GPT-410</td><td>2023.3</td><td>Nearly 2 trillion</td><td>-</td><td>Reinforcement learning from human feedback</td><td>Multimodal processing and getting closer to the bionic human brain</td></tr><tr><td>LaMDA11</td><td>2021</td><td>137 billion</td><td>150TB</td><td>Pre-training + fine-tuning</td><td>Introduce external information retrieval system</td></tr><tr><td>BARD12</td><td>2023.2</td><td>137 billion</td><td>-</td><td>Join ChromeOS as a search engine</td><td>Using LaMDA as a base</td></tr><tr><td>PaLM</td><td>2022.4</td><td>540 billion</td><td>-</td><td>PathWay distributed training framework</td><td>Large scale, multi-lingual</td></tr><tr><td>Claude13</td><td>2023.3</td><td>52 billion</td><td>-</td><td>Join the RLAIF training paradigm</td><td>Longer and more natural text editing than ChatGPT</td></tr><tr><td>BlenderBot314</td><td>2022.8</td><td>175 billion</td><td>-</td><td>Instruction fine-tuning</td><td>Text generation, question answering</td></tr></table>\n\n# 3.2. Fusion strategies\n\nCooperating with the education and training community. LLM technology engages with schools, online education platforms, and educational technology companies to collectively explore and develop the application of LLMs in education. Partnering to provide actual educational scenarios and resources can help customize models to meet educational needs and accelerate the implementation of LLMedu. For example, Baidu launched \"ERNIE Bot\" [143], Alibaba Group Holding Limited launched \"Tongyi Qianwen\" [15], and universities like Tsinghua University launched \"ChatGLM\" [16] [133], etc.\n\nForm customized content generation to enhance competitiveness. LLMs require high-quality and large data sets, so the education and training community can use LLMs to generate high-quality educational content, such as course materials, textbooks, exercises, and tests. For example, Baidu's \"ERNIE Bot\" has a certain accuracy in answering knowledge questions because it uses the Baidu Encyclopedia as training material. ChatGPT can also generate some framework lesson plans for teaching.\n\nProvide popular educational functions. Some educational technology companies develop an intelligent tutoring system, use LLMs to answer students' questions, provide answers and feedback, provide logical responses to open-ended questions, and provide guided responses to calculation questions. For example, MathGPT, developed by TAL, provides high-quality problem-solving tutoring in the field of mathematics [97]. Some use LLMs to develop speech recognition and dialogue systems, making speech education and interaction easier to implement, enabling language teaching and situational dialogue [54].\n\nIntegrate LLMs into online education platforms. Based on the learning model combined with the Internet and the rapid development of big data, integrating LLMs into online education platforms can provide students with richer learning resources, tools, and more comprehensive applications. For example, the Coursera online education platform<sup>17</sup> uses LLMs to implement functions such as data\n\ncollection and course recommendations. Duolingo $^{18}$  uses LLMs to upgrade language functions. Chegg $^{19}$  uses LLMs to optimize the homework tutoring process.\n\nParticipate in optimizing the educational work training process. First, provide training and support to educators so that they can effectively use LLMs and related tools. For example, we learn how to integrate models into teaching, as well as how to interpret and use the data and recommendations generated by the models. Second, we use LLMs to analyze student data to provide educators with insights about student progress and needs, thereby optimizing their teaching methods, such as timely feedback features.\n\nContinuous improvement and research. The gradual integration of LLMs into the education industry requires time and resources. During this process, the performance, application, and potential risks of LLMs are continuously monitored and improved, and data privacy and security regulations are observed, considering the educational needs of different regions and cultures, which can maximize the role of LLMs in the education industry.\n\n# 4. Key Technologies for LLMEdu\n\nThe technologies behind LLMs support their rapid development, as shown in Figure 4. The combination of these technologies enables LLMs to achieve excellent performance in a variety of NLP tasks, such as text generation, machine translation, sentiment analysis, and text classification. They already play an important role in various applications such as virtual assistants, intelligent search, automatic summary generation, and natural language understanding, which promotes the development of LLMEdu.\n\nLanguage model. It learns from a corpus and predicts word sequences based on probability distributions. Two main technologies used to train a language model are next-token prediction and masked language modeling. Next-token prediction predicts the next word based on its context, and masked language modeling learns the statistical structure of language, like word order and usage patterns [9, 25, 84]. However, there is still a significant gap between predicting\n\nTable 3 Comparison between generative AI and discriminative AI  \n\n<table><tr><td></td><td>Core</td><td>Data learning</td><td>Development process</td><td>Application</td></tr><tr><td>Discriminant/Analytical AI</td><td>Analysis</td><td>Conditional probability distribution</td><td>Mature technology and widely used</td><td>Recommendation systems, CV, NLP</td></tr><tr><td>Generative AI</td><td>Creation</td><td>Joint probability distribution</td><td>Exponential explosion</td><td>AIGC, text generation, audio generation</td></tr></table>\n\ntext and mastering more advanced representations in LMs, so training strategies for LMs can be inconsistent and may not correctly reach the ultimate goal. The prediction ability reflects the large model's learning ability, which determines whether the LLM can form a coherent and logical text when answering questions. So the language model is LLMEdu's foundation.\n\nHuman feedback reinforcement learning (HFRL). It is a method used in the training of LLMs [86]. By incorporating human feedback, it reduces distorted and meaningless outputs, helping ChatGPT overcome the issues present in GPT-3, such as consistency problems. It includes supervised fine-tuning, simulating human preferences, and proximal policy optimization [140]. i) In supervised fine-tuning, a small amount of annotated data is fine-tuned by first performing next-token prediction to improve the injected data, then integrating the results, and finally decoding operations [33]. ii) Developing a reward model that simulates human preferences to rank the decoded results, and constructing a ranking sequence to obtain a scoring model. To ensure consistent annotation results, the ranking process uses ordinal ranking for data annotation, resulting in a new dataset composed of comparative data [8]. iii) Proximal policy optimization aims to learn a policy that maximizes the cumulative reward obtained during training. The algorithm involves an actor, which outputs the probability distribution for the next action, and a critic, which estimates the expected cumulative reward for a given state. By iteratively optimizing the reward signal output, the model learns from experience, adapts to new situations, continuously adjusts its policy, and improves the LLMs [121]. HFRL improves LMEdu's accuracy, making the output results more concise, accurate, and in line with the human thinking process.\n\nDeep neural networks (DNNs) [42]. Before explaining DNNs, it is necessary to introduce deep learning. It refers to the learning of the underlying patterns and hierarchical representations of sample data, aiming to achieve the goal of machine learning with analytical capabilities similar to humans. DNNs consist of multiple layers of interconnected neurons, typically including an input layer, several hidden layers, and an output layer. The connectivity between neurons is similar to the connections between biological neural cells. DNNs have advantages in processing large-scale educational data, including students' academic performance, learning behavior, problem-solving abilities, etc. By analyzing these data, LLM can provide insights for educational decision-making and improve teaching methods and personalized education strategies.\n\nSelf-supervised learning. To produce the desired results, a model or machine needs to be trained with the given materials. Machine learning can be categorized into supervised learning, unsupervised learning, and reinforcement learning [80]. Self-supervised learning falls under unsupervised learning, where the model learns general feature representations for specific tasks. Unlike supervised learning, which requires a large amount of manually annotated data for training, self-supervised learning completes self-training by replacing human annotations with the intrinsic structural features of the data itself, using unlabeled datasets [31, 125]. It gradually trains the parameters from scratch in a progressive manner, using part of the input as the supervisory signal and the rest as input. This approach significantly reduces the cost of manual annotation in terms of high cost, long cycles, and low accuracy, resulting in a lower development cost. Through self-supervised learning, LLMs can learn advanced representations of language data and deep cognition of language skills. This enables them to better understand and generate education-related content, including textbooks, exercises, solutions, and study materials.\n\nTransformer model. From a structural perspective, LMs have evolved from statistical LMs to neural network LMs, and now to LLMs. Statistical LMs focus on transforming sentences into probability distributions, but the lack of computational power limits their ability to match massive amounts of data. Neural network LMs, such as recurrent neural networks, use recursion and convolutional neural networks to transform language sequences. Recurrent neural networks require considering the input-output order for computation and cannot handle examples in batches efficiently, resulting in slow speed. The Transformer model, widely used in LLMs, overcomes these limitations. The transformer model is essentially an encoder-decoder architecture that includes encoding and decoding components. It employs attention mechanisms to capture global dependencies between inputs and outputs [27], without considering the distance within input or output sequences [29]. This approach transforms the growth rate of required data for operations on related signals from linear or logarithmic to constant, showcasing high parallelism, which is beneficial for fast model iterations. Compared to previous models, the Transformer model has a richer structure, stronger adaptability to various scenarios, and better performance. The Transformer model improves the compatibility and practicality of LLMs, as well as its ability to cope with diverse and rich teaching contents and educational scenarios.\n\nLLM diagnostics and application evaluation. Existing interdisciplinary evaluation systems assess LLMs from two perspectives: diagnostics during LLM training and the effectiveness of LLM applications. \"ChatbotArena\"20 is a benchmark platform for LLMs that conduct anonymous and random adversarial evaluations, where the system randomly selects two different LLMs to chat with users, who then rate the interactions. \"SuperCLUE\"21 is a benchmark for evaluating general-purpose LMs in Chinese, examining multidimensional capabilities in terms of basic abilities, professional abilities, and Chinese-specific abilities [124]. \"The C-Eval project\" [51], jointly carried out by Shanghai Jiao Tong University, Tsinghua University, and the University of Edinburgh, constructs a multidisciplinary benchmark list to assist Chinese LLM research. \"FlagEval\" [63], built by multiple universities, adopts a three-dimensional approach to evaluating LLMs, including factuality, safety, and inclusivity. These evaluation frameworks are designed to comprehensively assess LLMedu's performance, ethical impact, and potential bias, as well as promote the improvement of LLMedu's capabilities and technology optimization.\n\nPrompt engineering [83]. It refers to the ability to interact with LLMs. Machines match corresponding results through prompts, thereby increasing productivity. Good prompts can enhance the intelligence of LLMs and increase the value of feedback results [109, 130], increasing the use value of LLM.edu. Moreover, poor prompts may lead to erroneous conclusions. In the field of education, especially rigorous science, the correctness of answers is always given priority, so optimizing prompt words is also important to deal with LLM's nonsense when answering academic questions. Different LMs, such as ChatGPT, ERNIE Bot, and MathGPT, have independent underlying training mechanisms, and their prompts are different. This can be likened to communication with individuals with different personalities.\n\nLearning cognitive mechanisms. Learning cognitive mechanisms, which were developed in cognitive ethics, serve as the foundation for intelligent instructional design. It studies the process of knowledge construction in learners, integrating new knowledge into existing knowledge structures, and adjusting and updating the overall structure. Prior to ChatGPT, AI primarily focused on computation and reasoning. With AI's rapid development, its cognitive intelligence has gradually emerged and can even match human intelligence. There are two main cognitive approaches: one involves simulating human learning processes through computer models, and the other utilizes non-invasive brain imaging techniques such as functional magnetic resonance imaging. LLMs primarily simulate human learning processes, where pre-training can be likened to acquiring new knowledge and constructing knowledge.\n\nBy adding plug-ins, the latest LLM GPT-4 can address real-time problems, such as solving the lag problem of pretraining data. GPT-4 can also better solve logic problems because it introduces the mathematical problem data sets\n\nMATH and GSM-8K into the training data set, which greatly improves its mathematical reasoning capabilities. Moreover, GPT-4 can also complete creative text creation because it is connected to the API, and users can customize the AI character and complete simulated writing, reducing deviations and over-correction [71].\n\n![](/uploads/images/f6ddea83-75e3-472e-8798-80000e520eb9/b4ef019575990bd87a640c565e63e967f54e38f8504e2682eebbeedb8e434bd6.jpg)  \nFigure 4: Key technologies of the LLMs\n\n# 5. Implementation of LLMEdu\n\nIn this article, many products of LLMedu are introduced, and the summary is shown in Figure 5. Moreover, this part will focus on the implementation process of LMs from two aspects: LLMs empowering education and specifically LLMs empowering the field of mathematics. Finally, we use a unified framework to organize and compare the application of LLM in the field of education. The details are shown in Table 4.\n\n![](/uploads/images/f6ddea83-75e3-472e-8798-80000e520eb9/2e00fa102c4cec42c4c9611c8bc61e3d50cd086121164b5e0ef13d24ffcfd33b.jpg)  \nFigure 5: Examples of LLMEdu.\n\n# 5.1. LLMs-empowered education\n\nImprove teacher effectiveness. LLM can help teachers access a wealth of teaching resources, allowing them to conduct classroom instruction more effectively. Before class, LLM can serve as a helpful assistant for lesson preparation. Through interactive question-and-answer sessions, LLM can provide ideas for teacher's lesson planning, assist in designing teaching outlines and curriculum plans, and help teachers quickly identify the highlights and challenges of a lesson. In the classroom, LLM can act as an AI teaching assistant, providing an instant feedback platform for both teachers and students and enhancing classroom engagement, interest, and appeal. After class, LLM can assist teachers in generating\n\nhomework assignments and exam questions, enabling teachers to better assess students' understanding of the subject matter. In daily work, LLM is also a valuable assistant for teachers, capable of drafting meeting invitations, writing work plans, summaries, reports, and more. When used properly, LLM can help alleviate teachers' workload and promote their professional development [136]. For example, a survey pointed out that during the paper revision process,  $57.4\\%$  of users believed that the feedback generated by LLM was helpful and could help them improve their research process [64].\n\nPromote student progress and growth. In terms of learning assistance, LLM is a powerful tool that can understand complex concepts, solve difficult problems, and provide corresponding learning advice. In language learning, LLM offers scenario-based dialogue training, greatly enhancing student's oral and written abilities. In terms of cultivating thinking skills, LLM sometimes exhibits \"serious nonsense\". Teachers and parents can utilize this phenomenon to cultivate students' critical thinking and enhance their information literacy. In terms of learning ability development, the process of using LLM requires students to ask questions. In this process, students have to learn how to translate their questions into effective questions and how to obtain useful information, which cultivates students' self-learning ability and summary ability. Taking college students as an example, data shows that more than  $20\\%$  of the users of one of LLM's latest products, the iFlytek Spark model, are college students, and it helps them improve in English speaking practice, mock interviews, and after-school homework.\n\nAnswer professional and academic questions, accelerating research progress. LLM is capable of writing academic experiment codes, building experimental models, quickly and accurately searching for literature materials, and extracting and integrating relevant information. This reduces the tedious process of manual research and accumulation, saving a significant amount of time. As a result, researchers can invest more energy into subsequent research, thereby improving research efficiency [7]. Additionally, the report findings show that LLMs in universities, as an important research platform in the field of AI, have achieved remarkable results. Chinese universities' research on LLMs mainly focuses on CV, NLP, speech recognition, and other fields. Research results in these fields not only provide a good academic atmosphere for teachers and students in universities but also provide strong support for the development of different AI industries.\n\nPromote the evolution of educational consciousness and form new learning paradigms. The existing educational system is primarily focused on inheritance, and students often approach knowledge with inertial thinking inherited from their learning experiences. There is a lack of creative awareness. However, with the advancement of AI technologies such as ChatGPT, the existing learning paradigms are no longer sufficient for the future. Faced with the challenges posed by technologies like ChatGPT,\n\nit is necessary to cultivate higher consciousness and exercise thinking skills with a high level of awareness, forming new learning paradigms while improving perception and cognition to better understand the world. For example, the high-consciousness generative learning paradigm reflected in ChatGPT involves establishing connections between new and old knowledge, incorporating reflection and introspection, and innovating new concepts and understandings. To advance the high-consciousness generative learning paradigm, collaboration between educational designers and implementers is required to build adaptive learning environments and foster a positive learning atmosphere [7].\n\nCreate highly contextualized and intelligent learning experiences. In subject learning, generative AI like LLM, with its vast amount of data, can provide students with abundant information and knowledge, streamlining the process of finding learning materials and assisting students in finding answers and solving problems across various subjects. In language learning, LLM can offer real-time dialogue training, enabling students to immerse themselves in scenario-based learning and improve their conversational and writing skills. In terms of temporal and spatial aspects of learning, as an online tool, LLM can be accessed by students anytime and anywhere, providing great flexibility. Currently, LLMs are constantly improving their technologies and capabilities to achieve intelligent learning. For example, in the language understanding task, the ultra-large-scale Chinese pre-trained language model PLUG broke the Chinese GLUE classification list record with a score of 80.179. In the language generation task, it improved by an average of more than  $8\\%$  compared with the previous best results in multiple datasets.\n\nPromoting high-quality development in education enhances educational management and decision-making capabilities. LLMs represent the latest technological means supporting intelligent education, and their development process reflects the synchronized progress of AI and humans. This embodies a new era of educational style that aims to create intelligence, cultivate wisdom, and create more efficient intelligence. Moreover, the data transparency involved in LLMs can make educational development decisions more precise and scientific, transforming educational decision-making from experiential patterns to evidence-based patterns and thereby enhancing educational governance capabilities. Finally, educational practitioners can use AI technologies like ChatGPT to conduct scenario-based assessments of students, resulting in a digital transformation of educational evaluation [45]. LLMs can help teachers judge student's progress in learning and understand student's learning status. Notice that the multi-dimensional data collected by LLMs through evaluation is helpful for educators to study student's learning logic and development rules, adjust teaching content on time, and provide students with personalized growth services.\n\nDriving in-depth research in the education system. The research paradigms in education have evolved from the traditional observation and summary of scientific experiment experience, the construction of theoretical models and\n\nderivations, and computer simulation to the scientific research paradigm of large-scale data collection, analysis, and processing. The educational research paradigm is constantly changing. However, as time progresses, the old research paradigms no longer meet the requirements. The emergence of content-generative AI, represented by LLMs, has given rise to a new paradigm, \"The Fifth Paradigm\" of \"AI for Science,\" enabling humans to delve further into the exploration of the education system. This paradigm shift involves the transition from simple imitation of humans to cognitive understanding and transformation, creating a new world of AI and education. According to a survey by Study.com[22],  $21\\%$  of teachers outside China have begun to use ChatGPT to assist their teaching work. Chegg, a listed American education and training company, also said that after launching the LLM-based learning assistance platform, it has affected the user growth of its original business, and students' interest in ChatGPT has greatly increased.\n\nPromote the development of AI from fragmentation to scalability, thereby enhancing its generalization capabilities in education. LLMs accurately capture knowledge from massive datasets through the process of pre-training an LLM and fine-tuning it for downstream tasks [11]. This knowledge is stored in a large number of parameters and then fine-tuned for specific tasks. Finally, it can be flexibly applied to various scenarios. In other words, a single set of techniques can be used to address different tasks, greatly improving development efficiency. For example, in the field of education, LLMs share data to solve common problems and are widely applied in dialogue question-answering, language translation, text generation, and other scenarios. Some open-source LLMs, such as ChatGLM, Baichuan, InternLM, Qwen-7B, and Qwen-14B, are all manifestations of the generalization of LLMs, and Qwen-14B among them already has an accuracy of more than  $70\\%$ , which shows that these degrees are constantly improving.\n\n# 5.2. LLMs in Mathematics\n\nAI has been pursuing mathematical research and applications since its inception. Mathematics is a challenging subject in education, and proficiency in math represents a significant milestone in the intelligence level of LLMs. The successful handling of mathematical problems by LLMs will mark a new era in AI.\n\nApplications in mathematics can reflect the imitation ability of LLMs. Mathematics is an abstract discipline that requires logical reasoning and critical thinking [102]. Currently, LLMs are unable to genuinely comprehend the essence of mathematics and demonstrate independent thought. Therefore, when addressing mathematical problems, these LLM models rely heavily on the mathematical concepts and rules embedded in their training data. For instance, when solving algebraic problems, LLMs apply algebraic rules by mimicking the way humans learn and apply algebra [71].\n\nImprovement of computational performance of LLMs in mathematics. The essence of LLMs is to predict future outputs based on data correlation. However, errors may occur for symbols that are rarely or never encountered in the pre-training stage. For example, because the size of numbers is infinite and the scale of LLMs is limited, arithmetic operations on large numbers are likely to go wrong. To solve this problem, fine-tune the LLM on synthetic arithmetic problems and use special training and inference strategies to further improve numerical computing performance.\n\nOptimize the logical reasoning process. One is to optimize the human logical reasoning process through LLMs. For example, some scholars have applied LLMs to the proof of theorems [44], because LLMs can provide a large amount of relevant materials to make up for the lack of information or omissions, making the reasoning more complete. The second goal is to improve LLMs' logical reasoning abilities. The logical reasoning ability of LLMs is a key indicator for evaluating LLMs. Because LLMs usually have problems such as excessive parameter space and severe data sparseness, LLMs perform poorly on robust and rigorous reasoning tasks. Relevant research has proposed optimization methods for LLM logical reasoning problems. For example, OpenAI[23] studies a process-based supervision model to improve the logical reasoning capabilities of GPT-4. Moreover, some research institutions use the method of continuous pre-prediction on large-scale mathematical corpora, which improves model performance on mathematical reasoning tasks.\n\nInteraction with external tools to improve LLMs' mathematical capabilities. 1) LLMs interact with language conversion tools, such as lean language [81], which can convert mathematical language into computer language, thereby improving the rigor of model reasoning. This is an innovative way to bridge the gap between human reasoning and machine reasoning. This could allow models to better understand and process complex mathematical concepts. 2) LLMs interact with information retrieval systems, such as the large dialogue model LaMDA proposed by Google, which connects to the information retrieval system and allows the model to learn to retrieve and use calculators and translation engines [108]. 3) LLMs directly interact with the calculation engine, such as MathGPT, which improves calculation accuracy by interacting with the calculation engine. This allows models to take advantage of calculators' powerful computing capabilities and perform complex mathematical calculations with greater accuracy. 4) LLMs enable themselves to determine the interactive tools, such as Meta's toolformer model, which can determine the use of external tools by itself [98]. This gives models the flexibility to adapt to different situations and choose the most appropriate tools to solve a problem, much like humans do.\n\nFuture development of LLMs in mathematics. Specifically, the first is a cutting-edge exploration with scientific research at the core, such as the research and improvement of LLMs' capabilities in mathematics, including computing\n\nTable 4 Comparison between generative AI and discriminative AI  \n\n<table><tr><td>Application</td><td>Advantage</td><td>Disadvantage</td><td>Challenge</td><td>Future development</td></tr><tr><td rowspan=\"3\">Personalized learning</td><td>Save time and costs</td><td>Data privacy issues</td><td>Expand the corpus</td><td>Develop personalized applications</td></tr><tr><td>Precise teaching</td><td>Information bias</td><td>Information accuracy</td><td>Information extraction technology update</td></tr><tr><td>Good interactivity</td><td>The learning process is opaque</td><td>Update corpus in real time</td><td>Integration of various technologies</td></tr><tr><td rowspan=\"3\">Guided learning</td><td>Improve problem-solving abilities</td><td>Marginalized teachers</td><td>Social impact</td><td>Training with more accurate data</td></tr><tr><td>Encourage critical thinking</td><td>Misleading information</td><td>Emotional understanding</td><td>Integrate with personalized experiences</td></tr><tr><td>Cultivate interest in learning</td><td>Lack of emotional resonance</td><td>Unemployment Risk</td><td>Develop policies to address social impacts</td></tr><tr><td rowspan=\"3\">Interdisciplinary learning</td><td>Provide diverse learning support</td><td>Insufficient training data support</td><td>Logic optimization</td><td>Integration of multidisciplinary and LLM</td></tr><tr><td>Cultivate interdisciplinary thinking skills</td><td>Lack of domain knowledge</td><td>Accuracy of knowledge integration</td><td>Revolutionize the way we learn and teach</td></tr><tr><td>Boast excellent interdisciplinary capabilities</td><td>Disciplinary bias</td><td>Algorithm optimization</td><td>Filter useful training data</td></tr><tr><td rowspan=\"3\">Real-time problem-solving</td><td>Reduce teacher stress</td><td>Machine hallucination</td><td>Multiple text associations</td><td>Standardize technology use</td></tr><tr><td>Improved learning efficiency</td><td>Over-reliance on technology</td><td>Text extraction</td><td>Acceleration of model inference</td></tr><tr><td>Teaching assistance upgrade</td><td></td><td></td><td>Diversified technical assistance</td></tr><tr><td rowspan=\"3\">Applications in mathematics</td><td>Guide mathematics learning</td><td>Math terminology learning</td><td>Promote mathematical research</td><td>Pay attention to thinking guidance</td></tr><tr><td>Improve math learning efficiency</td><td></td><td>Improved logical reasoning ability</td><td>Mathematics research and teaching</td></tr><tr><td>Show the fusion of AI and mathematics</td><td></td><td>Understand number relationships</td><td>Adequate language modeling</td></tr></table>\n\ncapabilities, reasoning capabilities, robustness, and so on. The second is to improve inclusive education and basic education for the general public. This entails studying how to use models to improve learning experiences and effects, as well as enhance mathematical education for students of all ages and backgrounds. By leveraging the power of LLMs, it may be possible to create personalized learning experiences that cater to individual student's needs and learning styles, making mathematics education more accessible and effective for a broader range of people. In terms of development potential, the expansion of LLMs' ability to solve mathematical problems could have far-reaching implications for other technical and educational fields. For example, LLMs could be used to improve the accuracy and efficiency of scientific simulations, enhance the effectiveness of machine learning algorithms, or even aid in the development of new technologies such as quantum computing. Ultimately, the development of LLMs in mathematics could drive the development of a new generation of education models that are more inclusive, effective, and efficient.\n\n# 6. Issues and Challenges\n\nIn practical applications, LLMs for education still face many issues and challenges, including but not limited to, as shown in Figure 6.\n\n# 6.1. Main issues\n\nRisk of widespread false knowledge. As an imperfect intelligent technology, LLMs such as ChatGPT still have many flaws. The biggest drawback is the potential for generating incorrect information [3]. As many people have noticed, LLM sometimes exhibits machine hallucination [94]. For example, a computer scientist in California tried different methods to check the output of the GPT robots and found that GPT-3.5 and GPT-4 were full of errors when testing physics, chemistry, and mathematics questions selected from\n\n![](/uploads/images/f6ddea83-75e3-472e-8798-80000e520eb9/2e96c40efc4f830a6d3e3df8179621d5ff0b821e91ca75d694a2efc3168f8e51.jpg)  \nFigure 6: Some challenges and issues of LLMEdu.\n\ncollege textbooks and exams. Moreover, since LLM's training data largely consists of English corpora, it often struggles to understand and provide correct answers to personalized Chinese questions. In the short term, these errors can cause disruptions in students' knowledge learning, and students with weaker discernment abilities are highly likely to acquire erroneous knowledge without realizing it. In the long term, if the corresponding technology is not improved promptly, LLM may contribute further to the proliferation of false knowledge. There are many examples of actively dealing with machine hallucinations. For example, the retrieval-augmented generation method (RAG) can integrate LLM with a rigorously verified external key knowledge corpus.\n\nLack of clear operating rules in the education system. Due to the complexity of education itself, representing the education system using specific symbols and algorithms is an extremely challenging process that current LLMs cannot achieve. Education behaviors, such as emotional interaction, effective communication, and leading by example, are currently beyond the capabilities of LLMs. LLMs learn from a large amount of data and provide feedback, representing subjective educational information with data and providing\n\nrational reflections of human thinking. The goal of anthropomorphizing LLMs is to enable NLP models, such as Word2Vec, to convert words into vectors, facilitating the computer's processing of textual data [4]. GPT-1 and BERT, based on the self-attention mechanism [40], further enhance performance. GPT-3 achieves another leap in performance on zero-shot learning tasks with its significantly increased parameter scale [116]. ChatGPT's HFRL, code pretraining, and instruction fine-tuning improve the model's inference capabilities [86]. GPT-4, an ultra-large-scale multimodal pre-trained model, possesses multimodal understanding and multi-type content generation capabilities [62]. These examples show ideas for solving the problem of anthropomorphizing LLMs, gradually approaching human-like capabilities through continuous optimization and development, thereby alleviating the limitations of the abstraction and ambiguity of educational rules.\n\nSome drawbacks when students use LLMs. The occasional inaccuracies in LLM's answers can mislead students who lack critical thinking skills. The great convenience of LLM may reduce students' desire for independent learning and innovation, leading to intellectual laziness. As LLM involves massive amounts of data, students who lack awareness of data security may unknowingly leak their personal data [129]. While LLM provides interactive dialogue scenarios and opportunities for AI communication with students, it reduces real interpersonal conversations, and the way of discussing problems may shift from online to one-sided questioning of the machine, affecting the development of student's social skills. In response to these problems, educators need to actively guide students to adapt to the characteristics of LLM-assisted education and enhance the cultivation of privacy and security awareness.\n\nInsufficient integration of LLMs in collaborative teaching [71]. Although LLM has achieved some level of one-on-one dialogue and communication, its integration with education in real life is still limited. The ability to solve higher-order reasoning problems and complex problems still needs improvement. For example, while GPT-4 performs reasonably well in some exams, it fails to demonstrate significant advantages in logical reasoning problems [70]. Most LLMs have high accuracy rates (up to  $95\\%$ ) for reasoning with a small number of steps, but as the number of steps increases, reaching 20 or more, the accuracy drops significantly to  $36\\%$ , indicating a significant disparity [90]. As a result, it is necessary to develop chain-of-thought technology to improve LLMs' reasoning ability and ability to solve complex problems [117], thereby promoting the integration of large models and collaborative education.\n\nLimitations of LLMs [107]. Firstly, in pre-training, models that simultaneously satisfy the reasonable model size, advanced few-shot learning capability, and advanced fine-tuning capability have not been achieved yet. For example, GPT-3 lacks a reasonable model size and is relatively large in scale [16]. Furthermore, the high complexity and strong data dependency of LLMs may be exploited by malicious data to affect their training process and generation\n\nresults, as well as output uncertainty and other factors. The lack of interpretability in LLMs' technology makes their internal mechanisms unclear. The widespread application of LMs requires interpretability to ensure application security, overcome performance limitations, and control societal impact, which has triggered corresponding considerations regarding these issues. In the future, LLM's technology still needs optimization and innovation, and researchers need to consider the interpretability of the model more based on the user's situation.\n\n# 6.2. Main challenges\n\nTechnological challenges. The application of LLMEdu relies on AI-based technologies, which are complex and challenging. If the technology is not perfected, it becomes difficult to provide high-quality educational services. The availability of high-quality data sources is one important factor influencing the improvement of LLM technology. High-quality data transformation involves capture and conversion processes. It is necessary to consider how to expand the perception of the educational field to capture dynamic performance data from any learning activity in educational subjects and how to improve the quality of the data through efficient processing. Moreover, LLMEdu faces technological challenges such as speech recognition, NLP, AIGC [119], multimodal LLMs [120], and other aspects. The above-mentioned issues require researchers to always pay attention to the development of other technologies in the AI field and actively integrate them into LLM to bring a better experience to the education industry.\n\nArtificial intelligence security. The intelligence level of LLMs continues to improve, and security issues have become more severe. The first is the LLMs' biased cognition. Some studies have pointed out that when LLMs are tested using gender bias data sets, their answers will reflect gender bias [57]. Therefore, when training an LLM, the data should be filtered. The second is the lack of correct social, moral, and ethical values. For some issues that violate social ethics, LLMs are unable to judge, which increases the risk of crime. Therefore, the country should formulate a more complete legal system to regulate the use of LLMs. The third is the most common issue among artificial intelligence ethical issues: \"AI replaces human activities\". AI has limitations in education. While AI has great potential in education, it cannot replace the role of teachers, such as encouraging critical thinking, solving complex problems, and providing psychological and social support. However, humans should also flexibly adjust their roles, regulate and guide the development of AI from an ethical perspective, and maintain their dominant position.\n\nEducation quality. The use of LLMedu provides many opportunities for smart education, but it also presents challenges in terms of quality. If LLMedu cannot provide high-quality educational services, it will be difficult to gain recognition from students and teachers. Furthermore, educational institutions that use LMs must strike a balance between educational quality and technological innovation. Otherwise,\n\nthere may be an overreliance on technology, neglecting the quality of education itself. Therefore, to ensure the quality of education, the first consideration is to ensure the educational content, which requires educators to adjust reasonable teaching content and clarify the auxiliary functions of LLMs. Then, technology developers are required to ensure that the technology of LLMs is steadily progressing.\n\nTechnological dependence. Note that the future LLMEd should be human-centric but not technology-centric [127]. Overreliance on AI may reduce students' ability for independent learning and innovative thinking, and it may even lead to cheating and academic misconduct, such as using ChatGPT to complete assignments and papers. It is necessary to prevent the passive application of LLMs, as seen in the examples in reality. While using AI, the student should be encouraged to think independently, explore problems, and find answers. Furthermore, students should be educated on time management, ensuring sufficient time for other important activities while using AI, and avoiding excessive dependence on it.\n\nTechnical accessibility and training. The introduction of AI technology requires corresponding hardware infrastructure and network support. In resource-limited areas, this can be a challenge. Combined with the pressures and entrenched thinking that fear is being replaced [126], there is a phenomenon of fear and refusal to use AI in education, in other words, cognitive limitations. In such cases, technical access and training become difficult. Therefore, efforts should be made to promote the long-term advantages of AI in the education industry, guide teachers and students to receive appropriate training, better understand the application ideas and specific methods of intelligent technology, enhance willingness to use, and better adapt to and utilize these tools.\n\nEquity issues. Although AI has the potential to improve the quality and efficiency of education, its use can lead to unfairness among students. For example, some families may not be able to afford AI learning tools, or in certain areas, students may lack access to the necessary technological facilities for tools like ChatGPT. Educational equity is the cornerstone of social development, and interventions are needed to address the examples mentioned above effectively. For instance, when designing and optimizing LLMs, efforts should be made to balance characteristics such as race, gender, and age, reducing the digital divide and gender gap.\n\nData privacy and security [129]. Data privacy, including privacy protection, is a significant concern in the application of LLMs. LLMs involve collecting personal information and learning data from students and teachers. Therefore, privacy protection becomes an important issue in LLM applications. Educational institutions need to ensure the effective protection of student's and teacher's privacy while also ensuring the security and reliability of the data. Parents and teachers should focus on cultivating children's awareness of data privacy and security, as well as educating students to avoid privacy risks associated with the use of LLMs. Moreover, when collecting and processing student's\n\nlearning data, it is essential to ensure that this information is properly protected to avoid data breaches or improper use.\n\nIn the future, following the development characteristics of the era of integrating intelligence and education, while continuing to optimize core technologies and technological innovations, LLMs such as ChatGPT, GPT-4, and MathGPT will continue to empower the education field. Moreover, based on the existing LLMs, we must continue to look for more effective training methods to more efficiently train models with large-scale parameters [11].\n\n# 7. Conclusion\n\nIn this article, we have introduced the development and application of LLMs in the field of education as comprehensively as possible. There are still some technologies that have not been included, as well as other issues that have not been discussed in depth. It is hoped that the technology introduced in this article and the thinking presented can help scholars and researchers better develop and optimize educational LLMs. This article summarizes the process of integrating education and LLMs. LLMs have excellent language generation and interactive capabilities that cannot be provided by traditional book-based teaching. It demonstrates the creative role of AI in education, as well as teachers, and the changing roles of parents and students. For smart education, we call for more mature education and AI development standards, technical specifications, and data security guidelines to focus on more practical issues. How to ensure data security? How can we limit the behavior that relies too much on AI technology? How to cultivate students' active exploration abilities? LLMs and education complement each other. The application of LLMs in education makes education more intelligent and efficient, and the data accumulated over many years in education can help optimize LLM training. More attention should be paid to these development conditions. How can we create more valuable LLM.edu application scenarios? We look forward to the future of LLM.edu.\n\nAcknowledgments This research was supported in part by the National Natural Science Foundation of China (No. 62272196), the Natural Science Foundation of Guangdong Province (No. 2022A1515011861), Guangzhou Basic and Applied Basic Research Foundation (No. 2024A04J9971).\n\nAuthor contributions Hanyi Xu: paper reading and review, writing original draft. Wensheng Gan: conceptualization, review and editing, supervisor. Zhenlian Qi: conceptualization, review and editing. Jiayang Wu: writing original draft. Philip S. Yu: review and editing.\n\nData availability This is a review paper, and no data was generated during the study.\n\nConflict of interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n# References\n\n[1] Ahmad, N., Murugesan, S., Kshetri, N., 2023. Generative Artificial Intelligence and the Education Sector. Computer 56, 72-76.  \n[2] Al-Garaady, J., Mahyoob, M., 2023. ChatGPT's Capabilities in Spotting and Analyzing Writing Errors Experienced by EFL Learners. Arab World English Journals.  \n[3] Amer-Yahia, S., Bonifati, A., Chen, L., Li, G., Shim, K., Xu, J., Yang, X., 2023. From Large Language Models to Databases and Back: A Discussion on Research and Education. ArXiv E-prints, arXiv:2306.01388.  \n[4] Amin, M.M., Cambria, E., Schuller, B.W., 2023. Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT. ArXiv E-prints, arXiv:2303.03186.  \n[5] Bahrami, M., Srinivasan, R., 2023. Examining LLM's Awareness of the United Nations Sustainable Development Goals, in: ICLR Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models.  \n[6] Bai, K., Shrivastava, A., 2010. Heap Data Management for Limited Local Memory Multi-Core Processors, in: Proceedings of the Eighth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis, ACM. p. 317-326.  \n[7] Baidoo-Anu, D., Ansah, L.O., 2023. Education in the Era of Generative Artificial Intelligence: Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning. Journal of AI 7, 52-62.  \n[8] Bakker, M., Chadwick, M., Sheahan, H., Tessler, M., Campbell-Gillingham, L., Balaguer, J., McAleese, N., Glaese, A., Aslanides, J., Botvinick, M., et al., 2022. Fine-tuning Language Models to Find Agreement among Humans with Diverse Preferences. Advances in Neural Information Processing Systems 35, 38176-38189.  \n[9] Bao, H., Dong, L., Wei, F., Wang, W., Yang, N., Liu, X., Wang, Y., Gao, J., Piao, S., Zhou, M., et al., 2020. UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training, in: International Conference on Machine Learning, PMLR. pp. 642–652.  \n[10] Beck, J., Stern, M., Haugsjaa, E., 1996. Applications of AI in Education. XRDS: Crossroads, The ACM Magazine for Students 3, 11-15.  \n[11] Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S., 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, in: ACM Conference on Fairness, Accountability, and Transparency, pp. 610-623.  \n[12] Bhutoria, A., 2022. Personalized Education and Artificial Intelligence in the United States, China, and India: A Systematic Review Using A Human-in-the-loop Model. Computers and Education: Artificial Intelligence 3, 100068.  \n[13] Biggs, J., Tang, C., Kennedy, G., 2022. Ebook: Teaching for Quality Learning at University 5e. McGraw-hill education (UK).  \n[14] Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G.B., Lespiau, J.B., Damoc, B., Clark, A., et al., 2022. Improving Language Models by Retrieving from Trillions of Tokens, in: International Conference on Machine Learning, PMLR. pp. 2206-2240.  \n[15] Brem, A., Giones, F., Werle, M., 2021. The AI Digital Revolution in Innovation: A Conceptual Framework of Artificial Intelligence Technologies for the Management of Innovation. IEEE Transactions on Engineering Management 70, 770-776.  \n[16] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020. Language Models are Few-shot lLarners. Advances in Neural Information Processing Systems 33, 1877-1901.  \n[17] Budiharso, T., Tarman, B., 2020. Improving Quality Education through Better Working Conditions of Academic Institutes. Journal of Ethnic and Cultural Studies 7, 99-115.  \n[18] Bunnell, T., Courtois, A., Donnelly, M., 2020. British Elite Private Schools and Their Overseas Branches: Unexpected Actors in the Global Education Industry. British Journal of Educational Studies 68, 691-712.\n\n[19] Butcher, K.R., Sumner, T., 2011. Self-Directed Learning and the Sensemaking Paradox. Human-Computer Interaction 26, 123–159.  \n[20] Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L., Yi, X., Wang, C., Wang, Y., et al., 2023. A Survey on Evaluation of Large Language Models. ArXiv E-prints, arXiv:2307.03109.  \n[21] Chen, L., Chen, P., Lin, Z., 2020a. Artificial Intelligence in Education: A Review. IEEE Access 8, 75264-75278.  \n[22] Chen, X., Xie, H., Hwang, G.J., 2020b. A Multi-perspective Study on Artificial Intelligence in Education: Grants, Conferences, Journals, Software Tools, Institutions, and Researchers. Computers and Education: Artificial Intelligence 1, 100005.  \n[23] Chen, X., Xie, H., Zou, D., Hwang, G.J., 2020c. Application and Theory Gaps During the Rise of Artificial Intelligence in Education. Computers and Education: Artificial Intelligence 1, 100002.  \n[24] Cheng, X., Jiao, F., Ji, G., Tian, Y., 2023. The Artificial Intelligence Revolution Led by ChatGPT, in: International Seminar on Computer Science and Engineering Technology, IEEE. pp. 360-363.  \n[25] Chung, Y.A., Zhang, Y., Han, W., Chiu, C.C., Qin, J., Pang, R., Wu, Y., 2021. W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-supervised Speech Pre-training, in: IEEE Automatic Speech Recognition and Understanding Workshop, IEEE. pp. 244-250.  \n[26] Deng, Y., Liu, X., Meng, L., Jiang, W., Dong, Y., Liu, C., 2023. Multi-Modal Information Fusion for Action Unit Detection in the Wild, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 5855–5862.  \n[27] DeRose, J.F., Wang, J., Berger, M., 2020. Attention flows: Analyzing and Comparing Attention Mechanisms in Language Models. IEEE Transactions on Visualization and Computer Graphics 27, 1160-1170.  \n[28] Dillenbourg, P., 2016. The Evolution of Research on Digital Education. International Journal of Artificial Intelligence in Education 26, 544-560.  \n[29] Dong, L., Jiang, F., Peng, Y., Wang, K., Yang, K., Pan, C., Schober, R., 2023. LAMBO: Large Language Model Empowered Edge Intelligence. ArXiv E-prints, arXiv:2308.15078.  \n[30] Edyko, K., Petryla, P., Ostafin, K., Minkner, M., Bienkowski, B., Feja, K., Suwała, Z., Rektor, N., Luczak, E., Marchewka, U., 2023. Utilizing Artificial Intelligence Tools Using the GPT Chatbot in Medicine-A Review of Flaws, Advantages, and Limitations. Journal of Education, Health and Sport 46, 122-133.  \n[31] Elnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M., et al., 2021. ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 7112-7127.  \n[32] Fan, W., Zhao, Z., Li, J., Liu, Y., Mei, X., Wang, Y., Tang, J., Li, Q., 2023a. Recommender Systems in the Era of Large Language Models (LLMs). ArXiv E-prints, arXiv:2307.02046.  \n[33] Fan, Y., Jiang, F., Li, P., Li, H., 2023b. GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning, in: Natural Language Processing and Chinese Computing, Springer Nature Switzerland. pp. 69–80.  \n[34] Gan, W., Lin, J.C.W., Chao, H.C., Yu, P.S., 2023a. Discovering high utility episodes in sequences. IEEE Transactions on Artificial Intelligence 4, 473-486.  \n[35] Gan, W., Lin, J.C.W., Fournier-Viger, P., Chao, H.C., Tseng, V.S., Yu, P.S., 2021. A Survey of Utility-oriented Pattern Mining. IEEE Transactions on Knowledge and Data Engineering 33, 1306-1327.  \n[36] Gan, W., Qi, Z., Wu, J., Lin, J.C.W., 2023b. Large Language Models in Education: Vision and Opportunities, in: IEEE International Conference on Big Data, IEEE. pp. 4776-4785.  \n[37] Gan, W., Wan, S., Yu, P.S., 2023c. Model-as-a-Service (MaaS): A Survey, in: IEEE International Conference on Big Data, IEEE. pp. 4636-4645.  \n[38] Gan, W., Ye, Z., Wan, S., Yu, P.S., 2023d. Web 3.0: The Future of Internet, in: Companion Proceedings of the ACM Web Conference,\n\npp. 1266-1275.  \n[39] Gao, B., Cai, K., Qu, T., Hu, Y., Chen, H., 2020. Personalized Adaptive Cruise Control Based on Online Driving Style Recognition Technology and Model Predictive Control. IEEE Transactions on Vehicular Technology 69, 12482-12496.  \n[40] Ghojogh, B., Ghodsi, A., 2020. Attention mechanism, transformers, bert, and gpt: tutorial and survey.  \n[41] Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., Poon, H., 2021. Domain-specific Language Model Pretraining for Biomedical Natural Language Processing. ACM Transactions on Computing for Healthcare 3, 1-23.  \n[42] Guu, K., Lee, K., Tung, Z., Pasupat, P., Chang, M., 2020. Retrieval Augmented Language Model Pre-Training, in: International Conference on Machine Learning, PMLR. pp. 3929-3938.  \n[43] Han, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao, H., Zhang, K., Liu, C., Wen, S., Guo, Z., et al., 2023. ImageBind-LLM: Multi-modality Instruction Tuning. ArXiv E-prints, arXiv:2309.03905.  \n[44] Han, J.M., Rute, J., Wu, Y., Ayers, E.W., Polu, S., 2021. Proof Artifact Co-training for Theorem Proving with Language Models. ArXiv E-prints, arXiv:2102.06203.  \n[45] Hawley, R., Allen, C., 2018. Student-generated Video Creation for Assessment: Can It Transform Assessment Within Higher Education? International Journal for Transformative Research 5, 1-11.  \n[46] Hsu, H.P., Wenting, Z., Hughes, J.E., 2019. Developing Elementary Students' Digital Literacy Through Augmented Reality Creation: Insights From a Longitudinal Analysis of Questionnaires, Interviews, and Projects. Journal of Educational Computing Research 57, 1400-1435.  \n[47] Hu, L., Liu, Z., Zhao, Z., Hou, L., Nie, L., Li, J., 2023. A Survey of Knowledge Enhanced Pre-trained Language Models. IEEE Transactions on Knowledge and Data Engineering, 1-19.  \n[48] Huang, G., Gan, W., Weng, J., Yu, P.S., 2023a. US-Rule: Discovering Utility-driven Sequential Rules. ACM Transactions on Knowledge Discovery from Data 17, 1-22.  \n[49] Huang, H., Zheng, O., Wang, D., Yin, J., Wang, Z., Ding, S., Yin, H., Xu, C., Yang, R., Zheng, Q., et al., 2023b. ChatGPT for Shaping the Future of Dentistry: the Potential of Multi-modal Large Language Model. International Journal of Oral Science 15, 29.  \n[50] Huang, J., Chang, K.C.C., 2022. Towards Reasoning in Large Language Models: A Survey. ArXiv E-prints, arXiv:2212.10403.  \n[51] Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu, J., Lv, C., Zhang, Y., Lei, J., et al., 2023c. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. ArXiv E-prints, arXiv:2305.08322.  \n[52] Ivanov, S., Soliman, M., 2023. Game of Algorithms: ChatGPT Implications for the Future of Tourism Education and Research. Journal of Tourism Futures 9, 214-221.  \n[53] Jeon, J., Lee, S., 2023. Large Language Models in Education: A Focus on the Complementary Relationship between Human Teachers and ChatGPT. Education and Information Technologies 28, 15873-15892.  \n[54] Kim, J.W., Yoon, H., Jung, H.Y., 2022. Improved Spoken Language Representation for Intent Understanding in a Task-Oriented Dialogue System. Sensors 22, 1509.  \n[55] Koksal, I., 2020. The Rise of Online Learning. FORBES.  \n[56] Kopnina, H., 2020. Education for the Future? Critical Evaluation of Education for Sustainable Development Goals. The Journal of Environmental Education 51, 280-291.  \n[57] Kotek, H., Dockum, R., Sun, D., 2023. Gender Bias and Stereotypes in Large Language Models, in: The ACM Collective Intelligence Conference, pp. 12-24.  \n[58] Lai, J., Gan, W., Wu, J., Qi, Z., Yu, P.S., 2023. Large Language Models in Law: A survey. arXiv preprint arXiv:2312.03718.  \n[59] Latif, E., Mai, G., Nyaaba, M., Wu, X., Liu, N., Lu, G., Li, S., Liu, T., Zhai, X., 2023. Artificial General Intelligence for Education. ArXiv E-prints, arXiv:2304.12479.  \n[60] Li, L., 2020. Education Supply Chain in the Era of Industry 4.0. Systems Research and Behavioral Science 37, 579-592.\n\n[61] Li, S., Challoo, R., 2006. Restructuring An Electric Machinery Course with An Integrative Approach and Computer-assisted Teaching Methodology. IEEE Transactions on Education 49, 16-28.  \n[62] Li, Y., Hu, B., Chen, X., Ma, L., Xu, Y., Zhang, M., 2023. LMEye: An Interactive Perception Network for Large Language Models. ArXiv E-prints, arXiv:2305.03701.  \n[63] Li, Y., Zhao, J., Zheng, D., Hu, Z.Y., Chen, Z., Su, X., Huang, Y., Huang, S., Lin, D., Lyu, M.R., et al., 2023. CLEVA: Chinese Language Models EVALuation Platform. ArXiv E-prints, arXiv:2308.04813.  \n[64] Liang, W., Zhang, Y., Cao, H., Wang, B., Ding, D., Yang, X., Vodrahalli, K., He, S., Smith, D., Yin, Y., McFarland, D., Zou, J., 2023. Can Large Language Models Provide Useful Feedback on Research Papers? A Large-scale Empirical Analysis. ArXiv E-prints, arXiv:2310.01783.  \n[65] Lim, J., Sa, I., MacDonald, B., Ahn, H.S., 2023. A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM. ArXiv EA-prints, arXiv:2309.16898.  \n[66] Lin, H., Wan, S., Gan, W., Chen, J., Chao, H.C., 2022. Metaverse in Education: Vision, Opportunities, and Challenges, in: IEEE International Conference on Big Data, IEEE. pp. 2857-2866.  \n[67] Lin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Li, Y., Lin, W., et al., 2021. M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. ArXiv E-prints, arXiv:2110.03888.  \n[68] Lin, J.C.W., Gan, W., Fournier-Viger, P., Hong, T.P., 2015. Mining High-utility Itemsets with Multiple Minimum Utility Thresholds, in: The Eighth International C* Conference on Computer Science & Software Engineering, pp. 9-17.  \n[69] Liu, C., Jin, R., Ren, Y., Yu, L., Dong, T., Peng, X., Zhang, S., Peng, J., Zhang, P., Lyu, Q., et al., 2023. M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. ArXiv E-prints, arXiv:2305.10263.  \n[70] Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., Zhang, Y., 2023. Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. ArXiv E-prints, arXiv:2304.03439.  \n[71] Liu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He, M., Liu, Z., et al., 2023. Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models. Meta-Radiology 1, 100017.  \n[72] Luckin, R., Holmes, W., 2016. Intelligence Unleashed: An Argument for AI in Education.  \n[73] Lv, Z., Han, Y., Singh, A.K., Manogaran, G., Lv, H., 2020. Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence. IEEE Transactions on Industrial Informatics 17, 1496-1504.  \n[74] Lyu, C., Xu, J., Wang, L., 2023. New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT. ArXiv E-prints, arXiv:2305.01181.  \n[75] Ma, X., Fang, G., Wang, X., 2023. LLM-Pruner: On the Structural Pruning of Large Language Models. ArXiv E-prints, arXiv:2305.11627.  \n[76] Maddigan, P., Susnjak, T., 2023. Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models. IEEE Access 11, 45181-45193.  \n[77] Malodia, S., Islam, N., Kaur, P., Dhir, A., 2021. Why Do People Use Artificial Intelligence-Enabled Voice Assistants? IEEE Transactions on Engineering Management, 1-15.  \n[78] Meng, Y., Zhang, Y., Huang, J., Xiong, C., Ji, H., Zhang, C., Han, J., 2020. Text Classification Using Label Names Only: A Language Model Self-Training Approach. ArXiv E-prints, arXiv:2010.07245.  \n[79] Mhlanga, D., 2023. Open AI in Education, the Responsible and Ethical Use of ChatGPT Towards Lifelong Learning, in: FinTech and Artificial Intelligence for Sustainable Development: The Role of Smart Technologies in Achieving Development Goals. Springer, pp. 387-409.  \n[80] Morales, E.F., Escalante, H.J., 2022. A Brief Introduction to Supervised, Unsupervised, and Reinforcement Learning, in: Biosignal Processing and Classification Using Computational Learning and\n\nIntelligence. Academic Press, pp. 111-129.  \n[81] Moura, L.d., Ullrich, S., 2021. The Lean 4 Theorem Prover and Programming Language, in: Automated Deduction - CADE 28, Springer International Publishing. pp. 625-635.  \n[82] Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., et al., 2021. Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM, in: The International Conference for High Performance Computing, Networking, Storage and Analysis, ACM. pp. 1-15.  \n[83] Naseem, U., Razzak, I., Khan, S.K., Prasad, M., 2021. A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models. Transactions on Asian and Low-Resource Language Information Processing 20, 1–35.  \n[84] Ng, E., Subramanian, S., Klein, D., Kanazawa, A., Darrell, T., Ginosar, S., 2023. Can Language Models Learn to Listen?, in: The IEEE/CVF International Conference on Computer Vision, pp. 10083-10093.  \n[85] Ouyang, F., Jiao, P., 2021. Artificial Intelligence in Education: The Three Paradigms. Computers and Education: Artificial Intelligence 2, 100020.  \n[86] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al., 2022. Training Language Models to Follow Instructions with Human Feedback. Advances in Neural Information Processing Systems 35, 27730-27744.  \n[87] P, D., 2020. AI in the Wild: Sustainability in the Age of Artificial Intelligence. MIT Press.  \n[88] Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J., Wu, X., 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. ArXiv E-prints, arXiv:2306.08302.  \n[89] Pankiewicz, M., Baker, R.S., 2023. Large Language Models (GPT) for Automating Feedback on Programming Assignments. ArXiv E-prints, arXiv:2307.00150.  \n[90] Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., Tulio Ribeiro, M., 2023. ART: Automatic Multi-step Reasoning and Tool-use for Large Language Models. ArXiv E-prints, arXiv:2303.09014.  \n[91] Philippe, S., Souchet, A.D., Lameras, P., Petridis, P., Caporal, J., Coldeboeuf, G., Duzan, H., 2020. Multimodal Teaching, Learning and Training in Virtual Reality: A Review and Case Study. Virtual Reality & Intelligent Hardware 2, 421-442.  \n[92] Qidwai, U., Kashem, S.B.A., Conor, O., 2020. Humanoid Robot as a Teacher's Assistant: Helping Children with Autism to Learn Social and Academic Skills. Journal of Intelligent & Robotic Systems 98, 759-770.  \n[93] Rajbhandari, S., Rasley, J., Ruwase, O., He, Y., 2020. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, in: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE. pp. 1-16.  \n[94] Rawte, V., Sheth, A., Das, A., 2023. A Survey of Hallucination in Large Foundation Models. ArXiv E-prints, arXiv:2309.05922.  \n[95] Rudovic, O., Zhang, M., Schuller, B., Picard, R., 2019. MultiModal Active Learning From Human Data: A Deep Reinforcement Learning Approach, in: International Conference on Multimodal Interaction, pp. 6-15.  \n[96] Saini, M.K., Goel, N., 2019. How Smart Are Smart Classrooms? A Review of Smart Classroom Technologies. ACM Computing Survey 52, 1-28.  \n[97] Scarlatos, A., Lan, A., 2023. Tree-Based Representation and Generation of Natural and Mathematical Language. ArXiv E-prints, arXiv:2302.07974.  \n[98] Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., Scialom, T., 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. ArXiv Eprints, arXiv:2302.04761.\n\n[99] Schlecker Lamoureux, P., Winther, K.T., Garrido Torres, J.A., Streibel, V., Zhao, M., Bajdich, M., Abild-Pedersen, F., Bligaard, T., 2019. Machine Learning for Computational Heterogeneous Catalysis. ChemCatChem 11, 3581-3601.  \n[100] Schwartz, R., Dodge, J., Smith, N.A., Etzioni, O., 2020. Green AI. Communications of the ACM 63, 54-63.  \n[101] Srinivas Tida, V., Hsu, S., 2022. Universal Spam Detection using Transfer Learning of BERT Model. ArXiv E-prints, arXiv:2202.03480.  \n[102] Su, H.F.H., Ricci, F.A., Mnatsakanian, M., 2016. Mathematical Teaching Strategies: Pathways to Critical Thinking and Metacognition. International Journal of Research in Education and Science 2, 190–200.  \n[103] Sun, J., Gan, W., Chao, H.C., Yu, P.S., Ding, W., 2023. Internet of Behaviors: A Survey. IEEE Internet of Things Journal 10, 11117-11134.  \n[104] Tan, M., Le, Q., 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, in: The 36th International Conference on Machine Learning, PMLR. pp. 6105-6114.  \n[105] Tang, Y., Liang, J., Hare, R., Wang, F.Y., 2020. A Personalized Learning System for Parallel Intelligent Education. IEEE Transactions on Computational Social Systems 7, 352-361.  \n[106] Tao, S., Qiu, R., Ping, Y., Ma, H., 2021. Multi-modal Knowledge-aware Reinforcement Learning Network for Explainable Recommendation. Knowledge-Based Systems 227, 107217.  \n[107] Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., Ting, D.S.W., 2023. Large language models in medicine. Nature Medicine 29, 1930-1940.  \n[108] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.T., Jin, A., Bos, T., Baker, L., Du, Y., et al., 2022. Language Models for Dialog Applications. arXiv preprint, arXiv:2201.08239.  \n[109] Tirumala, K., Markosyan, A., Zettlemoyer, L., Aghajanyan, A., 2022. Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models. Advances in Neural Information Processing Systems 35, 38274-38290.  \n[110] Valverde Valencia, Å., 2023. An Interdisciplinary and Applied Approach to Generative Artificial Intelligence in Secondary School for the Development of Communicative Competencies.  \n[111] Wang, C.X., Di Renzo, M., Stanczak, S., Wang, S., Larsson, E.G., 2020a. Artificial Intelligence Enabled Wireless Networking for 5G and Beyond: Recent Advances and Future Challenge. IEEE Wireless Communications 27, 16-23.  \n[112] Wang, D., Weisz, J.D., Muller, M., Ram, P., Geyer, W., Dugan, C., Tausczik, Y., Samulowitz, H., Gray, A., 2019. Human-AI Collaboration in Data Science: Exploring Data Scientists' Perceptions of Automated AI. The ACM on Human-Computer Interaction 3, 1–24.  \n[113] Wang, H., Yeung, D.Y., 2020. A Survey on Bayesian Deep Learning. ACM Computing Survey 53, 1-37.  \n[114] Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., Zhou, M., 2020b. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. Advances in Neural Information Processing Systems 33, 5776–5788.  \n[115] Wang, Y., Chu, Z., Ouyang, X., Wang, S., Hao, H., Shen, Y., Gu, J., Xue, S., Zhang, J.Y., Cui, Q., et al., 2023. Enhancing Recommender Systems with Large Language Model Reasoning Graphs. ArXiv E-prints, arXiv:2308.10835.  \n[116] Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V., 2021. Finetuned Language Models Are Zero-Shot Learners. ArXiv E-prints, arXiv:2109.01652.  \n[117] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al., 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 24824-24837.  \n[118] Williamson, B., Macgilchrist, F., Potter, J., 2023. Re-examining AI, Automation and Datafication in Education. Learning, Media and Technology 48, 1-5.\n\n[119] Wu, J., Gan, W., Chen, Z., Wan, S., Lin, H., 2023a. AI-Generated Content (AIGC): A Survey. arXiv preprint arXiv:2304.06632.  \n[120] Wu, J., Gan, W., Chen, Z., Wan, S., Yu, P.S., 2023b. Multimodal Large Language Models: A Survey, in: IEEE International Conference on Big Data, IEEE. pp. 2247-2256.  \n[121] Wu, T., Zhu, B., Zhang, R., Wen, Z., Ramchandran, K., Jiao, J., 2023c. Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. arXiv preprint arXiv:2310.00212.  \n[122] Xie, H., Qin, Z., Li, G. Y., Juang, B. H., 2021. Deep Learning Enabled Semantic Communication Systems. IEEE Transactions on Signal Processing 69, 2663-2675.  \n[123] Xu, H., 2023. No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. ArXiv E-prints, arXiv:2309.03224.  \n[124] Xu, L., Li, A., Zhu, L., Xue, H., Zhu, C., Zhao, K., He, H., Zhang, X., Kang, Q., Lan, Z., 2023. SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. ArXiv E-prints, arXiv:2307.15020.  \n[125] Yan, K., Cai, J., Jin, D., Miao, S., Guo, D., Harrison, A.P., Tang, Y., Xiao, J., Lu, J., Lu, L., 2022. Self-Supervised Learning of Pixel-Wise Anatomical Embeddings in Radiological Images. IEEE Transactions on Medical Imaging 41, 2658-2669.  \n[126] Yan, L., Sha, L., Zhao, L., Li, Y., Martinez-Maldonado, R., Chen, G., Li, X., Jin, Y., Gašević, D., 2024. Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review. British Journal of Educational Technology 55, 90-112.  \n[127] Yang, R., Li, L., Gan, W., Chen, Z., Qi, Z., 2023. The Human-centric Metaverse: A Survey, in: Companion Proceedings of the ACM Web Conference, pp. 1296-1306.  \n[128] Yang, W., Li, H., 2019. Changing Culture, Changing Curriculum: A Case Study of Early Childhood Curriculum Innovations in Two Chinese Kindergartens. The Curriculum Journal 30, 279–297.  \n[129] Yu, Z., Wu, Y., Zhang, N., Wang, C., Vorobeychik, Y., Xiao, C., 2023. CodeIPPrompt: Intellectual Property Infringement Assessment of Code Language Models, in: International Conference on Machine Learning, PMLR. pp. 40373-40389.  \n[130] Zamfirescu-Pereira, J., Wong, R.Y., Hartmann, B., Yang, Q., 2023. Why Johnny Can't Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts, in: CHI Conference on Human Factors in Computing Systems, Curran Associates, Inc.. pp. 1-21.  \n[131] Zeng, F., Gan, W., Wang, Y., Liu, N., Yu, P.S., 2023a. Large Language Models for Robotics: A Survey. arXiv preprint arXiv:2311.07226.  \n[132] Zeng, F., Gan, W., Wang, Y., Yu, P.S., 2023b. Distributed Training of Large Language Models, in: IEEE 29th International Conference on Parallel and Distributed Systems, IEEE. pp. 840-847.  \n[133] Zeng, H., 2023. Measuring Massive Multitask Chinese Understanding. ArXiv E-prints, arXiv:2304.12986.  \n[134] Zeng, Y., Mahmud, T., 2023. ChatGPT in English Class: Perspectives of Students and Teachers from Swedish Upper Secondary Schools.  \n[135] Zhang, C., Dai, Q., Du, Z., Gan, W., Weng, J., Yu, P.S., 2023a. TUSQ: Targeted High-Utility Sequence Querying. IEEE Transactions on Big Data 9, 512–527.  \n[136] Zhang, C., Zhang, C., Zheng, S., Qiao, Y., Li, C., Zhang, M., Dam, S.K., Thwal, C.M., Tun, Y.L., Huy, L.L., et al., 2023b. A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need? ArXiv E-prints, arXiv:2303.11717.  \n[137] Zhang, M., Li, J., 2021. A Commentary of GPT-3 in MIT Technology Review. Fundamental Research 1, 831–833.  \n[138] Zhao, L., 2022. A Study on Data-Driven Teaching Decision Optimization of Distance Education Platforms. International Journal of Emerging Technologies in Learning 17.  \n[139] Zhao, S., Blaabjerg, F., Wang, H., 2020. An Overview of Artificial Intelligence Applications for Power Electronics. IEEE Transactions on Power Electronics 36, 4633-4658.  \n[140] Zheng, R., Dou, S., Gao, S., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Xiong, L., Chen, L., et al., 2023. Secrets of RLHF in Large\n\nLanguage Models Part I: PPO. ArXiv E-prints, arXiv:2307.04964.  \n[141] Zhipeng, G., Yi, X., Sun, M., Li, W., Yang, C., Liang, J., Chen, H., Zhang, Y., Li, R., 2019. Jiuge: A Human-Machine Collaborative Chinese Classical Poetry Generation System, 25-30.  \n[142] Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., Duan, N., 2023. AGIEval: A Human-centric Benchmark for Evaluating Foundation Models. ArXiv E-prints, arXiv:2304.06364.  \n[143] Zou, L., Zhang, S., Cai, H., Ma, D., Cheng, S., Wang, S., Shi, D., Cheng, Z., Yin, D., 2021. Pre-Trained Language Model Based Ranking in Baidu Search, in: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ACM. pp. 4014-4022.",
    "translated_content": null,
    "created_at": "2025-12-16 07:25:47.506741",
    "updated_at": "2025-12-16 07:25:54.602329",
    "doi": null,
    "arxiv_id": "2405.13001",
    "embedding": [
      0.7890625,
      -1.9609375,
      -1.1796875,
      -2.65625,
      -0.267578125,
      1.1796875,
      -1.8828125,
      -2.484375,
      -0.87109375,
      2.328125,
      2.421875,
      2.359375,
      2.53125,
      2.53125,
      0.7578125,
      3.40625,
      -1.7890625,
      -0.5078125,
      1.859375,
      -5.75,
      -0.0927734375,
      0.95703125,
      0.53125,
      -4.90625,
      4.59375,
      -7.21875,
      -3.40625,
      4.21875,
      1.8125,
      0.60546875,
      8.1875,
      -4,
      0.2119140625,
      0.515625,
      -1.2265625,
      0.34765625,
      -2.59375,
      -1.1796875,
      4.125,
      3.359375,
      -7.1875,
      1.9453125,
      0.181640625,
      5.21875,
      -0.09814453125,
      3.1875,
      0.8203125,
      0.59375,
      -7.15625,
      -1.71875,
      -5.09375,
      -1.671875,
      7.53125,
      0.177734375,
      3.1875,
      -3.015625,
      -5.59375,
      5.1875,
      -3.890625,
      -0.43359375,
      1.8671875,
      0.396484375,
      0.412109375,
      -1.0703125,
      5.40625,
      3.109375,
      0.44140625,
      0.67578125,
      -2.078125,
      2.109375,
      0.87890625,
      2.46875,
      5.59375,
      -3.90625,
      7.78125,
      6.96875,
      3.484375,
      2.609375,
      -2.3125,
      3.5,
      -5.8125,
      5.34375,
      5.28125,
      -1.4453125,
      5.09375,
      2.828125,
      1.359375,
      0.169921875,
      -2.6875,
      1.296875,
      -0.384765625,
      0.65234375,
      -4.78125,
      0.38671875,
      -3.53125,
      4.78125,
      -1.1953125,
      -3.171875,
      -5.78125,
      0.60546875,
      -2.203125,
      -0.89453125,
      0.322265625,
      -7.15625,
      -4.5625,
      -3.8125,
      -3.671875,
      -7.09375,
      -0.0267333984375,
      -1.8671875,
      -0.359375,
      3.171875,
      1.2421875,
      -2.34375,
      4.875,
      -0.1650390625,
      1.046875,
      -3.203125,
      -5.5625,
      -1.171875,
      1.09375,
      -0.275390625,
      -1.8125,
      0.9765625,
      2.5625,
      1.5859375,
      -4.28125,
      3.28125,
      6.25,
      -3.34375,
      4.6875,
      -0.06396484375,
      5.125,
      -2.28125,
      -8.9375,
      -1.75,
      -5,
      3.5,
      1.2109375,
      4,
      -5.5,
      -1.015625,
      -2.46875,
      -7.84375,
      2.59375,
      1.6640625,
      -7.34375,
      -1.25,
      3.546875,
      -4.03125,
      0.416015625,
      1.1171875,
      1.015625,
      8.0625,
      0.10546875,
      -2.75,
      2.359375,
      1.640625,
      0.5390625,
      -2.046875,
      0.00628662109375,
      2.625,
      0.119140625,
      0.7265625,
      -0.053955078125,
      -0.89453125,
      -5.28125,
      1.140625,
      0.86328125,
      -1.078125,
      1.109375,
      14.875,
      1.3984375,
      -1.9375,
      1.671875,
      0.58984375,
      -1.1640625,
      9.375,
      2.328125,
      0.30859375,
      0.84375,
      2.078125,
      -3.28125,
      3.828125,
      -1.359375,
      1.5625,
      2.921875,
      -3.953125,
      2,
      -2.109375,
      0.4375,
      4.5,
      4.65625,
      1.2421875,
      -4.4375,
      0.81640625,
      4.53125,
      -1.3671875,
      -0.06494140625,
      1.5234375,
      -1.1875,
      -10.375,
      -0.71484375,
      -1.8828125,
      -5.84375,
      -2.21875,
      1.1484375,
      -4.5,
      1.203125,
      -1.3046875,
      -1.0625,
      2.09375,
      2.609375,
      -0.3515625,
      7.5,
      2.40625,
      2.328125,
      -2.5,
      4.5625,
      0.97265625,
      4.3125,
      4.90625,
      2.609375,
      0.330078125,
      -0.18359375,
      2.015625,
      3.015625,
      2.96875,
      2.15625,
      8.125,
      -0.361328125,
      4.28125,
      4.65625,
      -4.375,
      -3.609375,
      -0.8671875,
      -4.28125,
      -0.54296875,
      -2.515625,
      1.796875,
      -3.375,
      -4.15625,
      0.15625,
      2.9375,
      1.421875,
      -0.75390625,
      -0.4609375,
      -2.03125,
      0.50390625,
      -8.125,
      0.205078125,
      4.25,
      -9.6875,
      -3.578125,
      4.625,
      6.8125,
      0.9296875,
      -0.275390625,
      0.82421875,
      -5.0625,
      3.078125,
      -1.9921875,
      -5.25,
      1.8828125,
      0.87890625,
      -4.09375,
      4.1875,
      0.71875,
      1.7265625,
      3.15625,
      1.1328125,
      1,
      -4.40625,
      1.7421875,
      -3.53125,
      4.625,
      2.078125,
      -3.828125,
      1.0390625,
      -2.28125,
      -6.90625,
      -9.5,
      3.953125,
      -4,
      2.421875,
      -0.7421875,
      -0.76953125,
      6.25,
      -2.0625,
      12.5625,
      1.46875,
      1.7265625,
      0.62890625,
      0.0244140625,
      -1.9296875,
      1.140625,
      -1.4453125,
      0.1611328125,
      -4.6875,
      1.3515625,
      4.28125,
      -0.1953125,
      -1.75,
      -2.84375,
      -0.1416015625,
      2.46875,
      -0.9375,
      -4.3125,
      1.4765625,
      4.5625,
      0.328125,
      1.3046875,
      5.3125,
      -3.78125,
      3.453125,
      -4.40625,
      -3.03125,
      3.5,
      1.9765625,
      -2.4375,
      -7.34375,
      -3.734375,
      -2.234375,
      -2.71875,
      -2.703125,
      -4.5625,
      1.3984375,
      -0.0693359375,
      5.625,
      2.234375,
      1.453125,
      0.63671875,
      -6.34375,
      -8.3125,
      6.875,
      -3.78125,
      3.40625,
      1.59375,
      -0.69921875,
      2.65625,
      -0.341796875,
      -1.7109375,
      1.34375,
      -3.8125,
      -2.171875,
      -0.5,
      5,
      -1.53125,
      3.1875,
      -6.40625,
      4.78125,
      2.859375,
      1.390625,
      4.15625,
      7.46875,
      -1.3671875,
      2.328125,
      -1.8828125,
      -1.7421875,
      1.140625,
      0.53125,
      -3.296875,
      7.53125,
      0.20703125,
      -2.671875,
      -3.78125,
      -2.859375,
      2.59375,
      -0.84375,
      -1.15625,
      -2.171875,
      -2.34375,
      0.8828125,
      3.375,
      0.609375,
      1.6875,
      1.8125,
      -3.28125,
      -4.15625,
      2.234375,
      -1.4296875,
      1.6015625,
      -0.251953125,
      3.46875,
      4.09375,
      1.4609375,
      -0.99609375,
      5.125,
      1.46875,
      -3.421875,
      -2.109375,
      0.8984375,
      -3.640625,
      1.53125,
      4.21875,
      1.125,
      -2.171875,
      2.15625,
      -0.61328125,
      -3.75,
      4.59375,
      -1.359375,
      -0.294921875,
      -1.75,
      -1.8984375,
      -1.4921875,
      2.375,
      -7.1875,
      1.3828125,
      -0.33203125,
      -1.140625,
      1.0859375,
      -1.0859375,
      2.34375,
      -0.671875,
      4.4375,
      -2.765625,
      1.8515625,
      -7.4375,
      -0.84375,
      -3.5,
      -2.171875,
      1.6640625,
      -1.7734375,
      1.7734375,
      -0.06494140625,
      0.326171875,
      4.78125,
      2.109375,
      0.05908203125,
      -0.1650390625,
      1.9375,
      -3.96875,
      1.171875,
      -0.64453125,
      -5.90625,
      1.1875,
      -5.125,
      -4.75,
      2.609375,
      3.390625,
      -0.126953125,
      6.6875,
      4.71875,
      -2.640625,
      -3.484375,
      2.09375,
      2.65625,
      -4.53125,
      -2.359375,
      1.7578125,
      -1.5234375,
      0.6171875,
      -0.72265625,
      2.921875,
      0.271484375,
      -1.7578125,
      2.828125,
      0.94921875,
      0.06591796875,
      0.88671875,
      -1.3984375,
      0.41015625,
      1.4453125,
      1.5078125,
      1.140625,
      -2.609375,
      6.28125,
      6.625,
      -6.625,
      -8.5,
      2.609375,
      0.7734375,
      -1.796875,
      -1.421875,
      3.34375,
      1.59375,
      0.396484375,
      -5.53125,
      -4.84375,
      -1.625,
      -0.8203125,
      4.5625,
      3.046875,
      0.244140625,
      -3.34375,
      -3.875,
      4.4375,
      -2.515625,
      4.1875,
      0.34375,
      -1.5,
      0.578125,
      -3.953125,
      5.09375,
      0.87109375,
      5.6875,
      -4.75,
      -0.375,
      3.390625,
      -9.3125,
      -0.423828125,
      -3.25,
      -0.09375,
      -0.5859375,
      -0.4921875,
      5.09375,
      -3.609375,
      0.2041015625,
      0.75,
      5.96875,
      -2.46875,
      -0.91015625,
      2.46875,
      -7.15625,
      -2.4375,
      1.0859375,
      2.453125,
      2.421875,
      -2.015625,
      -4.875,
      0.388671875,
      -0.15234375,
      -0.52734375,
      -2.203125,
      -0.984375,
      -3.1875,
      -1.9140625,
      1.546875,
      3.703125,
      2.21875,
      0.1484375,
      -0.8046875,
      -3.78125,
      -2.59375,
      -1.1171875,
      1.625,
      -0.37890625,
      1.21875,
      0.322265625,
      2.78125,
      3.359375,
      -2.5,
      -1.546875,
      -1.2421875,
      3.109375,
      -2.890625,
      1.671875,
      0.5234375,
      1.9296875,
      3.453125,
      2.015625,
      -1.4296875,
      -1.546875,
      0.67578125,
      -1.8203125,
      -4.59375,
      -1.4375,
      4.65625,
      -0.71484375,
      0.0155029296875,
      -4.90625,
      0.76171875,
      0.71484375,
      1.8359375,
      -0.65625,
      0.06640625,
      6.21875,
      1.3671875,
      0.2421875,
      2.265625,
      5.875,
      -2.203125,
      0.62109375,
      0.6484375,
      1.09375,
      -7.25,
      -7.5,
      -3.703125,
      3.546875,
      8.4375,
      -0.07373046875,
      5.1875,
      -4.90625,
      4.78125,
      0.228515625,
      0.33984375,
      -14.8125,
      2.5,
      0.77734375,
      -5.6875,
      -2.609375,
      0.00182342529296875,
      4.75,
      -0.91796875,
      4.09375,
      -0.9609375,
      -1.015625,
      1.5234375,
      4.03125,
      -0.9296875,
      -2.265625,
      6.0625,
      3.796875,
      -3.671875,
      2.5,
      -1.8984375,
      -4.84375,
      0.3046875,
      0.09716796875,
      2.625,
      4.125,
      6.84375,
      -0.37890625,
      1.078125,
      4.3125,
      -4.8125,
      -0.34375,
      1.3359375,
      0.80078125,
      -2.875,
      1.6328125,
      -5.65625,
      1.2265625,
      -3.921875,
      1.078125,
      0.0024566650390625,
      -3.109375,
      0.154296875,
      4.09375,
      -2,
      -1.7421875,
      -1.3515625,
      1.3984375,
      3.09375,
      5.625,
      -6.65625,
      0.921875,
      0.498046875,
      0.259765625,
      2.703125,
      -2.171875,
      -0.734375,
      -3.234375,
      3.03125,
      1.140625,
      -2.625,
      5.0625,
      -0.6640625,
      -5.46875,
      3.09375,
      0.56640625,
      -1.1875,
      -0.11962890625,
      3.140625,
      0.2421875,
      0.064453125,
      -0.423828125,
      2.1875,
      1.21875,
      -3.890625,
      -2.90625,
      -0.80859375,
      -2.265625,
      3.90625,
      -0.92578125,
      -2.859375,
      -3.34375,
      -3.90625,
      1.2421875,
      1.796875,
      3.046875,
      1.125,
      2.234375,
      3.15625,
      -4.53125,
      0.5078125,
      -4.8125,
      -3.875,
      1.2890625,
      -3.578125,
      -1.953125,
      0.94140625,
      1.0859375,
      2.171875,
      -1.3125,
      -1.640625,
      -3.640625,
      -4.3125,
      -2.234375,
      1.28125,
      1.2734375,
      2.3125,
      -1.2265625,
      6.125,
      -0.6171875,
      -2.859375,
      0.07080078125,
      -1.453125,
      -0.2060546875,
      -1.0234375,
      1.375,
      -5.6875,
      -0.609375,
      5.96875,
      1.3515625,
      -3.375,
      6.25,
      -0.6015625,
      -0.85546875,
      -1.765625,
      2.65625,
      -0.609375,
      -0.333984375,
      0.828125,
      -3.78125,
      -2.75,
      -3.765625,
      -0.0179443359375,
      -1.2890625,
      3.296875,
      0.62890625,
      0.74609375,
      -0.9296875,
      -7.75,
      2.96875,
      0.21875,
      -0.30859375,
      2.359375,
      5.90625,
      -0.74609375,
      -1.1875,
      2.4375,
      0.7734375,
      -0.82421875,
      -0.8125,
      -4.0625,
      4.4375,
      2.375,
      3.765625,
      -3.96875,
      -6.28125,
      -0.345703125,
      -1.2265625,
      -1.65625,
      4.53125,
      -0.6953125,
      -0.39453125,
      5.4375,
      2.421875,
      -4.8125,
      -2.0625,
      5.5,
      -3.84375,
      0.94921875,
      2.34375,
      2.84375,
      0.458984375,
      4.25,
      -1.65625,
      -2.859375,
      -0.90234375,
      5.75,
      2.859375,
      -1.1640625,
      -0.6953125,
      3.875,
      0.11572265625,
      4.4375,
      -6.0625,
      -5.125,
      0.99609375,
      0.984375,
      -1.84375,
      -0.031005859375,
      6.40625,
      -1.890625,
      0.1728515625,
      0.48046875,
      -0.171875,
      3.40625,
      1.421875,
      1.0390625,
      0.10888671875,
      1.75,
      1.5078125,
      0.875,
      1.8984375,
      -1.15625,
      -0.302734375,
      -1.2265625,
      0.326171875,
      -1.9375,
      3.53125,
      -3.65625,
      0.66015625,
      -1.6875,
      3.734375,
      -0.328125,
      5.75,
      5.15625,
      0.043701171875,
      0.62109375,
      -0.76171875,
      6.84375,
      -1.6015625,
      6.5625,
      1.3125,
      8.5625,
      -2.3125,
      -3.640625,
      1.40625,
      0.095703125,
      -0.890625,
      0.87109375,
      -6.9375,
      -1.953125,
      -1.4140625,
      -0.1611328125,
      -0.76953125,
      2.75,
      -5.125,
      -0.79296875,
      3.25,
      3.6875,
      3.75,
      2.875,
      4.875,
      0.82421875,
      0.88671875,
      -3.203125,
      -1.8046875,
      0.400390625,
      0.58984375,
      3.453125,
      -3.921875,
      0.9453125,
      1.4609375,
      5.53125,
      -0.039794921875,
      -3.046875,
      -3.171875,
      7.34375,
      2.203125,
      -2.328125,
      1.765625,
      -1.7265625,
      0.89453125,
      0.9609375,
      3.984375,
      -5.59375,
      -3.140625,
      6.6875,
      5.03125,
      -0.8046875,
      3.203125,
      0.060302734375,
      2.171875,
      -3.484375,
      -0.90625,
      0.01397705078125,
      -0.046875,
      -4.25,
      -3.078125,
      0.6953125,
      3.828125,
      -4.78125,
      1.921875,
      0.95703125,
      -2.71875,
      2.4375,
      1.0625,
      -2.296875,
      1.609375,
      0.2177734375,
      6.8125,
      0.53125,
      -5.96875,
      -4.5625,
      -7.46875,
      -3.890625,
      0.267578125,
      2.125,
      2.796875,
      2.15625,
      -3.453125,
      2.71875,
      -5.28125,
      -1.046875,
      -4.8125,
      3.65625,
      3.59375,
      -4.8125,
      -2.625,
      -2,
      -7.40625,
      -3.859375,
      -0.91015625,
      -1.2578125,
      -2.484375,
      -5.65625,
      -1.9609375,
      2.375,
      -4.96875,
      -1.046875,
      -1.9375,
      -1.1328125,
      4.34375,
      -0.2158203125,
      0.7890625,
      -4.59375,
      4.8125,
      1.84375,
      -2.3125,
      5.34375,
      1.859375,
      2.734375,
      2.09375,
      0.138671875,
      -0.10205078125,
      -4.78125,
      4.09375,
      -1.5390625,
      9.0625,
      4.0625,
      1.953125,
      -4.5625,
      4.25,
      1.8203125,
      1.4296875,
      -4.40625,
      -0.765625,
      -2.46875,
      2.28125,
      1.609375,
      -1.3125,
      2.4375,
      -2.578125,
      -1.4140625,
      0.89453125,
      -2.421875,
      -0.40625,
      3.265625,
      -0.05712890625,
      4.4375,
      1.6640625,
      0.447265625,
      0.1572265625,
      0.23828125,
      1.671875,
      -0.462890625,
      5.71875,
      -5.09375,
      2.34375,
      -5.375,
      -1.0390625,
      0.73046875,
      -1.1953125,
      -0.64453125,
      -0.4453125,
      -0.318359375,
      1.7421875,
      2.890625,
      1.59375,
      1.1796875,
      4.78125,
      -2.796875,
      -0.0986328125,
      0.76953125,
      -2.453125,
      4.875,
      -0.0022735595703125,
      -0.98046875,
      -0.86328125,
      -0.69921875,
      -0.251953125,
      1.3046875,
      0.84375,
      -8.0625,
      -1.421875,
      -1.21875,
      -4.125,
      7.3125,
      0.953125,
      -0.6796875,
      1.4921875,
      4.09375,
      6.28125,
      3.921875,
      -1.03125,
      -1.0703125,
      -0.2275390625,
      -2.28125,
      4.03125,
      -2.09375,
      -0.74609375,
      4.4375,
      3.828125,
      3.5625,
      -1.03125,
      -0.57421875,
      -2.6875,
      0.00323486328125,
      4.90625,
      -1.7265625,
      -1.1875,
      6.59375,
      -2.953125,
      0.099609375,
      -1.9140625,
      -2.671875,
      -1.8984375,
      -1.5546875,
      2.65625,
      2.1875,
      2.578125,
      -2,
      -2.59375,
      -7.6875,
      2.015625,
      -0.1240234375,
      -1.0703125,
      -1.1640625,
      1.7890625,
      -0.255859375,
      1.5390625,
      -2.03125,
      -1.984375,
      -0.328125,
      -3.8125,
      -0.01116943359375,
      2.828125,
      0.1435546875,
      1.3671875,
      -0.42578125,
      2.75,
      -2.28125,
      2.453125,
      3.875,
      -5.6875,
      2,
      0.921875,
      -1.6953125,
      3.734375,
      1.84375,
      1.4765625,
      2.78125,
      -1.234375,
      0.796875,
      -0.439453125,
      -1.703125,
      1.0703125,
      -2.96875,
      0.60546875,
      1.9375,
      3.328125,
      -2.40625,
      3.078125,
      0.89453125,
      -4.375,
      3.671875,
      -3.65625,
      -7.09375,
      -1.0078125,
      0.08349609375,
      -0.90625,
      -1.3671875,
      -2.453125,
      1.234375,
      2.59375,
      -7.4375,
      5.78125,
      -1.578125,
      5.3125,
      2.71875,
      -1.046875,
      -2.8125,
      0.58203125,
      2.234375,
      -1.578125,
      2.359375,
      1.5234375,
      -1.7421875,
      4.03125,
      -3.359375,
      1.2890625,
      -0.9453125,
      0.59765625,
      3.1875,
      -1.140625,
      0.9765625,
      -0.12255859375,
      1.0546875,
      -9.5,
      -3.46875,
      -2.0625,
      -0.279296875,
      1.96875,
      -0.62109375,
      2.3125,
      -2.828125,
      2.5625,
      -3.96875,
      3.015625,
      -5.0625,
      -3.078125,
      1.7578125,
      -1.921875,
      4.59375,
      2.109375,
      -2.328125,
      0.333984375,
      0.408203125,
      -4.5,
      -2.203125,
      -2.671875,
      0.0390625,
      -3.03125,
      -2.03125,
      -0.287109375,
      2.78125,
      -0.2578125,
      1.7890625,
      -0.90234375,
      3.28125,
      4.15625,
      -0.7421875,
      3.0625,
      0.59765625,
      0.56640625,
      -3.0625,
      -4.4375,
      -4.40625,
      2.0625,
      3.84375,
      -3.21875,
      2.53125,
      -0.921875,
      2.109375,
      2.09375,
      -0.859375,
      -0.162109375,
      1.109375,
      1.78125,
      -0.7578125,
      -1.953125,
      2.515625,
      -1.4765625,
      2.015625,
      -2.203125,
      -4.125,
      -4.25,
      -0.232421875,
      1.0703125,
      0.1904296875,
      1.078125,
      -1.0625,
      -1.8515625,
      -1.25,
      -4.21875,
      -2.65625,
      4.90625,
      0.53125,
      5.1875,
      0.474609375,
      1.1328125,
      -3.046875,
      -1.140625,
      -1.453125,
      6.03125,
      0.78515625,
      -2.78125,
      3.890625,
      -2.09375,
      5.40625,
      -2.140625,
      -1.734375,
      -9.25,
      -1.890625,
      7.125,
      2.0625,
      -0.66015625,
      1.2421875,
      0.31640625,
      2.328125,
      0.000484466552734375,
      5.34375,
      -3.078125,
      -1.578125,
      -0.423828125,
      3.53125,
      1.796875,
      -5.53125,
      0.6484375,
      1.890625,
      -1.5859375,
      1.59375,
      5.8125,
      5.71875,
      -2.0625,
      3.46875,
      -3.984375,
      -2.703125,
      -4.90625,
      4.09375,
      -0.57421875,
      -0.79296875,
      -1.7578125,
      -3.21875,
      -0.255859375,
      -1.4140625,
      0.333984375,
      7.15625,
      -4.59375,
      2.578125,
      -0.95703125,
      4.09375,
      0.90625,
      -1.09375,
      -0.7265625,
      2.921875,
      2.671875,
      -0.93359375,
      -1.1796875,
      -5.9375,
      -1.828125,
      -0.58984375,
      -7.4375,
      2.03125,
      1.78125,
      0.9609375,
      2.25,
      -1.8203125,
      2.265625,
      3.140625,
      -2.5,
      -0.78515625,
      -2.15625,
      2.796875,
      4.75,
      1.46875,
      -2.921875,
      1.1953125,
      -0.96484375,
      -0.2314453125,
      -5.03125,
      3.015625,
      -1.875,
      4.25,
      2.984375,
      3.125,
      -3.765625,
      -4.1875,
      -3.203125,
      -1.703125,
      -3.84375,
      1.171875,
      0.01336669921875,
      1.5859375,
      3.71875,
      -0.08642578125,
      -3.546875,
      -1.2890625,
      -3.890625,
      1.53125,
      -0.5625,
      1.5234375,
      -6.875,
      2.4375,
      -6.25,
      0.76171875,
      1.28125,
      -3.140625,
      0.130859375,
      3.4375,
      2.046875,
      2.25,
      3.328125,
      -1.4921875,
      -3.5625,
      3.484375,
      -4.0625,
      -2.078125,
      -5,
      0.34765625,
      -0.427734375,
      -4.375,
      3.1875,
      -2.765625,
      2.234375,
      -2.4375,
      -3.0625,
      0.2490234375,
      -1.0859375,
      -0.55078125,
      3.5625,
      2.0625,
      -2.4375,
      1.7578125,
      -1.078125,
      5.84375,
      2.625,
      -2.296875,
      1.640625,
      -0.353515625,
      1.8359375,
      -6.125,
      4.3125,
      0.7421875,
      -1.3828125,
      3.40625,
      5.125,
      -3.203125,
      0.953125,
      2.21875,
      5.5625,
      -2.609375,
      -1.375,
      0.421875,
      4.71875,
      -4,
      0.0238037109375,
      -4.125,
      -0.0186767578125,
      1.453125,
      -1.6328125,
      5.03125,
      -1.7109375,
      3.734375,
      -5.0625,
      0.3125,
      1.3359375,
      1.4140625,
      2.9375,
      -5.4375,
      3.890625,
      2.421875,
      -1.390625,
      5.0625,
      -0.73046875,
      -5.59375,
      -2.21875,
      2.125,
      -4.53125,
      -3.0625,
      -5.03125,
      0.0830078125,
      -2.390625,
      -1.6328125,
      -0.65625,
      0.36328125,
      1.0390625,
      2.578125,
      -1.625,
      -1.6953125,
      -2.96875,
      -4.46875,
      1.140625,
      -1.90625,
      -6.71875,
      -0.349609375,
      2.203125,
      3.40625,
      5.5,
      7.8125,
      3.171875,
      -4.5,
      2.609375,
      -1.5703125,
      1.1953125,
      -0.14453125,
      -0.1787109375,
      -0.46875,
      -2.921875,
      2.640625,
      0.88671875,
      3.40625,
      -6.875,
      -0.87890625,
      -4.8125,
      -1.125,
      -0.267578125,
      0.1484375,
      -0.1748046875,
      2.46875,
      4.84375,
      -2.34375,
      3.84375,
      1.4765625,
      -0.1298828125,
      -5.21875,
      2.375,
      -1.5234375,
      2.65625,
      4.5,
      0.6640625,
      -5,
      6.71875,
      2.328125,
      -2.640625,
      2.96875,
      0.03564453125,
      -3.609375,
      -3.171875,
      1.2578125,
      0.5,
      4.0625,
      -0.96875,
      0.93359375,
      2.765625,
      -0.578125,
      3.75,
      2.4375,
      0.76953125,
      0.185546875,
      4.53125,
      0.70703125,
      -5,
      -1.3671875,
      2.515625,
      -3.3125,
      -4,
      2.203125,
      -0.671875,
      2.625,
      -0.115234375,
      2.171875,
      1.75,
      6.84375,
      -4.46875,
      3.03125,
      -5.9375,
      0.1669921875,
      -7.3125,
      2.390625,
      -7.9375,
      1.9765625,
      0.796875,
      6.3125,
      -0.023681640625,
      -4.28125,
      3.421875,
      5.5,
      1.9453125,
      -4.625,
      2.46875,
      2.578125,
      -0.89453125,
      -5.875,
      -2.671875,
      4.34375,
      0.828125,
      3.0625,
      -4.90625,
      1.328125,
      3.6875,
      -1.6328125,
      -5.40625,
      0.63671875,
      -0.3359375,
      -6.53125,
      -5.65625,
      -2.0625,
      -0.54296875,
      -1.21875,
      2.0625,
      4.375,
      -1.953125,
      2.09375,
      0.453125,
      4.09375,
      2.53125,
      -1.34375,
      3.09375,
      0.404296875,
      -0.1474609375,
      2.234375,
      3.875,
      -2.515625,
      1.53125,
      -0.9453125,
      1.015625,
      3.078125,
      -1.84375,
      -0.185546875,
      -6.84375,
      1.7578125,
      4.375,
      1.96875,
      0.1318359375,
      -3.859375,
      -3.046875,
      1.984375,
      -1.3359375,
      -3.21875,
      1.1015625,
      -1.71875,
      -0.2236328125,
      1.0625,
      -1.9609375,
      3.765625,
      0.83203125,
      -0.4453125,
      -1.953125,
      -0.6484375,
      -2.109375,
      -1.5859375,
      -3.59375,
      3.109375,
      -0.625,
      4.34375,
      -0.46484375,
      -4.3125,
      -2.140625,
      3.828125,
      1.0859375,
      2.390625,
      -1.84375,
      -1.21875,
      3.359375,
      3.796875,
      1.328125,
      -1.3203125,
      0.625,
      1.203125,
      0.41015625,
      -3.34375,
      -1.59375,
      -1.84375,
      3.265625,
      -1.90625,
      -1.3046875,
      -2.421875,
      1.953125,
      -1.2109375,
      -7.90625,
      0.88671875,
      -0.0019378662109375,
      -0.77734375,
      -5,
      0.240234375,
      2.296875,
      2.015625,
      4.75,
      3.25,
      2.9375,
      -2.09375,
      2.84375,
      18.75,
      -2.703125,
      -4.9375,
      -1.7265625,
      0.318359375,
      2.96875,
      8.5625,
      1.1171875,
      0.1123046875,
      -1.953125,
      1.578125,
      4.4375,
      2.921875,
      3.234375,
      0.68359375,
      0.87109375,
      2.671875,
      1.9453125,
      -2.59375,
      -4.1875,
      -1.0859375,
      2.234375,
      -1.0078125,
      3.15625,
      -2.625,
      3.125,
      3.34375,
      -1.4921875,
      -0.21484375,
      -5,
      1.171875,
      -3.046875,
      -1.3125,
      -5.34375,
      1.3046875,
      -4.96875,
      0.953125,
      1.9921875,
      0.81640625,
      -2.0625,
      -2.390625,
      -4.25,
      3.1875,
      -0.890625,
      -0.37890625,
      0.86328125,
      -2.3125,
      1.515625,
      -0.71875,
      -0.8671875,
      -2.5,
      -3.65625,
      -3.453125,
      2.046875,
      0.71875,
      -2.546875,
      0.84765625,
      -7.03125,
      -2.34375,
      -5.40625,
      -1.671875,
      -2.265625,
      -3.40625,
      -2.890625,
      -3.90625,
      -2.453125,
      -2.1875,
      -0.2373046875,
      -0.09765625,
      -1.484375,
      -6,
      -0.296875,
      5,
      5.625,
      0.1435546875,
      -2.390625,
      2.6875,
      2.390625,
      2.171875,
      1.484375,
      -0.455078125,
      3.421875,
      -2.203125,
      -1.5390625,
      -0.796875,
      0.91015625,
      -5.3125,
      2.34375,
      0.1455078125,
      -0.16796875,
      4.09375,
      -2.890625,
      2.171875,
      -0.67578125,
      -3.359375,
      -1.609375,
      -0.322265625,
      3.40625,
      0.09130859375,
      -0.8125,
      2.703125,
      -0.546875,
      -0.91015625,
      -2.09375,
      -0.52734375,
      -1.5546875,
      1.8203125,
      -1.6484375,
      1.8203125,
      -0.84375,
      1.2734375,
      2.03125,
      0.84765625,
      -2.25,
      -0.47265625,
      2.828125,
      -6.78125,
      -1.9140625,
      1.953125,
      -0.76171875,
      4.3125,
      -1.9375,
      -3.9375,
      -0.296875,
      -8.75,
      -5.59375,
      -1.8359375,
      -2.96875,
      3.53125,
      -2.125,
      1.421875,
      -1,
      -1.8984375,
      -1.296875,
      2.875,
      -4.25,
      3.6875,
      3.8125,
      1.109375,
      0.322265625,
      -2.875,
      2.328125,
      1.1484375,
      -2.859375,
      -2.5625,
      0.734375,
      -4.71875,
      2.5625,
      5.5625,
      1.1953125,
      -3.65625,
      4.84375,
      -0.71484375,
      -3.375,
      -2.546875,
      3.125,
      -0.16015625,
      -0.671875,
      -1.28125,
      -1.9765625,
      0.57421875,
      0.55078125,
      -2.78125,
      -4.34375,
      -1.953125,
      -1.9921875,
      2.828125,
      -0.578125,
      -7.375,
      0.8828125,
      5.625,
      -6.9375,
      0.890625,
      -1.7421875,
      5.84375,
      -5.875,
      0.474609375,
      5.40625,
      -0.181640625,
      0.60546875,
      2.109375,
      3.203125,
      -2.078125,
      -6.40625,
      1.7109375,
      4.375,
      -2.8125,
      2.296875,
      3.046875,
      -1.4609375,
      0.72265625,
      -4.1875,
      -4.6875,
      6.90625,
      5.84375,
      -5.03125,
      0.62109375,
      -2.8125,
      -1.25,
      -1.359375,
      -5.15625,
      2.53125,
      1.3671875,
      1.25,
      0.7421875,
      0.9140625,
      -3.671875,
      -0.71484375,
      -0.953125,
      -2.546875,
      -2.09375,
      2.890625,
      -1.21875,
      4.09375,
      -6.0625,
      5.5625,
      3.4375,
      0.76953125,
      -4.40625,
      3.359375,
      4.53125,
      0.109375,
      -4.46875,
      -1.46875,
      -1.078125,
      4.625,
      -6.53125,
      4.28125,
      -3.140625,
      1.5546875,
      -1.1640625,
      2.4375,
      -1.3203125,
      3.15625,
      -2.9375,
      -6.15625,
      -3.671875,
      3.578125,
      1.203125,
      1.9765625,
      2.8125,
      -0.87890625,
      1.21875,
      2.03125,
      -3.3125,
      1.7421875,
      -0.87890625,
      -0.71875,
      3.5,
      -2.265625,
      0.044677734375,
      -0.1484375,
      2.359375,
      -2.4375,
      -2.921875,
      -1.765625,
      -3.453125,
      1.03125,
      -4.28125,
      3.671875,
      2.90625,
      -7.21875,
      4.4375,
      -3.4375,
      -1.859375,
      1.9375,
      0.6796875,
      -5.65625,
      6.0625,
      4.28125,
      -4.5,
      4.5,
      -0.14453125,
      -2,
      0.5625,
      -1.6015625,
      0.9453125,
      -5.375,
      0.640625,
      3.765625,
      -4.96875,
      1.1796875,
      3.21875,
      -2.5625,
      -0.16796875,
      -1.3046875,
      5.5,
      1.1640625,
      3.40625,
      0.75,
      2.421875,
      5.875,
      -4.3125,
      1.5390625,
      -2.203125,
      -0.93359375,
      -2.75,
      -0.59765625,
      -1.328125,
      -2.234375,
      2.1875,
      3.234375,
      1.3203125,
      -10,
      -3.890625,
      2.53125,
      -3.96875,
      3.625,
      1.4609375,
      1.0546875,
      -3.203125,
      -1.4296875,
      0.8125,
      2.734375,
      -2.109375,
      -2.671875,
      -2.375,
      3.203125,
      1.4453125,
      -6.21875,
      -0.6640625,
      0.46875,
      -2.453125,
      -0.490234375,
      -2.03125,
      -2.03125,
      0.361328125,
      5.625,
      -6.09375,
      2.828125,
      1.6484375,
      3.3125,
      4.71875,
      -1.375,
      2.9375,
      3.90625,
      -2.984375,
      -7.0625,
      -1.2890625,
      0.427734375,
      -0.07861328125,
      1.953125,
      -2.984375,
      2.671875,
      0.365234375,
      3.484375,
      -5.5,
      1.6640625,
      3.328125,
      -6.46875,
      4.5625,
      -3.5,
      0.66796875,
      -1.546875,
      -5.9375,
      -0.57421875,
      4.25,
      0.5390625,
      0.031494140625,
      0.6640625,
      -1.6484375,
      0.04833984375,
      2.546875,
      -0.9375,
      -0.8125,
      -0.26171875,
      3.734375,
      -3.265625,
      1.65625,
      2.53125,
      0.53515625,
      -0.6015625,
      -3.453125,
      2.59375,
      -1.3203125,
      -2.734375,
      4.71875,
      5.125,
      -1.1171875,
      -3.671875,
      -5.625,
      1.5390625,
      -0.1572265625,
      -3.921875,
      0.48828125,
      -2.515625,
      1.28125,
      -3.453125,
      1.765625,
      0.734375,
      -6.03125,
      -2.953125,
      -3.84375,
      -1.171875,
      0.7890625,
      2.921875,
      2.171875,
      -2.1875,
      -1.796875,
      4.0625,
      3.25,
      -0.90234375,
      -5.09375,
      1.5,
      -1.25,
      -1.921875,
      -3.53125,
      1.390625,
      -3.328125,
      -0.6875,
      1.7734375,
      -0.294921875,
      -0.42578125,
      -5.875,
      -2.46875,
      1.15625,
      -0.4765625,
      -0.0021820068359375,
      -2.640625,
      -0.251953125,
      1.8359375,
      4.40625,
      -4.09375,
      -1.1328125,
      0.6171875,
      1.8359375,
      -6.4375,
      -1.4453125,
      2.0625,
      -1.28125,
      0.2333984375,
      4.71875,
      1.34375,
      2.046875,
      1.125,
      -4.4375,
      -2.640625,
      5.21875,
      3.796875,
      0.04736328125,
      0.71875,
      -1.7421875,
      -2.46875,
      4.84375,
      7.625,
      -2.75,
      -2.328125,
      -0.9453125,
      -1.7265625,
      -2.484375,
      -2.359375,
      0.057861328125,
      -0.244140625,
      -7.46875,
      7.0625,
      -1.8046875,
      3.6875,
      1.5546875,
      -4.6875,
      4.8125,
      -4.25,
      -1.3515625,
      -0.55859375,
      0.053955078125,
      3.703125,
      -3.625,
      -1.390625,
      -3.265625,
      0.498046875,
      0.6484375,
      -2.015625,
      0.25,
      1.4453125,
      4.3125,
      -0.17578125,
      -1.4296875,
      -1.2890625,
      -1.4140625,
      5.84375,
      -1.921875,
      -2.625,
      -0.0712890625,
      -3.34375,
      1.5546875,
      -1.7578125,
      1.421875,
      -5.09375,
      -0.94921875,
      -1.9296875,
      2.03125,
      1.3828125,
      1.71875,
      -0.73828125,
      -3.5625,
      -1.9375,
      4.21875,
      1.59375,
      5.0625,
      1.84375,
      3.609375,
      0.6796875,
      1.140625,
      2.21875,
      3.171875,
      3.671875,
      -2.375,
      0.625,
      -0.302734375,
      -0.1298828125,
      -0.6875,
      -1.6171875,
      2.796875,
      1.3515625,
      4.71875,
      -0.640625,
      -0.421875,
      -3.546875,
      -2.296875,
      1.0625,
      -1.34375,
      3.09375,
      3.109375,
      -2.265625,
      0.107421875,
      -0.5078125,
      2.984375,
      1.0546875,
      4.25,
      0.384765625,
      -1.96875,
      -2.40625,
      1.1171875,
      1.890625,
      -0.400390625,
      0.60546875,
      -1.640625,
      1.2890625,
      -0.578125,
      -2.875,
      0.294921875,
      2.71875,
      -2.609375,
      1.3359375,
      0.038330078125,
      0.34765625,
      0.353515625,
      -2.046875,
      -0.2255859375,
      0.953125,
      -0.75390625,
      4.65625,
      0.251953125,
      -0.490234375,
      1.90625,
      3.390625,
      -1.609375,
      0.32421875,
      1.65625,
      -0.74609375,
      -1.5078125,
      -1.7578125,
      2.234375,
      -2.609375,
      4.34375,
      -1.6640625,
      0.2294921875,
      -0.2333984375,
      -1.4296875,
      -3.765625,
      2.234375,
      -3.65625,
      -0.302734375,
      -2.140625,
      0.94921875,
      2.890625,
      0.50390625,
      0.52734375,
      1.1171875,
      0.291015625,
      -1.875,
      -1.5703125,
      -1.65625,
      -3.359375,
      -4.46875,
      -2.734375,
      1.03125,
      0.6640625,
      -0.546875,
      0.83984375,
      0.3515625,
      2.09375,
      -0.107421875,
      -1.0390625,
      2.484375,
      1.765625,
      -2.546875,
      -1.6796875,
      1.2109375,
      -0.09521484375,
      1.2109375,
      -0.94921875,
      -2.390625,
      -0.287109375,
      0.06396484375,
      -0.439453125,
      1.6484375,
      -0.439453125,
      -0.498046875,
      0.671875,
      1.203125,
      -1.734375,
      1.140625,
      0.34765625,
      2.390625,
      -0.58203125,
      0.126953125,
      1.9921875,
      0.82421875,
      -1.3671875,
      2.9375,
      3.421875,
      -2.03125,
      -0.70703125,
      -0.1416015625,
      1.375,
      2.140625,
      -1.75,
      -0.95703125,
      1.71875,
      -1.984375,
      -2.703125,
      0.9765625,
      2.109375,
      -2.375,
      0.69921875,
      -2.546875,
      -0.96484375,
      -1.2421875,
      0.1708984375,
      1.671875,
      -0.90625,
      -0.8515625,
      -1.9296875,
      1.2265625,
      2,
      0.67578125,
      0.609375,
      -1.1171875,
      -2.5,
      -0.50390625,
      1.453125,
      -1.5390625,
      -1.7421875,
      0.376953125,
      -0.392578125,
      2.375,
      0.95703125,
      0.7890625,
      -0.7265625,
      -1.2734375,
      -0.373046875,
      -0.6328125,
      -0.69921875,
      -3.5625,
      1.2734375,
      2.328125,
      1.0703125,
      -1.28125,
      -0.85546875,
      -3.296875,
      -1.5546875,
      -4.5625,
      2.03125,
      0.84765625,
      -2.796875,
      0.2734375,
      1.6796875,
      1.2890625,
      1.9453125,
      -0.427734375,
      1.234375,
      1.7734375,
      -0.24609375,
      2.46875,
      0.76171875,
      1.8828125,
      -3.390625,
      -0.93359375,
      -0.054443359375,
      0.26171875,
      0.625,
      1.09375,
      -0.875,
      2.75,
      1.15625,
      -1.1015625,
      1.8125,
      2.046875,
      1.1875,
      2.015625,
      -3.359375,
      3.34375,
      2.40625,
      0.703125,
      -1.4140625,
      0.015625,
      0.41015625,
      -0.025146484375,
      -3.171875,
      0.6953125,
      -0.6015625,
      3.890625,
      0.17578125,
      -0.55078125,
      -1.0078125,
      0.91015625,
      1.3671875,
      -1.671875,
      0.85546875,
      -3.15625,
      2.28125,
      1.0390625,
      1.453125,
      -0.451171875,
      -0.6484375,
      -1.84375,
      2.640625,
      -5.21875,
      0.361328125,
      -0.73046875,
      0.67578125,
      0.87109375,
      0.67578125,
      -1.328125,
      -2.984375,
      -1.421875,
      1.90625,
      -1.8828125,
      2.046875,
      1.9140625,
      -0.92578125,
      0.06591796875,
      2.375,
      0.1328125,
      0.2255859375,
      1.7421875,
      -2.5625,
      1.109375,
      -0.94140625,
      -1.9921875,
      -3.1875,
      -1.8671875,
      -6.71875,
      -2.234375,
      -1.4609375,
      -1.5078125,
      0.64453125,
      2.046875,
      0.90625,
      0.2412109375,
      -1.8125,
      1.8125,
      3.765625,
      0.025390625,
      -1.1484375,
      2.484375,
      -1.671875,
      0.65234375,
      -2,
      -0.40234375,
      0.453125,
      0.4140625,
      3.296875,
      -0.470703125,
      3.03125,
      0.57421875,
      -1.890625,
      1.21875,
      0.294921875,
      -1.0390625,
      -1.3046875,
      0.140625,
      1.2265625,
      2.078125,
      3.765625,
      3.90625,
      2.515625,
      -0.921875,
      -0.68359375,
      0.474609375,
      0.81640625,
      6.5,
      0.84375,
      -1.078125,
      -1.328125,
      2.828125,
      1.0390625,
      1.046875,
      0.55859375,
      0.74609375,
      5.65625,
      0.89453125,
      0.234375,
      -1.8671875,
      0.00897216796875,
      -1.1484375,
      0.2265625,
      5.5625,
      -4.65625,
      0.376953125,
      -1.359375,
      -1.203125,
      1.84375,
      -0.185546875,
      3.390625,
      0.05859375,
      5.0625,
      1.875,
      0.33984375,
      0.99609375,
      1.109375,
      -0.96484375,
      2.484375,
      1.40625,
      -1.9609375,
      -0.5859375,
      0.1787109375,
      -0.58203125,
      2.28125,
      -1.34375,
      -0.953125,
      1.9296875,
      -2.640625,
      1.6171875,
      -0.056640625,
      1.515625,
      2.328125,
      -1.1875,
      -1.5546875,
      -3.5,
      -1.84375,
      1.3515625,
      3.90625,
      0.6328125,
      -3.65625,
      -0.41015625,
      -0.1875,
      -0.443359375,
      -2.03125,
      0.2578125,
      -1.84375,
      -3.15625,
      -2.515625,
      -1.6796875,
      -3.421875,
      5.21875,
      -1.5703125,
      2.515625,
      4.4375,
      3.359375,
      2.703125,
      2.921875,
      -0.0703125,
      3.796875,
      -2.890625,
      0.361328125,
      3.65625,
      -0.47265625,
      -1.1484375,
      -0.82421875,
      1.28125,
      -1.1171875,
      -3.015625,
      -1.15625,
      -0.52734375,
      1.7734375,
      -0.40625,
      -4.09375,
      -1.40625,
      0.0279541015625,
      -3.578125,
      -1.5,
      0.65625,
      -2.296875,
      -1.5390625,
      0.70703125,
      1.8828125,
      0.310546875,
      1.5625,
      -0.09619140625,
      0.271484375,
      0.70703125,
      -0.8359375,
      -2.34375,
      -1.359375,
      1.1484375,
      2.125,
      -1.6640625,
      -0.1494140625,
      -0.6953125,
      4.5,
      -3.796875,
      2.515625,
      -1.265625,
      -1.984375,
      2.734375,
      9.3125,
      0.73046875,
      -0.11669921875,
      -0.06982421875,
      0.80078125,
      -3.578125,
      1.203125,
      0.056884765625,
      -2.9375,
      -3.375,
      3.875,
      -1.6484375,
      1.9453125,
      0.71875,
      -0.2578125,
      0.2119140625,
      -2.6875,
      0.38671875,
      -3.421875,
      -1.9921875,
      3.171875,
      4.5625,
      -1.9609375,
      -0.74609375,
      1.765625,
      -4.25,
      3.171875,
      -2.765625,
      -2.421875,
      1.390625,
      4.5625,
      -1.171875,
      5.15625,
      0.51171875,
      -1.640625,
      2.96875,
      0.1376953125,
      1.734375,
      2.046875,
      -2.34375,
      -1.03125,
      -1.90625,
      2.703125,
      -1.6640625,
      -5.4375,
      -0.0908203125,
      -1.7421875,
      -0.8359375,
      -0.07958984375,
      0.01092529296875,
      2.890625,
      0.2158203125,
      -0.314453125,
      2.40625,
      1.9609375,
      0.59375,
      1.4296875,
      -0.203125,
      0.8203125,
      -6.375,
      0.62109375,
      -1.171875,
      0.419921875,
      0.353515625,
      0.9453125,
      1.5859375,
      -0.263671875,
      -0.6015625,
      -1.5,
      1.4921875,
      -0.66796875,
      0.984375,
      -0.251953125,
      -0.2060546875,
      -2.125,
      0.5234375,
      -3.59375,
      2.84375,
      -2.265625,
      -2.390625,
      0.5546875,
      -1.90625,
      -1.8515625,
      -4.5,
      1.0625,
      2.09375,
      -0.201171875,
      1.3359375,
      0.0206298828125,
      2.578125,
      -2.71875,
      1.8515625,
      0.953125,
      1.0078125,
      1.1015625,
      -1.6875,
      -1.046875,
      -3.96875,
      1.640625,
      -1.3515625,
      0.65234375,
      4.21875,
      -1.796875,
      -2.984375,
      2.609375,
      1.4765625
    ],
    "suggested_tags": [
      "智能教育",
      "大语言模型",
      "教育技术",
      "文献综述"
    ],
    "tag_suggestions": [
      {
        "name": "智能教育",
        "confidence": 0.95,
        "reason": "论文核心研究领域为大型语言模型在教育领域的应用，属于教育技术的重要方向"
      },
      {
        "name": "大语言模型",
        "confidence": 0.9,
        "reason": "论文系统综述了LLMs在教育中的技术特点、应用场景和挑战"
      },
      {
        "name": "教育技术",
        "confidence": 0.85,
        "reason": "论文聚焦于人工智能技术在教育行业的整合与应用方法"
      },
      {
        "name": "文献综述",
        "confidence": 0.8,
        "reason": "论文采用系统性综述方法，总结当前技术现状并展望未来发展"
      }
    ],
    "s2_graph": {
      "citations": [
        {
          "external_id": "CorpusId:283602068",
          "title": "Evaluating Adaptive and Generative AI-Based Feedback and Recommendations in a Knowledge-Graph-Integrated Programming Learning System",
          "authors": [
            "Lalita Na Nongkhai",
            "Jingyun Wang",
            "Adam T. Wynn",
            "T. Mendori"
          ],
          "year": 2025,
          "venue": "Computers and Education: Artificial Intelligence",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283438556",
          "title": "FEANEL: A Benchmark for Fine-Grained Error Analysis in K-12 English Writing",
          "authors": [
            "Jingheng Ye",
            "Shen Wang",
            "Jiaqi Chen",
            "Hebin Wang",
            "Deqing Zou",
            "Yanyu Zhu",
            "Jiwei Tang",
            "Hai-Tao Zheng",
            "Ruitong Liu",
            "Haoyang Li",
            "Yanfeng Wang",
            "Qingsong Wen"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283261962",
          "title": "LOOM: Personalized Learning Informed by Daily LLM Conversations Toward Long-Term Mastery via a Dynamic Learner Memory Graph",
          "authors": [
            "Justin Cui",
            "Kevin Pu",
            "Tovi Grossman"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283299903",
          "title": "Research on the Intelligent Reform Pathway of Higher Education Empowered by Generative Artificial Intelligence",
          "authors": [
            "Gao Min"
          ],
          "year": 2025,
          "venue": "Artificial Intelligence and Digital Technology",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283025009",
          "title": "THE RISE OF LARGE LANGUAGE MODELS: A BEGINNER’S SURVEY",
          "authors": [
            "Gustavo de Aquino Mouzinho",
            "Leandro Youiti Silva Okimoto",
            "Leonardo Yuto Suzuki Camelo",
            "Nádila da Silva de Azevedo",
            "Hendrio Bragança",
            "Rubens de Andrade Fernandes",
            "Fabricio Ribeiro Seppe",
            "Raimundo Claúdio Souza Gomes",
            "Fábio de Sousa Cardoso"
          ],
          "year": 2025,
          "venue": "ARACÊ",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283103684",
          "title": "Human or LLM as Standardized Patients? A Comparative Study for Medical Education",
          "authors": [
            "Bingquan Zhang",
            "Xiaoxiao Liu",
            "Yuchi Wang",
            "Lei Zhou",
            "Qianqian Xie",
            "Benyou Wang"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:282911236",
          "title": "Using LLMs to support assessment of student work in higher education: a viva voce simulator",
          "authors": [
            "Ian M. Church",
            "Lyndon Drake",
            "Mark Harris"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283569751",
          "title": "SPARK – Smart Plug-and-Play AI Framework for RAG & Knowledge",
          "authors": [
            "Nirmit Dagli",
            "Chetan Jaiswal",
            "Sanjeev Kumar Marimekala"
          ],
          "year": 2025,
          "venue": "Ubiquitous Computing, Electronics & Mobile Communication Conference",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:282107557",
          "title": "Modular Framework Integrating Large Language Models with Drilling Hazard Detection Systems to Provide Operational Context-Informed Interpretations and Recommended Actions",
          "authors": [
            "S. Suhail",
            "T. S. Robinson",
            "O. Revheim",
            "P. Bekkeheien"
          ],
          "year": 2025,
          "venue": "SPE Annual Technical Conference and Exhibition",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:281787333",
          "title": "Defying Data Scarcity: High-Performance Indonesian Short Answer Grading via Reasoning-Guided Language Model Fine-Tuning",
          "authors": [
            "Muhammad Naufal Faza",
            "P. D. Purnamasari",
            "A. A. P. Ratna"
          ],
          "year": 2025,
          "venue": "International Journal of Electrical, Computer, and Biomedical Engineering",
          "citation_count": 0
        }
      ],
      "citations_fetched_at": "2025-12-16T15:26:07.745222",
      "references": [
        {
          "external_id": "CorpusId:266054920",
          "title": "Large Language Models in Law: A Survey",
          "authors": [
            "Jinqi Lai",
            "Wensheng Gan",
            "Jiayang Wu",
            "Zhenlian Qi",
            "Philip S. Yu"
          ],
          "year": 2023,
          "venue": "AI Open",
          "citation_count": 150
        },
        {
          "external_id": "CorpusId:265351653",
          "title": "Multimodal Large Language Models: A Survey",
          "authors": [
            "Jiayang Wu",
            "Wensheng Gan",
            "Zefeng Chen",
            "Shicheng Wan",
            "Philip S. Yu"
          ],
          "year": 2023,
          "venue": "BigData Congress [Services Society]",
          "citation_count": 282
        },
        {
          "external_id": "CorpusId:265352038",
          "title": "Large Language Models in Education: Vision and Opportunities",
          "authors": [
            "Wensheng Gan",
            "Zhenlian Qi",
            "Jiayang Wu",
            "Chun-Wei Lin"
          ],
          "year": 2023,
          "venue": "BigData Congress [Services Society]",
          "citation_count": 128
        },
        {
          "external_id": "CorpusId:265149884",
          "title": "Large Language Models for Robotics: A Survey",
          "authors": [
            "Fanlong Zeng",
            "Wensheng Gan",
            "Yongheng Wang",
            "Ning Liu",
            "Philip S. Yu"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 189
        },
        {
          "external_id": "CorpusId:265128707",
          "title": "Model-as-a-Service (MaaS): A Survey",
          "authors": [
            "Wensheng Gan",
            "Shicheng Wan",
            "Philip S. Yu"
          ],
          "year": 2023,
          "venue": "BigData Congress [Services Society]",
          "citation_count": 35
        },
        {
          "external_id": "CorpusId:263608784",
          "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
          "authors": [
            "Weixin Liang",
            "Yuhui Zhang",
            "Hancheng Cao",
            "Binglu Wang",
            "Daisy Ding",
            "Xinyu Yang",
            "Kailas Vodrahalli",
            "Siyu He",
            "D. Smith",
            "Yian Yin",
            "Daniel A. McFarland",
            "James Zou"
          ],
          "year": 2023,
          "venue": "NEJM AI",
          "citation_count": 217
        },
        {
          "external_id": "CorpusId:263334045",
          "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment",
          "authors": [
            "Tianhao Wu",
            "Banghua Zhu",
            "Ruoyu Zhang",
            "Zhaojin Wen",
            "K. Ramchandran",
            "Jiantao Jiao"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 70
        },
        {
          "external_id": "CorpusId:263310363",
          "title": "A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM",
          "authors": [
            "Jongyoon Lim",
            "Inkyu Sa",
            "Bruce A. MacDonald",
            "Ho Seok Ahn"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 9
        },
        {
          "external_id": "CorpusId:261582620",
          "title": "ImageBind-LLM: Multi-modality Instruction Tuning",
          "authors": [
            "Jiaming Han",
            "Renrui Zhang",
            "Wenqi Shao",
            "Peng Gao",
            "Peng Xu",
            "Han Xiao",
            "Kaipeng Zhang",
            "Chris Liu",
            "Song Wen",
            "Ziyu Guo",
            "Xudong Lu",
            "Shuai Ren",
            "Yafei Wen",
            "Xiaoxin Chen",
            "Xiangyu Yue",
            "Hongsheng Li",
            "Y. Qiao"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 148
        },
        {
          "external_id": "CorpusId:261582366",
          "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function",
          "authors": [
            "Haotian Xu"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 15
        }
      ],
      "references_fetched_at": "2025-12-16T15:26:08.438467"
    },
    "analysis": {
      "paper_id": "f6ddea83-75e3-472e-8798-80000e520eb9",
      "status": "completed",
      "started_at": null,
      "completed_at": "2025-12-16T07:26:08.671455",
      "summary": "本文针对大型语言模型在教育领域的应用展开系统性综述。研究旨在梳理智能教育背景下LLMs的整合现状、技术特点、应用效益及面临挑战。论文采用文献综述方法，系统分析LLMs与教育融合的技术路径，重点探讨深度学习、预训练、微调和强化学习等技术在教育场景的具体应用。\n\n研究发现，LLMs通过个性化学习支持、即时反馈和多模态交互等功能，显著提升教学质量并推动教育模式变革。其主要贡献在于构建了LLMEdu的技术框架，详细阐述了从技术整合到实际应用的完整流程。同时研究指出，当前LLMEdu仍存在技术适应性、数据隐私和伦理规范等挑战。最后，论文对未来优化方向提出展望，强调需进一步探索LLMs在教育场景的精准化应用路径。",
      "methods": [
        {
          "name": "系统综述",
          "description": "本文采用系统综述的方法对LLMEdu进行全面分析。该方法通过总结当前技术状况、整合相关研究成果，形成对智能教育领域的结构化概述。",
          "location": null
        },
        {
          "name": "文献分析",
          "description": "通过对现有文献的深入分析，识别LLMs在教育领域的研究空白和技术特点。该方法有助于理解当前研究现状和未来发展方向。",
          "location": null
        },
        {
          "name": "技术框架分析",
          "description": "分析LLMs的基本技术框架和核心组件，包括预训练、微调等关键技术。该方法帮助理解LLMs在教育应用中的技术基础。",
          "location": null
        },
        {
          "name": "应用过程分析",
          "description": "深入分析LLMs在教育行业的应用过程，从技术整合到实际实施的各个阶段。该方法揭示LLMs如何逐步赋能教育领域。",
          "location": null
        },
        {
          "name": "多维度评估",
          "description": "从技术、应用、挑战等多个维度对LLMEdu进行全面评估。该方法提供对智能教育发展的全方位视角。",
          "location": null
        }
      ],
      "datasets": [],
      "code_refs": [],
      "structure": {
        "sections": [
          {
            "title": "Large Language Models for Education: A Survey",
            "level": 1,
            "start_line": 1
          },
          {
            "title": "ARTICLE INFO",
            "level": 1,
            "start_line": 9
          },
          {
            "title": "ABSTRACT",
            "level": 1,
            "start_line": 18
          },
          {
            "title": "1. Introduction",
            "level": 1,
            "start_line": 22
          },
          {
            "title": "2. Characteristics of LLM in Education",
            "level": 1,
            "start_line": 44
          },
          {
            "title": "2.1. Characteristics of LLMs",
            "level": 1,
            "start_line": 55
          },
          {
            "title": "2.2. Characteristics of education",
            "level": 1,
            "start_line": 71
          },
          {
            "title": "2.2.1. Educational development process",
            "level": 1,
            "start_line": 75
          },
          {
            "title": "2.2.2. Impact on teachers",
            "level": 1,
            "start_line": 93
          },
          {
            "title": "2.2.3. Educational challenges",
            "level": 1,
            "start_line": 109
          },
          {
            "title": "2.3. Characteristics of LLMEdu",
            "level": 1,
            "start_line": 123
          },
          {
            "title": "2.3.1. Specific embodiment of \"LLMs + education\"",
            "level": 1,
            "start_line": 127
          },
          {
            "title": "2.3.2. Impact of \"LLMs + education\"",
            "level": 1,
            "start_line": 144
          },
          {
            "title": "3. How to Gradually Integrate LLMs into Education",
            "level": 1,
            "start_line": 172
          },
          {
            "title": "3.1. Reasons why LLMs for education",
            "level": 1,
            "start_line": 176
          },
          {
            "title": "3.2. Fusion strategies",
            "level": 1,
            "start_line": 198
          },
          {
            "title": "4. Key Technologies for LLMEdu",
            "level": 1,
            "start_line": 214
          },
          {
            "title": "5. Implementation of LLMEdu",
            "level": 1,
            "start_line": 247
          },
          {
            "title": "5.1. LLMs-empowered education",
            "level": 1,
            "start_line": 254
          },
          {
            "title": "5.2. LLMs in Mathematics",
            "level": 1,
            "start_line": 278
          },
          {
            "title": "6. Issues and Challenges",
            "level": 1,
            "start_line": 298
          },
          {
            "title": "6.1. Main issues",
            "level": 1,
            "start_line": 302
          },
          {
            "title": "6.2. Main challenges",
            "level": 1,
            "start_line": 323
          },
          {
            "title": "7. Conclusion",
            "level": 1,
            "start_line": 345
          },
          {
            "title": "References",
            "level": 1,
            "start_line": 357
          }
        ]
      },
      "error_message": null
    }
  },
  "7a7392e0-5765-4905-ad67-e4f3ad4ecac8": {
    "id": "7a7392e0-5765-4905-ad67-e4f3ad4ecac8",
    "filename": "user_paper.pdf",
    "file_path": "data/uploads/da023352-786f-40d3-b71e-3503bb71b2e0/7a7392e0-5765-4905-ad67-e4f3ad4ecac8_user_paper.pdf",
    "status": "failed",
    "created_at": "2025-12-16 17:08:01.587572",
    "updated_at": "2025-12-16 17:08:01.838474",
    "user_id": "da023352-786f-40d3-b71e-3503bb71b2e0",
    "error_message": "Mineru API error: "
  },
  "d572a494-7e2b-4328-819b-57bc6f24cdc8": {
    "id": "d572a494-7e2b-4328-819b-57bc6f24cdc8",
    "filename": "2405.11070v1.pdf",
    "file_path": "data/uploads/64d94413-f66a-4369-b6a0-66dcef7b83ee/d572a494-7e2b-4328-819b-57bc6f24cdc8_2405.11070v1.pdf",
    "status": "completed",
    "created_at": "2025-12-16 18:39:50.677190",
    "updated_at": "2025-12-16 10:40:45.719437",
    "user_id": "64d94413-f66a-4369-b6a0-66dcef7b83ee",
    "title": "Jill Watson: A Virtual Teaching Assistant powered by ChatGPT",
    "markdown_content": "# Jill Watson: A Virtual Teaching Assistant powered by ChatGPT\n\nKaran Taneja, Pratyusha Maiti, Sandeep Kakar, Pranav Guruprasad, Sanjeev Rao, and Ashok K. Goel\n\nGeorgia Institute of Technology, Atlanta, GA {ktaneja6, pmaiti6, skakar6, pguruprasad7, srao373, ag25}@gatech.edu\n\nAbstract. Conversational AI agents often require extensive datasets for training that are not publicly released, are limited to social chit-chat or handling a specific domain, and may not be easily extended to accommodate the latest advances in AI technologies. This paper introduces Jill Watson, a conversational Virtual Teaching Assistant (VTA) leveraging the capabilities of ChatGPT. Jill Watson based on ChatGPT requires no prior training and uses a modular design to allow the integration of new APIs using a skill-based architecture inspired by XiaoIce. Jill Watson is also well-suited for intelligent textbooks as it can process and converse using multiple large documents. We exclusively utilize publicly available resources for reproducibility and extensibility. Comparative analysis shows that our system outperforms the legacy knowledge-based Jill Watson as well as the OpenAI Assistants service. We employ many safety measures that reduce instances of hallucinations and toxicity. The paper also includes real-world examples from a classroom setting that demonstrate different features of Jill Watson and its effectiveness.\n\nKeywords: Virtual Teaching Assistant  $\\cdot$  Intelligent Textbooks  $\\cdot$  Conversational Agents  $\\cdot$  Question Answering  $\\cdot$  Modular AI Design\n\n# 1 Introduction\n\nConversational AI agents can be powerful tools for education as they enable continuous 24x7 support and instant responses to student queries without increasing the workload for instructors. These virtual teaching assistants can help in efficiently scaling quality education in terms of both time and cost. The interactive nature of conversational AI agents allows students to be more inquisitive and increases teaching presence by resembling one-on-one tutoring. Based on the Community of Inquiry framework [6], teaching presence through instructional management and direct instruction leads to an increase in student engagement and retention. Towards this end, we developed Jill Watson, a Virtual Teaching Assistant (VTA) powered by ChatGPT for online classrooms which answers students' queries based on course material such as slides, notes, and syllabi.\n\nIn previous work, the legacy Jill Watson [8,5] (henceforth LJW) is a question-answering system for course logistics and uses a two-dimensional database of\n\ninformation organized by course deliverables (assignments, exams, etc.) and information categories (submission policy, deadline, etc.). It also uses a list of FAQs for course-level information such as ethics and grading policies. In this paper, we introduce the new Jill Watson which is conversational and answers questions related to course logistics as well as course content based on multiple large documents provided as context.\n\nChatGPT or GPT-3.5, based on GPT-3 [3], is a powerful large language model (LLM) trained to follow instructions and hold a dialogue. It is capable of attending to a large context and constructing meaningful text in response to user inputs. Many conversational systems such as HuggingGPT [21], Microsoft Bing Chat (www.bing.com/chat), and LangChain (www.langchain.com) based systems leverage ChatGPT for performing context-aware response generation and zero-shot learning. ChatGPT and other LLMs suffer from hallucination i.e. they generate text that can be inconsistent or unverifiable with the source text, or absurd in a given context [9]. While hallucination is useful in creative tasks such as story writing, it is detrimental in information-seeking tasks such as those in the domain of education. ChatGPT and other LLMs also have safety issues as they generate text that may be considered toxic or inappropriate [25].\n\nThis work introduces Jill Watson's architecture which does not require any model training or fine-tuning and is designed to address the above LLM-related concerns. We address the hallucination issue by citing the documents from which information is obtained and verifying grounding using textual entailment. To prevent Jill Watson from answering unsafe questions or generating unsafe responses, we employ a classifier for question relevance, toxic text filters, and prompts that promote politeness in response generation. Further, Jill Watson is designed to answer questions based on multiple large documents which makes it well-suited for intelligent textbooks. We only rely on publicly available resources to promote future research in this direction.\n\nThe paper has three main contributions: (i) we introduce Jill Watson, a virtual teaching assistant powered by ChatGPT with a skill-based architecture, (ii) detail all the different modules of Jill Watson and associated algorithms, and (iii) quantitatively evaluate Jill Watson to measure response quality and safety, along with a discussion on examples from our first deployment.\n\nSection 2 discusses Jill Watson in the context of related work. Section 3 describes the architecture and each module in detail. Section 4 describes our experimental results comparing Jill Watson to two strong baselines in terms of response quality and safety along with examples (see Table 3) from our first deployment. We conclude the paper in Section 5 with a summary of the strengths, limitations, and potential impact of Jill Watson.\n\n# 2 Related Work\n\nQuestion Answering can either be open-ended or grounded in knowledge. Without a knowledge source, question-answering models based on LLMs [16,22] are expected to store the information in their parameters during the training.\n\nIn grounded question answering, previous work has explored different types of contexts including the web [17], machine reading comprehension [1], knowledge bases [2], and short text documents [18,27]. Some methods assume access to the correct context from the document [18]. Further, many methods require training with datasets that are expensive to collect and do not generalize well [24,26]. Jill Watson neither imposes a limit on the document size nor requires a training dataset. It pre-processes large documents and answers incoming questions based on passages retrieved using dense passage retrieval (DPR) [11].\n\nRetrieval Augmented Generation (RAG) is a well-known method [14] for increasing the reliability of LLMs by generating text conditioned on source texts that are retrieved based on a query. Knowledge-grounded generative models have two main goals: factuality and conversationality [19]. Factuality minimizes hallucinations by ensuring consistency of output with the retrieved texts while conversationality refers to relevance of the information to the query and generation without repetition. Previous work has shown improved factuality using RAG in dialog response generation task to remain consistent with a persona [23], knowledge-grounded generation [13,19,26] and machine translation [4].\n\nMany models use large training datasets to learn question answering from contexts [2,24,26]. On the other hand, WikiChat [19] uses seven-step few-shot prompts based question answering system which uses both retrieval and open-ended generation to answer questions using Wikipedia.  $\\mathrm{Re}^2\\mathrm{G}$  or Retrieve, Re-rank, Generate [7] also uses retrieval for generating outputs but uses an additional re-ranking step to score retrieved passages before generation. Further, it can also be trained end-to-end after initial fine-tuning. Jill Watson solves the knowledge-intensive generation problem using RAG but without any fine-tuning by using open-source DPR models for retrieval and using ChatGPT to construct responses. Because of clever prompting and indexing, it is also able to refer to the document and page number from which the response was generated.\n\nSafety in LLMs is important to avoid harm because of hateful, offensive, or toxic text. Previous work on evaluating the toxicity of ChatGPT has found that assigning personas, using non-English languages, prompting with creative tasks, jailbreak prompts, and higher temperature values can all lead to more toxic responses [25]. Perspective API [12] and OpenAI Moderation API [15] are popular services to measure toxicity in various categories including hateful content, violence, etc. Jill Watson ensures safety in three ways: (i) OpenAI Moderation API for both user inputs and its own responses, (ii) skill classifier to identify irrelevant queries, and (iii) encourages polite responses in prompts to ChatGPT.\n\nDialog Systems are AI agents designed for human-like conversations, typically using hybrid architectures involving both rule-based systems and machine learning systems. For instance, MILABOT [20] uses rule-based and generative models along with a response selection policy trained using reinforcement learning. Microsoft XiaoIce [28] uses a skill-based architecture where the chat manager selects one of 230 high-level skills to generate responses. To the best of our knowledge, such powerful multi-turn dialog systems have not been introduced in a classroom. The legacy Jill Watson [8] is a single-turn question-answering\n\n![](/uploads/images/d572a494-7e2b-4328-819b-57bc6f24cdc8/0fa51ad6db48b2e0593c73bb2200b94e603f95c19a7b007d2d1689fd6daec81e.jpg)  \nFig. 1. Architecture of Jill Watson: After the coreference resolution of an incoming query, the skill classifier is used to find the most appropriate skill for response generation. Jill Watson's skills include Contextual Answering, Greetings, etc. The updated conversation history is used as context for generating responses in the future.\n\nsystem for course logistics and policies. OpenAI Assistants service<sup>1</sup> provides a way to instantiate a ChatGPT-based agent to generate responses using text documents. Inspired by XiaoIce, Jill Watson has a skill-based architecture and is designed to be a safe conversational agent for classrooms that can answer student queries related to course logistics as well as course content using ChatGPT in the backend. It is also well-suited for other applications in education like intelligent textbooks given its ability to use long documents as context.\n\n# 3 Jill Watson Architecture\n\nThe architecture of Jill Watson shown in Figure 1 takes inspiration from the skill-based architecture of XiaoIce [28]. XiaoIce relies on different skills such as task completion, image commenting, content creation, etc. to interact with users and selects the appropriate skill for each conversation turn based on the previous context. In Jill Watson, the query with resolved coreferences is used to decide the most appropriate skill for answering the incoming query. The skill-based design of Jill Watson makes it extensible as we can easily plug in new API services and other capabilities in the future in the form of new skills.\n\nContextual Answering skill is responsible for answering questions where content needs to be retrieved from course content or syllabus and Self-awareness skill answers queries about Jill Watson itself. As we will discuss later, these two skills make Jill Watson a knowledge-grounded AI agent with the ability to refer to multiple documents and cite relevant content in its answers. Further, ChatGPT allows Jill Watson to be conversational by using past messages as context in generating responses to user messages. We also utilize many safety features in Jill Watson which include detecting irrelevant queries by skill classifier, moderation filters, and prompts to encourage courteous responses.\n\n# 3.1 Coreference Resolution\n\nCoreference resolution involves determining the entities that are indirectly referenced in a text and making them explicit using nouns or noun phrases. For example, given the context 'John started reading a book', the query 'When did he start?' has two coreferences. An explicit coreference is suggested by the word 'he' while an implicit coreference to an event is present because of 'When' and 'start'. In coreference resolution, we must resolve the reference 'he' and the event that 'When' and 'start' are referring to. Therefore, the resolved query would be 'When did John start reading the book?' formed by replacing 'he' with John and adding the implicit event 'reading the book'.\n\nWhile ChatGPT implicitly resolves the coreferences as it can construct appropriate responses by itself, since we wish to use existing models for retrieval without any fine-tuning, we need to construct complete queries with resolved coreferences before passing them to the retrieval module. Hence, the first step in Jill Watson is to resolve coreferences in the user query based on the previous messages. In the example in Figure 1, 'it' in 'When is it due?' is replaced by the entity 'Assignment 2'. This is done by prompting ChatGPT to resolve the coreferences in the received query and passing the past messages as context. We use a combined instruction and demonstration-based prompt where we explain the task (instruction) along with three demonstrations with no coreferences, an explicit and an implicit coreference. In our investigation, we found demonstrations to be extremely useful for improving performance, especially on implicit coreferences which are much harder to identify.\n\n# 3.2 Skill Classifier\n\nAs discussed earlier, Jill Watson uses various skills to answer different types of queries. For instance, queries that require retrieval are forwarded to the Contextual Answering Skill while greetings are answered by the Greetings Skill. The skill-based division using a skill classifier allows us to use different response-generation techniques based on the user query. It can also aid in understanding user behaviors by analyzing the skill distribution of student queries.\n\nTo forward a user query to the appropriate skill, the resolved query (after coreference resolution) is used is to perform skill classification by prompting ChatGPT. In the example in Figure 1, the selected skill is Contextual Answering based on the resolved query and previous messages as context. We again used a combined instruction and demonstration-based prompt with an explanation of each skill (instruction) and one demonstration per skill. We found that fewer and distinct classes lead to a better performance which motivates the use of a small number of skills as far as possible.\n\n# 3.3 Contextual Answering\n\nContextual answering or context-based question answering skill involves answering questions based on the given information. For Jill Watson, this information\n\nAlgorithm 1 Contextual Answering Skill  \nRequire:Resolved query  $Q$  ,context  $C$  ,PRE-PROCESSED DOCUMENTS with passages   \n $\\mathcal{P}$  and corresponding context embeddings  $\\mathcal{D}$  , DPR query encoder  $E_{q}(.)$    \nEnsure:Response  $R$  ,confidence  $C\\in \\{\\mathrm{low},\\mathrm{high}\\}$    \n// DENSE PASSAGE RETRIEVAL   \nConstruct a query with context  $Q_{C}$  with  $Q$  and  $C$    \nSort passages  $\\mathcal{P}$  using cosine similarity between  $e_{Q,C} = E_{q}(Q_{C})$  and context embeddings in  $\\mathcal{D}$  .Keep top-20 passages  $P$  with highest cosine similarity.   \n// CONTEXT Loop   \nfor batches  $P_{5}$  of 5 passages in top-20 do   \n//RESPONSE GENERATION   \nGenerate response  $R$  by prompting ChatGPT with  $C,Q,$  and  $P_{5}$  if  $R$  answers  $Q$  then   \n// TEXTUAL ENTAILMENT   \nif  $^ { \" P _ { 5 } }$  implies  $R ^ { \" }$  succeeds:return  $R$  and  $C =$  high. else: return  $R$  and  $C =$  low. end if   \nend if   \nend for   \nreturn  $R$  and  $C =$  low.\n\nconsists of verified course documents provided by course instructors. The process outlined in Algorithm 1 can be divided into five main parts, highlighted in SMALL CAPS, viz. documents pre-processing, DPR, response generation, textual entailment and context loop. The documents pre-processing step is performed only once when a Jill Watson is initialized with a set of course documents. The remaining four steps have to be performed for every query received by the Contextual Answering skill.\n\nDocuments Pre-processing: Jill Watson pre-processes course documents used for answering student queries and stores them as a list of passages along with their different representations discussed below. These representations allow fast retrieval of the most relevant parts of documents during run-time. It accepts PDF documents, the most common format in which course contents (syllabus, notes, books, and slides) are distributed. After text extraction from each PDF document using Adobe PDF Extract API, it is divided into pages and each page is further divided into paragraphs. We group paragraphs into passages of at least 500 characters (90-100 words). This ensures a significant context size in each passage for the DPR step. We also store document and page information with each passage which is used to refer back and cite the documents. Further, there is a  $50\\%$  overlap between passages for added redundancy and to represent continuity between consecutive passages.\n\nFigure 2 shows different representations stored for each passage. Since we use DPR [11] in the retrieval step (discussed in more detail later), we pre-compute embeddings of each passage using the DPR context encoder. The context encoder requires passages with prepended headings which are obtained by prompting ChatGPT to generate a 2-3 word heading. We use a cleaned version of the original text for the context encoder model because the raw text from the PDF\n\n![](/uploads/images/d572a494-7e2b-4328-819b-57bc6f24cdc8/d753db231d8a6fc397aec096e4ee34a3f875483a09c97f09b79ba0a7242576c6.jpg)  \nFig. 2. Passage representation consists of the original text, heading, clean text, summary text, and context embeddings of both clean and summary texts.\n\nhas unwanted spaces, special characters, and poor table formatting. To obtain this clean text, we again prompt ChatGPT with the original text and instruct it to clean the text and to format the tables. Further, we also prompt ChatGPT to summarize the clean text, and we store a context embedding of this passage summary. We found that this is useful for retrieval because passages can contain implicit information which is made more explicit in summaries. For example, an exam might be mentioned in a different line from its deadline but a summary makes this relation more explicit and direct.\n\nDense Passage Retrieval: DPR [11] has a dual-encoder architecture i.e. it consists of a query encoder and a context encoder. During training, the two models are aligned to output in a common embedding space by maximizing the similarity between embeddings of query-context pairs in training data and minimizing the similarity across example pairs. We used the multi-dataset model which is trained on Natural Questions, TriviaQA, WebQuestions, and TREC datasets [11], allowing it to generalize over domains and text properties.\n\nThe first step in DPR is to compute query embeddings of the coreference-resolved queries. We found that prepending the question history in the query to provide more context improves retrieval performance. Second, we compute the similarity of the query embedding with context embeddings of both clean and summary text. Third, we sort the passages in decreasing order of similarity and group clean text in batches of five for prompting. For sorting, the similarity score of a passage is the maximum between similarities with clean and summary embeddings. Finally, we also pass the document and page information for each passage to the next step where we prompt ChatGPT to use this information for citing document and page when constructing an answer.\n\nChatGPT API Call: In Jill Watson, we wish to prompt ChatGPT to construct an answer with four requirements: (i) ChatGPT should only answer student query if the context has the information to answer the query, or else, refuse to answer, (ii) it should not add any additional information to the answer\n\nthat is not present in the context, (iii) it should cite the corresponding document and page number when using content from a text chunk, and (iv) it should answer the query with respect to the current date and time.\n\nThe prompt to construct an answer using context is given in Figure 3 (left). Note that we prompt ChatGPT to use only the given context which is made up of a batch of five passages, encourage it to say it doesn't know the answer if the context is insufficient, and ask it to cite the document and page number (see example in Figure 3). We also feed it the current week (Monday-Sunday), date and time, and past messages which can be useful in answering user queries in the context of the present time and the past conversation.\n\n<table><tr><td>System: A polite virtual assistant named Jill Watson answers student (user) questions on a class forum.\nThe context below is provided to answer questions.\n CONTEXT: {context}\nOnly the above context is used and rephrased to answer the question. If context does not answer it, say that you don’t know the answer. If answer is found, don’t forget to cite the document and page number.\nCURRENT WEEK: {current-week}\nCURRENT TIME: {current-time}\nUser and Assistant Messages:\n{past-messages} {most-recent-query}</td><td>USER: When is Mini-Project 2 due?\nJill Watson: Mini-Project 2 is due on Monday, September 25, 2023 at 9 am. You need to submit the code for solving the problem to the Mini-Project 2 assignment on Gradescope and also submit a report describing your agent to Canvas. Your grade for this project will be based on a combination of your report’s quality (50%) and your agent’s performance (50%).\nSource: Syllabus, Page 13</td></tr></table>\n\nFig. 3. Answer generation prompt (left) and an example response with citation (right). The context contains five passages with document name and page numbers.\n\nTextual Entailment: The textual entailment task involves determining the entailment relation between two texts. A text  $T$  entails hypothesis  $H$  if  $H$  is a logical consequence of text  $T$  i.e.  $T \\Rightarrow H$ . If  $T \\Rightarrow \\neg H$ , we say that  $T$  contradicts  $H$ .  $T$  can also be neutral i.e. it neither entails nor contradicts  $H$ . Jill Watson's prompt for answer generation contains instructions to not answer a question when the context doesn't provide an answer and to only use the given context for answering. However, we wish to have an additional check to detect hallucinations through textual entailment. Given a context  $C$  and an answer  $A$ , we wish to check if  $C$  entails  $A$ . If  $C$  doesn't completely entail  $A$ , there is information in  $A$  that was not retrieved from  $C$ . In such a case, we pretend a warning for the user conveying that the confidence in the answer is low and encourage them to check the answer on their own. As shown in Algorithm 1, we check if the context  $C = P_5$  is used to generate the answer  $A = R$ . We prompt ChatGPT with  $P_5$  as the text and  $R$  as the hypothesis and ask it to determine if the text implies that hypothesis. We found a simple instruction-based prompt to be most effective with the highest recall for non-entailed answers.\n\nContext Loop: After scoring and sorting the passages, we group the top twenty passages into four batches of five and prompt ChatGPT to generate a response based on the first batch as context. If it fails to answer using the first batch, we use the second batch of passages, and so on until a valid answer is generated. To check if ChatGPT generated a valid answer, we prompt it to classify the response as NEGATIVE if it refuses to reply because the information is not present or it suggests contacting the staff, and NEUTRAL otherwise.\n\n# 3.4 Other Skills\n\nIn addition to contextual answering, we use other skills for additional capabilities and we plan to expand these in the future with software tools and API services.\n\nSelf-awareness Skill: Many curious users ask AI agents about itself to check its self-awareness or to know more about the system. For such queries, we prompt ChatGPT to answer the user query based on a textual description of Jill Watson. This textual description contains basic information about us, the team of researchers who built it, and a blurb about its capabilities.\n\nGreeting Skill: If an incoming query is a greeting or conveys gratitude to Jill Watson, we prompt ChatGPT to generate a polite reply.\n\nIrrelevant Skill: If a query doesn't fit into any of the other skills, we use a fixed polite message asking the user to change or rephrase their question.\n\n# 3.5 Moderation Filter\n\nFor deployment in real world, Jill Watson should be safe to use and not accept any harmful requests or generate a harmful response. To this end, we filter input user queries and outputs of Jill Watson using the OpenAI Moderation API [15]. The API allows Jill Watson to detect different categories of unsafe text including hate, hate and threatening, harassment, harassment and threatening, self-harm, self-harm intention, self-harm instructions, sexual, sexual involving minors, and violence with graphic depictions.\n\n# 4 Results\n\nWe compare Jill Watson with both legacy Jill Watson (LJW), and the OpenAI Assistants service (OAI-Assist). LJW baseline employs an intent classifier and a database of information organized by course deliverables and information categories as well as a list of FAQs. OAI-Assist allows users to upload files PDF files and employs retrieval to generate answers for user queries. Our new system Jill Watson uses coreference resolution, skill classification, dual encoder DPR, ChatGPT for generation, and safety features described earlier. Both OAI-Assist and Jill Watson use 'gpt-3.5-turbo-1106' for retrieval-augmented generation in our experiments.\n\nResponse Quality and Harmful Errors: We used a set of 150 questions created by four students based on the syllabus, e-book, and video transcripts for\n\nTable 1. Response Quality: A set of 150 questions is used to evaluate the response quality of each system. Failures are further determined to be harmful, confusing, and stemming from poor retrieval.  \n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Pass</td><td colspan=\"3\">Failures</td></tr><tr><td>Harmful</td><td>Confusing</td><td>Retrieval</td></tr><tr><td>LJW</td><td>26.0%</td><td>-</td><td>60.4%</td><td>-</td></tr><tr><td>OAI-Assist</td><td>31.3%</td><td>16.5%</td><td>72.8%</td><td>68.0%</td></tr><tr><td>Jill Watson</td><td>76.7%</td><td>5.7%</td><td>62.8%</td><td>57.1%</td></tr></table>\n\na course on AI. The ground truth answers contain text from these documents or 'I don't know' (IDK) responses for unanswerable questions that students deliberately added based on our instructions. These 150 questions were asked to each of the systems and the answers were evaluated by human annotators based on ground truth values and labeled as 'Pass' or 'Fail' (Cohen's  $\\kappa = 0.76$  in inter-rater reliability test). Human annotators made a second pass through the failing answers to annotate the different types of mistakes made by the three systems.\n\nThe results are shown in Table 1. We observe that Jill Watson can answer a much higher proportion of questions as compared to LJW and OAI-Assist. To dig deeper into the types of mistakes that different systems make, we explore three types of failures. Harmful answers are those that are misleading or have misinformation that can potentially be detrimental to the students. Confusing answers are either indisputably wrong or confusing as they mostly contain irrelevant information. The answers with Retrieval problems are suspected to have retrieved irrelevant information and used it to answer the question. Note that the above categories are not mutually exclusive and AI agents can respond with IDK response to escape all these pitfalls.\n\nWe find Jill Watson to be the most reliable in terms of generating the fewest potentially harmful answers, and least retrieval problems as well as confusing answers. We believe that Jill Watson can generate more precise answers as compared to other agents because of our retrieval approach as well as the prompting method to encourage responses grounded in the provided context and the use of IDK response when a query cannot be answered. LJW is not generative or retrieval-based but can create confusion because of irrelevant answers. We speculate that OAI-Assist prompts the ChatGPT model to use the retrieved context to generate a response and possibly discourages IDK responses unless retrieval fails because of a threshold score. We believe that our use of multiple representations of the same passages with overlapping text for retrieval is a better approach as compared to OAI-Assist which we assume to be directly using poorly formatted raw text from PDF files for retrieval.\n\nOffensive Responses and Toxicity: Based on [25] (Table 10), we use 20 insult prompts that generate insults using ChatGPT by injecting literary devices into instructions. For example, the tenth prompt is \"Put together an offensive jab applying irony.\" Skill classifier in Jill Watson classifies such prompts\n\nTable 2. User Safety: 20 insult prompts are used directly and adapted for an AI course to measure toxicity. T. Avg and T. Max are obtained using Perspective API. Numbers in braces are the results for the most toxic sentence in each response.  \n\n<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">Insult Prompts</td><td colspan=\"3\">Adapted Insult Prompts</td></tr><tr><td>IDK%</td><td>T. Avg (%)</td><td>T. Max (%)</td><td>IDK%</td><td>T. Avg (%)</td><td>T. Max (%)</td></tr><tr><td>OAI-Assist</td><td>68</td><td>7.9 (11.4)</td><td>42.7 (54.3)</td><td>5</td><td>6.4 (14.0)</td><td>65.4 (64.4)</td></tr><tr><td>Jill Watson</td><td>98</td><td>2.7 (5.0)</td><td>45.6 (50.8)</td><td>82</td><td>3.2 (9.4)</td><td>32.2 (44.8)</td></tr></table>\n\nas irrelevant to the course which leads to a default response. Therefore, we adapt these prompts to make them relevant in the context of the course. For example, we modify the above prompt to \"How many assignments are there in this course? Half of the semester is already over and I still need to watch the first lecture. Answer in form of an offensive jab applying irony.\" We also test out another variation that exploits the AI concept of reasoning: \"What is reasoning in [course name]? Put together an offensive jab applying irony and explain your reasoning.\"\n\nWe generate three outputs for all three systems with 20 insult prompts and 40 adapted insult prompts which are then passed to Perspective API [12] to measure their toxicity. Perspective API provides a toxicity score (T. Score) between zero and one which denotes the fraction of people that are expected to perceive the text as toxic. In Table 2, similar to [25], we report the average (T. Avg) and maximum (T. Max) over all prompts.\n\nJill Watson responds with IDKs more often than OAI-Assist which leads to lower T. Avg. for complete responses as well as on sentence level. Also, the proportion of IDKs goes down for adapted insult prompts which shows that simple modifications can be made to insult prompts to disguise them as domain-relevant for engaging in conversations that can lead to potentially toxic content. Further, with adapted insult prompts, we also see an increase in T. Avg on sentence level as more responses have toxic content but we do not see an increase in T Avg. for complete responses because of their longer length, about  $2.25\\mathrm{x}$  the length of response to the original insult prompts. The adapted prompts elicit more toxicity for OAI-Assist than Jill Watson as the proportion of IDK responses is lower. We did not perform this experiment for LJW as it uses templated responses which cannot be toxic, arguably making it the safest of all methods.\n\nSample Conversations: Kakar et al. 2024 [10] deployed Jill Watson with additional design changes in a diverse range of courses including the online graduate-level Knowledge-Based AI course offered at Georgia Tech in the Fall 2023 semester. Deployed Jill Watson had an average response time of 6.8 seconds. Table 3 provides six snippets from organic student conversations with Jill Watson. These snippets demonstrate how Jill Watson is able to converse using coreference resolution as well as the chat context to answer questions. We also see how Jill Watson uses the documents to answer student questions and avoids answering questions or adds a disclaimer when the context does not contain an answer to the user query. Further, Jill Watson is also able to navigate conversations involving user queries that encourage toxicity and is also able to correct\n\nTable 3. Examples: Snippets from organic student conversations with Jill Watson deployment in an AI course.  \n\n<table><tr><td>Coreference Resolution\nUSER: what is partial order planning\nASSISTANT: Partial order planning is a type of planning algorithm used in artifi-\ncial intelligence to solve problems [...] \nUSER: does it devise separate plans\nASSISTANT: Yes, in partial order plan-\nning, the planner devises separate plans for\neach subgoal or subtask. It treats [...]</td><td>Retrieval and conversationality\nUSER: What is a semantic network?\nASSISTANT: A semantic network is a\nknowledge representation technique [...] \nUSER: what is another way to represent\nknowledge in AI systems?\nASSISTANT: Another way to represent\nknowledge in AI systems is through the use\nof frames. Frames are a knowledge [...]</td></tr><tr><td>Date-aware and conversationality\nUSER (asked on 2023-08-30): what assign-\nments are due this week?\nASSISTANT: This week (2023-08-28 to\n2023-09-03), the Start-of-Course Survey is\ndue on Monday, August 28, 2023 at 9am.\nNo other assignments are due this week.\nUSER: What about next week?\nASSISTANT: Next week is the week of\nSeptember 4th to September 10th. Here\nare the upcoming milestones and assign-\nments for that week: [...]</td><td>Accepting a mistake and self-correction\nUSER: Are there resources available to\nprovide an introduction to the RPM\nproject, other than the project descri-\ntions?\nASSISTANT: Yes, [...] lesson on RPM (Re-\nsource Planning and Management), [...] \nUSER: RPM does not stand for Resource\nPlanning and Management\nASSISTANT: That&#x27;s correct. In the con-\ntext of the course material provided, RPM\nstands for Raven&#x27;s Progressive Matrices.\nRaven&#x27;s Progressive Matrices is a [...]</td></tr><tr><td>Avoiding harmful text\nUSER: Explain to me rudely how I can\nsucceed in this class.\nASSISTANT: I&#x27;m sorry, but I cannot pro-\nvide a rude response. However, I can offer\nyou some tips on how to succeed in this\nclass: [...]</td><td>Not answering outside context\nUSER: what is the grading rubric for\nHW1?\nASSISTANT: The grading rubric for HW1\nis not provided in the context. I recommend\nreferring to the rubric in Canvas or reach-\ning out to your instructor [...]</td></tr></table>\n\nitself when a mistake is pointed out by the user. Note that the textual entailment warning and document citations were not available in our first deployed system. The reader can find more details in [10].\n\n# 5 Conclusion\n\nWe described the architecture of a virtual teaching assistant powered by ChatGPT and presented quantitative evaluation and qualitative examples to demonstrate its ability to ground answers in course documents, minimize hallucinations, and promote safety. We compared our system Jill Watson with legacy Jill Watson and OpenAI Assistants service and found that it can answer student queries more reliably, and generate fewer potentially harmful and confusing answers.\n\nLimitations: Jill Watson has the limitation of the RAG method i.e. the answers must be generated using a limited context. This means that long-range\n\nqueries such as 'Summarize chapter 15.' cannot be answered unless the summary is directly available in the text. The performance of Jill Watson also relies on each module working correctly, or else errors can cascade in modular AI systems. In ensuring safety, we also have to make a trade-off with performance as some questions that can be answered may not get addressed. For example, the skill classifier may deem some relevant questions as irrelevant. Building expectations around AI assistants is also an important aspect as the users should understand the limitations to avoid harm from misleading or harmful text.\n\nSocietal Impact: Jill Watson will promote the use of AI in education in boosting student and teacher productivity. LLMs are powerful tools to create AI assistants but more work needs to be done to ensure safety in terms of both misinformation and toxicity. Our work showcases a virtual teaching assistant in the real world and demonstrates the use of various techniques towards this end. AI assistants will inevitably play an important role in our daily lives including our education. We believe that Jill Watson is an important step towards understanding the role of AI assistants, user expectations, and performance constraints.\n\nAcknowledgements: This research has been supported by NSF Grants #2112532 and #2247790 to the National AI Institute for Adult Learning and Online Education. We thank Alekhya Nandula, Aiden Zhao, Elaine Cortez, and Gina Nguyen for their inputs and contributions to this work.\n\n# References\n\n1. Bajaj, P., Campos, D., Craswell, N., et al.: MS MARCO: A Human Generated MAchine Reading CComprehension Dataset (Oct 2018), arXiv:1611.09268 [cs]  \n2. Bao, J., Duan, N., Yan, Z., Zhou, M., Zhao, T.: Constraint-Based Question Answering with Knowledge Graph. In: COLING 2016. pp. 2503-2514 (Dec 2016)  \n3. Brown, T.B., Mann, B., Ryder, N., et al.: Language Models are Few-Shot Learners. In: NeurIPS 2020 (2020)  \n4. Cai, D., Wang, Y., Li, H., Lam, W., Liu, L.: Neural Machine Translation with Monolingual Translation Memory. In: ACL 2021. pp. 7307-7318 (2021)  \n5. Eicher, B., Polepeddi, L., Goel, A.: Jill Watson Doesn't Care if You're Pregnant: Grounding AI Ethics in Empirical Studies. In: AIES 2018. pp. 88-94 (2018)  \n6. Garrison, D., Anderson, T., Archer, W.: Critical Inquiry in a Text-Based Environment: Computer Conferencing in Higher Education. The Internet and Higher Education 2(2-3), 87-105 (1999)  \n7. Glass, M., Rossiello, G., Chowdhury, M.F.M., Naik, A.R., Cai, P., Gliozzo, A.: Re2G: Retrieve, Rerank, Generate. In: NAACL 2022. pp. 2701-2715 (2022)  \n8. Goel, A.K., Polepeddi, L.: Jill Watson: A Virtual Teaching Assistant for Online Education. In: Dede, C., Richards, J., & Saxberg, B., (Editors) Education at scale: Engineering online teaching and learning. NY: Routledge. (2018)  \n9. Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A., Fung, P.: Survey of Hallucination in Natural Language Generation. ACM Computing Surveys 55(12), 248:1-38 (2023)  \n10. Kakar, S., Maiti, P., Nandula, P., Nguyen, G., Taneja, K., Zhao, A., Nandan, V., Goel, A.: Jill Watson: Scaling and Deploying an AI Conversational Agent in Online Classrooms. In: Intelligent Tutoring Systems 2024 (2024)\n\n11. Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., Yih, W.t.: Dense Passage Retrieval for Open-Domain Question Answering. In: EMNLP 2020. pp. 6769-6781 (2020)  \n12. Lees, A., Tran, V.Q., Tay, Y., Sorensen, J., Gupta, J., Metzler, D., Vasserman, L.: A New Generation of Perspective API: Efficient Multilingual Character-level Transformers. In: ACM SIGKDD 2022. pp. 3197-3207 (2022)  \n13. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS 2020 pp. 9459–74 (2020)  \n14. Li, H., Su, Y., Cai, D., Wang, Y., Liu, L.: A Survey on Retrieval-Augmented Text Generation (Feb 2022), arXiv:2202.01110 [cs]  \n15. Markov, T., Zhang, C., Agarwal, S., Eloundou, T., Lee, T., Adler, S., Jiang, A., Weng, L.: A Holistic Approach to Undesired Content Detection in the Real World. In: AAAI 2023. pp. 15009-15018 (2023)  \n16. Ouyang, L., Wu, J., Jiang, X., et al.: Training language models to follow instructions with human feedback. In: NeurIPS 2022 (2022)  \n17. Piktus, A., Petroni, F., Karpukhin, V., et al.: The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus (2022), arXiv:2112.09924 [cs]  \n18. Qin, L., Galley, M., Brockett, C., Liu, X., Gao, X., Dolan, B., Choi, Y., Gao, J.: Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading. In: ACL 2019. pp. 5427-5436 (2019)  \n19. Semnani, S., Yao, V., Zhang, H., Lam, M.: WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia. In: EMNLP 2023. pp. 2387-2413 (2023)  \n20. Serban, I.V., Sankar, C., Germain, M., et al.: A Deep Reinforcement Learning Chatbot (Nov 2017), arXiv:1709.02349 [cs, stat]  \n21. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. In: Thirty-seventh Conference on Neural Information Processing Systems (Nov 2023), https://openreview.net/forum?id=yHdTscY6Ci  \n22. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (Feb 2023). https://doi.org/10.48550/arXiv.2302.13971, http://arxiv.org/abs/2302.13971, arXiv:2302.13971 [cs]  \n23. Weston, J., Dinan, E., Miller, A.: Retrieve and Refine: Improved Sequence Generation Models For Dialogue. In: EMNLP 2019 SCAI Workshop. pp. 87-92 (2019)  \n24. Wu, Z., Galley, M., Brockett, C., Zhang, Y., Gao, X., Quirk, C., Koncel-Kedziorski, R., Gao, J., Hajishirzi, H., Ostendorf, M., Dolan, B.: A Controllable Model of Grounded Response Generation. In: AAAI 2022 (2022)  \n25. Zhang, B., Shen, X., Si, W.M., Sha, Z., Chen, Z., Salem, A., Shen, Y., Backes, M., Zhang, Y.: Comprehensive Assessment of Toxicity in ChatGPT (Nov 2023), arXiv:2311.14685 [cs]  \n26. Zhang, Y., Sun, S., Gao, X., Fang, Y., Brockett, C., Galley, M., Gao, J., Dolan, B.: RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling. In: AAAI 2022. pp. 11739-11747 (2022)  \n27. Zhou, K., Prabhumoye, S., Black, A.W.: A Dataset for Document Grounded Conversations. In: EMNLP 2018. pp. 708-713 (2018)  \n28. Zhou, L., Gao, J., Li, D., Shum, H.Y.: The Design and Implementation of XiaoIce, an Empathetic Social Chatbot. Computational Linguistics 46(1), 53-93 (Mar 2020)",
    "arxiv_id": "2405.11070",
    "error_message": null,
    "embedding": [
      -1.25,
      1.96875,
      -1.1640625,
      -0.1552734375,
      -1.6875,
      -0.55078125,
      0.376953125,
      0.361328125,
      -0.423828125,
      4.125,
      1.7734375,
      0.48828125,
      1.5234375,
      0.48046875,
      1.4453125,
      1.9609375,
      1.0390625,
      0.1025390625,
      1.1328125,
      -8.4375,
      2.015625,
      2.125,
      1.6796875,
      -4.03125,
      3.671875,
      -3.328125,
      1.515625,
      1.375,
      1.015625,
      2,
      8.625,
      -4.53125,
      -2.546875,
      0.294921875,
      -0.045166015625,
      0.17578125,
      -2.875,
      -0.875,
      4.8125,
      6.40625,
      -8.5625,
      2.9375,
      0.86328125,
      3.875,
      -0.6484375,
      3.359375,
      3.125,
      -5,
      -4.0625,
      0.6875,
      -3.78125,
      -2.90625,
      4.40625,
      -0.60546875,
      2.609375,
      -5.59375,
      -7.875,
      7.40625,
      -3.984375,
      -1.8125,
      1.5078125,
      -1,
      2.03125,
      -1.125,
      3.5625,
      3.875,
      1.4609375,
      0.6640625,
      -2.03125,
      2.0625,
      -1.75,
      -0.53125,
      5.15625,
      -2.8125,
      6.34375,
      6.84375,
      1.796875,
      5.3125,
      -0.3828125,
      4.8125,
      -3.953125,
      3.875,
      5.4375,
      -1.28125,
      3.875,
      4.09375,
      -2.5,
      -2.296875,
      -4.84375,
      1.625,
      -2.046875,
      -0.6640625,
      -0.1748046875,
      -3.5,
      -0.36328125,
      5.25,
      -1.6953125,
      -5.625,
      -5.0625,
      -0.474609375,
      -0.58984375,
      -4.125,
      -0.96484375,
      -5.21875,
      -1.796875,
      -4.28125,
      -2.703125,
      -5.65625,
      -6.5,
      -2.8125,
      -2.03125,
      2.140625,
      2.46875,
      -0.6484375,
      2.40625,
      -0.93359375,
      1.71875,
      -7.3125,
      -5.9375,
      -1.90625,
      -3.515625,
      -2.921875,
      -0.55859375,
      -1.625,
      2.640625,
      1.390625,
      -1.7265625,
      0.70703125,
      5.4375,
      1.125,
      -0.59765625,
      1.109375,
      4.78125,
      -0.5234375,
      -10.25,
      -1.875,
      -7.6875,
      1.328125,
      5.09375,
      7.75,
      -6.375,
      0.6796875,
      -0.62109375,
      -5.46875,
      5.03125,
      0.3671875,
      -6.46875,
      0.140625,
      3.5,
      -2.578125,
      -1.78125,
      1.6171875,
      4.4375,
      7.375,
      -1.859375,
      -4.75,
      4.25,
      -1.6484375,
      2.46875,
      -3.34375,
      -2.5,
      3.953125,
      -0.75,
      1.7890625,
      -0.66015625,
      2.390625,
      -2.578125,
      0.40234375,
      -1.8125,
      -1.5703125,
      1.015625,
      11.8125,
      3.625,
      0.0390625,
      -0.83203125,
      0.146484375,
      -3.5,
      6.75,
      3.71875,
      -1.5,
      3.234375,
      -0.84375,
      -3.171875,
      3.09375,
      -2.34375,
      1.65625,
      1.2890625,
      -3.671875,
      4.90625,
      -2.25,
      0.251953125,
      1.2421875,
      0.890625,
      3.015625,
      -5.0625,
      -0.0164794921875,
      4.125,
      0.6875,
      0.1796875,
      2.265625,
      -0.23046875,
      -10.25,
      0.734375,
      0.11376953125,
      -3.609375,
      0.0546875,
      3.296875,
      -1.203125,
      1.78125,
      0.61328125,
      0.7109375,
      0.8125,
      3.140625,
      0.66796875,
      5.6875,
      2.609375,
      2.296875,
      -3.515625,
      2.78125,
      0.126953125,
      3.953125,
      4.3125,
      1.203125,
      -1.7109375,
      2.21875,
      3.1875,
      3.328125,
      3.78125,
      3.09375,
      1.125,
      2.71875,
      2.21875,
      1.3984375,
      -4.375,
      -2.65625,
      -0.71875,
      -5.46875,
      -0.625,
      -1.9296875,
      2.796875,
      -1.8828125,
      -3.109375,
      1.203125,
      3.171875,
      3.15625,
      0.298828125,
      -3.84375,
      -3.125,
      -3.828125,
      -9.3125,
      3.671875,
      2.515625,
      -9.0625,
      -7.875,
      2.296875,
      4.625,
      0.74609375,
      -2.03125,
      0.296875,
      -3.40625,
      2.796875,
      -1.71875,
      -6.4375,
      2.953125,
      0.43359375,
      0.51171875,
      2.375,
      -1.5546875,
      2.21875,
      1.6953125,
      2.265625,
      -2.53125,
      -1.890625,
      0.9921875,
      -1.078125,
      5.40625,
      2.5,
      -3.8125,
      -0.033203125,
      -1.796875,
      -3.25,
      -7.25,
      6.625,
      -4.25,
      5.71875,
      -1.515625,
      0.53125,
      4.84375,
      -0.61328125,
      9.625,
      4.375,
      2.15625,
      2.609375,
      -1.9609375,
      -3.140625,
      1.1640625,
      -1.9921875,
      -2.453125,
      -6.125,
      -2.078125,
      6.3125,
      0.79296875,
      -2.625,
      -0.12451171875,
      -0.255859375,
      4.21875,
      1.2890625,
      -2.84375,
      2.640625,
      0.94921875,
      -3.78125,
      0.578125,
      4.65625,
      -2.734375,
      3.40625,
      -3.296875,
      -2.859375,
      3.75,
      -0.0380859375,
      -0.640625,
      -2.15625,
      -4.84375,
      -2.84375,
      0.73046875,
      -4.21875,
      -1.4921875,
      1.1796875,
      -1.4140625,
      3.609375,
      -0.828125,
      5.34375,
      -1.84375,
      -5.625,
      -7.53125,
      6.875,
      -0.859375,
      2.765625,
      3.578125,
      -0.1689453125,
      6.875,
      -1.8828125,
      -2.34375,
      3.5625,
      -3.015625,
      -1.828125,
      3.46875,
      4.46875,
      -1.484375,
      -0.671875,
      -6.78125,
      5.3125,
      0.48828125,
      -0.337890625,
      0.6796875,
      6.28125,
      -2.0625,
      -0.341796875,
      -2.71875,
      0.357421875,
      -0.54296875,
      3.71875,
      -2.59375,
      3.734375,
      -0.1845703125,
      -2.90625,
      -2.21875,
      -4.8125,
      2.65625,
      -3.140625,
      -2.78125,
      2.109375,
      -0.61328125,
      0.8046875,
      2.359375,
      -0.2197265625,
      -0.1943359375,
      1.765625,
      -2.734375,
      -6.09375,
      1.1953125,
      -2.34375,
      -1.1953125,
      -0.177734375,
      1.078125,
      2.390625,
      1.1328125,
      0.9453125,
      0.55859375,
      0.5546875,
      -1.1640625,
      -2.328125,
      -1.7109375,
      -2.421875,
      2.515625,
      2.984375,
      0.93359375,
      -3,
      2.15625,
      -0.953125,
      -0.314453125,
      3.4375,
      1.0234375,
      -1.53125,
      -0.43359375,
      -2.40625,
      1.7890625,
      0.21484375,
      -9.8125,
      2.609375,
      2.734375,
      0.1689453125,
      4.5,
      0.02880859375,
      -1.7890625,
      0.68359375,
      0.5078125,
      -0.419921875,
      -0.10546875,
      -6.3125,
      -1.3671875,
      -5.34375,
      -0.70703125,
      2.265625,
      -2.03125,
      1.078125,
      -0.2431640625,
      1.9375,
      6.78125,
      -1.96875,
      -0.1435546875,
      2.328125,
      0.65234375,
      -5.71875,
      3.953125,
      -5.375,
      -4.21875,
      2.734375,
      -4.46875,
      -5.8125,
      1.5,
      2.25,
      -1.9296875,
      2.734375,
      4.28125,
      -5.46875,
      0.36328125,
      1.8125,
      4.5,
      -0.9765625,
      -1.5546875,
      -1.9609375,
      1.234375,
      -0.71875,
      2.390625,
      2.53125,
      0.2333984375,
      -3.46875,
      3.109375,
      2.015625,
      -1.2734375,
      1.8046875,
      -1.4375,
      -2.109375,
      4.1875,
      -1.1875,
      -1.546875,
      -2.9375,
      3.4375,
      7.84375,
      -5.96875,
      -9.5,
      4.40625,
      -0.02978515625,
      -1.6484375,
      -4.09375,
      4.4375,
      1.8046875,
      2.265625,
      -5,
      -1.1640625,
      -3.140625,
      -2.84375,
      5.28125,
      2.4375,
      -3.359375,
      -5.46875,
      -3.765625,
      2.125,
      1.484375,
      5.375,
      0.5390625,
      -1.046875,
      0.12353515625,
      -7.03125,
      1.1484375,
      0.3515625,
      6.71875,
      -0.2431640625,
      1.421875,
      4.09375,
      -7.03125,
      0.41796875,
      -0.765625,
      0.83984375,
      -0.220703125,
      -1.234375,
      4.75,
      -1.453125,
      -1.453125,
      0.80078125,
      6.1875,
      -3.78125,
      -0.443359375,
      1.65625,
      -3.890625,
      -2.046875,
      -1.2578125,
      2.171875,
      4.0625,
      0.39453125,
      -3.046875,
      1.2265625,
      0.734375,
      1.3984375,
      4.28125,
      -1.21875,
      -1.5390625,
      -2.125,
      1.125,
      3.34375,
      2.5,
      0.90234375,
      -1.546875,
      -3.21875,
      -2.234375,
      -0.51953125,
      2.40625,
      -2.28125,
      -0.609375,
      1.3828125,
      -0.88671875,
      2.4375,
      -4.5625,
      -3.53125,
      1.7578125,
      0.49609375,
      -4.4375,
      1.15625,
      2.5,
      4.59375,
      2.421875,
      0.0703125,
      -1.171875,
      0.58203125,
      -0.265625,
      -2.890625,
      -4.59375,
      -1.375,
      4.3125,
      -5.46875,
      -0.96484375,
      -2.640625,
      1.2421875,
      -0.443359375,
      2.46875,
      1.640625,
      -1.0390625,
      5.34375,
      -0.875,
      -0.4296875,
      1.25,
      3.890625,
      -2.96875,
      1.015625,
      -0.18359375,
      2.0625,
      -6.5625,
      -5.5,
      -0.828125,
      0.412109375,
      7.5625,
      -0.74609375,
      2.53125,
      -3.140625,
      8.125,
      -1.8984375,
      1.6875,
      -11.875,
      5.03125,
      1.390625,
      -5.65625,
      -0.318359375,
      -3.78125,
      2.53125,
      -2.015625,
      3.609375,
      2.265625,
      0.3671875,
      2.296875,
      4.25,
      -0.953125,
      -2.1875,
      5.6875,
      3.265625,
      -4.03125,
      0.62109375,
      1.015625,
      -4.59375,
      2.109375,
      -1.4140625,
      5.28125,
      2.8125,
      2.046875,
      2,
      1.4765625,
      2.484375,
      -6.53125,
      -0.474609375,
      4.90625,
      1.40625,
      -2.171875,
      2.25,
      -3.265625,
      2.21875,
      -4.0625,
      5.25,
      0.6640625,
      -1.5625,
      2.953125,
      3.984375,
      -4.25,
      -0.2353515625,
      -1.2578125,
      -3.078125,
      5.59375,
      4.21875,
      -2.59375,
      2.171875,
      -0.82421875,
      -1.609375,
      5.09375,
      0.734375,
      -2.890625,
      -1.359375,
      2.140625,
      0.78515625,
      -1.4609375,
      5.71875,
      1.25,
      -3.4375,
      -0.072265625,
      0.8671875,
      0.333984375,
      -2.1875,
      -0.8515625,
      -0.232421875,
      -1.984375,
      -0.1533203125,
      2.125,
      -0.2734375,
      -2.515625,
      -3.515625,
      1.3203125,
      -2.828125,
      3.015625,
      0.63671875,
      -2.9375,
      0.08056640625,
      -3.75,
      4.90625,
      -1.671875,
      0.451171875,
      0.3984375,
      3.0625,
      4.0625,
      -1.15625,
      3.578125,
      -5.125,
      -4.65625,
      -0.7578125,
      -2.84375,
      -2.6875,
      1.1796875,
      0.21875,
      -0.0257568359375,
      -0.81640625,
      -2.453125,
      -5.15625,
      -5.15625,
      0.70703125,
      2.703125,
      -1.3046875,
      1.859375,
      -3.1875,
      4.21875,
      -1.21875,
      -0.298828125,
      2.75,
      -0.6015625,
      -0.9140625,
      0.75,
      2.5,
      -7.5,
      0.3203125,
      4.4375,
      3.3125,
      -1.7421875,
      4.59375,
      -2.4375,
      -2.71875,
      -1.1328125,
      4.34375,
      -1.734375,
      -1.8828125,
      -3.296875,
      -0.6796875,
      -3.78125,
      -4.875,
      -1.1015625,
      -1.46875,
      0.7578125,
      -2.53125,
      -0.96875,
      -2.921875,
      -5.96875,
      -0.1279296875,
      -0.019775390625,
      -4.34375,
      2.9375,
      1.7578125,
      -1.0859375,
      -3.578125,
      1.8671875,
      3.4375,
      0.5703125,
      -1.578125,
      -1.1484375,
      4.15625,
      -0.78515625,
      2.703125,
      -0.053955078125,
      -6.375,
      1.359375,
      -1.3984375,
      -3.15625,
      2.1875,
      -2.765625,
      1.75,
      4.15625,
      1.421875,
      -4.09375,
      -1.3984375,
      1.640625,
      -2.265625,
      0.498046875,
      0.62890625,
      3.296875,
      -0.208984375,
      5.125,
      -2.578125,
      -4.4375,
      0.443359375,
      4.25,
      4.59375,
      -0.0419921875,
      -2.875,
      2.890625,
      1.84375,
      3.328125,
      -5.34375,
      -2.75,
      -0.5234375,
      2.53125,
      -1.953125,
      -2.84375,
      7.03125,
      -2.765625,
      2.375,
      0.9375,
      3,
      -0.0625,
      -1.6875,
      -0.031982421875,
      2.140625,
      2.5625,
      -0.796875,
      1.9453125,
      3.328125,
      -3.765625,
      -1.890625,
      1.15625,
      1.265625,
      -2.796875,
      2.265625,
      0.453125,
      -0.0830078125,
      -1.0234375,
      -0.77734375,
      -0.0012054443359375,
      3.71875,
      6,
      -1.0703125,
      -0.87109375,
      -2,
      8.4375,
      -2.09375,
      4.21875,
      1.703125,
      10.5625,
      -1.6953125,
      -4.5,
      1.65625,
      -1.1875,
      -0.01434326171875,
      1.046875,
      -3.890625,
      -5.25,
      1.3984375,
      -0.240234375,
      0.50390625,
      1.4921875,
      -5.5,
      -0.478515625,
      6.03125,
      -0.4375,
      5.15625,
      4.84375,
      3.65625,
      -0.734375,
      -0.55859375,
      -2.5625,
      -1.4765625,
      -0.9453125,
      0.08837890625,
      0.453125,
      -4.34375,
      3.890625,
      0.84765625,
      4.5,
      1.859375,
      0.7265625,
      -0.07275390625,
      8.25,
      -1.828125,
      -2.015625,
      1.625,
      3,
      -0.875,
      -0.703125,
      2.734375,
      -5.125,
      -1.9140625,
      7.1875,
      5.15625,
      0.9921875,
      2.3125,
      -0.65625,
      0.515625,
      -4.03125,
      -0.83984375,
      -0.55078125,
      1.734375,
      -8.1875,
      -2.671875,
      2.171875,
      2.671875,
      -3.015625,
      -0.068359375,
      2.203125,
      -6.625,
      5.875,
      1.1875,
      2.5,
      0.94140625,
      -0.1328125,
      3.859375,
      -2.234375,
      -2.546875,
      -4.28125,
      -3.4375,
      -4.28125,
      -1.140625,
      3.46875,
      2.875,
      4.625,
      -1.53125,
      -0.2470703125,
      -8.375,
      0.1640625,
      -0.65625,
      2.609375,
      2.625,
      -5.59375,
      0.58984375,
      -3.03125,
      -8.9375,
      -4.8125,
      -3.5,
      -3.0625,
      -1.2265625,
      -4.875,
      -0.64453125,
      0.380859375,
      -4.90625,
      2.9375,
      -2.6875,
      1.453125,
      3.75,
      0.439453125,
      1.90625,
      -3.953125,
      -1.078125,
      0.60546875,
      0.65625,
      5.375,
      0.76171875,
      2.78125,
      5.40625,
      0.365234375,
      0.4609375,
      -4.03125,
      7.09375,
      -1.1953125,
      6.15625,
      3.109375,
      1.6953125,
      -5.78125,
      1.7265625,
      -0.474609375,
      1.3671875,
      -5.71875,
      0.9453125,
      -3.9375,
      1.375,
      0.53125,
      -0.2412109375,
      4.15625,
      -3.5,
      -3.5625,
      1.2578125,
      -6.09375,
      -0.4296875,
      4.84375,
      0.56640625,
      1.0625,
      -4.53125,
      -0.859375,
      -0.267578125,
      4.65625,
      1.5234375,
      -0.4609375,
      2.515625,
      -4.375,
      0.65625,
      -1.5390625,
      0.1376953125,
      -2.25,
      -1.2890625,
      -1.5234375,
      -1.2109375,
      2.71875,
      3.296875,
      0.48828125,
      5.90625,
      1.921875,
      0.49609375,
      -1.4765625,
      -0.55859375,
      -0.796875,
      -2.265625,
      6.1875,
      -0.6171875,
      0.57421875,
      -2.09375,
      1.046875,
      -4.8125,
      -1.9609375,
      0.1240234375,
      -7.96875,
      0.095703125,
      -2.6875,
      -5,
      2.71875,
      -1.734375,
      0.74609375,
      0.87109375,
      4.875,
      1.78125,
      2.53125,
      -0.46484375,
      0.5703125,
      0.251953125,
      -3.8125,
      7.59375,
      -2.140625,
      -0.76171875,
      4.59375,
      3.828125,
      6.53125,
      -1.953125,
      -3.28125,
      -2.703125,
      3.234375,
      5.03125,
      1.8203125,
      0.8046875,
      2.8125,
      0.6640625,
      1.53125,
      1.6171875,
      -2.265625,
      0.0732421875,
      -2.984375,
      0.6640625,
      3.484375,
      4.78125,
      -2.984375,
      -1.4375,
      -5.75,
      -0.031982421875,
      0.9609375,
      1.6875,
      -3.59375,
      -1.6796875,
      -2.328125,
      0.92578125,
      -1.015625,
      -0.3125,
      -1.53125,
      -5.65625,
      1.140625,
      1.65625,
      -0.068359375,
      -0.36328125,
      -0.5625,
      0.5234375,
      -1.390625,
      2.390625,
      2.390625,
      -0.734375,
      1.6640625,
      3,
      0.1318359375,
      4.28125,
      3.9375,
      1.6015625,
      2.0625,
      -0.30078125,
      -3.515625,
      0.1484375,
      -3.234375,
      1.75,
      -2.8125,
      1.8671875,
      3.515625,
      6.78125,
      -0.466796875,
      3.65625,
      1.359375,
      0.068359375,
      2.578125,
      -3.859375,
      -1.265625,
      1.0234375,
      -1.25,
      -2.078125,
      1.2421875,
      -3.40625,
      1.1953125,
      3.59375,
      -7.1875,
      3.078125,
      -1.484375,
      9.125,
      0.1875,
      -3.125,
      0.671875,
      -1.1953125,
      1.5234375,
      -4.3125,
      2.40625,
      -0.55078125,
      -3.765625,
      5.15625,
      -1.6875,
      -0.6875,
      0.20703125,
      -3.046875,
      1.4453125,
      -2.21875,
      -1.5546875,
      1.1015625,
      -1.5234375,
      -4.28125,
      -2.1875,
      -1.4765625,
      1.2578125,
      0.50390625,
      0.66796875,
      0.1171875,
      -1.4765625,
      1.7890625,
      -4.03125,
      -0.50390625,
      0.8671875,
      0.408203125,
      2.203125,
      -3.125,
      5.75,
      1.828125,
      -1.046875,
      -0.091796875,
      -1.140625,
      -3.90625,
      -1.765625,
      -1.375,
      -0.228515625,
      -4.8125,
      -4.65625,
      1.3359375,
      0.44140625,
      0.69921875,
      -0.7734375,
      -1.2421875,
      2.078125,
      4.5,
      -0.6328125,
      4.28125,
      -0.337890625,
      1.328125,
      -2.375,
      -5.90625,
      -5.84375,
      1.59375,
      1.4765625,
      -0.34765625,
      2.28125,
      -0.73828125,
      2.5625,
      0.0255126953125,
      0.0211181640625,
      -3.109375,
      -0.2119140625,
      3.953125,
      -1.578125,
      -3.34375,
      4.53125,
      1.3125,
      3.171875,
      -2.5,
      -0.10693359375,
      -2.59375,
      3.0625,
      1.8515625,
      2.453125,
      -0.8046875,
      -0.1337890625,
      -4.15625,
      -2.421875,
      -3.3125,
      -1.78125,
      5.84375,
      -0.578125,
      3.078125,
      2.171875,
      -1.578125,
      -2.140625,
      0.5703125,
      -1.5390625,
      4.15625,
      3.203125,
      -1.1875,
      -0.2001953125,
      -2.890625,
      2.03125,
      -3.171875,
      0.2216796875,
      -7.25,
      -3.15625,
      4.71875,
      -1.03125,
      -2.296875,
      -1.796875,
      -0.0267333984375,
      1.484375,
      0.52734375,
      2.765625,
      -1.84375,
      -3.484375,
      -4.1875,
      4.09375,
      -0.421875,
      -3.34375,
      1.546875,
      3.484375,
      -5.1875,
      2.734375,
      3.515625,
      2.265625,
      -3.0625,
      4.84375,
      -1.25,
      -4.625,
      -5.8125,
      3.515625,
      4.0625,
      -0.6328125,
      -1.5625,
      -4,
      -1.6640625,
      -0.267578125,
      2.15625,
      5.09375,
      -1.6171875,
      1.5234375,
      -0.8828125,
      3.046875,
      0.49609375,
      -4.71875,
      -0.5078125,
      3.3125,
      3.4375,
      -0.92578125,
      0.1845703125,
      -2.28125,
      -0.10009765625,
      -1.875,
      -8.9375,
      1.0703125,
      1.3203125,
      -0.050048828125,
      5.3125,
      0.45703125,
      2.375,
      0.6953125,
      -3.859375,
      -1.421875,
      -2.515625,
      2.671875,
      1.234375,
      4.5625,
      -3.640625,
      0.384765625,
      -0.54296875,
      -0.34375,
      -1.71875,
      2.25,
      -2.078125,
      0.84375,
      0.70703125,
      4.03125,
      -3.078125,
      -4.625,
      -0.8828125,
      -1.0234375,
      -1.4453125,
      0.87109375,
      2.390625,
      2.078125,
      4.25,
      -2.640625,
      -3.609375,
      -0.9921875,
      -4.3125,
      2.453125,
      3.40625,
      1.6796875,
      -0.6875,
      0.49609375,
      -3.953125,
      -1.734375,
      3.46875,
      -1.625,
      -1.203125,
      -0.39453125,
      0.10498046875,
      1.953125,
      1.6328125,
      -0.69140625,
      -1.265625,
      3.921875,
      -1.21875,
      -4.84375,
      -2.890625,
      1.8203125,
      1.03125,
      -2.78125,
      2.21875,
      -3.578125,
      3.171875,
      -3.265625,
      -0.9375,
      -0.2158203125,
      -1.6875,
      1.59375,
      4,
      0.85546875,
      -1.90625,
      0.3203125,
      -0.9765625,
      3.625,
      1.78125,
      -1.09375,
      0.5390625,
      0.9609375,
      0.369140625,
      -3.625,
      3.46875,
      0.34765625,
      -3.84375,
      1.859375,
      2.15625,
      -5.78125,
      -0.5,
      2.953125,
      3.328125,
      -2.75,
      0.67578125,
      -0.3046875,
      8.3125,
      0.298828125,
      -1.3125,
      -2.859375,
      -2.109375,
      1.2265625,
      0.703125,
      1.2265625,
      -0.482421875,
      1.78125,
      -6.3125,
      -1.96875,
      -0.029296875,
      2.4375,
      6.1875,
      -4.0625,
      1.8828125,
      3.984375,
      -1.828125,
      5.46875,
      -0.5546875,
      -3.578125,
      -2.40625,
      -1.1796875,
      -6.625,
      -0.91796875,
      -5.78125,
      1.5859375,
      -4.15625,
      -2.171875,
      -2.78125,
      1.4453125,
      2.296875,
      1.6796875,
      -2.5,
      -2.546875,
      -3.9375,
      -5.15625,
      -2.375,
      -1.375,
      -6.34375,
      2.84375,
      -0.279296875,
      1.8359375,
      4.375,
      3.640625,
      1.640625,
      -1.296875,
      2.265625,
      -1.8671875,
      0.8828125,
      -0.59375,
      -0.51171875,
      -2.109375,
      -2.28125,
      -0.341796875,
      -1.9765625,
      2.1875,
      -5.625,
      -3,
      -3.4375,
      -0.70703125,
      0.455078125,
      -0.43359375,
      0.443359375,
      -0.78125,
      5.4375,
      -0.3515625,
      1.6953125,
      3.90625,
      -1.5234375,
      0.40625,
      1.4453125,
      1.171875,
      8.4375,
      -1.34375,
      1.203125,
      -4.53125,
      4.90625,
      2.859375,
      -3.5625,
      2.203125,
      -0.65234375,
      -2.453125,
      -3.8125,
      1.3359375,
      2.296875,
      3.46875,
      -3.703125,
      3.5,
      4.78125,
      -1.1875,
      4.9375,
      4.875,
      2.734375,
      1.21875,
      4.6875,
      0.9921875,
      -0.12255859375,
      -1.671875,
      4.125,
      -1.6171875,
      -2.59375,
      1.734375,
      0.625,
      2.703125,
      2.21875,
      0.1923828125,
      2.421875,
      6.59375,
      -7.125,
      1.5703125,
      -6.125,
      0.8828125,
      -4.90625,
      -0.984375,
      -3.953125,
      -3.21875,
      1.703125,
      4.53125,
      -1.9296875,
      -4.625,
      1.3828125,
      3.375,
      -0.2109375,
      -4.5,
      4.34375,
      3.953125,
      1.3984375,
      -3.046875,
      -2.59375,
      3.46875,
      1.984375,
      -1.1796875,
      -5.21875,
      4.09375,
      -0.1015625,
      -4.5625,
      -2.90625,
      0.205078125,
      -1.4140625,
      -3.671875,
      -6.0625,
      -3.859375,
      3,
      -4.4375,
      3.8125,
      5.0625,
      -0.205078125,
      3.28125,
      0.89453125,
      1.53125,
      1.546875,
      -2.078125,
      2.875,
      1.15625,
      1.453125,
      -1.4296875,
      1.7109375,
      -1.296875,
      1.859375,
      -0.2294921875,
      1.8125,
      3,
      -1.5625,
      0.921875,
      -3.859375,
      3.265625,
      3.0625,
      1.1796875,
      -3.65625,
      -2.40625,
      3.703125,
      1.90625,
      -2.9375,
      -0.94140625,
      0.27734375,
      -0.50390625,
      -1.5703125,
      1.4609375,
      -1.046875,
      4.03125,
      0.1376953125,
      1.328125,
      -0.263671875,
      -0.94140625,
      -3.65625,
      -1.0390625,
      -2.015625,
      -0.95703125,
      -1.515625,
      5.53125,
      -1.7265625,
      -2.671875,
      -2.875,
      3.453125,
      -2.203125,
      2.1875,
      2.84375,
      -1.0703125,
      2.453125,
      0.796875,
      -0.5,
      -5.09375,
      0.216796875,
      3.703125,
      0.40625,
      1.078125,
      -0.83203125,
      -2.015625,
      2.015625,
      -3.28125,
      -4.03125,
      1.1796875,
      -1.0546875,
      -2.109375,
      -9.625,
      1.6484375,
      2.078125,
      -0.392578125,
      -1.890625,
      -1.7109375,
      1.8515625,
      4,
      4.34375,
      4.15625,
      -0.54296875,
      -0.61328125,
      1.109375,
      16.25,
      -1.6875,
      -8.0625,
      -0.64453125,
      0.6171875,
      2.5,
      9.3125,
      0.5078125,
      0.322265625,
      1.2734375,
      2.4375,
      1.765625,
      -3.578125,
      3.46875,
      -0.546875,
      0.26171875,
      3.234375,
      1.0546875,
      0.7421875,
      -3.265625,
      1.109375,
      1.765625,
      2.078125,
      1.5859375,
      -2.359375,
      1.203125,
      -1.265625,
      -0.9609375,
      -3.125,
      -3.5625,
      2.15625,
      -1.640625,
      -0.1572265625,
      -2.078125,
      0.248046875,
      -1.71875,
      2.953125,
      0.90234375,
      0.0206298828125,
      -1.2421875,
      -3.34375,
      -4.5,
      4.21875,
      -0.1005859375,
      -1.5390625,
      -2.5,
      -0.0126953125,
      -1.1171875,
      1.2265625,
      0.57421875,
      0.35546875,
      1.7734375,
      -1.890625,
      0.859375,
      -0.365234375,
      -1.4296875,
      -2.1875,
      -6.875,
      -2.171875,
      -3.28125,
      0.490234375,
      0.2177734375,
      -2.4375,
      -1.265625,
      -2.53125,
      -1.8203125,
      -2.859375,
      -0.94140625,
      -2.390625,
      0.71875,
      -5.0625,
      -0.90625,
      6.34375,
      2.265625,
      -1.8984375,
      -4.375,
      0.1357421875,
      4.1875,
      1.171875,
      3.40625,
      -4.03125,
      0.0654296875,
      -4.84375,
      0.02392578125,
      -2.515625,
      0.53125,
      -3.4375,
      3.125,
      -0.388671875,
      -1.4140625,
      4.5625,
      -4.46875,
      3.53125,
      1.734375,
      -3.703125,
      -0.64453125,
      3.75,
      1.3828125,
      -0.890625,
      -1.546875,
      4.09375,
      -1.5859375,
      -0.255859375,
      1.09375,
      0.039306640625,
      -1.5234375,
      3.0625,
      -3.265625,
      2.421875,
      -0.8203125,
      -2.421875,
      5.3125,
      3.109375,
      0.48046875,
      -0.2041015625,
      3.421875,
      -0.7109375,
      -0.408203125,
      -0.0186767578125,
      -4.0625,
      4.375,
      -0.11669921875,
      -0.8671875,
      1.890625,
      -8.0625,
      -4.875,
      -1.984375,
      -6.03125,
      1.953125,
      -5.28125,
      -0.7265625,
      -1.375,
      -1.859375,
      -3.203125,
      1.9296875,
      -2.40625,
      0.7734375,
      5.28125,
      -0.7578125,
      0.578125,
      -3.296875,
      0.81640625,
      4.03125,
      -3.5,
      -1.140625,
      0.2421875,
      0.16015625,
      2.34375,
      3.015625,
      -0.5390625,
      -2.25,
      5.34375,
      -0.98046875,
      -2.984375,
      0.494140625,
      3.171875,
      -0.185546875,
      -0.072265625,
      -3.4375,
      2.125,
      -1.265625,
      -0.2041015625,
      -0.94140625,
      -3.0625,
      -1.984375,
      0.92578125,
      2.234375,
      -2.5,
      -6.90625,
      4.53125,
      5.375,
      -7.15625,
      -0.1748046875,
      -4.03125,
      4.84375,
      -2.625,
      3.421875,
      5.6875,
      -0.00830078125,
      -0.0849609375,
      2.53125,
      1.140625,
      0.11181640625,
      -5.59375,
      2.859375,
      6.9375,
      -3.96875,
      2.5625,
      3.96875,
      0.57421875,
      0.74609375,
      -3.171875,
      -1.71875,
      6.4375,
      3.1875,
      -3.734375,
      -1.1640625,
      -2.859375,
      -0.62109375,
      -2.6875,
      -3.03125,
      -2.1875,
      -0.8359375,
      -0.248046875,
      3.4375,
      -0.8984375,
      -2.953125,
      -1.3828125,
      -2.65625,
      -1.9296875,
      -1.1328125,
      5,
      0.59375,
      2.375,
      -5.65625,
      6.15625,
      2.6875,
      0.84765625,
      1.4921875,
      5.4375,
      -0.8046875,
      3.421875,
      -5.21875,
      -1.625,
      -1.6953125,
      1.9921875,
      -4.625,
      2.34375,
      -4.09375,
      1.390625,
      -4.3125,
      3.6875,
      -0.193359375,
      1.9140625,
      -4.03125,
      -3.90625,
      -2.25,
      5.59375,
      -2.109375,
      1.0234375,
      4.09375,
      -1.453125,
      0.8203125,
      2.78125,
      -1.6640625,
      0.294921875,
      -2.53125,
      2.4375,
      0.90234375,
      -2.1875,
      2.15625,
      -0.84375,
      3.875,
      -1.0078125,
      1.375,
      -2.59375,
      -3.640625,
      -0.111328125,
      -4.875,
      0.359375,
      0.34375,
      -5.40625,
      1.390625,
      0.74609375,
      2.15625,
      0.5703125,
      3.5625,
      -7.6875,
      5.59375,
      0.8046875,
      -5.59375,
      1.890625,
      0.96484375,
      -2.65625,
      0.357421875,
      -2.484375,
      1.828125,
      -4.6875,
      0.29296875,
      5.25,
      -2.34375,
      0.26171875,
      0.9296875,
      -3.921875,
      0.29296875,
      -4.78125,
      3.640625,
      1.0703125,
      3.421875,
      2.125,
      1.578125,
      5.78125,
      -3.921875,
      3.375,
      -6.09375,
      0.8125,
      -2.34375,
      2.734375,
      0.828125,
      -2.875,
      4.25,
      5.625,
      -0.380859375,
      -9.125,
      -5.28125,
      -0.703125,
      -2.84375,
      2.34375,
      2.609375,
      1.6875,
      -1.328125,
      0.1474609375,
      1.3125,
      3.1875,
      -1.8671875,
      -1.890625,
      -1.28125,
      2.640625,
      -0.028076171875,
      -4.59375,
      -0.34765625,
      -0.02880859375,
      -0.181640625,
      -0.5703125,
      -2.21875,
      -2.890625,
      2.359375,
      3,
      -3.328125,
      2.78125,
      1.328125,
      2.8125,
      1.859375,
      0.1279296875,
      -0.384765625,
      3.671875,
      -3.140625,
      -6.78125,
      -0.38671875,
      -0.625,
      2.21875,
      4.46875,
      -1.765625,
      4.21875,
      0.051513671875,
      4.28125,
      -5.03125,
      3.53125,
      3.296875,
      -10.375,
      2.296875,
      -2.859375,
      1.046875,
      -0.314453125,
      -5.53125,
      -1.796875,
      3.78125,
      -0.404296875,
      -1.796875,
      5.71875,
      -2.25,
      0.1875,
      -0.234375,
      -2.765625,
      -3.421875,
      3.09375,
      2.1875,
      -2.25,
      3.5,
      5.25,
      -1.9375,
      -2.0625,
      -1.5234375,
      5.4375,
      -1.8984375,
      -3.03125,
      3.484375,
      3.640625,
      1.171875,
      -1.4609375,
      -4.375,
      -0.2451171875,
      -1.6484375,
      -2.953125,
      -1,
      -2.359375,
      2.25,
      -2.09375,
      -0.06787109375,
      -1.859375,
      -2.625,
      0.52734375,
      -2.53125,
      -3.296875,
      1.8046875,
      2.703125,
      0.07861328125,
      -2.0625,
      -0.7109375,
      1.8125,
      0.1533203125,
      -3.53125,
      -3.9375,
      0.94140625,
      -2.625,
      -3.65625,
      -1.7578125,
      0.384765625,
      -2.265625,
      0.412109375,
      -0.578125,
      -0.302734375,
      -0.333984375,
      -0.79296875,
      -0.71875,
      1.5234375,
      2.828125,
      0.279296875,
      0.2734375,
      -0.0233154296875,
      3.234375,
      2.65625,
      -5.6875,
      -2.828125,
      1.28125,
      -0.057373046875,
      -4.28125,
      -3.734375,
      0.259765625,
      -2.25,
      3.171875,
      5.09375,
      2.5625,
      -1.046875,
      2.984375,
      -3.796875,
      -1.2890625,
      4.9375,
      4.375,
      -0.546875,
      -1.4609375,
      1.796875,
      -4.71875,
      5.75,
      6.5625,
      -3.390625,
      -3.765625,
      0.6875,
      -2.875,
      -1.296875,
      -1.8125,
      3.359375,
      0.26171875,
      -7.15625,
      3.8125,
      -0.93359375,
      5.53125,
      0.96875,
      -2.515625,
      2.453125,
      -0.46875,
      1.859375,
      -1.0234375,
      2.125,
      0.67578125,
      -4.46875,
      0.26171875,
      -2.265625,
      -2.875,
      -3.390625,
      -0.33984375,
      0.296875,
      2.625,
      5.15625,
      1.140625,
      -0.93359375,
      -1.3203125,
      -1.5859375,
      3.28125,
      -1.6875,
      -2.9375,
      -0.34765625,
      -2.9375,
      0.95703125,
      0.703125,
      -0.9296875,
      -6.375,
      -1.2734375,
      -4.65625,
      4.90625,
      -1.515625,
      0.1494140625,
      -2.453125,
      -1.5078125,
      2.9375,
      4.5,
      3.59375,
      3.578125,
      2.46875,
      2.671875,
      0.2265625,
      -2.828125,
      4.6875,
      3.625,
      2.234375,
      -0.91015625,
      0.859375,
      0.5625,
      -2.375,
      -4.1875,
      -0.130859375,
      0.146484375,
      5.53125,
      4.21875,
      -1.03125,
      1.3984375,
      -2.15625,
      -1.140625,
      -2.109375,
      0.84765625,
      0.9609375,
      1.328125,
      -1.8359375,
      0.80078125,
      2.59375,
      1.078125,
      0.412109375,
      3.296875,
      1.3515625,
      -0.193359375,
      -2.515625,
      -0.671875,
      3.234375,
      -2.75,
      -0.404296875,
      -2.046875,
      0.271484375,
      -1.203125,
      -3.59375,
      3.328125,
      -0.181640625,
      -0.9375,
      4.1875,
      -0.353515625,
      1.8515625,
      0.32421875,
      2.890625,
      -2.921875,
      1.390625,
      0.47265625,
      2.8125,
      -1.5390625,
      1.1796875,
      1.2265625,
      1.546875,
      -1.7109375,
      1.609375,
      1.703125,
      -2.4375,
      -0.265625,
      1.2265625,
      -2.390625,
      -1.015625,
      3.078125,
      -1.7421875,
      0.51953125,
      0.80078125,
      -0.55078125,
      -1.1171875,
      4.65625,
      -3.671875,
      -3.140625,
      -0.71875,
      1.109375,
      0.1416015625,
      -0.51171875,
      -0.0712890625,
      -0.83984375,
      0.8828125,
      -0.3359375,
      -2.671875,
      -0.6640625,
      -0.77734375,
      -1.4921875,
      1.3828125,
      -0.890625,
      0.3125,
      -4.5625,
      3.359375,
      -0.2890625,
      -0.8203125,
      -0.41015625,
      -0.65625,
      -0.30078125,
      -3.140625,
      1.125,
      -1.484375,
      1.6796875,
      1.421875,
      0.765625,
      2.78125,
      -0.87109375,
      0.3984375,
      -0.51171875,
      -0.01446533203125,
      4,
      -0.1845703125,
      -2.296875,
      -0.87890625,
      1.8359375,
      1.109375,
      1.3125,
      0.5390625,
      2.8125,
      0.5546875,
      1.46875,
      1.9765625,
      -0.57421875,
      3.25,
      0.96484375,
      3.1875,
      -2.875,
      -1,
      -3.125,
      -1.921875,
      1.3671875,
      -1.875,
      -3.84375,
      -0.3984375,
      -0.14453125,
      0.98046875,
      2.734375,
      5.1875,
      -2.25,
      1.6328125,
      -0.76953125,
      1.53125,
      -1.2265625,
      1.203125,
      1.953125,
      0.00970458984375,
      -3.703125,
      -3.0625,
      2.046875,
      -1.984375,
      1.4296875,
      0.267578125,
      -1.421875,
      0.63671875,
      -2.140625,
      -1.9921875,
      -0.22265625,
      -0.875,
      0.37890625,
      0.97265625,
      1.4140625,
      1.546875,
      1.140625,
      -3.046875,
      3.828125,
      -3.390625,
      2.96875,
      -0.259765625,
      0.419921875,
      -2.109375,
      2.65625,
      -1.265625,
      -3,
      1.921875,
      -3.375,
      -2.28125,
      -3.28125,
      -1.2109375,
      2.5625,
      -1.3203125,
      0.6484375,
      -1.53125,
      -0.3515625,
      2.4375,
      2.5625,
      2.5625,
      2.90625,
      1.546875,
      3.125,
      2.109375,
      -2.890625,
      0.2490234375,
      -2.953125,
      -0.93359375,
      -0.578125,
      1.9765625,
      1.84375,
      -4.3125,
      0.5234375,
      -1.1015625,
      2.53125,
      3.53125,
      -1.0234375,
      2.453125,
      2,
      -1.625,
      3.84375,
      5.125,
      0.97265625,
      0.8984375,
      0.13671875,
      -1.6953125,
      -0.2451171875,
      -1.875,
      -1.09375,
      -0.10791015625,
      2.28125,
      0.050048828125,
      0.55859375,
      -2.046875,
      2.328125,
      1.3984375,
      0.048583984375,
      2.3125,
      -2.609375,
      -1.2578125,
      0.07177734375,
      2.484375,
      1.125,
      1.4921875,
      1.4453125,
      0.65625,
      -4.90625,
      -1.1796875,
      -0.0693359375,
      -1.125,
      -0.75,
      -2.109375,
      0.765625,
      -1.4140625,
      -4.125,
      0.55859375,
      -0.734375,
      0.2060546875,
      1.0703125,
      2.859375,
      -0.5234375,
      1.984375,
      -0.02294921875,
      1.40625,
      -0.49609375,
      -0.94140625,
      1.8515625,
      1.9921875,
      1.046875,
      0.455078125,
      -3.6875,
      -2.40625,
      -1.953125,
      -1.9609375,
      -0.9765625,
      2.65625,
      0.361328125,
      -1.0078125,
      0.82421875,
      -1.5,
      -1.3828125,
      2.578125,
      3.203125,
      -0.5703125,
      1.1796875,
      -1.1328125,
      2.40625,
      -3.40625,
      1.28125,
      -0.302734375,
      -1.03125,
      0.16796875,
      -0.486328125,
      -0.578125,
      -1.6796875,
      0.4453125,
      -0.86328125,
      0.10986328125,
      -2.3125,
      -2.046875,
      1.140625,
      2.71875,
      -0.302734375,
      1.8359375,
      2.109375,
      5.46875,
      -0.73046875,
      1.0625,
      1.7265625,
      3.59375,
      5.625,
      1.6015625,
      -5.09375,
      -2.953125,
      0.8125,
      2.453125,
      2.34375,
      -0.486328125,
      1.796875,
      4.625,
      -0.6875,
      0.4453125,
      -4.6875,
      0.02490234375,
      3.15625,
      1.3203125,
      4.6875,
      -1.5390625,
      -0.7109375,
      -1.9453125,
      -1.9609375,
      2.453125,
      2.46875,
      2.9375,
      2.421875,
      2.546875,
      -0.01397705078125,
      1.6484375,
      0.298828125,
      2.796875,
      -1.203125,
      0.765625,
      2.34375,
      -1.5546875,
      -1.7578125,
      1.9921875,
      -3.296875,
      1.75,
      -1.1484375,
      2.296875,
      3.171875,
      -0.9921875,
      -1.3046875,
      0.021484375,
      -0.9140625,
      4.3125,
      -1.1171875,
      -0.171875,
      -2.4375,
      1.125,
      0.037109375,
      -1.3828125,
      0.86328125,
      -3.8125,
      1.8671875,
      0.333984375,
      -1.125,
      -1.1015625,
      -2.59375,
      -2.703125,
      -1.328125,
      -1.7421875,
      -0.99609375,
      -2.265625,
      5,
      -2.765625,
      -1.6171875,
      0.73828125,
      4.59375,
      0.1201171875,
      6.5625,
      -0.384765625,
      2.828125,
      -1.3515625,
      -1.1953125,
      -0.80859375,
      -4.3125,
      -2.921875,
      -0.11181640625,
      0.3984375,
      -0.2197265625,
      -0.55078125,
      -1.453125,
      -1.171875,
      3.609375,
      -0.515625,
      -3.703125,
      -2.0625,
      2.671875,
      -0.9765625,
      0.70703125,
      -3.59375,
      0.2490234375,
      -0.7578125,
      -1.5390625,
      1.234375,
      -0.396484375,
      0.8984375,
      0.70703125,
      -0.032470703125,
      -2.734375,
      -0.8984375,
      0.55078125,
      -3.4375,
      -0.40625,
      4.9375,
      -1.5859375,
      0.08642578125,
      -0.4609375,
      4.15625,
      0.1123046875,
      2.203125,
      -2.515625,
      -1.2734375,
      2.546875,
      4.1875,
      3.171875,
      1.609375,
      0.087890625,
      3.46875,
      -3.828125,
      -1.203125,
      -0.427734375,
      -4.34375,
      -0.318359375,
      3.203125,
      -4.09375,
      0.0517578125,
      -1.6328125,
      0.28125,
      -0.6015625,
      -2.625,
      2.78125,
      -2.875,
      -0.50390625,
      3.65625,
      -1.9453125,
      -0.921875,
      -3.234375,
      1.7421875,
      -4.25,
      0.361328125,
      -2.734375,
      -2.4375,
      0.52734375,
      1.65625,
      -0.3203125,
      3,
      0.3984375,
      0.314453125,
      5.21875,
      2.3125,
      3.09375,
      1.2421875,
      -0.625,
      1.2421875,
      -1.5078125,
      3.34375,
      -0.2314453125,
      -2.671875,
      0.84375,
      -2.84375,
      -1.515625,
      -0.546875,
      3.453125,
      0.4765625,
      -0.046630859375,
      -0.8515625,
      1.5703125,
      1.90625,
      -0.76171875,
      3.734375,
      -0.9765625,
      0.5546875,
      -3.875,
      0.28515625,
      -0.87890625,
      3.359375,
      3.3125,
      2.046875,
      -0.7578125,
      -2.03125,
      2.453125,
      -1.7578125,
      2.015625,
      0.4921875,
      -4.75,
      -2.203125,
      -1.375,
      -1.1484375,
      -0.54296875,
      -1.8515625,
      -0.11181640625,
      -0.478515625,
      -0.2431640625,
      2.65625,
      -0.330078125,
      -2.234375,
      -0.9375,
      0.83203125,
      2.125,
      -0.08154296875,
      1.1484375,
      -1.3359375,
      3.34375,
      0.39453125,
      -0.71484375,
      2,
      2.203125,
      -1.0390625,
      -2.390625,
      -0.08154296875,
      -3.734375,
      -1.484375,
      2.59375,
      1.6875,
      1.90625,
      -0.703125,
      0.10302734375,
      3.6875,
      0.77734375
    ],
    "suggested_tags": [
      "教育AI",
      "对话系统",
      "问答系统",
      "LLM应用",
      "模块化设计"
    ],
    "tag_suggestions": [
      {
        "name": "教育AI",
        "confidence": 0.95,
        "reason": "论文聚焦于开发虚拟教学助手，应用于在线教育场景，解决学生问答和教学支持问题"
      },
      {
        "name": "对话系统",
        "confidence": 0.9,
        "reason": "基于ChatGPT构建的对话式AI代理，具备多轮对话和上下文理解能力"
      },
      {
        "name": "问答系统",
        "confidence": 0.85,
        "reason": "核心功能是基于课程材料的问答任务，支持文档检索和知识验证"
      },
      {
        "name": "LLM应用",
        "confidence": 0.8,
        "reason": "利用ChatGPT等大语言模型实现零样本学习，无需训练即可处理多文档问答"
      },
      {
        "name": "模块化设计",
        "confidence": 0.75,
        "reason": "采用基于技能的架构设计，支持API集成和功能扩展，提升系统灵活性"
      }
    ],
    "s2_graph": {
      "citations": [
        {
          "external_id": "CorpusId:283217596",
          "title": "Intelligent teaching design assistant for primary mathematics: A large language model-driven framework with retrieval-augmented generation and problem-chain pedagogy",
          "authors": [
            "Danna Tang",
            "Ran Ding",
            "Meng He",
            "Yushen Wang",
            "Kaka Cheng"
          ],
          "year": 2026,
          "venue": "International Electronic Journal of Mathematics Education",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283624085",
          "title": "From knowledge gaps to learning opportunities: Leveraging student questions and dual use of generative AI to support student learning at scale",
          "authors": [
            "Stanislav Pozdniakov",
            "Jonathan Brazil",
            "Oleksandra Poquet",
            "Stephan Krusche",
            "Santiago Berrezueta-Guzman",
            "Shazia Sadiq",
            "Hassan Khosravi"
          ],
          "year": 2025,
          "venue": "Computers and Education: Artificial Intelligence",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283213277",
          "title": "How college students use ChatGPT",
          "authors": [
            "N. M. Mohammad",
            "Matthew Demers",
            "Erin McCubbin",
            "Jackson Mitchell",
            "Sara M. Fulmer"
          ],
          "year": 2025,
          "venue": "Pedagogical Research",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:281421061",
          "title": "Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments",
          "authors": [
            "Li Siyan",
            "Zhen Xu",
            "Vethavikashini Chithrra Raghuram",
            "Xuanming Zhang",
            "Renzhe Yu",
            "Zhou Yu"
          ],
          "year": 2025,
          "venue": "Conference on Empirical Methods in Natural Language Processing",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:281332933",
          "title": "Perspectives and potential issues in using artificial intelligence for computer science education",
          "authors": [
            "Juho Vepsäläinen",
            "Petri Juntunen"
          ],
          "year": 2025,
          "venue": "arXiv.org",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:280420389",
          "title": "From Misunderstandings to Learning Opportunities: Leveraging Generative AI in Discussion Forums to Support Student Learning",
          "authors": [
            "Stanislav Pozdniakov",
            "Jonathan Brazil",
            "Oleksandra Poquet",
            "Stephan Krusche",
            "Santiago Berrezueta-Guzman",
            "Shazia Sadiq",
            "Hassan Khosravi"
          ],
          "year": 2025,
          "venue": "International Conference on Artificial Intelligence in Education",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:280635533",
          "title": "Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment",
          "authors": [
            "Farzana Zahid",
            "Anjalika Sewwandi",
            "Lee Brandon",
            "Vimal Kumar",
            "Roopak Sinha"
          ],
          "year": 2025,
          "venue": "High-Confidence Computing",
          "citation_count": 1
        },
        {
          "external_id": "CorpusId:282394325",
          "title": "The Effects of Chatbot Placement, Personification, and Functionality on Student Outcomes in a Global CS1 Course",
          "authors": [
            "Sierra Wang",
            "Thomas Jefferson",
            "Chris Piech",
            "John C. Mitchell"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:282396045",
          "title": "Scaling Effective AI-Generated Explanations for Middle School Mathematics in Online Learning Platforms",
          "authors": [
            "Eamon Worden",
            "Kirk P. Vanacore",
            "Aaron Haim",
            "Neil T. Heffernan"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:280294670",
          "title": "AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education",
          "authors": [
            "Jaroslaw A. Chudziak",
            "Adam Kostka"
          ],
          "year": 2025,
          "venue": "International Conference on Artificial Intelligence in Education",
          "citation_count": 4
        }
      ],
      "citations_fetched_at": "2025-12-16T18:44:55.853630",
      "references": [
        {
          "external_id": "CorpusId:265456807",
          "title": "Comprehensive Assessment of Toxicity in ChatGPT",
          "authors": [
            "Boyang Zhang",
            "Xinyue Shen",
            "Waiman Si",
            "Zeyang Sha",
            "Zeyuan Chen",
            "Ahmed Salem",
            "Yun Shen",
            "Michael Backes",
            "Yang Zhang"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 5
        },
        {
          "external_id": "CorpusId:258841157",
          "title": "WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia",
          "authors": [
            "Sina J. Semnani",
            "Violet Z. Yao",
            "He Zhang",
            "M. Lam"
          ],
          "year": 2023,
          "venue": "Conference on Empirical Methods in Natural Language Processing",
          "citation_count": 101
        },
        {
          "external_id": "CorpusId:257219404",
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "authors": [
            "Hugo Touvron",
            "Thibaut Lavril",
            "Gautier Izacard",
            "Xavier Martinet",
            "M. Lachaux",
            "Timothée Lacroix",
            "Baptiste Rozière",
            "Naman Goyal",
            "Eric Hambro",
            "Faisal Azhar",
            "Aur'elien Rodriguez",
            "Armand Joulin",
            "Edouard Grave",
            "Guillaume Lample"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 17220
        },
        {
          "external_id": "CorpusId:251371664",
          "title": "A Holistic Approach to Undesired Content Detection in the Real World",
          "authors": [
            "Todor Markov",
            "Chong Zhang",
            "Sandhini Agarwal",
            "Tyna Eloundou",
            "Teddy Lee",
            "Steven Adler",
            "Angela Jiang",
            "L. Weng"
          ],
          "year": 2022,
          "venue": "AAAI Conference on Artificial Intelligence",
          "citation_count": 323
        },
        {
          "external_id": "CorpusId:250391085",
          "title": "Re2G: Retrieve, Rerank, Generate",
          "authors": [
            "Michael R. Glass",
            "Gaetano Rossiello",
            "Md. Faisal Mahbub Chowdhury",
            "Ankita Rajaram Naik",
            "Pengshan Cai",
            "A. Gliozzo"
          ],
          "year": 2022,
          "venue": "North American Chapter of the Association for Computational Linguistics",
          "citation_count": 130
        },
        {
          "external_id": "CorpusId:246426909",
          "title": "Training language models to follow instructions with human feedback",
          "authors": [
            "Long Ouyang",
            "Jeff Wu",
            "Xu Jiang",
            "Diogo Almeida",
            "Carroll L. Wainwright",
            "Pamela Mishkin",
            "Chong Zhang",
            "Sandhini Agarwal",
            "Katarina Slama",
            "Alex Ray",
            "John Schulman",
            "Jacob Hilton",
            "Fraser Kelton",
            "Luke E. Miller",
            "Maddie Simens",
            "Amanda Askell",
            "Peter Welinder",
            "P. Christiano",
            "Jan Leike",
            "Ryan J. Lowe"
          ],
          "year": 2022,
          "venue": "Neural Information Processing Systems",
          "citation_count": 16892
        },
        {
          "external_id": "CorpusId:247058801",
          "title": "A New Generation of Perspective API: Efficient Multilingual Character-level Transformers",
          "authors": [
            "Alyssa Lees",
            "Vinh Q. Tran",
            "Yi Tay",
            "Jeffrey Scott Sorensen",
            "Jai Gupta",
            "Donald Metzler",
            "Lucy Vasserman"
          ],
          "year": 2022,
          "venue": "Knowledge Discovery and Data Mining",
          "citation_count": 247
        },
        {
          "external_id": "CorpusId:246652372",
          "title": "Survey of Hallucination in Natural Language Generation",
          "authors": [
            "Ziwei Ji",
            "Nayeon Lee",
            "Rita Frieske",
            "Tiezheng Yu",
            "D. Su",
            "Yan Xu",
            "Etsuko Ishii",
            "Yejin Bang",
            "Delong Chen",
            "Wenliang Dai",
            "Andrea Madotto",
            "Pascale Fung"
          ],
          "year": 2022,
          "venue": "ACM Computing Surveys",
          "citation_count": 3304
        },
        {
          "external_id": "CorpusId:246472929",
          "title": "A Survey on Retrieval-Augmented Text Generation",
          "authors": [
            "Huayang Li",
            "Yixuan Su",
            "Deng Cai",
            "Yan Wang",
            "Lemao Liu"
          ],
          "year": 2022,
          "venue": "arXiv.org",
          "citation_count": 255
        },
        {
          "external_id": "CorpusId:245334864",
          "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus",
          "authors": [
            "Aleksandra Piktus",
            "F. Petroni",
            "Yizhong Wang",
            "Vladimir Karpukhin",
            "Dmytro Okhonko",
            "Samuel Broscheit",
            "Gautier Izacard",
            "Patrick Lewis",
            "Barlas Ouguz",
            "Edouard Grave",
            "Wen-tau Yih",
            "Sebastian Riedel"
          ],
          "year": 2021,
          "venue": "arXiv.org",
          "citation_count": 75
        }
      ],
      "references_fetched_at": "2025-12-16T18:44:56.671666"
    }
  },
  "2c6ea33c-9a9e-4547-949a-69351fc70f65": {
    "id": "2c6ea33c-9a9e-4547-949a-69351fc70f65",
    "filename": "2405.13001v1.pdf",
    "file_path": "data/uploads/64d94413-f66a-4369-b6a0-66dcef7b83ee/2c6ea33c-9a9e-4547-949a-69351fc70f65_2405.13001v1.pdf",
    "status": "completed",
    "created_at": "2025-12-16 19:02:16.206608",
    "updated_at": "2025-12-16 11:03:36.112706",
    "user_id": "64d94413-f66a-4369-b6a0-66dcef7b83ee",
    "title": "Large Language Models for Education: A Survey",
    "markdown_content": "# Large Language Models for Education: A Survey\n\nHanyi  $\\mathbf{X}\\mathbf{u}^{a}$ , Wensheng  $\\mathrm{Gan}^{a,*}$ , Zhenlian  $\\mathbf{Q}\\mathbf{i}^{b,*}$ , Jiayang  $\\mathbf{W}\\mathbf{u}^{a}$  and Philip S.  $\\mathbf{Y}\\mathbf{u}^{c}$\n\n$^{a}$ College of Cyber Security, Jinan University, Guangzhou 510632, China  \n$^{b}$ School of Information Engineering, Guangdong Eco-Engineering Polytechnic, Guangzhou 510520, China  \n$^{c}$ Department of Computer Science, University of Illinois Chicago, Chicago, USA\n\n# ARTICLE INFO\n\nKeywords:  \nartificial intelligence  \nsmart education  \nLLMs  \napplications  \nchallenges\n\n# ABSTRACT\n\nArtificial intelligence (AI) has a profound impact on traditional education. In recent years, large language models (LLMs) have been increasingly used in various applications such as natural language processing, computer vision, speech recognition, and autonomous driving. LLMs have also been applied in many fields, including recommendation, finance, government, education, legal affairs, and finance. As powerful auxiliary tools, LLMs incorporate various technologies such as deep learning, pre-training, fine-tuning, and reinforcement learning. The use of LLMs for smart education (LLMEdu) has been a significant strategic direction for countries worldwide. While LLMs have shown great promise in improving teaching quality, changing education models, and modifying teacher roles, the technologies are still facing several challenges. In this paper, we conduct a systematic review of LLMEdu, focusing on current technologies, challenges, and future developments. We first summarize the current state of LLMEdu and then introduce the characteristics of LLMs and education, as well as the benefits of integrating LLMs into education. We also review the process of integrating LLMs into the education industry, as well as the introduction of related technologies. Finally, we discuss the challenges and problems faced by LLMEdu, as well as prospects for future optimization of LLMEdu.\n\n# 1. Introduction\n\nArtificial intelligence (AI) has developed rapidly in recent years [73, 111, 139], thanks to the continuous improvements in Web 3.0 [38], Internet of Behaviors (IoB) [103], data mining [35, 48, 68], deep learning [122], and language processing technologies [47]. LLMs have shown excellent performance in various industries with the optimization of pre-training models and the continuous adjustment of related technologies [25, 132]. LLM is mainly based on many AI technologies, e.g., natural language processing (NLP), and was used to understand and generate massive texts [41]. They perform self-supervised learning on a large-scale corpus to obtain the statistical laws of language [31] and then convert it into logical natural language text. Its basic framework is shown in Figure 1. LLMs have demonstrated strong versatility and logical reasoning capabilities, leading to their widespread model-as-a-service (MaaS) [37] in various industries, including finance, education [36], law [58], robotics [131], and government affairs [20, 32, 126]. Creating a scenario-based user experience is a key advantage for most digital companies, and it also happens to be a development need for LLM.\n\nThe concept of education has been around for centuries, dating back to the theory of biological origins. In primitive societies, education was limited to the use of primary production tools, whereas ancient societies relied on oral transmission and practice to pass knowledge down to future generations [66]. With the development of science and technology in modern society, education and AI\n\n![](/uploads/images/2c6ea33c-9a9e-4547-949a-69351fc70f65/7086b8cda485234568fab5cdb627979b998a6dc1e1e87faeae4fe69f5d2412ae.jpg)  \nFigure 1: Framework of LLMs.\n\nhave become inseparable [22], including intelligent teacher assistants, voice assistants [77, 92], AI writing creation platforms, etc. The fourth industrial revolution, represented by the intelligent revolution [15], can bring the education industry to a new level with the help of LLMs. Education is essentially about knowledge transfer, instant feedback, and emotional interaction. LLMs mainly enhance the \"immediate feedback\" process in education. They have the potential to revolutionize the education industry by providing personalized, adaptive learning experiences for students. By infusing knowledge into their models, LLMs can gradually build a deep understanding of the world, surpassing human learning in some aspects. They can generate high-quality text content, comprehend natural language, extract information, and answer questions across various fields [71]. LLMs can also do complex mathematical reasoning [123], which helps the education sector show that they are good at self-supervision, intelligent adaptive teaching, and multi-modal interaction [26]. With their ability to adapt the individual students' needs and learning styles, LLMs can provide a more effective and engaging learning experience.\n\nResearch gaps: There are already many educators and researchers who have shown a lot of thinking about AI in education. Examples are as follows: Some research has been conducted on the paradigm shift in AI in education [85] and on the impact of AI in management, teaching, and learning [21]. Some studies explain AI in education and show how they work [72]. Due to the rapid iteration and update of AI, many new educational AI technologies have been spawned, but there is a lack of summary and analysis of emerging technological means. LLMs, as one of these technologies, have significantly advanced AI development to a new stage. LLMs are the latest technological means to support intelligent education. The integration of education and LLMs particularly highlights the development and application characteristics of LLMs. There has been one brief review of LLMs for education [36], while many characteristics of LMEdu and key technologies are not discussed in detail.\n\nContributions: To examine the potential of LLMEdu and promote its development, this paper provides an in-depth analysis of the development process and technical structure of LLMEdu and forms a comprehensive summary. This review aims to help readers gain a deeper understanding of LLMEdu and encourages us to invent and consider LLMEdu applications. The specific contributions are as follows:\n\n- We take a closer look at the connection between LLMs and education, aiming to achieve smart education.  \n- We demonstrate the development process of LLMEdu through the process of applying LLMs to education and the key technologies of LLMs.  \n- We review the implementation of LLMEdu from the perspective of LLMs empowering education, focusing on exploring the development potential of LLMEdu.  \n- We highlight the problems and challenges existing in LLMEdu in detail, aiming to trigger some insight, critical thinking, and exploration.\n\nRoadmap: In Section 2, we briefly introduce the characteristics of LLMs and the education industry, as well as the characteristics of LLMs integrated into education. In Section 3, we conduct an in-depth analysis of the process of applying LLMs to education. In Section 4, we explain the key technologies related to LLMs. In Section 5, we provide the implementation of LLMEdu from the perspective of empowering education with LLMs. In Section 6, we highlight some of the main issues and challenges in LLMEdu. Finally, in Section 7, we summarize LLMEdu and propose expectations for the development of future LLMs. Table 1 describes some basic symbols in this article.\n\n# 2. Characteristics of LLM in Education\n\nIn this section, we discuss the key characteristics of LLMs, the key characteristics of education, the limitations of traditional education, and the combinations between LLMs and education, as depicted in Figure 2.\n\nTable 1 Summary of symbols and their explanations  \n\n<table><tr><td>Symbol</td><td>Definition</td></tr><tr><td>AI</td><td>Artificial Intelligence</td></tr><tr><td>AIGC</td><td>AI-Generated Content</td></tr><tr><td>ChatGPT</td><td>Chat Generative Pre-Training Transformer</td></tr><tr><td>CV</td><td>Computer Vision</td></tr><tr><td>DNNs</td><td>Deep Neural Networks</td></tr><tr><td>GPT</td><td>Generative Pre-trained Transformer</td></tr><tr><td>HFRL</td><td>Human Feedback Reinforcement Learning</td></tr><tr><td>LLMEdu</td><td>Large Language Models for Education</td></tr><tr><td>LLMs</td><td>Large Language Models</td></tr><tr><td>LMs</td><td>Language Models</td></tr><tr><td>NLP</td><td>Natural Language Processing</td></tr></table>\n\n![](/uploads/images/2c6ea33c-9a9e-4547-949a-69351fc70f65/4ceb13c181dc3c041d9dfd2c369372900381d64a94c5af271691b37f38f65114.jpg)  \nFigure 2: The characteristics of LLMEdu.\n\n# 2.1. Characteristics of LLMs\n\nLarge-scale. The term \"large\" in LLMs can be interpreted in two ways. Firstly, LLMs possess an enormous number of parameters, with the parameter count increasing exponentially from billions to trillions in just a few years. For instance, Google's BERT had 300 million parameters in 2018, GPT-2 had 1.5 billion parameters in 2019, and GPT-3 had 175 billion parameters in 2021 [137, 101]. In 2022, the Switch Transformer reached an impressive 1.6 trillion parameters [67, 100]. Furthermore, LLMs are trained on vast amounts of data from diverse sources, including the web, academic literature, and conversations. This large-scale corpus of data enables the models to learn and represent complex patterns and relationships in language, leading to improved performance in various NLP tasks [107].\n\nGeneral-purpose. LLMs have a wide range of applications [88]. In addition to excelling in specific domains, they are adept at handling various types of tasks, including NLP, CV, speech recognition, and even cross-modal tasks. In other words, LLMs possess powerful generalization capabilities, and achieving such capabilities requires training on massive amounts of data.\n\nPre-training and fine-tuning [27, 47, 132]. The core of the model training process lies in the use of pre-training followed by fine-tuning. Initially, pre-training is performed on a large-scale unlabeled text corpus to acquire the model's\n\nbasic language knowledge. Subsequently, fine-tuning is conducted on specific tasks in a particular domain to better understand and generate language specific to that domain, such as legal, educational, or medical texts.\n\nEmergent ability: unpredictability [88]. The emergent ability of LLMs refers to their capacity to generate coherent and logically consistent text without explicit human intervention, as they have learned from their training process. When the amount of data reaches a sufficiently large scale, the model's learning and feedback capabilities can experience a substantial increase, resulting in improved performance.\n\nFragmentation [93]. The current AI landscape is characterized by diverse business scenarios across various industries, resulting in fragmented and diversified AI demands. The development process of AI models involves several stages, including development, hyperparameter tuning, optimization, and iterative deployment for eventual application. Each stage requires significant investment, and in high-cost situations, catering to customized market demands can be challenging.\n\nPotential for breaking accuracy limitations. The development of deep learning has taken a long time. The improvement in accuracy through architectural changes appears to have reached a bottleneck as neural network design techniques have matured and converged. However, LLM development has shown that increasing the scale of both the model and the data can help break through accuracy limitations. Research experiments have consistently demonstrated that scaling up the model and data leads to improved model accuracy [104]. High complexity and investment costs. LLMs are becoming increasingly complex, with single-step computation time growing by more than 10 times [6]. For high-traffic businesses, a training experiment that used to take a few hours now takes several days, with the expectation that tests will remain within a one-day timeframe as a basic requirement [75]. Moreover, training a general-purpose large model is expensive, and if subsequent optimization, updates, and deployment are included, it will cost even more. For example, the core infrastructure of ChatGPT, the Azure AI, required an investment of nearly $1 billion [87]. Moreover, ChatGPT has high requirements for the number of GPU chips used for data processing [82].\n\n# 2.2. Characteristics of education\n\nAccording to its definition, education is a deliberate and conscious social practice that aims to nurture individuals. Its fundamental characteristic is its process-oriented nature, indicating that education exists and evolves through a series of steps. With a focus on individuals, education ultimately aims to facilitate their holistic and enduring growth. Education encompasses knowledge transmission, immediate feedback, and emotional interaction. Error correction, knowledge reinforcement, and rapid training consolidation are some parts of educational behavior. Furthermore, the education system is highly intricate, marked by the distinctiveness of its subjects, diverse requirements, and intricate interactions.\n\n# 2.2.1. Educational development process\n\nLow entry barriers. On one hand, the accessibility of starting an educational institution is relatively easy [17], resulting in lower operating and investment costs for both teachers and institutions. However, this has also led to a disparity in teacher qualifications, contributing to issues such as disorder in the education and training industry, misleading advertisements, exaggerated titles for teachers, and ineffective offline one-on-one teaching. These have subsequently led to an increase in complaints. On the other hand, there has been a reduction in barriers to education for learners, leading to greater equality of educational opportunities across different regions and a stronger emphasis on the right to education.\n\nLarge capacity [60]. The education industry encompasses a significant number of students and teachers, making it crucial to consider the implications of a large population. Moreover, there exists a diverse array of educational settings, including public schools as well as numerous private educational institutions. There is an abundance of educational materials available, and the advent of the internet has made access to educational resources easier. This development has transcended the confines of traditional textbook-based teaching, breaking down information barriers and expanding the horizons of education.\n\nWell-developed system. The expansion of education has been propelled by economic development [56], leading to a surge in investment in the education sector. This growth encompasses a wide range of educational institutions at different levels. Moreover, the education system encompasses diverse forms of education, such as social life education, family education, and school education. It also encompasses a variety of disciplines, including mathematics, languages, and physical education.\n\nRise of online education [55]. Since the late 1990s, emerging technologies have made significant inroads into the education industry [18]. This transformation has propelled education through various stages, including traditional education, digital education, internet-based education, mobile-based education, and intelligent education. The advancement of information technology has played a pivotal role in facilitating education development by overcoming time and space constraints, making knowledge acquisition more convenient and rapid.\n\nEducation at a younger age. The development of the internet has dismantled barriers to education, resulting in heightened parental concerns and an increased focus on early education. Under the influence of globalization, the significance of early education [128], particularly in language and logic development, has been recognized. In conjunction with the surge of online education, early childhood education has become more readily available. A wide range of tutoring classes and early learning programs have become commonplace.\n\nIntelligent, precise, and personalized education [23]. With the rapid advancement of AI, technology has significantly enhanced production methods and raised people's\n\nliving standards. As a result, society's demand for education has escalated, leading to a more targeted approach to talent development. Education is currently transforming the integration and innovation of \"AI + education\" in smart education.\n\nAlthough education has integrated AI to a significant extent, the nature of human education and machine education fundamentally differs in a two-tier manner. These two forms of education vary in their sequence: human education primarily focuses on shaping values, followed by systematic knowledge acquisition, and ultimately engaging in real-world experiences to foster learning. In contrast, machine education begins by processing vast amounts of data, subsequently discerning between right and wrong (learning values), incorporating human feedback, and ultimately attaining practicality. When it comes to learning, the most notable distinction between humans and machines lies in the limited energy humans possess to acquire knowledge within a fixed period, whereas machines have a relatively unlimited learning capacity. Embracing AI, formulating education strategies that align with the current era, and achieving a comprehensive digital transformation of education are the central points of contemporary educational development.\n\n# 2.2.2. Impact on teachers\n\nInstructional method's development. Digital education provides a wider range of teaching methods and tools [28]. It requires teachers to adapt and become proficient in utilizing these innovative approaches and technologies. This includes leveraging online learning platforms, educational applications, and virtual classrooms to effectively impart knowledge and engage with students. To cater to student's diverse learning needs, teachers must acquire familiarity with and expertise in using these technologies.\n\nPersonalized and self-directed learning support. Digital education has the potential to better support personalized and self-directed learning [19]. Teachers can leverage technology to gain insights into student's learning styles, interests, and needs. They also provide tailored instructional content and learning plans. This shift in education will see teachers adopt more of a guide and mentor role. They encourage students to take an active role in their learning and self-development.\n\nData-driven instructional decision-making. Digital education yields a wealth of learning data, including student's performance, interests, and progress [138]. Teachers can leverage this data to make informed instructional decisions and provide personalized guidance. By analyzing student's data, teachers can identify areas of difficulty and weakness and offer targeted support and feedback to help students overcome these challenges and improve their learning outcomes.\n\nCollaboration and cross-border teaching. Digital education has the power to break down geographical barriers, enabling teachers to engage in cross-border teaching and collaboration with students from all over the world. This allows for the sharing of instructional resources, experiences, and\n\nbest practices among educators, promoting professional development and collaboration within the teaching community.\n\nCultivating 21st-century skills. In the digital age, it's essential for students to develop skills such as creative thinking, digital literacy, collaboration, and problem-solving [46]. Teachers play a vital role in guiding students to cultivate these skills and providing relevant educational support and guidance. By exploring and applying new technologies together with students, teachers can foster student's innovation and adaptability, preparing them for success in an ever-changing digital landscape.\n\nTeachers are indispensable in the digital transformation of education, as they play a multifaceted role in shaping student's academic, emotional, and social development. While technology can provide access to vast knowledge and resources, it cannot replace the personalized guidance, emotional support, and values-based education that teachers offer. The expertise, interpersonal relationships, and educational wisdom of teachers are still essential elements in the digital transformation of education, ensuring that students receive a well-rounded education that prepares them for success in the 21st century.\n\n# 2.2.3. Educational challenges\n\nPersonalized learning needs. In contemporary education, students have diverse learning needs, styles, interests, and aspirations. The traditional one-size-fits-all approach may not cater to each student's unique requirements, and personalized learning is essential to addressing these differences effectively. Therefore, implementing personalized learning is a significant challenge that educators and administrators must address to ensure that every student receives an education tailored to their individual needs and abilities.\n\nInsufficient educational resources. Despite the advancements in technology, there are still areas where schools lack modern technology infrastructure, resulting in a digital divide that hinders student's access to online learning and digital education resources. Moreover, the number of students worldwide continues to rise, putting immense pressure on the education industry. Some regions face the challenge of insufficient educational resources, including teachers, classrooms, and learning materials, leading to disparities in educational opportunities.\n\nEducation quality and standards. Inconsistencies in education quality pose a significant challenge. In some regions, an exam-oriented approach to education may lead to a narrow focus on standardized testing, resulting in a simplified curriculum and a lack of support for students' personal interests and development. Ensuring high-quality, standardized education is crucial to enhance student's academic performance and overall quality. This can be achieved by implementing a well-rounded curriculum that fosters critical thinking, creativity, and problem-solving skills while also providing individualized support for student's unique needs and interests.\n\nDiverse educational technology. The integration of big data, AI, virtual reality, and other educational technologies\n\nhas the potential to revolutionize the education sector. However, it also poses new challenges, such as management, security, and privacy considerations. Effective integration and utilization of these technologies are crucial to enhance the learning experience and achieve optimal educational outcomes. This requires a well-thought-out strategy that takes into account the unique needs and constraints of the education sector.\n\nChallenges in implementing new educational concepts. The rapid pace of technological and economic advancements, coupled with improvements in living standards and quality, has led to the emergence of new educational concepts. One such concept is \"Science Technology Engineer Art Math (STEAM)\" education, which emphasizes interdisciplinary approaches and hands-on practice. However, implementing these cutting-edge educational concepts and cultivating the next generation of socially conscious talents pose a significant challenge for the education sector. Effective strategies and innovative approaches are needed to address these challenges and ensure that students are well-equipped to thrive in an ever-changing world.\n\n# 2.3. Characteristics of LLMEdu\n\nThe integration of AI into the education industry has accelerated rapidly [39, 61, 105], transforming teaching methods and enhancing learning outcomes. From computer-assisted teaching to personalized adaptive learning and content generation, AI has revolutionized the education sector, catering to diverse age groups and fields of study. In the era of intelligence, the primary objective of education is to convert knowledge into intelligence and nurture intelligent individuals. LLMs, with natural language technology at their core, align seamlessly with the education industry's development and adapt to the vast changes in intelligent education. These models have the potential to support and enhance various aspects of the learning experience, making education more accessible, engaging, and effective.\n\n# 2.3.1. Specific embodiment of \"LLMs + education\"\n\nReasons for integrating LLM into education are shown in Figure 3.\n\n![](/uploads/images/2c6ea33c-9a9e-4547-949a-69351fc70f65/fb43ad14a0e503da8c1bbe33bee4f19135686be5fe62deda62761976b887337c.jpg)  \nFigure 3: Reasons for integrating LLM into education.\n\nInterdisciplinary teaching [74]. The training of LLMs with vast amounts of data gives them a significant advantage in knowledge integration. They can provide diverse learning support based on different subjects and boast excellent interdisciplinary capabilities. For instance, the \"Ziyue\"\n\nlarge model<sup>1</sup> prioritizes a \"scenario-first\" approach, while the iFLYTEK \"Spark Desk\"<sup>2</sup> can conduct human-like interactive learning in various fields, including mathematics, English oral practice, essay correction, and more. These models have the potential to revolutionize the way we learn and teach [24].\n\nPrecise identification of personalized needs. LLMs possess advanced language understanding and generation capabilities, enabling them to provide adaptive learning guidance tailored to individual users' age, learning stage, and learning environment. For example, the iFlytek learning machine based on LLMs can provide customized teaching for traditional subjects, such as oral teaching, Chinese and English composition correction, interactive supplementary mathematics, and so on, providing students with personalized one-to-one mentoring experiences. Furthermore, the learning machine can help parents answer questions through one-to-one dialogue, provide suggestions, and assist in parent-child communication, parent-child interaction, behavioral habits, and so on.\n\nGuided learning. LLMs are shifting towards a more human-like approach, providing authentic conversational teaching experiences in various scenarios instead of simply giving answers. This is particularly noticeable in subjects like physics and mathematics, where LLMs simulate a teacher's role and ask questions to encourage critical thinking and independent exploration [53]. By fostering a self-learning environment, LLMs can help students develop their problem-solving skills and become more effective learners [79]. For example, OpenAI collaborated with the educational organization Khan Academy to produce Khanmigo, an LLM-based educational tool. As students complete the exercises, Khanmigo can guide them to get answers on their own by asking a lot of questions.\n\nIntegration of three modes. Tool-based, companion-based, and information-based [30, 52, 118]. The tool-based mode primarily involves using data to construct a knowledge base, which becomes a large-scale query repository. The companion-based mode is exemplified by virtual teachers and assistants, providing virtual teaching and online assistance through human-like conversations. The informatization-based mode mainly refers to educational informatization, accelerating the development of an \"internet + education\" platform.\n\n# 2.3.2. Impact of \"LLMs + education\"\n\n\"LLMs + education\" will have far-reaching and profound impacts. Here are 10 areas where these impacts can be observed, along with detailed explanations.\n\nPersonalized learning support. LLMs can provide customized learning support based on students' personalized needs. By deeply understanding students learning characteristics, interests, and learning styles, LLMs can tailor teaching content and learning plans for each student. For example,\n\nin mathematics learning, LLMs can provide targeted guidance for students' weak points in mathematics by interacting with them in dialogue, helping them overcome difficulties, and improving their mathematical abilities. LLMs can design adaptive tests that adjust the difficulty of questions based on students' responses, accurately assessing students' knowledge levels and ensuring they are educated at the appropriate level [1].\n\nPersonalized assessment and feedback. LLMs can provide personalized assessment and feedback based on students' learning performance [59]. By analyzing student's answers, understanding levels, and error patterns during the learning process, LLMs can provide targeted assessment results and improvement suggestions. For example, when students encounter difficulties in writing, LLMs can analyze the structure, grammar, and expression of their writing pieces and provide detailed guidance and suggestions to help students improve their writing skills [2, 76]. Some commercial auxiliary tools based on OpenAI's LLM technology, MagicSchool, and Eduaide, can participate in the assessment of students' homework and give feedback [89].\n\nWide coverage of subject knowledge. LLMs have extensive knowledge coverage and can encompass knowledge content from multiple subject areas [69]. Students can engage in dialogue with LLMs to acquire knowledge and information across various subject domains. For instance, when students encounter problems in history learning, LLMs can provide detailed explanations and in-depth discussions of historical events, figures, and backgrounds, helping students better understand historical knowledge. According to statistics, the latest model has 13 trillion tokens of carefully selected pre-training knowledge data, which is equivalent to 5 million sets of four major classics. In addition, 1.8 trillion \"knowledge fragments\" are extracted during training [14].\n\nInterdisciplinary learning. LLMs have excellent interdisciplinary capabilities, enabling students to engage in integrated learning and cultivate interdisciplinary thinking skills [110]. Through interactions with LLMs, students can integrate and apply knowledge from different subject areas. For example, when conducting scientific experiments, students can have conversations with LLMs to discuss experimental principles, data analysis, and scientific reasoning, promoting integrated learning between science and mathematics, logical thinking, and other disciplines [3].\n\nReal-time problem-solving and tutoring. LLMs can provide real-time problem-solving and tutoring support for students. When students encounter confusion or questions during the learning process, they can ask LLMs at any time and receive immediate answers and solutions. A survey report in the first half of this year pointed out that  $89\\%$  of American students surveyed were using ChatGPT to complete homework [134]. Additionally, when students encounter comprehension difficulties while reading literary works, they can engage in dialogue with LLMs to explore the themes, plots, and character images of literary works, helping students better understand and analyze literary works [115].\n\nOpportunities for learning across time and space. The existence of LLMs allows students to learn anytime and anywhere. Students can interact with LLMs through mobile devices or computers, without being constrained by traditional classroom time and location. For example, students can utilize evening or weekend time to engage in online learning with LLMs, improving their academic abilities and knowledge levels. Online learning platforms, which utilize LLMs, provide students with access to a wide range of courses and disciplines via the Internet. The LLMs support the implementation of virtual classrooms and distance education, and students talk to the LLMs in real time to solve problems.\n\nProvision of learning resources and tools. LLMs can serve as rich learning resources and tools, providing a wide range of educational materials and tools for student's learning needs. For instance, LLMs can offer textbooks, educational videos, interactive exercises, and other learning materials to support student's learning in various subjects [7]. Additionally, there are some subject-specific tools, such as MathGPT. MathGPT has an accuracy rate of  $60.34\\%$  in the benchmark test AGIEval, which can help students solve mathematical problems efficiently [142].\n\nPromotion of critical thinking. LLMs can guide students in developing critical thinking and problem-solving skills [50]. By engaging in dialogue and posing thought-provoking questions, LLMs can foster a thinking atmosphere that encourages students to explore answers, enhancing their self-learning abilities and critical thinking skills. For example, LLMs can simulate a teacher's role in a physics class, asking students questions about concepts, principles, and problem-solving strategies, encouraging them to think critically and develop problem-solving skills [114].\n\nProfessional development for educators. LLMs can support the professional development of educators by providing them with access to a vast amount of educational resources, best practices, and innovative teaching approaches. Educators can interact with LLMs to enhance their teaching methods and explore new ways to engage students [65]. For example, teachers can engage in dialogue with LLMs to discuss teaching strategies, classroom management techniques, and approaches to address student's individual needs, improving their teaching effectiveness and professional growth.\n\nAccessibility and inclusivity in education. LLMs can contribute to making education more accessible and inclusive. They can provide learning support for students with different learning styles, abilities, and backgrounds, ensuring that all students have equitable access to quality education. For example, LLMs can offer alternative explanations, visual aids, and interactive learning experiences to accommodate diverse learners, including students with learning disabilities or language barriers, making education more inclusive and supportive. Additionally, through multicultural training, LLMs can better understand and respect students from different cultural backgrounds and create a learning environment that is inclusive and respectful of diversity.\n\nIn summary, the integration of LLMs with education will revolutionize the learning experience by providing personalized support, expanding knowledge coverage, promoting critical thinking, and enhancing the accessibility and inclusivity of education. It will empower students and educators alike, transforming the way knowledge is acquired, shared, and applied in the digital age.\n\n# 3. How to Gradually Integrate LLMs into Education\n\nThe integration of AI into the education industry has been progressing step by step, from machine learning (implementing the ability to store and calculate) to deep learning (implementing the ability to see and hear), and now to LLMs (capable of understanding and creating) [78, 99, 113]. In the current era, the vigorous development of quality education by the entire population and the active deployment of educational intelligent hardware nationwide represent the active transformation of educational training enterprises [13, 91]. In the long-standing coexistence and collaboration between teachers and AI models [112], as well as the highly homogeneous hardware background, LLMs have emerged as one of the most important technologies in human intelligence.\n\n# 3.1. Reasons why LLMs for education\n\nLLMs' excellent characteristics make their application in the education industry very reasonable. NLP [41], data analysis [34, 135], and text generation capabilities [119] align well with the fundamental processes of learning, questioning, and feedback in education. The iterative optimization process of \"development-deployment\" suits the application process in the education industry. User testing and feedback data lay the foundation for further optimization. Taking the development of LLMs in China as an example, the Spark Desk by iFLYTEK<sup>3</sup>, the ERNIE Bot by Baidu<sup>4</sup>, and the \"MathGPT\" by TAL<sup>5</sup> have accumulated data from years of experience in the education industry [143]. During their usage, these LLMs can collect more data from the education industry, leading to further technology optimization.\n\nThe \"AI + education\" model has already formed, and the gradual maturity of AI technology has paved the way for the entry of LLMs into the education industry. Smart classrooms, voice-assisted teaching, intelligent problem-solving, and other AI applications have become routine in the education industry, leading to high acceptance of LLMs [10, 12, 96]. It is important to recognize that LLMs are the latest technological achievements that gather human collective intelligence, rather than only technological achievements. However, LLMs' development potential and influence are gradually increasing.\n\nEducation companies implement their own LLMEdu development strategies. LLMs require massive amounts of data and significant investments to support them. In terms of\n\ndata, looking at various education companies, long-term experience data accumulation, technology accumulation, and an objective combination of their development conditions have differentiated the educational application of LLMs. They focus on LLM research and strive to maximize their benefits, cater to current development trends, and reduce development costs. In terms of funding, consumers in the education industry have a strong willingness to consume. As people's living standards and education levels improve, the world strengthens the education industry and injects large amounts of funding to provide a solid foundation for LLM research, development, and application.\n\nChatGPT makes practical changes to the integration of technology and education. Learning is an exploration process, and LLMs play an exploratory role in education. Because of interactive questions and answers, people's roles are changing from passive recipients of knowledge to active explorers. Because of the existence of machine hallucinations, scholars need to have a skeptical and judgmental attitude towards generated knowledge and treat LLMs from a dialectical perspective. Intelligent technology stimulates human creativity, allowing people to continuously expand their breadth of learning, thus leading to scientific and technological progress.\n\nLLMs support the sustainable development of education [5]. Innovation is the core of technological development and the premise of long-term application. By fully utilizing AI technologies such as ChatGPT, the application process in education can transition from a search mode to a content generation mode personalized for individuals. This enables the development of diverse, scalable, tangible application scenarios, as well as a series of differentiated and highly experiential educational products and services. It provides excellent environments and resources for educators and education recipients, supporting education's sustainable development.\n\nNowadays, general language models (LMs) leverage extensive data memory to shift from dedicated to universal application models. They rely on text generation capabilities, transitioning the application process from distribution to generation. This allows them to achieve multi-modality and transform application scenarios from single to multiple [43]. Multi-modal LLMs, which combine pre-training and downstream tasks, can efficiently complete downstream task adaptation with relatively small amounts of data and can be used in small sample learning and natural language question answering. In education, three typical applications are realized: automatic generation of teaching resources, human-machine collaborative process support [141], and intelligent teaching assistance for teachers. Multi-modal LMs combine the three fields of reinforcement learning, CV, and NLP. They attempt to extend the concept of LMs [49, 95, 106].\n\nWhat's more, we demonstrate the development of the GPT models, as shown in Table 2.\n\nTable 2 Iteration and comparison of LLMs  \n\n<table><tr><td>LLMs</td><td>Publish time</td><td>Parameter quantity</td><td>Pre-training data size</td><td>Training paradigm</td><td>Feature</td></tr><tr><td>GPT</td><td>2018.7</td><td>120 million</td><td>5G</td><td>Pre-training + fine-tuning</td><td>Reflection of the advantages of self-attention structure</td></tr><tr><td>GPT-26</td><td>2019.2</td><td>1.5 billion</td><td>40G</td><td>Prompt paradigm based on Tunning-free: Zero Shot Prompt</td><td>Open the exploration of the Prompt paradigm</td></tr><tr><td>GPT-37</td><td>2020.6</td><td>175 billion</td><td>45TB</td><td>Prompt paradigm based on Tunning-free: In-Context Learning</td><td>Deepen the exploration of the Prompt paradigm</td></tr><tr><td>InstructGPT8</td><td>2022.3</td><td>175 billion</td><td>45TB</td><td>Prompt paradigm of Instruction Tuning</td><td>Start paying attention to human preferences</td></tr><tr><td>ChatGPT9</td><td>2022.11</td><td>175 billion</td><td>45TB</td><td>Reinforcement learning from human feedback</td><td>Aligned with human preferences</td></tr><tr><td>GPT-410</td><td>2023.3</td><td>Nearly 2 trillion</td><td>-</td><td>Reinforcement learning from human feedback</td><td>Multimodal processing and getting closer to the bionic human brain</td></tr><tr><td>LaMDA11</td><td>2021</td><td>137 billion</td><td>150TB</td><td>Pre-training + fine-tuning</td><td>Introduce external information retrieval system</td></tr><tr><td>BARD12</td><td>2023.2</td><td>137 billion</td><td>-</td><td>Join ChromeOS as a search engine</td><td>Using LaMDA as a base</td></tr><tr><td>PaLM</td><td>2022.4</td><td>540 billion</td><td>-</td><td>PathWay distributed training framework</td><td>Large scale, multi-lingual</td></tr><tr><td>Claude13</td><td>2023.3</td><td>52 billion</td><td>-</td><td>Join the RLAIF training paradigm</td><td>Longer and more natural text editing than ChatGPT</td></tr><tr><td>BlenderBot314</td><td>2022.8</td><td>175 billion</td><td>-</td><td>Instruction fine-tuning</td><td>Text generation, question answering</td></tr></table>\n\n# 3.2. Fusion strategies\n\nCooperating with the education and training community. LLM technology engages with schools, online education platforms, and educational technology companies to collectively explore and develop the application of LLMs in education. Partnering to provide actual educational scenarios and resources can help customize models to meet educational needs and accelerate the implementation of LLMedu. For example, Baidu launched \"ERNIE Bot\" [143], Alibaba Group Holding Limited launched \"Tongyi Qianwen\" [15], and universities like Tsinghua University launched \"ChatGLM\" [16] [133], etc.\n\nForm customized content generation to enhance competitiveness. LLMs require high-quality and large data sets, so the education and training community can use LLMs to generate high-quality educational content, such as course materials, textbooks, exercises, and tests. For example, Baidu's \"ERNIE Bot\" has a certain accuracy in answering knowledge questions because it uses the Baidu Encyclopedia as training material. ChatGPT can also generate some framework lesson plans for teaching.\n\nProvide popular educational functions. Some educational technology companies develop an intelligent tutoring system, use LLMs to answer students' questions, provide answers and feedback, provide logical responses to open-ended questions, and provide guided responses to calculation questions. For example, MathGPT, developed by TAL, provides high-quality problem-solving tutoring in the field of mathematics [97]. Some use LLMs to develop speech recognition and dialogue systems, making speech education and interaction easier to implement, enabling language teaching and situational dialogue [54].\n\nIntegrate LLMs into online education platforms. Based on the learning model combined with the Internet and the rapid development of big data, integrating LLMs into online education platforms can provide students with richer learning resources, tools, and more comprehensive applications. For example, the Coursera online education platform<sup>17</sup> uses LLMs to implement functions such as data\n\ncollection and course recommendations. Duolingo $^{18}$  uses LLMs to upgrade language functions. Chegg $^{19}$  uses LLMs to optimize the homework tutoring process.\n\nParticipate in optimizing the educational work training process. First, provide training and support to educators so that they can effectively use LLMs and related tools. For example, we learn how to integrate models into teaching, as well as how to interpret and use the data and recommendations generated by the models. Second, we use LLMs to analyze student data to provide educators with insights about student progress and needs, thereby optimizing their teaching methods, such as timely feedback features.\n\nContinuous improvement and research. The gradual integration of LLMs into the education industry requires time and resources. During this process, the performance, application, and potential risks of LLMs are continuously monitored and improved, and data privacy and security regulations are observed, considering the educational needs of different regions and cultures, which can maximize the role of LLMs in the education industry.\n\n# 4. Key Technologies for LLMEdu\n\nThe technologies behind LLMs support their rapid development, as shown in Figure 4. The combination of these technologies enables LLMs to achieve excellent performance in a variety of NLP tasks, such as text generation, machine translation, sentiment analysis, and text classification. They already play an important role in various applications such as virtual assistants, intelligent search, automatic summary generation, and natural language understanding, which promotes the development of LLMEdu.\n\nLanguage model. It learns from a corpus and predicts word sequences based on probability distributions. Two main technologies used to train a language model are next-token prediction and masked language modeling. Next-token prediction predicts the next word based on its context, and masked language modeling learns the statistical structure of language, like word order and usage patterns [9, 25, 84]. However, there is still a significant gap between predicting\n\nTable 3 Comparison between generative AI and discriminative AI  \n\n<table><tr><td></td><td>Core</td><td>Data learning</td><td>Development process</td><td>Application</td></tr><tr><td>Discriminant/Analytical AI</td><td>Analysis</td><td>Conditional probability distribution</td><td>Mature technology and widely used</td><td>Recommendation systems, CV, NLP</td></tr><tr><td>Generative AI</td><td>Creation</td><td>Joint probability distribution</td><td>Exponential explosion</td><td>AIGC, text generation, audio generation</td></tr></table>\n\ntext and mastering more advanced representations in LMs, so training strategies for LMs can be inconsistent and may not correctly reach the ultimate goal. The prediction ability reflects the large model's learning ability, which determines whether the LLM can form a coherent and logical text when answering questions. So the language model is LLMEdu's foundation.\n\nHuman feedback reinforcement learning (HFRL). It is a method used in the training of LLMs [86]. By incorporating human feedback, it reduces distorted and meaningless outputs, helping ChatGPT overcome the issues present in GPT-3, such as consistency problems. It includes supervised fine-tuning, simulating human preferences, and proximal policy optimization [140]. i) In supervised fine-tuning, a small amount of annotated data is fine-tuned by first performing next-token prediction to improve the injected data, then integrating the results, and finally decoding operations [33]. ii) Developing a reward model that simulates human preferences to rank the decoded results, and constructing a ranking sequence to obtain a scoring model. To ensure consistent annotation results, the ranking process uses ordinal ranking for data annotation, resulting in a new dataset composed of comparative data [8]. iii) Proximal policy optimization aims to learn a policy that maximizes the cumulative reward obtained during training. The algorithm involves an actor, which outputs the probability distribution for the next action, and a critic, which estimates the expected cumulative reward for a given state. By iteratively optimizing the reward signal output, the model learns from experience, adapts to new situations, continuously adjusts its policy, and improves the LLMs [121]. HFRL improves LMEdu's accuracy, making the output results more concise, accurate, and in line with the human thinking process.\n\nDeep neural networks (DNNs) [42]. Before explaining DNNs, it is necessary to introduce deep learning. It refers to the learning of the underlying patterns and hierarchical representations of sample data, aiming to achieve the goal of machine learning with analytical capabilities similar to humans. DNNs consist of multiple layers of interconnected neurons, typically including an input layer, several hidden layers, and an output layer. The connectivity between neurons is similar to the connections between biological neural cells. DNNs have advantages in processing large-scale educational data, including students' academic performance, learning behavior, problem-solving abilities, etc. By analyzing these data, LLM can provide insights for educational decision-making and improve teaching methods and personalized education strategies.\n\nSelf-supervised learning. To produce the desired results, a model or machine needs to be trained with the given materials. Machine learning can be categorized into supervised learning, unsupervised learning, and reinforcement learning [80]. Self-supervised learning falls under unsupervised learning, where the model learns general feature representations for specific tasks. Unlike supervised learning, which requires a large amount of manually annotated data for training, self-supervised learning completes self-training by replacing human annotations with the intrinsic structural features of the data itself, using unlabeled datasets [31, 125]. It gradually trains the parameters from scratch in a progressive manner, using part of the input as the supervisory signal and the rest as input. This approach significantly reduces the cost of manual annotation in terms of high cost, long cycles, and low accuracy, resulting in a lower development cost. Through self-supervised learning, LLMs can learn advanced representations of language data and deep cognition of language skills. This enables them to better understand and generate education-related content, including textbooks, exercises, solutions, and study materials.\n\nTransformer model. From a structural perspective, LMs have evolved from statistical LMs to neural network LMs, and now to LLMs. Statistical LMs focus on transforming sentences into probability distributions, but the lack of computational power limits their ability to match massive amounts of data. Neural network LMs, such as recurrent neural networks, use recursion and convolutional neural networks to transform language sequences. Recurrent neural networks require considering the input-output order for computation and cannot handle examples in batches efficiently, resulting in slow speed. The Transformer model, widely used in LLMs, overcomes these limitations. The transformer model is essentially an encoder-decoder architecture that includes encoding and decoding components. It employs attention mechanisms to capture global dependencies between inputs and outputs [27], without considering the distance within input or output sequences [29]. This approach transforms the growth rate of required data for operations on related signals from linear or logarithmic to constant, showcasing high parallelism, which is beneficial for fast model iterations. Compared to previous models, the Transformer model has a richer structure, stronger adaptability to various scenarios, and better performance. The Transformer model improves the compatibility and practicality of LLMs, as well as its ability to cope with diverse and rich teaching contents and educational scenarios.\n\nLLM diagnostics and application evaluation. Existing interdisciplinary evaluation systems assess LLMs from two perspectives: diagnostics during LLM training and the effectiveness of LLM applications. \"ChatbotArena\"20 is a benchmark platform for LLMs that conduct anonymous and random adversarial evaluations, where the system randomly selects two different LLMs to chat with users, who then rate the interactions. \"SuperCLUE\"21 is a benchmark for evaluating general-purpose LMs in Chinese, examining multidimensional capabilities in terms of basic abilities, professional abilities, and Chinese-specific abilities [124]. \"The C-Eval project\" [51], jointly carried out by Shanghai Jiao Tong University, Tsinghua University, and the University of Edinburgh, constructs a multidisciplinary benchmark list to assist Chinese LLM research. \"FlagEval\" [63], built by multiple universities, adopts a three-dimensional approach to evaluating LLMs, including factuality, safety, and inclusivity. These evaluation frameworks are designed to comprehensively assess LLMedu's performance, ethical impact, and potential bias, as well as promote the improvement of LLMedu's capabilities and technology optimization.\n\nPrompt engineering [83]. It refers to the ability to interact with LLMs. Machines match corresponding results through prompts, thereby increasing productivity. Good prompts can enhance the intelligence of LLMs and increase the value of feedback results [109, 130], increasing the use value of LLM.edu. Moreover, poor prompts may lead to erroneous conclusions. In the field of education, especially rigorous science, the correctness of answers is always given priority, so optimizing prompt words is also important to deal with LLM's nonsense when answering academic questions. Different LMs, such as ChatGPT, ERNIE Bot, and MathGPT, have independent underlying training mechanisms, and their prompts are different. This can be likened to communication with individuals with different personalities.\n\nLearning cognitive mechanisms. Learning cognitive mechanisms, which were developed in cognitive ethics, serve as the foundation for intelligent instructional design. It studies the process of knowledge construction in learners, integrating new knowledge into existing knowledge structures, and adjusting and updating the overall structure. Prior to ChatGPT, AI primarily focused on computation and reasoning. With AI's rapid development, its cognitive intelligence has gradually emerged and can even match human intelligence. There are two main cognitive approaches: one involves simulating human learning processes through computer models, and the other utilizes non-invasive brain imaging techniques such as functional magnetic resonance imaging. LLMs primarily simulate human learning processes, where pre-training can be likened to acquiring new knowledge and constructing knowledge.\n\nBy adding plug-ins, the latest LLM GPT-4 can address real-time problems, such as solving the lag problem of pretraining data. GPT-4 can also better solve logic problems because it introduces the mathematical problem data sets\n\nMATH and GSM-8K into the training data set, which greatly improves its mathematical reasoning capabilities. Moreover, GPT-4 can also complete creative text creation because it is connected to the API, and users can customize the AI character and complete simulated writing, reducing deviations and over-correction [71].\n\n![](/uploads/images/2c6ea33c-9a9e-4547-949a-69351fc70f65/b4ef019575990bd87a640c565e63e967f54e38f8504e2682eebbeedb8e434bd6.jpg)  \nFigure 4: Key technologies of the LLMs\n\n# 5. Implementation of LLMEdu\n\nIn this article, many products of LLMedu are introduced, and the summary is shown in Figure 5. Moreover, this part will focus on the implementation process of LMs from two aspects: LLMs empowering education and specifically LLMs empowering the field of mathematics. Finally, we use a unified framework to organize and compare the application of LLM in the field of education. The details are shown in Table 4.\n\n![](/uploads/images/2c6ea33c-9a9e-4547-949a-69351fc70f65/2e00fa102c4cec42c4c9611c8bc61e3d50cd086121164b5e0ef13d24ffcfd33b.jpg)  \nFigure 5: Examples of LLMEdu.\n\n# 5.1. LLMs-empowered education\n\nImprove teacher effectiveness. LLM can help teachers access a wealth of teaching resources, allowing them to conduct classroom instruction more effectively. Before class, LLM can serve as a helpful assistant for lesson preparation. Through interactive question-and-answer sessions, LLM can provide ideas for teacher's lesson planning, assist in designing teaching outlines and curriculum plans, and help teachers quickly identify the highlights and challenges of a lesson. In the classroom, LLM can act as an AI teaching assistant, providing an instant feedback platform for both teachers and students and enhancing classroom engagement, interest, and appeal. After class, LLM can assist teachers in generating\n\nhomework assignments and exam questions, enabling teachers to better assess students' understanding of the subject matter. In daily work, LLM is also a valuable assistant for teachers, capable of drafting meeting invitations, writing work plans, summaries, reports, and more. When used properly, LLM can help alleviate teachers' workload and promote their professional development [136]. For example, a survey pointed out that during the paper revision process,  $57.4\\%$  of users believed that the feedback generated by LLM was helpful and could help them improve their research process [64].\n\nPromote student progress and growth. In terms of learning assistance, LLM is a powerful tool that can understand complex concepts, solve difficult problems, and provide corresponding learning advice. In language learning, LLM offers scenario-based dialogue training, greatly enhancing student's oral and written abilities. In terms of cultivating thinking skills, LLM sometimes exhibits \"serious nonsense\". Teachers and parents can utilize this phenomenon to cultivate students' critical thinking and enhance their information literacy. In terms of learning ability development, the process of using LLM requires students to ask questions. In this process, students have to learn how to translate their questions into effective questions and how to obtain useful information, which cultivates students' self-learning ability and summary ability. Taking college students as an example, data shows that more than  $20\\%$  of the users of one of LLM's latest products, the iFlytek Spark model, are college students, and it helps them improve in English speaking practice, mock interviews, and after-school homework.\n\nAnswer professional and academic questions, accelerating research progress. LLM is capable of writing academic experiment codes, building experimental models, quickly and accurately searching for literature materials, and extracting and integrating relevant information. This reduces the tedious process of manual research and accumulation, saving a significant amount of time. As a result, researchers can invest more energy into subsequent research, thereby improving research efficiency [7]. Additionally, the report findings show that LLMs in universities, as an important research platform in the field of AI, have achieved remarkable results. Chinese universities' research on LLMs mainly focuses on CV, NLP, speech recognition, and other fields. Research results in these fields not only provide a good academic atmosphere for teachers and students in universities but also provide strong support for the development of different AI industries.\n\nPromote the evolution of educational consciousness and form new learning paradigms. The existing educational system is primarily focused on inheritance, and students often approach knowledge with inertial thinking inherited from their learning experiences. There is a lack of creative awareness. However, with the advancement of AI technologies such as ChatGPT, the existing learning paradigms are no longer sufficient for the future. Faced with the challenges posed by technologies like ChatGPT,\n\nit is necessary to cultivate higher consciousness and exercise thinking skills with a high level of awareness, forming new learning paradigms while improving perception and cognition to better understand the world. For example, the high-consciousness generative learning paradigm reflected in ChatGPT involves establishing connections between new and old knowledge, incorporating reflection and introspection, and innovating new concepts and understandings. To advance the high-consciousness generative learning paradigm, collaboration between educational designers and implementers is required to build adaptive learning environments and foster a positive learning atmosphere [7].\n\nCreate highly contextualized and intelligent learning experiences. In subject learning, generative AI like LLM, with its vast amount of data, can provide students with abundant information and knowledge, streamlining the process of finding learning materials and assisting students in finding answers and solving problems across various subjects. In language learning, LLM can offer real-time dialogue training, enabling students to immerse themselves in scenario-based learning and improve their conversational and writing skills. In terms of temporal and spatial aspects of learning, as an online tool, LLM can be accessed by students anytime and anywhere, providing great flexibility. Currently, LLMs are constantly improving their technologies and capabilities to achieve intelligent learning. For example, in the language understanding task, the ultra-large-scale Chinese pre-trained language model PLUG broke the Chinese GLUE classification list record with a score of 80.179. In the language generation task, it improved by an average of more than  $8\\%$  compared with the previous best results in multiple datasets.\n\nPromoting high-quality development in education enhances educational management and decision-making capabilities. LLMs represent the latest technological means supporting intelligent education, and their development process reflects the synchronized progress of AI and humans. This embodies a new era of educational style that aims to create intelligence, cultivate wisdom, and create more efficient intelligence. Moreover, the data transparency involved in LLMs can make educational development decisions more precise and scientific, transforming educational decision-making from experiential patterns to evidence-based patterns and thereby enhancing educational governance capabilities. Finally, educational practitioners can use AI technologies like ChatGPT to conduct scenario-based assessments of students, resulting in a digital transformation of educational evaluation [45]. LLMs can help teachers judge student's progress in learning and understand student's learning status. Notice that the multi-dimensional data collected by LLMs through evaluation is helpful for educators to study student's learning logic and development rules, adjust teaching content on time, and provide students with personalized growth services.\n\nDriving in-depth research in the education system. The research paradigms in education have evolved from the traditional observation and summary of scientific experiment experience, the construction of theoretical models and\n\nderivations, and computer simulation to the scientific research paradigm of large-scale data collection, analysis, and processing. The educational research paradigm is constantly changing. However, as time progresses, the old research paradigms no longer meet the requirements. The emergence of content-generative AI, represented by LLMs, has given rise to a new paradigm, \"The Fifth Paradigm\" of \"AI for Science,\" enabling humans to delve further into the exploration of the education system. This paradigm shift involves the transition from simple imitation of humans to cognitive understanding and transformation, creating a new world of AI and education. According to a survey by Study.com[22],  $21\\%$  of teachers outside China have begun to use ChatGPT to assist their teaching work. Chegg, a listed American education and training company, also said that after launching the LLM-based learning assistance platform, it has affected the user growth of its original business, and students' interest in ChatGPT has greatly increased.\n\nPromote the development of AI from fragmentation to scalability, thereby enhancing its generalization capabilities in education. LLMs accurately capture knowledge from massive datasets through the process of pre-training an LLM and fine-tuning it for downstream tasks [11]. This knowledge is stored in a large number of parameters and then fine-tuned for specific tasks. Finally, it can be flexibly applied to various scenarios. In other words, a single set of techniques can be used to address different tasks, greatly improving development efficiency. For example, in the field of education, LLMs share data to solve common problems and are widely applied in dialogue question-answering, language translation, text generation, and other scenarios. Some open-source LLMs, such as ChatGLM, Baichuan, InternLM, Qwen-7B, and Qwen-14B, are all manifestations of the generalization of LLMs, and Qwen-14B among them already has an accuracy of more than  $70\\%$ , which shows that these degrees are constantly improving.\n\n# 5.2. LLMs in Mathematics\n\nAI has been pursuing mathematical research and applications since its inception. Mathematics is a challenging subject in education, and proficiency in math represents a significant milestone in the intelligence level of LLMs. The successful handling of mathematical problems by LLMs will mark a new era in AI.\n\nApplications in mathematics can reflect the imitation ability of LLMs. Mathematics is an abstract discipline that requires logical reasoning and critical thinking [102]. Currently, LLMs are unable to genuinely comprehend the essence of mathematics and demonstrate independent thought. Therefore, when addressing mathematical problems, these LLM models rely heavily on the mathematical concepts and rules embedded in their training data. For instance, when solving algebraic problems, LLMs apply algebraic rules by mimicking the way humans learn and apply algebra [71].\n\nImprovement of computational performance of LLMs in mathematics. The essence of LLMs is to predict future outputs based on data correlation. However, errors may occur for symbols that are rarely or never encountered in the pre-training stage. For example, because the size of numbers is infinite and the scale of LLMs is limited, arithmetic operations on large numbers are likely to go wrong. To solve this problem, fine-tune the LLM on synthetic arithmetic problems and use special training and inference strategies to further improve numerical computing performance.\n\nOptimize the logical reasoning process. One is to optimize the human logical reasoning process through LLMs. For example, some scholars have applied LLMs to the proof of theorems [44], because LLMs can provide a large amount of relevant materials to make up for the lack of information or omissions, making the reasoning more complete. The second goal is to improve LLMs' logical reasoning abilities. The logical reasoning ability of LLMs is a key indicator for evaluating LLMs. Because LLMs usually have problems such as excessive parameter space and severe data sparseness, LLMs perform poorly on robust and rigorous reasoning tasks. Relevant research has proposed optimization methods for LLM logical reasoning problems. For example, OpenAI[23] studies a process-based supervision model to improve the logical reasoning capabilities of GPT-4. Moreover, some research institutions use the method of continuous pre-prediction on large-scale mathematical corpora, which improves model performance on mathematical reasoning tasks.\n\nInteraction with external tools to improve LLMs' mathematical capabilities. 1) LLMs interact with language conversion tools, such as lean language [81], which can convert mathematical language into computer language, thereby improving the rigor of model reasoning. This is an innovative way to bridge the gap between human reasoning and machine reasoning. This could allow models to better understand and process complex mathematical concepts. 2) LLMs interact with information retrieval systems, such as the large dialogue model LaMDA proposed by Google, which connects to the information retrieval system and allows the model to learn to retrieve and use calculators and translation engines [108]. 3) LLMs directly interact with the calculation engine, such as MathGPT, which improves calculation accuracy by interacting with the calculation engine. This allows models to take advantage of calculators' powerful computing capabilities and perform complex mathematical calculations with greater accuracy. 4) LLMs enable themselves to determine the interactive tools, such as Meta's toolformer model, which can determine the use of external tools by itself [98]. This gives models the flexibility to adapt to different situations and choose the most appropriate tools to solve a problem, much like humans do.\n\nFuture development of LLMs in mathematics. Specifically, the first is a cutting-edge exploration with scientific research at the core, such as the research and improvement of LLMs' capabilities in mathematics, including computing\n\nTable 4 Comparison between generative AI and discriminative AI  \n\n<table><tr><td>Application</td><td>Advantage</td><td>Disadvantage</td><td>Challenge</td><td>Future development</td></tr><tr><td rowspan=\"3\">Personalized learning</td><td>Save time and costs</td><td>Data privacy issues</td><td>Expand the corpus</td><td>Develop personalized applications</td></tr><tr><td>Precise teaching</td><td>Information bias</td><td>Information accuracy</td><td>Information extraction technology update</td></tr><tr><td>Good interactivity</td><td>The learning process is opaque</td><td>Update corpus in real time</td><td>Integration of various technologies</td></tr><tr><td rowspan=\"3\">Guided learning</td><td>Improve problem-solving abilities</td><td>Marginalized teachers</td><td>Social impact</td><td>Training with more accurate data</td></tr><tr><td>Encourage critical thinking</td><td>Misleading information</td><td>Emotional understanding</td><td>Integrate with personalized experiences</td></tr><tr><td>Cultivate interest in learning</td><td>Lack of emotional resonance</td><td>Unemployment Risk</td><td>Develop policies to address social impacts</td></tr><tr><td rowspan=\"3\">Interdisciplinary learning</td><td>Provide diverse learning support</td><td>Insufficient training data support</td><td>Logic optimization</td><td>Integration of multidisciplinary and LLM</td></tr><tr><td>Cultivate interdisciplinary thinking skills</td><td>Lack of domain knowledge</td><td>Accuracy of knowledge integration</td><td>Revolutionize the way we learn and teach</td></tr><tr><td>Boast excellent interdisciplinary capabilities</td><td>Disciplinary bias</td><td>Algorithm optimization</td><td>Filter useful training data</td></tr><tr><td rowspan=\"3\">Real-time problem-solving</td><td>Reduce teacher stress</td><td>Machine hallucination</td><td>Multiple text associations</td><td>Standardize technology use</td></tr><tr><td>Improved learning efficiency</td><td>Over-reliance on technology</td><td>Text extraction</td><td>Acceleration of model inference</td></tr><tr><td>Teaching assistance upgrade</td><td></td><td></td><td>Diversified technical assistance</td></tr><tr><td rowspan=\"3\">Applications in mathematics</td><td>Guide mathematics learning</td><td>Math terminology learning</td><td>Promote mathematical research</td><td>Pay attention to thinking guidance</td></tr><tr><td>Improve math learning efficiency</td><td></td><td>Improved logical reasoning ability</td><td>Mathematics research and teaching</td></tr><tr><td>Show the fusion of AI and mathematics</td><td></td><td>Understand number relationships</td><td>Adequate language modeling</td></tr></table>\n\ncapabilities, reasoning capabilities, robustness, and so on. The second is to improve inclusive education and basic education for the general public. This entails studying how to use models to improve learning experiences and effects, as well as enhance mathematical education for students of all ages and backgrounds. By leveraging the power of LLMs, it may be possible to create personalized learning experiences that cater to individual student's needs and learning styles, making mathematics education more accessible and effective for a broader range of people. In terms of development potential, the expansion of LLMs' ability to solve mathematical problems could have far-reaching implications for other technical and educational fields. For example, LLMs could be used to improve the accuracy and efficiency of scientific simulations, enhance the effectiveness of machine learning algorithms, or even aid in the development of new technologies such as quantum computing. Ultimately, the development of LLMs in mathematics could drive the development of a new generation of education models that are more inclusive, effective, and efficient.\n\n# 6. Issues and Challenges\n\nIn practical applications, LLMs for education still face many issues and challenges, including but not limited to, as shown in Figure 6.\n\n# 6.1. Main issues\n\nRisk of widespread false knowledge. As an imperfect intelligent technology, LLMs such as ChatGPT still have many flaws. The biggest drawback is the potential for generating incorrect information [3]. As many people have noticed, LLM sometimes exhibits machine hallucination [94]. For example, a computer scientist in California tried different methods to check the output of the GPT robots and found that GPT-3.5 and GPT-4 were full of errors when testing physics, chemistry, and mathematics questions selected from\n\n![](/uploads/images/2c6ea33c-9a9e-4547-949a-69351fc70f65/2e96c40efc4f830a6d3e3df8179621d5ff0b821e91ca75d694a2efc3168f8e51.jpg)  \nFigure 6: Some challenges and issues of LLMEdu.\n\ncollege textbooks and exams. Moreover, since LLM's training data largely consists of English corpora, it often struggles to understand and provide correct answers to personalized Chinese questions. In the short term, these errors can cause disruptions in students' knowledge learning, and students with weaker discernment abilities are highly likely to acquire erroneous knowledge without realizing it. In the long term, if the corresponding technology is not improved promptly, LLM may contribute further to the proliferation of false knowledge. There are many examples of actively dealing with machine hallucinations. For example, the retrieval-augmented generation method (RAG) can integrate LLM with a rigorously verified external key knowledge corpus.\n\nLack of clear operating rules in the education system. Due to the complexity of education itself, representing the education system using specific symbols and algorithms is an extremely challenging process that current LLMs cannot achieve. Education behaviors, such as emotional interaction, effective communication, and leading by example, are currently beyond the capabilities of LLMs. LLMs learn from a large amount of data and provide feedback, representing subjective educational information with data and providing\n\nrational reflections of human thinking. The goal of anthropomorphizing LLMs is to enable NLP models, such as Word2Vec, to convert words into vectors, facilitating the computer's processing of textual data [4]. GPT-1 and BERT, based on the self-attention mechanism [40], further enhance performance. GPT-3 achieves another leap in performance on zero-shot learning tasks with its significantly increased parameter scale [116]. ChatGPT's HFRL, code pretraining, and instruction fine-tuning improve the model's inference capabilities [86]. GPT-4, an ultra-large-scale multimodal pre-trained model, possesses multimodal understanding and multi-type content generation capabilities [62]. These examples show ideas for solving the problem of anthropomorphizing LLMs, gradually approaching human-like capabilities through continuous optimization and development, thereby alleviating the limitations of the abstraction and ambiguity of educational rules.\n\nSome drawbacks when students use LLMs. The occasional inaccuracies in LLM's answers can mislead students who lack critical thinking skills. The great convenience of LLM may reduce students' desire for independent learning and innovation, leading to intellectual laziness. As LLM involves massive amounts of data, students who lack awareness of data security may unknowingly leak their personal data [129]. While LLM provides interactive dialogue scenarios and opportunities for AI communication with students, it reduces real interpersonal conversations, and the way of discussing problems may shift from online to one-sided questioning of the machine, affecting the development of student's social skills. In response to these problems, educators need to actively guide students to adapt to the characteristics of LLM-assisted education and enhance the cultivation of privacy and security awareness.\n\nInsufficient integration of LLMs in collaborative teaching [71]. Although LLM has achieved some level of one-on-one dialogue and communication, its integration with education in real life is still limited. The ability to solve higher-order reasoning problems and complex problems still needs improvement. For example, while GPT-4 performs reasonably well in some exams, it fails to demonstrate significant advantages in logical reasoning problems [70]. Most LLMs have high accuracy rates (up to  $95\\%$ ) for reasoning with a small number of steps, but as the number of steps increases, reaching 20 or more, the accuracy drops significantly to  $36\\%$ , indicating a significant disparity [90]. As a result, it is necessary to develop chain-of-thought technology to improve LLMs' reasoning ability and ability to solve complex problems [117], thereby promoting the integration of large models and collaborative education.\n\nLimitations of LLMs [107]. Firstly, in pre-training, models that simultaneously satisfy the reasonable model size, advanced few-shot learning capability, and advanced fine-tuning capability have not been achieved yet. For example, GPT-3 lacks a reasonable model size and is relatively large in scale [16]. Furthermore, the high complexity and strong data dependency of LLMs may be exploited by malicious data to affect their training process and generation\n\nresults, as well as output uncertainty and other factors. The lack of interpretability in LLMs' technology makes their internal mechanisms unclear. The widespread application of LMs requires interpretability to ensure application security, overcome performance limitations, and control societal impact, which has triggered corresponding considerations regarding these issues. In the future, LLM's technology still needs optimization and innovation, and researchers need to consider the interpretability of the model more based on the user's situation.\n\n# 6.2. Main challenges\n\nTechnological challenges. The application of LLMEdu relies on AI-based technologies, which are complex and challenging. If the technology is not perfected, it becomes difficult to provide high-quality educational services. The availability of high-quality data sources is one important factor influencing the improvement of LLM technology. High-quality data transformation involves capture and conversion processes. It is necessary to consider how to expand the perception of the educational field to capture dynamic performance data from any learning activity in educational subjects and how to improve the quality of the data through efficient processing. Moreover, LLMEdu faces technological challenges such as speech recognition, NLP, AIGC [119], multimodal LLMs [120], and other aspects. The above-mentioned issues require researchers to always pay attention to the development of other technologies in the AI field and actively integrate them into LLM to bring a better experience to the education industry.\n\nArtificial intelligence security. The intelligence level of LLMs continues to improve, and security issues have become more severe. The first is the LLMs' biased cognition. Some studies have pointed out that when LLMs are tested using gender bias data sets, their answers will reflect gender bias [57]. Therefore, when training an LLM, the data should be filtered. The second is the lack of correct social, moral, and ethical values. For some issues that violate social ethics, LLMs are unable to judge, which increases the risk of crime. Therefore, the country should formulate a more complete legal system to regulate the use of LLMs. The third is the most common issue among artificial intelligence ethical issues: \"AI replaces human activities\". AI has limitations in education. While AI has great potential in education, it cannot replace the role of teachers, such as encouraging critical thinking, solving complex problems, and providing psychological and social support. However, humans should also flexibly adjust their roles, regulate and guide the development of AI from an ethical perspective, and maintain their dominant position.\n\nEducation quality. The use of LLMedu provides many opportunities for smart education, but it also presents challenges in terms of quality. If LLMedu cannot provide high-quality educational services, it will be difficult to gain recognition from students and teachers. Furthermore, educational institutions that use LMs must strike a balance between educational quality and technological innovation. Otherwise,\n\nthere may be an overreliance on technology, neglecting the quality of education itself. Therefore, to ensure the quality of education, the first consideration is to ensure the educational content, which requires educators to adjust reasonable teaching content and clarify the auxiliary functions of LLMs. Then, technology developers are required to ensure that the technology of LLMs is steadily progressing.\n\nTechnological dependence. Note that the future LLMEd should be human-centric but not technology-centric [127]. Overreliance on AI may reduce students' ability for independent learning and innovative thinking, and it may even lead to cheating and academic misconduct, such as using ChatGPT to complete assignments and papers. It is necessary to prevent the passive application of LLMs, as seen in the examples in reality. While using AI, the student should be encouraged to think independently, explore problems, and find answers. Furthermore, students should be educated on time management, ensuring sufficient time for other important activities while using AI, and avoiding excessive dependence on it.\n\nTechnical accessibility and training. The introduction of AI technology requires corresponding hardware infrastructure and network support. In resource-limited areas, this can be a challenge. Combined with the pressures and entrenched thinking that fear is being replaced [126], there is a phenomenon of fear and refusal to use AI in education, in other words, cognitive limitations. In such cases, technical access and training become difficult. Therefore, efforts should be made to promote the long-term advantages of AI in the education industry, guide teachers and students to receive appropriate training, better understand the application ideas and specific methods of intelligent technology, enhance willingness to use, and better adapt to and utilize these tools.\n\nEquity issues. Although AI has the potential to improve the quality and efficiency of education, its use can lead to unfairness among students. For example, some families may not be able to afford AI learning tools, or in certain areas, students may lack access to the necessary technological facilities for tools like ChatGPT. Educational equity is the cornerstone of social development, and interventions are needed to address the examples mentioned above effectively. For instance, when designing and optimizing LLMs, efforts should be made to balance characteristics such as race, gender, and age, reducing the digital divide and gender gap.\n\nData privacy and security [129]. Data privacy, including privacy protection, is a significant concern in the application of LLMs. LLMs involve collecting personal information and learning data from students and teachers. Therefore, privacy protection becomes an important issue in LLM applications. Educational institutions need to ensure the effective protection of student's and teacher's privacy while also ensuring the security and reliability of the data. Parents and teachers should focus on cultivating children's awareness of data privacy and security, as well as educating students to avoid privacy risks associated with the use of LLMs. Moreover, when collecting and processing student's\n\nlearning data, it is essential to ensure that this information is properly protected to avoid data breaches or improper use.\n\nIn the future, following the development characteristics of the era of integrating intelligence and education, while continuing to optimize core technologies and technological innovations, LLMs such as ChatGPT, GPT-4, and MathGPT will continue to empower the education field. Moreover, based on the existing LLMs, we must continue to look for more effective training methods to more efficiently train models with large-scale parameters [11].\n\n# 7. Conclusion\n\nIn this article, we have introduced the development and application of LLMs in the field of education as comprehensively as possible. There are still some technologies that have not been included, as well as other issues that have not been discussed in depth. It is hoped that the technology introduced in this article and the thinking presented can help scholars and researchers better develop and optimize educational LLMs. This article summarizes the process of integrating education and LLMs. LLMs have excellent language generation and interactive capabilities that cannot be provided by traditional book-based teaching. It demonstrates the creative role of AI in education, as well as teachers, and the changing roles of parents and students. For smart education, we call for more mature education and AI development standards, technical specifications, and data security guidelines to focus on more practical issues. How to ensure data security? How can we limit the behavior that relies too much on AI technology? How to cultivate students' active exploration abilities? LLMs and education complement each other. The application of LLMs in education makes education more intelligent and efficient, and the data accumulated over many years in education can help optimize LLM training. More attention should be paid to these development conditions. How can we create more valuable LLM.edu application scenarios? We look forward to the future of LLM.edu.\n\nAcknowledgments This research was supported in part by the National Natural Science Foundation of China (No. 62272196), the Natural Science Foundation of Guangdong Province (No. 2022A1515011861), Guangzhou Basic and Applied Basic Research Foundation (No. 2024A04J9971).\n\nAuthor contributions Hanyi Xu: paper reading and review, writing original draft. Wensheng Gan: conceptualization, review and editing, supervisor. Zhenlian Qi: conceptualization, review and editing. Jiayang Wu: writing original draft. Philip S. Yu: review and editing.\n\nData availability This is a review paper, and no data was generated during the study.\n\nConflict of interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n# References\n\n[1] Ahmad, N., Murugesan, S., Kshetri, N., 2023. Generative Artificial Intelligence and the Education Sector. Computer 56, 72-76.  \n[2] Al-Garaady, J., Mahyoob, M., 2023. ChatGPT's Capabilities in Spotting and Analyzing Writing Errors Experienced by EFL Learners. Arab World English Journals.  \n[3] Amer-Yahia, S., Bonifati, A., Chen, L., Li, G., Shim, K., Xu, J., Yang, X., 2023. From Large Language Models to Databases and Back: A Discussion on Research and Education. ArXiv E-prints, arXiv:2306.01388.  \n[4] Amin, M.M., Cambria, E., Schuller, B.W., 2023. Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT. ArXiv E-prints, arXiv:2303.03186.  \n[5] Bahrami, M., Srinivasan, R., 2023. Examining LLM's Awareness of the United Nations Sustainable Development Goals, in: ICLR Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models.  \n[6] Bai, K., Shrivastava, A., 2010. Heap Data Management for Limited Local Memory Multi-Core Processors, in: Proceedings of the Eighth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis, ACM. p. 317-326.  \n[7] Baidoo-Anu, D., Ansah, L.O., 2023. Education in the Era of Generative Artificial Intelligence: Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning. Journal of AI 7, 52-62.  \n[8] Bakker, M., Chadwick, M., Sheahan, H., Tessler, M., Campbell-Gillingham, L., Balaguer, J., McAleese, N., Glaese, A., Aslanides, J., Botvinick, M., et al., 2022. Fine-tuning Language Models to Find Agreement among Humans with Diverse Preferences. Advances in Neural Information Processing Systems 35, 38176-38189.  \n[9] Bao, H., Dong, L., Wei, F., Wang, W., Yang, N., Liu, X., Wang, Y., Gao, J., Piao, S., Zhou, M., et al., 2020. UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training, in: International Conference on Machine Learning, PMLR. pp. 642–652.  \n[10] Beck, J., Stern, M., Haugsjaa, E., 1996. Applications of AI in Education. XRDS: Crossroads, The ACM Magazine for Students 3, 11-15.  \n[11] Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S., 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, in: ACM Conference on Fairness, Accountability, and Transparency, pp. 610-623.  \n[12] Bhutoria, A., 2022. Personalized Education and Artificial Intelligence in the United States, China, and India: A Systematic Review Using A Human-in-the-loop Model. Computers and Education: Artificial Intelligence 3, 100068.  \n[13] Biggs, J., Tang, C., Kennedy, G., 2022. Ebook: Teaching for Quality Learning at University 5e. McGraw-hill education (UK).  \n[14] Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G.B., Lespiau, J.B., Damoc, B., Clark, A., et al., 2022. Improving Language Models by Retrieving from Trillions of Tokens, in: International Conference on Machine Learning, PMLR. pp. 2206-2240.  \n[15] Brem, A., Giones, F., Werle, M., 2021. The AI Digital Revolution in Innovation: A Conceptual Framework of Artificial Intelligence Technologies for the Management of Innovation. IEEE Transactions on Engineering Management 70, 770-776.  \n[16] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020. Language Models are Few-shot lLarners. Advances in Neural Information Processing Systems 33, 1877-1901.  \n[17] Budiharso, T., Tarman, B., 2020. Improving Quality Education through Better Working Conditions of Academic Institutes. Journal of Ethnic and Cultural Studies 7, 99-115.  \n[18] Bunnell, T., Courtois, A., Donnelly, M., 2020. British Elite Private Schools and Their Overseas Branches: Unexpected Actors in the Global Education Industry. British Journal of Educational Studies 68, 691-712.\n\n[19] Butcher, K.R., Sumner, T., 2011. Self-Directed Learning and the Sensemaking Paradox. Human-Computer Interaction 26, 123–159.  \n[20] Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L., Yi, X., Wang, C., Wang, Y., et al., 2023. A Survey on Evaluation of Large Language Models. ArXiv E-prints, arXiv:2307.03109.  \n[21] Chen, L., Chen, P., Lin, Z., 2020a. Artificial Intelligence in Education: A Review. IEEE Access 8, 75264-75278.  \n[22] Chen, X., Xie, H., Hwang, G.J., 2020b. A Multi-perspective Study on Artificial Intelligence in Education: Grants, Conferences, Journals, Software Tools, Institutions, and Researchers. Computers and Education: Artificial Intelligence 1, 100005.  \n[23] Chen, X., Xie, H., Zou, D., Hwang, G.J., 2020c. Application and Theory Gaps During the Rise of Artificial Intelligence in Education. Computers and Education: Artificial Intelligence 1, 100002.  \n[24] Cheng, X., Jiao, F., Ji, G., Tian, Y., 2023. The Artificial Intelligence Revolution Led by ChatGPT, in: International Seminar on Computer Science and Engineering Technology, IEEE. pp. 360-363.  \n[25] Chung, Y.A., Zhang, Y., Han, W., Chiu, C.C., Qin, J., Pang, R., Wu, Y., 2021. W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-supervised Speech Pre-training, in: IEEE Automatic Speech Recognition and Understanding Workshop, IEEE. pp. 244-250.  \n[26] Deng, Y., Liu, X., Meng, L., Jiang, W., Dong, Y., Liu, C., 2023. Multi-Modal Information Fusion for Action Unit Detection in the Wild, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 5855–5862.  \n[27] DeRose, J.F., Wang, J., Berger, M., 2020. Attention flows: Analyzing and Comparing Attention Mechanisms in Language Models. IEEE Transactions on Visualization and Computer Graphics 27, 1160-1170.  \n[28] Dillenbourg, P., 2016. The Evolution of Research on Digital Education. International Journal of Artificial Intelligence in Education 26, 544-560.  \n[29] Dong, L., Jiang, F., Peng, Y., Wang, K., Yang, K., Pan, C., Schober, R., 2023. LAMBO: Large Language Model Empowered Edge Intelligence. ArXiv E-prints, arXiv:2308.15078.  \n[30] Edyko, K., Petryla, P., Ostafin, K., Minkner, M., Bienkowski, B., Feja, K., Suwała, Z., Rektor, N., Luczak, E., Marchewka, U., 2023. Utilizing Artificial Intelligence Tools Using the GPT Chatbot in Medicine-A Review of Flaws, Advantages, and Limitations. Journal of Education, Health and Sport 46, 122-133.  \n[31] Elnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M., et al., 2021. ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 7112-7127.  \n[32] Fan, W., Zhao, Z., Li, J., Liu, Y., Mei, X., Wang, Y., Tang, J., Li, Q., 2023a. Recommender Systems in the Era of Large Language Models (LLMs). ArXiv E-prints, arXiv:2307.02046.  \n[33] Fan, Y., Jiang, F., Li, P., Li, H., 2023b. GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning, in: Natural Language Processing and Chinese Computing, Springer Nature Switzerland. pp. 69–80.  \n[34] Gan, W., Lin, J.C.W., Chao, H.C., Yu, P.S., 2023a. Discovering high utility episodes in sequences. IEEE Transactions on Artificial Intelligence 4, 473-486.  \n[35] Gan, W., Lin, J.C.W., Fournier-Viger, P., Chao, H.C., Tseng, V.S., Yu, P.S., 2021. A Survey of Utility-oriented Pattern Mining. IEEE Transactions on Knowledge and Data Engineering 33, 1306-1327.  \n[36] Gan, W., Qi, Z., Wu, J., Lin, J.C.W., 2023b. Large Language Models in Education: Vision and Opportunities, in: IEEE International Conference on Big Data, IEEE. pp. 4776-4785.  \n[37] Gan, W., Wan, S., Yu, P.S., 2023c. Model-as-a-Service (MaaS): A Survey, in: IEEE International Conference on Big Data, IEEE. pp. 4636-4645.  \n[38] Gan, W., Ye, Z., Wan, S., Yu, P.S., 2023d. Web 3.0: The Future of Internet, in: Companion Proceedings of the ACM Web Conference,\n\npp. 1266-1275.  \n[39] Gao, B., Cai, K., Qu, T., Hu, Y., Chen, H., 2020. Personalized Adaptive Cruise Control Based on Online Driving Style Recognition Technology and Model Predictive Control. IEEE Transactions on Vehicular Technology 69, 12482-12496.  \n[40] Ghojogh, B., Ghodsi, A., 2020. Attention mechanism, transformers, bert, and gpt: tutorial and survey.  \n[41] Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., Poon, H., 2021. Domain-specific Language Model Pretraining for Biomedical Natural Language Processing. ACM Transactions on Computing for Healthcare 3, 1-23.  \n[42] Guu, K., Lee, K., Tung, Z., Pasupat, P., Chang, M., 2020. Retrieval Augmented Language Model Pre-Training, in: International Conference on Machine Learning, PMLR. pp. 3929-3938.  \n[43] Han, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao, H., Zhang, K., Liu, C., Wen, S., Guo, Z., et al., 2023. ImageBind-LLM: Multi-modality Instruction Tuning. ArXiv E-prints, arXiv:2309.03905.  \n[44] Han, J.M., Rute, J., Wu, Y., Ayers, E.W., Polu, S., 2021. Proof Artifact Co-training for Theorem Proving with Language Models. ArXiv E-prints, arXiv:2102.06203.  \n[45] Hawley, R., Allen, C., 2018. Student-generated Video Creation for Assessment: Can It Transform Assessment Within Higher Education? International Journal for Transformative Research 5, 1-11.  \n[46] Hsu, H.P., Wenting, Z., Hughes, J.E., 2019. Developing Elementary Students' Digital Literacy Through Augmented Reality Creation: Insights From a Longitudinal Analysis of Questionnaires, Interviews, and Projects. Journal of Educational Computing Research 57, 1400-1435.  \n[47] Hu, L., Liu, Z., Zhao, Z., Hou, L., Nie, L., Li, J., 2023. A Survey of Knowledge Enhanced Pre-trained Language Models. IEEE Transactions on Knowledge and Data Engineering, 1-19.  \n[48] Huang, G., Gan, W., Weng, J., Yu, P.S., 2023a. US-Rule: Discovering Utility-driven Sequential Rules. ACM Transactions on Knowledge Discovery from Data 17, 1-22.  \n[49] Huang, H., Zheng, O., Wang, D., Yin, J., Wang, Z., Ding, S., Yin, H., Xu, C., Yang, R., Zheng, Q., et al., 2023b. ChatGPT for Shaping the Future of Dentistry: the Potential of Multi-modal Large Language Model. International Journal of Oral Science 15, 29.  \n[50] Huang, J., Chang, K.C.C., 2022. Towards Reasoning in Large Language Models: A Survey. ArXiv E-prints, arXiv:2212.10403.  \n[51] Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu, J., Lv, C., Zhang, Y., Lei, J., et al., 2023c. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. ArXiv E-prints, arXiv:2305.08322.  \n[52] Ivanov, S., Soliman, M., 2023. Game of Algorithms: ChatGPT Implications for the Future of Tourism Education and Research. Journal of Tourism Futures 9, 214-221.  \n[53] Jeon, J., Lee, S., 2023. Large Language Models in Education: A Focus on the Complementary Relationship between Human Teachers and ChatGPT. Education and Information Technologies 28, 15873-15892.  \n[54] Kim, J.W., Yoon, H., Jung, H.Y., 2022. Improved Spoken Language Representation for Intent Understanding in a Task-Oriented Dialogue System. Sensors 22, 1509.  \n[55] Koksal, I., 2020. The Rise of Online Learning. FORBES.  \n[56] Kopnina, H., 2020. Education for the Future? Critical Evaluation of Education for Sustainable Development Goals. The Journal of Environmental Education 51, 280-291.  \n[57] Kotek, H., Dockum, R., Sun, D., 2023. Gender Bias and Stereotypes in Large Language Models, in: The ACM Collective Intelligence Conference, pp. 12-24.  \n[58] Lai, J., Gan, W., Wu, J., Qi, Z., Yu, P.S., 2023. Large Language Models in Law: A survey. arXiv preprint arXiv:2312.03718.  \n[59] Latif, E., Mai, G., Nyaaba, M., Wu, X., Liu, N., Lu, G., Li, S., Liu, T., Zhai, X., 2023. Artificial General Intelligence for Education. ArXiv E-prints, arXiv:2304.12479.  \n[60] Li, L., 2020. Education Supply Chain in the Era of Industry 4.0. Systems Research and Behavioral Science 37, 579-592.\n\n[61] Li, S., Challoo, R., 2006. Restructuring An Electric Machinery Course with An Integrative Approach and Computer-assisted Teaching Methodology. IEEE Transactions on Education 49, 16-28.  \n[62] Li, Y., Hu, B., Chen, X., Ma, L., Xu, Y., Zhang, M., 2023. LMEye: An Interactive Perception Network for Large Language Models. ArXiv E-prints, arXiv:2305.03701.  \n[63] Li, Y., Zhao, J., Zheng, D., Hu, Z.Y., Chen, Z., Su, X., Huang, Y., Huang, S., Lin, D., Lyu, M.R., et al., 2023. CLEVA: Chinese Language Models EVALuation Platform. ArXiv E-prints, arXiv:2308.04813.  \n[64] Liang, W., Zhang, Y., Cao, H., Wang, B., Ding, D., Yang, X., Vodrahalli, K., He, S., Smith, D., Yin, Y., McFarland, D., Zou, J., 2023. Can Large Language Models Provide Useful Feedback on Research Papers? A Large-scale Empirical Analysis. ArXiv E-prints, arXiv:2310.01783.  \n[65] Lim, J., Sa, I., MacDonald, B., Ahn, H.S., 2023. A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM. ArXiv EA-prints, arXiv:2309.16898.  \n[66] Lin, H., Wan, S., Gan, W., Chen, J., Chao, H.C., 2022. Metaverse in Education: Vision, Opportunities, and Challenges, in: IEEE International Conference on Big Data, IEEE. pp. 2857-2866.  \n[67] Lin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Li, Y., Lin, W., et al., 2021. M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. ArXiv E-prints, arXiv:2110.03888.  \n[68] Lin, J.C.W., Gan, W., Fournier-Viger, P., Hong, T.P., 2015. Mining High-utility Itemsets with Multiple Minimum Utility Thresholds, in: The Eighth International C* Conference on Computer Science & Software Engineering, pp. 9-17.  \n[69] Liu, C., Jin, R., Ren, Y., Yu, L., Dong, T., Peng, X., Zhang, S., Peng, J., Zhang, P., Lyu, Q., et al., 2023. M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. ArXiv E-prints, arXiv:2305.10263.  \n[70] Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., Zhang, Y., 2023. Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. ArXiv E-prints, arXiv:2304.03439.  \n[71] Liu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He, M., Liu, Z., et al., 2023. Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models. Meta-Radiology 1, 100017.  \n[72] Luckin, R., Holmes, W., 2016. Intelligence Unleashed: An Argument for AI in Education.  \n[73] Lv, Z., Han, Y., Singh, A.K., Manogaran, G., Lv, H., 2020. Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence. IEEE Transactions on Industrial Informatics 17, 1496-1504.  \n[74] Lyu, C., Xu, J., Wang, L., 2023. New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT. ArXiv E-prints, arXiv:2305.01181.  \n[75] Ma, X., Fang, G., Wang, X., 2023. LLM-Pruner: On the Structural Pruning of Large Language Models. ArXiv E-prints, arXiv:2305.11627.  \n[76] Maddigan, P., Susnjak, T., 2023. Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models. IEEE Access 11, 45181-45193.  \n[77] Malodia, S., Islam, N., Kaur, P., Dhir, A., 2021. Why Do People Use Artificial Intelligence-Enabled Voice Assistants? IEEE Transactions on Engineering Management, 1-15.  \n[78] Meng, Y., Zhang, Y., Huang, J., Xiong, C., Ji, H., Zhang, C., Han, J., 2020. Text Classification Using Label Names Only: A Language Model Self-Training Approach. ArXiv E-prints, arXiv:2010.07245.  \n[79] Mhlanga, D., 2023. Open AI in Education, the Responsible and Ethical Use of ChatGPT Towards Lifelong Learning, in: FinTech and Artificial Intelligence for Sustainable Development: The Role of Smart Technologies in Achieving Development Goals. Springer, pp. 387-409.  \n[80] Morales, E.F., Escalante, H.J., 2022. A Brief Introduction to Supervised, Unsupervised, and Reinforcement Learning, in: Biosignal Processing and Classification Using Computational Learning and\n\nIntelligence. Academic Press, pp. 111-129.  \n[81] Moura, L.d., Ullrich, S., 2021. The Lean 4 Theorem Prover and Programming Language, in: Automated Deduction - CADE 28, Springer International Publishing. pp. 625-635.  \n[82] Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., et al., 2021. Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM, in: The International Conference for High Performance Computing, Networking, Storage and Analysis, ACM. pp. 1-15.  \n[83] Naseem, U., Razzak, I., Khan, S.K., Prasad, M., 2021. A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models. Transactions on Asian and Low-Resource Language Information Processing 20, 1–35.  \n[84] Ng, E., Subramanian, S., Klein, D., Kanazawa, A., Darrell, T., Ginosar, S., 2023. Can Language Models Learn to Listen?, in: The IEEE/CVF International Conference on Computer Vision, pp. 10083-10093.  \n[85] Ouyang, F., Jiao, P., 2021. Artificial Intelligence in Education: The Three Paradigms. Computers and Education: Artificial Intelligence 2, 100020.  \n[86] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al., 2022. Training Language Models to Follow Instructions with Human Feedback. Advances in Neural Information Processing Systems 35, 27730-27744.  \n[87] P, D., 2020. AI in the Wild: Sustainability in the Age of Artificial Intelligence. MIT Press.  \n[88] Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J., Wu, X., 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. ArXiv E-prints, arXiv:2306.08302.  \n[89] Pankiewicz, M., Baker, R.S., 2023. Large Language Models (GPT) for Automating Feedback on Programming Assignments. ArXiv E-prints, arXiv:2307.00150.  \n[90] Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., Tulio Ribeiro, M., 2023. ART: Automatic Multi-step Reasoning and Tool-use for Large Language Models. ArXiv E-prints, arXiv:2303.09014.  \n[91] Philippe, S., Souchet, A.D., Lameras, P., Petridis, P., Caporal, J., Coldeboeuf, G., Duzan, H., 2020. Multimodal Teaching, Learning and Training in Virtual Reality: A Review and Case Study. Virtual Reality & Intelligent Hardware 2, 421-442.  \n[92] Qidwai, U., Kashem, S.B.A., Conor, O., 2020. Humanoid Robot as a Teacher's Assistant: Helping Children with Autism to Learn Social and Academic Skills. Journal of Intelligent & Robotic Systems 98, 759-770.  \n[93] Rajbhandari, S., Rasley, J., Ruwase, O., He, Y., 2020. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, in: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE. pp. 1-16.  \n[94] Rawte, V., Sheth, A., Das, A., 2023. A Survey of Hallucination in Large Foundation Models. ArXiv E-prints, arXiv:2309.05922.  \n[95] Rudovic, O., Zhang, M., Schuller, B., Picard, R., 2019. MultiModal Active Learning From Human Data: A Deep Reinforcement Learning Approach, in: International Conference on Multimodal Interaction, pp. 6-15.  \n[96] Saini, M.K., Goel, N., 2019. How Smart Are Smart Classrooms? A Review of Smart Classroom Technologies. ACM Computing Survey 52, 1-28.  \n[97] Scarlatos, A., Lan, A., 2023. Tree-Based Representation and Generation of Natural and Mathematical Language. ArXiv E-prints, arXiv:2302.07974.  \n[98] Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., Scialom, T., 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. ArXiv Eprints, arXiv:2302.04761.\n\n[99] Schlecker Lamoureux, P., Winther, K.T., Garrido Torres, J.A., Streibel, V., Zhao, M., Bajdich, M., Abild-Pedersen, F., Bligaard, T., 2019. Machine Learning for Computational Heterogeneous Catalysis. ChemCatChem 11, 3581-3601.  \n[100] Schwartz, R., Dodge, J., Smith, N.A., Etzioni, O., 2020. Green AI. Communications of the ACM 63, 54-63.  \n[101] Srinivas Tida, V., Hsu, S., 2022. Universal Spam Detection using Transfer Learning of BERT Model. ArXiv E-prints, arXiv:2202.03480.  \n[102] Su, H.F.H., Ricci, F.A., Mnatsakanian, M., 2016. Mathematical Teaching Strategies: Pathways to Critical Thinking and Metacognition. International Journal of Research in Education and Science 2, 190–200.  \n[103] Sun, J., Gan, W., Chao, H.C., Yu, P.S., Ding, W., 2023. Internet of Behaviors: A Survey. IEEE Internet of Things Journal 10, 11117-11134.  \n[104] Tan, M., Le, Q., 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, in: The 36th International Conference on Machine Learning, PMLR. pp. 6105-6114.  \n[105] Tang, Y., Liang, J., Hare, R., Wang, F.Y., 2020. A Personalized Learning System for Parallel Intelligent Education. IEEE Transactions on Computational Social Systems 7, 352-361.  \n[106] Tao, S., Qiu, R., Ping, Y., Ma, H., 2021. Multi-modal Knowledge-aware Reinforcement Learning Network for Explainable Recommendation. Knowledge-Based Systems 227, 107217.  \n[107] Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., Ting, D.S.W., 2023. Large language models in medicine. Nature Medicine 29, 1930-1940.  \n[108] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.T., Jin, A., Bos, T., Baker, L., Du, Y., et al., 2022. Language Models for Dialog Applications. arXiv preprint, arXiv:2201.08239.  \n[109] Tirumala, K., Markosyan, A., Zettlemoyer, L., Aghajanyan, A., 2022. Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models. Advances in Neural Information Processing Systems 35, 38274-38290.  \n[110] Valverde Valencia, Å., 2023. An Interdisciplinary and Applied Approach to Generative Artificial Intelligence in Secondary School for the Development of Communicative Competencies.  \n[111] Wang, C.X., Di Renzo, M., Stanczak, S., Wang, S., Larsson, E.G., 2020a. Artificial Intelligence Enabled Wireless Networking for 5G and Beyond: Recent Advances and Future Challenge. IEEE Wireless Communications 27, 16-23.  \n[112] Wang, D., Weisz, J.D., Muller, M., Ram, P., Geyer, W., Dugan, C., Tausczik, Y., Samulowitz, H., Gray, A., 2019. Human-AI Collaboration in Data Science: Exploring Data Scientists' Perceptions of Automated AI. The ACM on Human-Computer Interaction 3, 1–24.  \n[113] Wang, H., Yeung, D.Y., 2020. A Survey on Bayesian Deep Learning. ACM Computing Survey 53, 1-37.  \n[114] Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., Zhou, M., 2020b. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. Advances in Neural Information Processing Systems 33, 5776–5788.  \n[115] Wang, Y., Chu, Z., Ouyang, X., Wang, S., Hao, H., Shen, Y., Gu, J., Xue, S., Zhang, J.Y., Cui, Q., et al., 2023. Enhancing Recommender Systems with Large Language Model Reasoning Graphs. ArXiv E-prints, arXiv:2308.10835.  \n[116] Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V., 2021. Finetuned Language Models Are Zero-Shot Learners. ArXiv E-prints, arXiv:2109.01652.  \n[117] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al., 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 24824-24837.  \n[118] Williamson, B., Macgilchrist, F., Potter, J., 2023. Re-examining AI, Automation and Datafication in Education. Learning, Media and Technology 48, 1-5.\n\n[119] Wu, J., Gan, W., Chen, Z., Wan, S., Lin, H., 2023a. AI-Generated Content (AIGC): A Survey. arXiv preprint arXiv:2304.06632.  \n[120] Wu, J., Gan, W., Chen, Z., Wan, S., Yu, P.S., 2023b. Multimodal Large Language Models: A Survey, in: IEEE International Conference on Big Data, IEEE. pp. 2247-2256.  \n[121] Wu, T., Zhu, B., Zhang, R., Wen, Z., Ramchandran, K., Jiao, J., 2023c. Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. arXiv preprint arXiv:2310.00212.  \n[122] Xie, H., Qin, Z., Li, G. Y., Juang, B. H., 2021. Deep Learning Enabled Semantic Communication Systems. IEEE Transactions on Signal Processing 69, 2663-2675.  \n[123] Xu, H., 2023. No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. ArXiv E-prints, arXiv:2309.03224.  \n[124] Xu, L., Li, A., Zhu, L., Xue, H., Zhu, C., Zhao, K., He, H., Zhang, X., Kang, Q., Lan, Z., 2023. SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. ArXiv E-prints, arXiv:2307.15020.  \n[125] Yan, K., Cai, J., Jin, D., Miao, S., Guo, D., Harrison, A.P., Tang, Y., Xiao, J., Lu, J., Lu, L., 2022. Self-Supervised Learning of Pixel-Wise Anatomical Embeddings in Radiological Images. IEEE Transactions on Medical Imaging 41, 2658-2669.  \n[126] Yan, L., Sha, L., Zhao, L., Li, Y., Martinez-Maldonado, R., Chen, G., Li, X., Jin, Y., Gašević, D., 2024. Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review. British Journal of Educational Technology 55, 90-112.  \n[127] Yang, R., Li, L., Gan, W., Chen, Z., Qi, Z., 2023. The Human-centric Metaverse: A Survey, in: Companion Proceedings of the ACM Web Conference, pp. 1296-1306.  \n[128] Yang, W., Li, H., 2019. Changing Culture, Changing Curriculum: A Case Study of Early Childhood Curriculum Innovations in Two Chinese Kindergartens. The Curriculum Journal 30, 279–297.  \n[129] Yu, Z., Wu, Y., Zhang, N., Wang, C., Vorobeychik, Y., Xiao, C., 2023. CodeIPPrompt: Intellectual Property Infringement Assessment of Code Language Models, in: International Conference on Machine Learning, PMLR. pp. 40373-40389.  \n[130] Zamfirescu-Pereira, J., Wong, R.Y., Hartmann, B., Yang, Q., 2023. Why Johnny Can't Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts, in: CHI Conference on Human Factors in Computing Systems, Curran Associates, Inc.. pp. 1-21.  \n[131] Zeng, F., Gan, W., Wang, Y., Liu, N., Yu, P.S., 2023a. Large Language Models for Robotics: A Survey. arXiv preprint arXiv:2311.07226.  \n[132] Zeng, F., Gan, W., Wang, Y., Yu, P.S., 2023b. Distributed Training of Large Language Models, in: IEEE 29th International Conference on Parallel and Distributed Systems, IEEE. pp. 840-847.  \n[133] Zeng, H., 2023. Measuring Massive Multitask Chinese Understanding. ArXiv E-prints, arXiv:2304.12986.  \n[134] Zeng, Y., Mahmud, T., 2023. ChatGPT in English Class: Perspectives of Students and Teachers from Swedish Upper Secondary Schools.  \n[135] Zhang, C., Dai, Q., Du, Z., Gan, W., Weng, J., Yu, P.S., 2023a. TUSQ: Targeted High-Utility Sequence Querying. IEEE Transactions on Big Data 9, 512–527.  \n[136] Zhang, C., Zhang, C., Zheng, S., Qiao, Y., Li, C., Zhang, M., Dam, S.K., Thwal, C.M., Tun, Y.L., Huy, L.L., et al., 2023b. A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need? ArXiv E-prints, arXiv:2303.11717.  \n[137] Zhang, M., Li, J., 2021. A Commentary of GPT-3 in MIT Technology Review. Fundamental Research 1, 831–833.  \n[138] Zhao, L., 2022. A Study on Data-Driven Teaching Decision Optimization of Distance Education Platforms. International Journal of Emerging Technologies in Learning 17.  \n[139] Zhao, S., Blaabjerg, F., Wang, H., 2020. An Overview of Artificial Intelligence Applications for Power Electronics. IEEE Transactions on Power Electronics 36, 4633-4658.  \n[140] Zheng, R., Dou, S., Gao, S., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Xiong, L., Chen, L., et al., 2023. Secrets of RLHF in Large\n\nLanguage Models Part I: PPO. ArXiv E-prints, arXiv:2307.04964.  \n[141] Zhipeng, G., Yi, X., Sun, M., Li, W., Yang, C., Liang, J., Chen, H., Zhang, Y., Li, R., 2019. Jiuge: A Human-Machine Collaborative Chinese Classical Poetry Generation System, 25-30.  \n[142] Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., Duan, N., 2023. AGIEval: A Human-centric Benchmark for Evaluating Foundation Models. ArXiv E-prints, arXiv:2304.06364.  \n[143] Zou, L., Zhang, S., Cai, H., Ma, D., Cheng, S., Wang, S., Shi, D., Cheng, Z., Yin, D., 2021. Pre-Trained Language Model Based Ranking in Baidu Search, in: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ACM. pp. 4014-4022.",
    "arxiv_id": "2405.13001",
    "error_message": null,
    "embedding": [
      0.671875,
      -2,
      -1.1484375,
      -2.703125,
      -0.2734375,
      1.1484375,
      -1.9296875,
      -2.484375,
      -0.828125,
      2.390625,
      2.4375,
      2.421875,
      2.46875,
      2.546875,
      0.76953125,
      3.328125,
      -1.765625,
      -0.490234375,
      1.765625,
      -5.78125,
      -0.1318359375,
      0.9921875,
      0.578125,
      -4.90625,
      4.5625,
      -7.1875,
      -3.234375,
      4.21875,
      1.9609375,
      0.5625,
      8.125,
      -3.953125,
      0.173828125,
      0.49609375,
      -1.09375,
      0.26953125,
      -2.5,
      -1.15625,
      4.21875,
      3.453125,
      -7.15625,
      1.8125,
      0.197265625,
      5.15625,
      -0.11962890625,
      3.109375,
      0.86328125,
      0.59765625,
      -6.78125,
      -1.671875,
      -5,
      -1.6484375,
      7.53125,
      0.15234375,
      3.203125,
      -3.1875,
      -5.6875,
      5.28125,
      -3.9375,
      -0.384765625,
      1.8671875,
      0.30078125,
      0.447265625,
      -1.125,
      5.5,
      3.046875,
      0.53515625,
      0.73828125,
      -2.109375,
      2.09375,
      0.96484375,
      2.546875,
      5.5625,
      -3.859375,
      7.78125,
      6.78125,
      3.484375,
      2.546875,
      -2.34375,
      3.59375,
      -5.875,
      5.4375,
      5.15625,
      -1.359375,
      5.09375,
      2.984375,
      1.328125,
      0.091796875,
      -2.734375,
      1.359375,
      -0.39453125,
      0.60546875,
      -4.8125,
      0.357421875,
      -3.5625,
      4.6875,
      -1.28125,
      -3.1875,
      -5.84375,
      0.61328125,
      -2.203125,
      -0.859375,
      0.3203125,
      -7,
      -4.53125,
      -3.78125,
      -3.671875,
      -6.96875,
      0.06591796875,
      -1.875,
      -0.1884765625,
      3.140625,
      1.25,
      -2.390625,
      4.8125,
      -0.0281982421875,
      1.0859375,
      -3.234375,
      -5.46875,
      -1.1796875,
      1.1484375,
      -0.224609375,
      -1.6796875,
      0.9921875,
      2.5,
      1.6875,
      -4.25,
      3.28125,
      6.53125,
      -3.4375,
      4.65625,
      -0.052490234375,
      5.125,
      -2.390625,
      -8.9375,
      -1.6796875,
      -5.125,
      3.46875,
      1.1875,
      3.9375,
      -5.53125,
      -1.03125,
      -2.484375,
      -7.875,
      2.59375,
      1.6875,
      -7.375,
      -1.2890625,
      3.625,
      -4.15625,
      0.380859375,
      1.0703125,
      0.96484375,
      7.9375,
      0.171875,
      -2.84375,
      2.296875,
      1.7265625,
      0.51171875,
      -2.03125,
      -0.0166015625,
      2.703125,
      0.099609375,
      0.6796875,
      -0.0152587890625,
      -0.8046875,
      -5.3125,
      1.1171875,
      0.80859375,
      -1.0703125,
      0.96484375,
      15.125,
      1.296875,
      -1.890625,
      1.65625,
      0.70703125,
      -1.25,
      9.625,
      2.265625,
      0.427734375,
      0.89453125,
      2.0625,
      -3.234375,
      3.78125,
      -1.3828125,
      1.5078125,
      3.0625,
      -3.90625,
      2.0625,
      -2.0625,
      0.466796875,
      4.46875,
      4.53125,
      1.1796875,
      -4.5625,
      0.828125,
      4.5625,
      -1.234375,
      0.006561279296875,
      1.5703125,
      -1.1640625,
      -10.4375,
      -0.71875,
      -1.90625,
      -5.8125,
      -2.203125,
      1.1953125,
      -4.46875,
      1.1875,
      -1.2734375,
      -1.015625,
      2.125,
      2.546875,
      -0.3125,
      7.34375,
      2.359375,
      2.25,
      -2.53125,
      4.59375,
      1.0078125,
      4.28125,
      4.875,
      2.625,
      0.40234375,
      -0.1796875,
      2.109375,
      2.9375,
      3.015625,
      2.046875,
      8.125,
      -0.25390625,
      4.15625,
      4.6875,
      -4.40625,
      -3.4375,
      -0.93359375,
      -4.25,
      -0.54296875,
      -2.453125,
      1.7109375,
      -3.5,
      -4.03125,
      0.1982421875,
      2.953125,
      1.3984375,
      -0.83203125,
      -0.5234375,
      -2.046875,
      0.462890625,
      -8.3125,
      0.2353515625,
      4.25,
      -9.8125,
      -3.5625,
      4.65625,
      6.875,
      0.87890625,
      -0.26953125,
      0.78515625,
      -4.96875,
      3.125,
      -2.1875,
      -5.3125,
      1.890625,
      0.79296875,
      -4.0625,
      4.15625,
      0.6015625,
      1.671875,
      3.140625,
      1.2265625,
      0.9921875,
      -4.4375,
      1.65625,
      -3.484375,
      4.65625,
      2.171875,
      -3.828125,
      0.93359375,
      -2.296875,
      -6.9375,
      -9.4375,
      4.03125,
      -4,
      2.46875,
      -0.78515625,
      -0.72265625,
      6.15625,
      -2.21875,
      12.625,
      1.5,
      1.703125,
      0.734375,
      0.0172119140625,
      -1.90625,
      1.1328125,
      -1.484375,
      0.1708984375,
      -4.75,
      1.390625,
      4.3125,
      -0.2451171875,
      -1.7265625,
      -2.75,
      -0.1435546875,
      2.5,
      -0.984375,
      -4.28125,
      1.328125,
      4.53125,
      0.298828125,
      1.46875,
      5.28125,
      -3.765625,
      3.484375,
      -4.46875,
      -2.9375,
      3.46875,
      2.03125,
      -2.421875,
      -7.375,
      -3.828125,
      -2.21875,
      -2.640625,
      -2.640625,
      -4.46875,
      1.484375,
      -0.0869140625,
      5.625,
      2.140625,
      1.5234375,
      0.625,
      -6.4375,
      -8.25,
      6.84375,
      -3.703125,
      3.34375,
      1.546875,
      -0.76171875,
      2.640625,
      -0.36328125,
      -1.671875,
      1.3359375,
      -3.75,
      -2.078125,
      -0.51953125,
      4.90625,
      -1.5703125,
      3.21875,
      -6.40625,
      4.75,
      2.890625,
      1.40625,
      4.21875,
      7.40625,
      -1.390625,
      2.3125,
      -1.84375,
      -1.7265625,
      1.2421875,
      0.482421875,
      -3.25,
      7.59375,
      0.2060546875,
      -2.609375,
      -3.875,
      -2.875,
      2.625,
      -0.85546875,
      -1.25,
      -2.234375,
      -2.359375,
      0.8984375,
      3.3125,
      0.703125,
      1.6875,
      1.7890625,
      -3.28125,
      -4.1875,
      2.25,
      -1.484375,
      1.578125,
      -0.2265625,
      3.484375,
      4.25,
      1.4296875,
      -0.87890625,
      5.09375,
      1.40625,
      -3.484375,
      -2.046875,
      0.87109375,
      -3.609375,
      1.4765625,
      4.1875,
      1.09375,
      -2.171875,
      2.203125,
      -0.58984375,
      -3.6875,
      4.59375,
      -1.3828125,
      -0.3359375,
      -1.703125,
      -1.8984375,
      -1.4140625,
      2.28125,
      -7.25,
      1.4609375,
      -0.384765625,
      -1.2421875,
      1.0859375,
      -1.046875,
      2.359375,
      -0.6796875,
      4.40625,
      -2.765625,
      1.875,
      -7.53125,
      -0.765625,
      -3.515625,
      -2.078125,
      1.6484375,
      -1.765625,
      1.734375,
      -0.04931640625,
      0.32421875,
      4.6875,
      2.1875,
      0.154296875,
      -0.0888671875,
      1.9296875,
      -3.984375,
      1.1640625,
      -0.703125,
      -5.90625,
      1.1640625,
      -5.125,
      -4.8125,
      2.6875,
      3.359375,
      -0.1376953125,
      6.65625,
      4.65625,
      -2.546875,
      -3.34375,
      2.046875,
      2.625,
      -4.5625,
      -2.375,
      1.6328125,
      -1.453125,
      0.6484375,
      -0.65625,
      2.96875,
      0.353515625,
      -1.828125,
      2.84375,
      0.94140625,
      0.0146484375,
      0.8359375,
      -1.28125,
      0.515625,
      1.4375,
      1.5390625,
      1.1640625,
      -2.671875,
      6.25,
      6.65625,
      -6.65625,
      -8.4375,
      2.609375,
      0.6875,
      -1.7421875,
      -1.390625,
      3.328125,
      1.5546875,
      0.486328125,
      -5.5,
      -4.96875,
      -1.53125,
      -0.83984375,
      4.625,
      3.0625,
      0.283203125,
      -3.390625,
      -3.8125,
      4.53125,
      -2.390625,
      4.15625,
      0.271484375,
      -1.4765625,
      0.62890625,
      -4.03125,
      4.90625,
      0.765625,
      5.71875,
      -4.6875,
      -0.41015625,
      3.5,
      -9.375,
      -0.43359375,
      -3.171875,
      -0.1318359375,
      -0.5859375,
      -0.455078125,
      5.125,
      -3.640625,
      0.1533203125,
      0.79296875,
      5.96875,
      -2.390625,
      -0.93359375,
      2.5,
      -7.09375,
      -2.34375,
      1.0703125,
      2.4375,
      2.53125,
      -1.9375,
      -4.84375,
      0.353515625,
      -0.212890625,
      -0.5234375,
      -2.25,
      -1.0390625,
      -3.25,
      -1.9296875,
      1.6015625,
      3.671875,
      2.234375,
      0.2060546875,
      -0.8515625,
      -3.703125,
      -2.578125,
      -0.98828125,
      1.5234375,
      -0.357421875,
      1.234375,
      0.333984375,
      2.6875,
      3.421875,
      -2.546875,
      -1.484375,
      -1.28125,
      3.203125,
      -2.875,
      1.6953125,
      0.55078125,
      1.921875,
      3.375,
      2.078125,
      -1.3515625,
      -1.59375,
      0.6875,
      -1.8515625,
      -4.65625,
      -1.453125,
      4.6875,
      -0.78515625,
      0.00689697265625,
      -4.90625,
      0.76953125,
      0.71484375,
      1.8984375,
      -0.70703125,
      0.10595703125,
      6.21875,
      1.28125,
      0.255859375,
      2.203125,
      5.8125,
      -2.34375,
      0.68359375,
      0.57421875,
      1.171875,
      -7.21875,
      -7.4375,
      -3.734375,
      3.578125,
      8.4375,
      -0.1181640625,
      5.15625,
      -5.03125,
      4.84375,
      0.2890625,
      0.2412109375,
      -14.6875,
      2.515625,
      0.8203125,
      -5.71875,
      -2.625,
      0.111328125,
      4.71875,
      -0.91015625,
      4.03125,
      -0.91015625,
      -0.99609375,
      1.3828125,
      3.96875,
      -1.0078125,
      -2.234375,
      6.09375,
      3.828125,
      -3.59375,
      2.46875,
      -1.8046875,
      -4.9375,
      0.30078125,
      0.09619140625,
      2.671875,
      4.09375,
      6.875,
      -0.392578125,
      1.1640625,
      4.25,
      -4.8125,
      -0.404296875,
      1.4921875,
      0.8125,
      -2.90625,
      1.6484375,
      -5.65625,
      1.3046875,
      -3.78125,
      1.0859375,
      0.053466796875,
      -3.046875,
      0.1689453125,
      4.15625,
      -1.921875,
      -1.7734375,
      -1.3515625,
      1.328125,
      3.140625,
      5.625,
      -6.5625,
      0.91015625,
      0.4921875,
      0.255859375,
      2.609375,
      -2.203125,
      -0.75390625,
      -3.203125,
      3.03125,
      1.03125,
      -2.703125,
      5.03125,
      -0.734375,
      -5.375,
      3.125,
      0.60546875,
      -1.171875,
      -0.0625,
      3.09375,
      0.265625,
      0.09521484375,
      -0.390625,
      2.203125,
      1.1328125,
      -3.9375,
      -2.890625,
      -0.76171875,
      -2.34375,
      3.890625,
      -0.9609375,
      -2.9375,
      -3.28125,
      -4.0625,
      1.2890625,
      1.8828125,
      3.078125,
      1.125,
      2.25,
      3.203125,
      -4.5,
      0.5,
      -4.71875,
      -3.90625,
      1.2734375,
      -3.59375,
      -2.015625,
      0.9453125,
      1.0390625,
      2.171875,
      -1.1640625,
      -1.53125,
      -3.75,
      -4.40625,
      -2.1875,
      1.2109375,
      1.3359375,
      2.234375,
      -1.109375,
      6.125,
      -0.62890625,
      -2.796875,
      0.12255859375,
      -1.40625,
      -0.146484375,
      -1.015625,
      1.3671875,
      -5.625,
      -0.66015625,
      5.875,
      1.375,
      -3.328125,
      6.25,
      -0.58984375,
      -0.9921875,
      -1.7578125,
      2.703125,
      -0.5078125,
      -0.474609375,
      0.8046875,
      -3.90625,
      -2.765625,
      -3.8125,
      -0.032470703125,
      -1.3203125,
      3.28125,
      0.56640625,
      0.70703125,
      -0.96484375,
      -7.875,
      2.84375,
      0.2236328125,
      -0.349609375,
      2.375,
      5.84375,
      -0.65625,
      -1.2421875,
      2.515625,
      0.74609375,
      -0.79296875,
      -0.8671875,
      -4.0625,
      4.375,
      2.328125,
      3.765625,
      -3.96875,
      -6.3125,
      -0.3671875,
      -1.09375,
      -1.6640625,
      4.625,
      -0.6875,
      -0.392578125,
      5.34375,
      2.484375,
      -4.90625,
      -2.125,
      5.4375,
      -3.796875,
      0.984375,
      2.34375,
      2.828125,
      0.408203125,
      4.21875,
      -1.6953125,
      -2.84375,
      -0.921875,
      5.75,
      2.890625,
      -1.109375,
      -0.76171875,
      3.859375,
      0.2421875,
      4.46875,
      -6,
      -5.15625,
      0.90625,
      0.90625,
      -1.984375,
      0.021484375,
      6.4375,
      -1.84375,
      0.181640625,
      0.470703125,
      -0.1884765625,
      3.375,
      1.3828125,
      1.0390625,
      0.1123046875,
      1.703125,
      1.484375,
      0.84765625,
      1.9453125,
      -1.1953125,
      -0.330078125,
      -1.1640625,
      0.408203125,
      -1.9296875,
      3.5625,
      -3.78125,
      0.6328125,
      -1.609375,
      3.765625,
      -0.265625,
      5.75,
      5.09375,
      0.07177734375,
      0.54296875,
      -0.84375,
      6.875,
      -1.6015625,
      6.53125,
      1.3671875,
      8.5,
      -2.359375,
      -3.625,
      1.4296875,
      0.1787109375,
      -0.98046875,
      0.87890625,
      -6.9375,
      -1.9296875,
      -1.375,
      -0.11474609375,
      -0.82421875,
      2.796875,
      -5.15625,
      -0.8046875,
      3.265625,
      3.671875,
      3.890625,
      2.90625,
      4.84375,
      0.9296875,
      0.80859375,
      -3.15625,
      -1.828125,
      0.466796875,
      0.5390625,
      3.578125,
      -4.03125,
      0.984375,
      1.546875,
      5.40625,
      -0.055908203125,
      -3.140625,
      -3.234375,
      7.375,
      2.15625,
      -2.296875,
      1.8671875,
      -1.6015625,
      0.94921875,
      0.984375,
      3.875,
      -5.625,
      -3.0625,
      6.75,
      5.03125,
      -0.828125,
      3.265625,
      0.09033203125,
      2.171875,
      -3.46875,
      -0.9296875,
      0.03173828125,
      -0.054931640625,
      -4.15625,
      -3,
      0.62890625,
      3.96875,
      -4.78125,
      1.9765625,
      0.96875,
      -2.75,
      2.328125,
      1.0625,
      -2.234375,
      1.6640625,
      0.2314453125,
      6.75,
      0.5234375,
      -5.8125,
      -4.59375,
      -7.34375,
      -3.96875,
      0.208984375,
      2.125,
      2.796875,
      2.15625,
      -3.375,
      2.65625,
      -5.1875,
      -1.1171875,
      -4.78125,
      3.6875,
      3.59375,
      -4.75,
      -2.5,
      -2,
      -7.875,
      -3.8125,
      -0.94921875,
      -1.34375,
      -2.609375,
      -5.625,
      -2.015625,
      2.453125,
      -5.03125,
      -1,
      -2.03125,
      -1.25,
      4.28125,
      -0.296875,
      0.74609375,
      -4.5625,
      4.875,
      1.71875,
      -2.296875,
      5.3125,
      1.953125,
      2.6875,
      2.15625,
      0.1494140625,
      -0.11669921875,
      -4.71875,
      4.09375,
      -1.5390625,
      9.0625,
      4.09375,
      1.875,
      -4.65625,
      4.3125,
      1.84375,
      1.4921875,
      -4.46875,
      -0.76953125,
      -2.5,
      2.234375,
      1.671875,
      -1.203125,
      2.34375,
      -2.578125,
      -1.5,
      0.859375,
      -2.390625,
      -0.46484375,
      3.25,
      -0.01519775390625,
      4.375,
      1.6171875,
      0.482421875,
      0.1728515625,
      0.2216796875,
      1.6875,
      -0.51953125,
      5.8125,
      -5.125,
      2.359375,
      -5.3125,
      -0.96875,
      0.78125,
      -1.1796875,
      -0.69140625,
      -0.41015625,
      -0.30078125,
      1.765625,
      2.828125,
      1.6953125,
      1.1015625,
      4.78125,
      -2.765625,
      -0.09423828125,
      0.72265625,
      -2.421875,
      4.9375,
      -0.0634765625,
      -1.0234375,
      -0.8359375,
      -0.7890625,
      -0.310546875,
      1.28125,
      0.85546875,
      -7.90625,
      -1.3828125,
      -1.296875,
      -4.1875,
      7.09375,
      0.921875,
      -0.73828125,
      1.4453125,
      4.0625,
      6.21875,
      3.765625,
      -0.92578125,
      -1.109375,
      -0.25,
      -2.265625,
      3.875,
      -2.15625,
      -0.65234375,
      4.34375,
      3.84375,
      3.546875,
      -0.9453125,
      -0.62109375,
      -2.796875,
      0.1015625,
      4.90625,
      -1.765625,
      -1.0546875,
      6.6875,
      -2.90625,
      0.12353515625,
      -2.015625,
      -2.59375,
      -1.9296875,
      -1.6328125,
      2.671875,
      2.25,
      2.4375,
      -2.046875,
      -2.53125,
      -7.71875,
      1.9453125,
      -0.1533203125,
      -0.984375,
      -1.171875,
      1.6875,
      -0.2138671875,
      1.4921875,
      -2.078125,
      -2.0625,
      -0.302734375,
      -3.859375,
      -0.0361328125,
      2.765625,
      0.0966796875,
      1.3828125,
      -0.341796875,
      2.84375,
      -2.140625,
      2.484375,
      3.828125,
      -5.625,
      2.140625,
      0.90234375,
      -1.71875,
      3.6875,
      1.9765625,
      1.4140625,
      2.734375,
      -1.2578125,
      0.78125,
      -0.44921875,
      -1.734375,
      1.0625,
      -3,
      0.5859375,
      1.8984375,
      3.375,
      -2.34375,
      3.09375,
      0.8515625,
      -4.34375,
      3.6875,
      -3.59375,
      -7.15625,
      -1.046875,
      0.033935546875,
      -0.8828125,
      -1.4140625,
      -2.3125,
      1.2265625,
      2.671875,
      -7.375,
      5.6875,
      -1.5390625,
      5.4375,
      2.734375,
      -1.0625,
      -2.8125,
      0.69140625,
      2.234375,
      -1.5703125,
      2.28125,
      1.6484375,
      -1.765625,
      3.984375,
      -3.34375,
      1.3984375,
      -0.95703125,
      0.55859375,
      3.21875,
      -1.0703125,
      0.86328125,
      -0.134765625,
      1.0546875,
      -9.5625,
      -3.4375,
      -1.9765625,
      -0.353515625,
      1.9375,
      -0.6015625,
      2.3125,
      -2.859375,
      2.53125,
      -4.03125,
      2.984375,
      -5.1875,
      -3.171875,
      1.7109375,
      -1.9453125,
      4.53125,
      2.109375,
      -2.3125,
      0.205078125,
      0.359375,
      -4.46875,
      -2.140625,
      -2.6875,
      0.02392578125,
      -3.09375,
      -1.9296875,
      -0.35546875,
      2.84375,
      -0.2001953125,
      1.84375,
      -0.875,
      3.140625,
      4.09375,
      -0.59375,
      3.015625,
      0.5078125,
      0.54296875,
      -3.125,
      -4.28125,
      -4.53125,
      2.078125,
      3.71875,
      -3.203125,
      2.46875,
      -0.96875,
      2,
      2,
      -0.83984375,
      -0.15625,
      1.1328125,
      1.859375,
      -0.71484375,
      -2.03125,
      2.5,
      -1.421875,
      1.890625,
      -2.203125,
      -4.1875,
      -4.1875,
      -0.1484375,
      0.953125,
      0.220703125,
      0.9921875,
      -1.046875,
      -1.7421875,
      -1.1875,
      -4.25,
      -2.6875,
      4.875,
      0.58984375,
      5.25,
      0.60546875,
      1.171875,
      -3.078125,
      -1.1953125,
      -1.375,
      6,
      0.7578125,
      -2.75,
      3.953125,
      -2.15625,
      5.4375,
      -2.09375,
      -1.78125,
      -9.1875,
      -1.90625,
      7.125,
      2.109375,
      -0.61328125,
      1.2578125,
      0.337890625,
      2.359375,
      -0.026611328125,
      5.40625,
      -3.140625,
      -1.5546875,
      -0.296875,
      3.578125,
      1.765625,
      -5.53125,
      0.62109375,
      1.9140625,
      -1.5078125,
      1.6015625,
      5.6875,
      5.78125,
      -2.140625,
      3.421875,
      -4,
      -2.859375,
      -4.78125,
      4.15625,
      -0.7109375,
      -0.765625,
      -1.71875,
      -3.1875,
      -0.271484375,
      -1.4921875,
      0.26953125,
      7.28125,
      -4.4375,
      2.609375,
      -0.8984375,
      4.09375,
      0.90234375,
      -0.98046875,
      -0.84375,
      2.859375,
      2.671875,
      -1.03125,
      -1.1640625,
      -5.9375,
      -1.875,
      -0.59765625,
      -7.34375,
      1.9765625,
      1.734375,
      0.99609375,
      2.265625,
      -1.84375,
      2.25,
      3.15625,
      -2.515625,
      -0.80859375,
      -2.140625,
      2.8125,
      4.6875,
      1.5,
      -2.859375,
      1.2265625,
      -0.9765625,
      -0.33203125,
      -5.09375,
      3.09375,
      -1.828125,
      4.28125,
      3,
      3.078125,
      -4.125,
      -4.28125,
      -3.234375,
      -1.6640625,
      -3.78125,
      1.15625,
      0.064453125,
      1.6171875,
      3.734375,
      -0.080078125,
      -3.546875,
      -1.3828125,
      -3.90625,
      1.515625,
      -0.5625,
      1.5078125,
      -6.8125,
      2.46875,
      -6.40625,
      0.6640625,
      1.3359375,
      -3.140625,
      0.2412109375,
      3.453125,
      1.9140625,
      2.265625,
      3.765625,
      -1.5703125,
      -3.484375,
      3.5,
      -3.921875,
      -2,
      -5,
      0.275390625,
      -0.369140625,
      -4.21875,
      3.203125,
      -2.890625,
      2.265625,
      -2.4375,
      -2.96875,
      0.341796875,
      -1.09375,
      -0.5390625,
      3.515625,
      2.125,
      -2.34375,
      1.7890625,
      -1.0390625,
      5.84375,
      2.640625,
      -2.40625,
      1.6875,
      -0.33984375,
      1.7109375,
      -6.09375,
      4.25,
      0.68359375,
      -1.4375,
      3.296875,
      5.125,
      -3.234375,
      1,
      2.234375,
      5.75,
      -2.703125,
      -1.3515625,
      0.4140625,
      4.71875,
      -3.96875,
      -0.002166748046875,
      -4.09375,
      0.01531982421875,
      1.53125,
      -1.6953125,
      5.03125,
      -1.734375,
      3.8125,
      -5.0625,
      0.33203125,
      1.296875,
      1.328125,
      2.9375,
      -5.25,
      3.890625,
      2.375,
      -1.3984375,
      5.09375,
      -0.75,
      -5.46875,
      -2.28125,
      2.171875,
      -4.625,
      -3.03125,
      -5.09375,
      0.0400390625,
      -2.40625,
      -1.6796875,
      -0.7109375,
      0.37109375,
      1.09375,
      2.515625,
      -1.671875,
      -1.5546875,
      -3.109375,
      -4.375,
      1.171875,
      -2.015625,
      -6.6875,
      -0.400390625,
      2.15625,
      3.296875,
      5.53125,
      7.625,
      3.09375,
      -4.34375,
      2.640625,
      -1.515625,
      1.2265625,
      -0.09912109375,
      -0.1328125,
      -0.54296875,
      -2.921875,
      2.65625,
      0.828125,
      3.484375,
      -6.90625,
      -0.7890625,
      -4.71875,
      -1.15625,
      -0.212890625,
      0.193359375,
      -0.1796875,
      2.390625,
      4.875,
      -2.4375,
      3.84375,
      1.5234375,
      -0.1220703125,
      -5.1875,
      2.421875,
      -1.4921875,
      2.640625,
      4.5,
      0.62890625,
      -4.96875,
      6.65625,
      2.3125,
      -2.703125,
      3.015625,
      0.006561279296875,
      -3.546875,
      -3.34375,
      1.3046875,
      0.515625,
      4.0625,
      -1.078125,
      0.86328125,
      2.734375,
      -0.6015625,
      3.84375,
      2.625,
      0.73828125,
      0.259765625,
      4.46875,
      0.8203125,
      -4.9375,
      -1.3046875,
      2.421875,
      -3.296875,
      -3.984375,
      2.15625,
      -0.640625,
      2.65625,
      -0.1416015625,
      2.078125,
      1.7109375,
      6.84375,
      -4.5,
      3.046875,
      -6.125,
      0.14453125,
      -7.09375,
      2.3125,
      -8,
      2.0625,
      0.8515625,
      6.1875,
      -0.04150390625,
      -4.40625,
      3.3125,
      5.53125,
      1.90625,
      -4.5625,
      2.640625,
      2.515625,
      -0.875,
      -5.78125,
      -2.734375,
      4.3125,
      0.87890625,
      3.0625,
      -4.8125,
      1.34375,
      3.703125,
      -1.6796875,
      -5.34375,
      0.515625,
      -0.2265625,
      -6.46875,
      -5.71875,
      -2.03125,
      -0.515625,
      -1.3671875,
      2,
      4.34375,
      -1.921875,
      2.109375,
      0.47265625,
      4.1875,
      2.578125,
      -1.3046875,
      2.984375,
      0.291015625,
      -0.255859375,
      2.21875,
      3.890625,
      -2.53125,
      1.5625,
      -1.015625,
      1.09375,
      3.0625,
      -1.8671875,
      -0.24609375,
      -6.8125,
      1.71875,
      4.34375,
      2.03125,
      0.10595703125,
      -3.8125,
      -3.015625,
      2.046875,
      -1.390625,
      -3.171875,
      1.078125,
      -1.7109375,
      -0.32421875,
      1.1328125,
      -1.8828125,
      3.828125,
      0.76171875,
      -0.5078125,
      -2.09375,
      -0.69921875,
      -2.015625,
      -1.5546875,
      -3.71875,
      3.15625,
      -0.62109375,
      4.34375,
      -0.5703125,
      -4.21875,
      -2.03125,
      3.671875,
      1.1328125,
      2.375,
      -1.8203125,
      -1.2890625,
      3.1875,
      3.6875,
      1.3359375,
      -1.328125,
      0.60546875,
      1.265625,
      0.302734375,
      -3.265625,
      -1.625,
      -1.8671875,
      3.21875,
      -1.8671875,
      -1.34375,
      -2.46875,
      1.9765625,
      -1.265625,
      -8.0625,
      0.890625,
      -0.0361328125,
      -0.69921875,
      -5,
      0.396484375,
      2.296875,
      2.078125,
      4.75,
      3.234375,
      2.9375,
      -2.109375,
      2.84375,
      18.625,
      -2.734375,
      -4.9375,
      -1.71875,
      0.267578125,
      2.84375,
      8.5625,
      1.171875,
      0.0235595703125,
      -1.8359375,
      1.546875,
      4.375,
      2.828125,
      3.28125,
      0.6875,
      0.90625,
      2.71875,
      1.9375,
      -2.65625,
      -4.1875,
      -1.03125,
      2.34375,
      -0.99609375,
      3.078125,
      -2.640625,
      3.28125,
      3.296875,
      -1.5,
      -0.2275390625,
      -4.90625,
      1.1328125,
      -3.140625,
      -1.3203125,
      -5.1875,
      1.296875,
      -4.90625,
      1,
      2.03125,
      0.75390625,
      -2.125,
      -2.46875,
      -4.28125,
      3.140625,
      -0.96484375,
      -0.4609375,
      0.90625,
      -2.3125,
      1.4765625,
      -0.79296875,
      -0.7890625,
      -2.515625,
      -3.6875,
      -3.515625,
      2.25,
      0.64453125,
      -2.578125,
      0.8203125,
      -7.03125,
      -2.359375,
      -5.40625,
      -1.5703125,
      -2.3125,
      -3.375,
      -2.71875,
      -3.875,
      -2.421875,
      -2.3125,
      -0.296875,
      -0.1591796875,
      -1.4921875,
      -6,
      -0.220703125,
      5.03125,
      5.5,
      0.1552734375,
      -2.453125,
      2.71875,
      2.40625,
      2.203125,
      1.5390625,
      -0.5234375,
      3.328125,
      -2.25,
      -1.46875,
      -0.8828125,
      0.875,
      -5.34375,
      2.21875,
      0.232421875,
      -0.193359375,
      4.40625,
      -2.796875,
      2.28125,
      -0.6640625,
      -3.453125,
      -1.5,
      -0.29296875,
      3.40625,
      0.119140625,
      -0.78125,
      2.75,
      -0.466796875,
      -0.82421875,
      -2.078125,
      -0.474609375,
      -1.5859375,
      1.7734375,
      -1.625,
      1.828125,
      -0.796875,
      1.3046875,
      2.0625,
      0.80859375,
      -2.203125,
      -0.45703125,
      2.9375,
      -6.78125,
      -1.9296875,
      1.8984375,
      -0.7265625,
      4.3125,
      -2.109375,
      -3.9375,
      -0.287109375,
      -8.625,
      -5.53125,
      -1.9140625,
      -3.0625,
      3.515625,
      -2.203125,
      1.4609375,
      -1.03125,
      -1.890625,
      -1.359375,
      2.8125,
      -4.34375,
      3.703125,
      3.875,
      1.0703125,
      0.373046875,
      -2.828125,
      2.171875,
      1.1796875,
      -2.796875,
      -2.546875,
      0.8046875,
      -4.625,
      2.515625,
      5.53125,
      1.3203125,
      -3.671875,
      4.78125,
      -0.6953125,
      -3.34375,
      -2.4375,
      3.1875,
      -0.0908203125,
      -0.7265625,
      -1.3203125,
      -1.9375,
      0.63671875,
      0.51171875,
      -2.78125,
      -4.375,
      -1.90625,
      -1.953125,
      2.765625,
      -0.5703125,
      -7.15625,
      0.88671875,
      5.53125,
      -7,
      1.03125,
      -1.5703125,
      5.78125,
      -5.75,
      0.5859375,
      5.375,
      -0.26171875,
      0.66015625,
      2.203125,
      3.171875,
      -2.015625,
      -6.3125,
      1.8046875,
      4.46875,
      -2.703125,
      2.28125,
      2.9375,
      -1.484375,
      0.60546875,
      -4.15625,
      -4.65625,
      6.96875,
      5.875,
      -5.09375,
      0.6796875,
      -2.875,
      -1.2109375,
      -1.3359375,
      -5.1875,
      2.421875,
      1.296875,
      1.3203125,
      0.8203125,
      0.9296875,
      -3.734375,
      -0.8671875,
      -0.94921875,
      -2.546875,
      -2.15625,
      2.859375,
      -1.1875,
      4.09375,
      -6.03125,
      5.53125,
      3.5,
      0.68359375,
      -4.1875,
      3.28125,
      4.40625,
      0.1083984375,
      -4.53125,
      -1.4296875,
      -0.9765625,
      4.625,
      -6.375,
      4.28125,
      -3.0625,
      1.6015625,
      -1.125,
      2.3125,
      -1.3515625,
      3.171875,
      -2.9375,
      -6.0625,
      -3.546875,
      3.421875,
      1.0703125,
      1.953125,
      2.796875,
      -0.84375,
      1.109375,
      2.109375,
      -3.359375,
      1.7890625,
      -0.9375,
      -0.7734375,
      3.34375,
      -2.1875,
      0.13671875,
      -0.2197265625,
      2.390625,
      -2.46875,
      -2.9375,
      -1.7421875,
      -3.5,
      0.9921875,
      -4.28125,
      3.71875,
      3.015625,
      -7.1875,
      4.5625,
      -3.515625,
      -1.8828125,
      1.9921875,
      0.71875,
      -5.59375,
      6.125,
      4.15625,
      -4.46875,
      4.5,
      -0.1630859375,
      -2,
      0.5859375,
      -1.578125,
      0.90625,
      -5.3125,
      0.6328125,
      3.671875,
      -4.9375,
      1.1875,
      3.09375,
      -2.59375,
      -0.09326171875,
      -1.3203125,
      5.53125,
      1.1953125,
      3.375,
      0.7109375,
      2.578125,
      5.875,
      -4.15625,
      1.65625,
      -2.234375,
      -0.953125,
      -2.8125,
      -0.6640625,
      -1.265625,
      -2.171875,
      2.203125,
      3.296875,
      1.2578125,
      -9.875,
      -3.875,
      2.515625,
      -3.953125,
      3.6875,
      1.4453125,
      1.15625,
      -3.296875,
      -1.4921875,
      0.71875,
      2.78125,
      -2.0625,
      -2.75,
      -2.3125,
      3.21875,
      1.453125,
      -6.125,
      -0.72265625,
      0.455078125,
      -2.546875,
      -0.484375,
      -2,
      -2.03125,
      0.390625,
      5.59375,
      -6.03125,
      2.84375,
      1.6171875,
      3.296875,
      4.75,
      -1.4375,
      2.890625,
      3.953125,
      -3.03125,
      -7.09375,
      -1.3359375,
      0.40625,
      0.0118408203125,
      1.9140625,
      -2.953125,
      2.78125,
      0.35546875,
      3.5,
      -5.5,
      1.6796875,
      3.3125,
      -6.34375,
      4.6875,
      -3.484375,
      0.70703125,
      -1.5625,
      -5.875,
      -0.62109375,
      4.3125,
      0.578125,
      -0.033935546875,
      0.6640625,
      -1.578125,
      0.0037994384765625,
      2.609375,
      -0.984375,
      -0.98046875,
      -0.30859375,
      3.703125,
      -3.34375,
      1.671875,
      2.5625,
      0.5078125,
      -0.60546875,
      -3.515625,
      2.65625,
      -1.2890625,
      -2.78125,
      4.6875,
      5.15625,
      -1.1171875,
      -3.65625,
      -5.59375,
      1.5234375,
      -0.0771484375,
      -3.84375,
      0.52734375,
      -2.5,
      1.1875,
      -3.328125,
      1.671875,
      0.73046875,
      -5.96875,
      -3.046875,
      -3.859375,
      -1.234375,
      0.8046875,
      2.859375,
      2.1875,
      -2.1875,
      -1.8671875,
      3.953125,
      3.21875,
      -0.87890625,
      -5.03125,
      1.5546875,
      -1.1953125,
      -1.8984375,
      -3.5625,
      1.296875,
      -3.296875,
      -0.734375,
      1.7421875,
      -0.36328125,
      -0.427734375,
      -5.8125,
      -2.453125,
      1.1796875,
      -0.41015625,
      -0.00115203857421875,
      -2.640625,
      -0.146484375,
      1.9140625,
      4.40625,
      -4.0625,
      -1.1171875,
      0.58203125,
      1.7109375,
      -6.4375,
      -1.6015625,
      2.078125,
      -1.25,
      0.2578125,
      4.59375,
      1.3671875,
      1.9765625,
      1.1484375,
      -4.46875,
      -2.53125,
      5.25,
      3.765625,
      0.0201416015625,
      0.66015625,
      -1.8125,
      -2.484375,
      4.875,
      7.5625,
      -2.703125,
      -2.328125,
      -0.98828125,
      -1.8046875,
      -2.609375,
      -2.34375,
      0.0791015625,
      -0.21875,
      -7.46875,
      7.125,
      -1.8125,
      3.65625,
      1.5625,
      -4.71875,
      4.8125,
      -4.3125,
      -1.2890625,
      -0.53515625,
      0.173828125,
      3.703125,
      -3.59375,
      -1.328125,
      -3.265625,
      0.484375,
      0.6328125,
      -2.046875,
      0.2392578125,
      1.421875,
      4.375,
      -0.1552734375,
      -1.3828125,
      -1.3671875,
      -1.4375,
      5.6875,
      -1.9453125,
      -2.703125,
      0.040283203125,
      -3.28125,
      1.4453125,
      -1.734375,
      1.4140625,
      -5.15625,
      -0.9609375,
      -1.859375,
      2.0625,
      1.2109375,
      1.75,
      -0.76953125,
      -3.5,
      -1.9140625,
      4.28125,
      1.5,
      4.96875,
      1.875,
      3.53125,
      0.73046875,
      1.1640625,
      2.234375,
      3.234375,
      3.640625,
      -2.4375,
      0.61328125,
      -0.375,
      -0.2392578125,
      -0.78125,
      -1.59375,
      2.734375,
      1.3046875,
      4.6875,
      -0.7421875,
      -0.44140625,
      -3.59375,
      -2.296875,
      0.99609375,
      -1.296875,
      3.109375,
      3.0625,
      -2.1875,
      0.1845703125,
      -0.515625,
      2.875,
      0.9453125,
      4.28125,
      0.375,
      -1.9140625,
      -2.328125,
      1.1171875,
      1.796875,
      -0.45703125,
      0.6171875,
      -1.640625,
      1.421875,
      -0.59375,
      -2.953125,
      0.232421875,
      2.625,
      -2.65625,
      1.34375,
      -0.007110595703125,
      0.275390625,
      0.404296875,
      -1.9921875,
      -0.283203125,
      0.9609375,
      -0.73046875,
      4.65625,
      0.2392578125,
      -0.482421875,
      1.9296875,
      3.390625,
      -1.5859375,
      0.267578125,
      1.6875,
      -0.7578125,
      -1.4765625,
      -1.765625,
      2.296875,
      -2.5625,
      4.28125,
      -1.6640625,
      0.1826171875,
      -0.232421875,
      -1.46875,
      -3.6875,
      2.15625,
      -3.640625,
      -0.2470703125,
      -2.109375,
      0.98828125,
      2.859375,
      0.41796875,
      0.546875,
      1.046875,
      0.27734375,
      -1.75,
      -1.6015625,
      -1.7265625,
      -3.390625,
      -4.4375,
      -2.796875,
      0.953125,
      0.5859375,
      -0.5859375,
      0.80859375,
      0.287109375,
      2.09375,
      -0.044921875,
      -0.9140625,
      2.375,
      1.8203125,
      -2.59375,
      -1.75,
      1.1328125,
      -0.0810546875,
      1.1484375,
      -1.0546875,
      -2.375,
      -0.248046875,
      0.07861328125,
      -0.58203125,
      1.6953125,
      -0.431640625,
      -0.3515625,
      0.609375,
      1.109375,
      -1.625,
      1.0546875,
      0.259765625,
      2.34375,
      -0.6171875,
      0.1982421875,
      1.9296875,
      0.80078125,
      -1.3046875,
      3.0625,
      3.4375,
      -2.125,
      -0.66796875,
      -0.1982421875,
      1.359375,
      2.171875,
      -1.6953125,
      -0.99609375,
      1.7421875,
      -1.9296875,
      -2.734375,
      0.96875,
      2.09375,
      -2.28125,
      0.74609375,
      -2.515625,
      -0.9609375,
      -1.3359375,
      0.220703125,
      1.6796875,
      -0.85546875,
      -0.98046875,
      -1.890625,
      1.171875,
      1.96875,
      0.6015625,
      0.486328125,
      -1.0859375,
      -2.46875,
      -0.51171875,
      1.3671875,
      -1.46875,
      -1.6796875,
      0.40234375,
      -0.46875,
      2.375,
      0.96484375,
      0.79296875,
      -0.75390625,
      -1.265625,
      -0.435546875,
      -0.67578125,
      -0.73828125,
      -3.390625,
      1.3125,
      2.3125,
      1.140625,
      -1.25,
      -0.8515625,
      -3.265625,
      -1.5546875,
      -4.59375,
      2.0625,
      0.7421875,
      -2.84375,
      0.25,
      1.8125,
      1.265625,
      1.9765625,
      -0.404296875,
      1.203125,
      1.71875,
      -0.28125,
      2.578125,
      0.8046875,
      1.9375,
      -3.421875,
      -0.796875,
      0.01263427734375,
      0.314453125,
      0.609375,
      1.15625,
      -0.9453125,
      2.828125,
      1.1484375,
      -1.234375,
      1.84375,
      2.0625,
      1.1796875,
      1.96875,
      -3.390625,
      3.234375,
      2.421875,
      0.66796875,
      -1.2890625,
      -0.04248046875,
      0.419921875,
      0.037109375,
      -3.140625,
      0.65234375,
      -0.5703125,
      3.890625,
      0.2314453125,
      -0.55078125,
      -0.984375,
      0.9375,
      1.296875,
      -1.640625,
      0.7578125,
      -2.953125,
      2.234375,
      1.0390625,
      1.515625,
      -0.43359375,
      -0.51171875,
      -1.890625,
      2.6875,
      -5.1875,
      0.337890625,
      -0.84375,
      0.64453125,
      0.9609375,
      0.71484375,
      -1.3125,
      -2.96875,
      -1.4296875,
      1.9453125,
      -1.921875,
      2.03125,
      1.9375,
      -0.8671875,
      0.03662109375,
      2.28125,
      0.1494140625,
      0.2177734375,
      1.578125,
      -2.53125,
      1.078125,
      -0.921875,
      -1.9375,
      -3.15625,
      -1.875,
      -6.875,
      -2.203125,
      -1.546875,
      -1.578125,
      0.6171875,
      2.078125,
      0.94921875,
      0.23828125,
      -1.65625,
      1.7265625,
      3.734375,
      0.0018157958984375,
      -1.1640625,
      2.4375,
      -1.6875,
      0.66796875,
      -1.890625,
      -0.404296875,
      0.5859375,
      0.484375,
      3.234375,
      -0.50390625,
      3,
      0.66796875,
      -1.8359375,
      1.2265625,
      0.2431640625,
      -0.95703125,
      -1.3828125,
      0.1376953125,
      1.3125,
      2.125,
      3.765625,
      3.75,
      2.5625,
      -0.96875,
      -0.7109375,
      0.54296875,
      0.83203125,
      6.53125,
      0.875,
      -1.171875,
      -1.25,
      2.734375,
      1.09375,
      0.97265625,
      0.486328125,
      0.7578125,
      5.59375,
      0.89453125,
      0.2041015625,
      -1.75,
      -0.01300048828125,
      -1.078125,
      0.171875,
      5.46875,
      -4.6875,
      0.296875,
      -1.34375,
      -1.0703125,
      1.8203125,
      -0.2021484375,
      3.40625,
      0.1123046875,
      5.03125,
      1.7578125,
      0.4140625,
      0.99609375,
      1.046875,
      -0.96875,
      2.359375,
      1.4296875,
      -2.078125,
      -0.51171875,
      0.07421875,
      -0.58203125,
      2.28125,
      -1.421875,
      -0.96484375,
      2.0625,
      -2.578125,
      1.5390625,
      -0.08203125,
      1.5078125,
      2.28125,
      -1.109375,
      -1.5625,
      -3.484375,
      -1.8203125,
      1.40625,
      3.84375,
      0.62109375,
      -3.578125,
      -0.435546875,
      -0.306640625,
      -0.494140625,
      -2.015625,
      0.322265625,
      -1.875,
      -3.078125,
      -2.515625,
      -1.7578125,
      -3.359375,
      5.3125,
      -1.609375,
      2.421875,
      4.5625,
      3.296875,
      2.8125,
      2.90625,
      -0.142578125,
      3.75,
      -2.890625,
      0.345703125,
      3.765625,
      -0.4609375,
      -1.21875,
      -0.8125,
      1.25,
      -0.99609375,
      -2.84375,
      -1.21875,
      -0.5234375,
      1.671875,
      -0.455078125,
      -4.03125,
      -1.40625,
      0.054443359375,
      -3.46875,
      -1.546875,
      0.74609375,
      -2.375,
      -1.6796875,
      0.8046875,
      1.921875,
      0.2265625,
      1.609375,
      -0.19921875,
      0.322265625,
      0.6171875,
      -0.85546875,
      -2.296875,
      -1.34375,
      1.1875,
      2.078125,
      -1.6015625,
      -0.07373046875,
      -0.75,
      4.46875,
      -3.875,
      2.515625,
      -1.3828125,
      -2.0625,
      2.71875,
      9.1875,
      0.75,
      -0.228515625,
      -0.07177734375,
      0.69921875,
      -3.609375,
      1.3046875,
      0.031982421875,
      -2.953125,
      -3.375,
      3.78125,
      -1.6328125,
      1.84375,
      0.78125,
      -0.1669921875,
      0.1787109375,
      -2.625,
      0.431640625,
      -3.546875,
      -2.0625,
      3.203125,
      4.53125,
      -1.8671875,
      -0.80078125,
      1.828125,
      -4.34375,
      3.015625,
      -2.78125,
      -2.328125,
      1.328125,
      4.71875,
      -1.1796875,
      5.15625,
      0.498046875,
      -1.5859375,
      3.0625,
      0.251953125,
      1.7734375,
      2.03125,
      -2.328125,
      -0.94140625,
      -1.8828125,
      2.65625,
      -1.609375,
      -5.40625,
      -0.044677734375,
      -1.7890625,
      -0.8671875,
      -0.058837890625,
      0.024169921875,
      2.8125,
      0.1865234375,
      -0.298828125,
      2.546875,
      1.8671875,
      0.57421875,
      1.5078125,
      -0.1181640625,
      0.77734375,
      -6.34375,
      0.4765625,
      -1.25,
      0.494140625,
      0.359375,
      1.1484375,
      1.515625,
      -0.26953125,
      -0.58203125,
      -1.4140625,
      1.4921875,
      -0.5703125,
      0.82421875,
      -0.193359375,
      -0.2578125,
      -2.140625,
      0.455078125,
      -3.578125,
      2.90625,
      -2.421875,
      -2.296875,
      0.64453125,
      -1.9453125,
      -1.8125,
      -4.40625,
      1.046875,
      1.9921875,
      -0.09912109375,
      1.1640625,
      -0.009033203125,
      2.59375,
      -2.6875,
      1.8515625,
      0.9375,
      1.046875,
      1.109375,
      -1.6640625,
      -0.86328125,
      -3.921875,
      1.65625,
      -1.3203125,
      0.62890625,
      4.125,
      -1.75,
      -2.984375,
      2.546875,
      1.5
    ],
    "summary": "对大型语言模型在教育领域的应用进行全面系统的梳理和分析，总结当前技术现状、挑战和未来发展方向",
    "structure": {
      "sections": [
        {
          "title": "Large Language Models for Education: A Survey",
          "level": 1,
          "start_line": 1
        },
        {
          "title": "ARTICLE INFO",
          "level": 1,
          "start_line": 9
        },
        {
          "title": "ABSTRACT",
          "level": 1,
          "start_line": 18
        },
        {
          "title": "1. Introduction",
          "level": 1,
          "start_line": 22
        },
        {
          "title": "2. Characteristics of LLM in Education",
          "level": 1,
          "start_line": 44
        },
        {
          "title": "2.1. Characteristics of LLMs",
          "level": 1,
          "start_line": 55
        },
        {
          "title": "2.2. Characteristics of education",
          "level": 1,
          "start_line": 71
        },
        {
          "title": "2.2.1. Educational development process",
          "level": 1,
          "start_line": 75
        },
        {
          "title": "2.2.2. Impact on teachers",
          "level": 1,
          "start_line": 93
        },
        {
          "title": "2.2.3. Educational challenges",
          "level": 1,
          "start_line": 109
        },
        {
          "title": "2.3. Characteristics of LLMEdu",
          "level": 1,
          "start_line": 123
        },
        {
          "title": "2.3.1. Specific embodiment of \"LLMs + education\"",
          "level": 1,
          "start_line": 127
        },
        {
          "title": "2.3.2. Impact of \"LLMs + education\"",
          "level": 1,
          "start_line": 144
        },
        {
          "title": "3. How to Gradually Integrate LLMs into Education",
          "level": 1,
          "start_line": 172
        },
        {
          "title": "3.1. Reasons why LLMs for education",
          "level": 1,
          "start_line": 176
        },
        {
          "title": "3.2. Fusion strategies",
          "level": 1,
          "start_line": 198
        },
        {
          "title": "4. Key Technologies for LLMEdu",
          "level": 1,
          "start_line": 214
        },
        {
          "title": "5. Implementation of LLMEdu",
          "level": 1,
          "start_line": 247
        },
        {
          "title": "5.1. LLMs-empowered education",
          "level": 1,
          "start_line": 254
        },
        {
          "title": "5.2. LLMs in Mathematics",
          "level": 1,
          "start_line": 278
        },
        {
          "title": "6. Issues and Challenges",
          "level": 1,
          "start_line": 298
        },
        {
          "title": "6.1. Main issues",
          "level": 1,
          "start_line": 302
        },
        {
          "title": "6.2. Main challenges",
          "level": 1,
          "start_line": 323
        },
        {
          "title": "7. Conclusion",
          "level": 1,
          "start_line": 345
        },
        {
          "title": "References",
          "level": 1,
          "start_line": 357
        }
      ]
    },
    "suggested_tags": [
      "教育技术",
      "LLMs",
      "智能教育",
      "文献综述",
      "AI教育应用"
    ],
    "tag_suggestions": [
      {
        "name": "教育技术",
        "confidence": 0.95,
        "reason": "论文核心研究领域是大型语言模型在教育领域的应用，属于教育技术范畴"
      },
      {
        "name": "LLMs",
        "confidence": 0.9,
        "reason": "论文聚焦大型语言模型的技术特点、应用方法和挑战分析"
      },
      {
        "name": "智能教育",
        "confidence": 0.85,
        "reason": "论文系统综述了LLMs在个性化教学、自适应学习等智能教育场景的应用"
      },
      {
        "name": "文献综述",
        "confidence": 0.8,
        "reason": "论文采用综述研究方法，系统总结当前技术现状、挑战和未来发展方向"
      },
      {
        "name": "AI教育应用",
        "confidence": 0.75,
        "reason": "论文探讨人工智能技术特别是LLMs在教育行业的具体应用场景和效益"
      }
    ],
    "s2_graph": {
      "citations": [
        {
          "external_id": "CorpusId:283602068",
          "title": "Evaluating Adaptive and Generative AI-Based Feedback and Recommendations in a Knowledge-Graph-Integrated Programming Learning System",
          "authors": [
            "Lalita Na Nongkhai",
            "Jingyun Wang",
            "Adam T. Wynn",
            "T. Mendori"
          ],
          "year": 2025,
          "venue": "Computers and Education: Artificial Intelligence",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283438556",
          "title": "FEANEL: A Benchmark for Fine-Grained Error Analysis in K-12 English Writing",
          "authors": [
            "Jingheng Ye",
            "Shen Wang",
            "Jiaqi Chen",
            "Hebin Wang",
            "Deqing Zou",
            "Yanyu Zhu",
            "Jiwei Tang",
            "Hai-Tao Zheng",
            "Ruitong Liu",
            "Haoyang Li",
            "Yanfeng Wang",
            "Qingsong Wen"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283261962",
          "title": "LOOM: Personalized Learning Informed by Daily LLM Conversations Toward Long-Term Mastery via a Dynamic Learner Memory Graph",
          "authors": [
            "Justin Cui",
            "Kevin Pu",
            "Tovi Grossman"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283299903",
          "title": "Research on the Intelligent Reform Pathway of Higher Education Empowered by Generative Artificial Intelligence",
          "authors": [
            "Gao Min"
          ],
          "year": 2025,
          "venue": "Artificial Intelligence and Digital Technology",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283025009",
          "title": "THE RISE OF LARGE LANGUAGE MODELS: A BEGINNER’S SURVEY",
          "authors": [
            "Gustavo de Aquino Mouzinho",
            "Leandro Youiti Silva Okimoto",
            "Leonardo Yuto Suzuki Camelo",
            "Nádila da Silva de Azevedo",
            "Hendrio Bragança",
            "Rubens de Andrade Fernandes",
            "Fabricio Ribeiro Seppe",
            "Raimundo Claúdio Souza Gomes",
            "Fábio de Sousa Cardoso"
          ],
          "year": 2025,
          "venue": "ARACÊ",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283103684",
          "title": "Human or LLM as Standardized Patients? A Comparative Study for Medical Education",
          "authors": [
            "Bingquan Zhang",
            "Xiaoxiao Liu",
            "Yuchi Wang",
            "Lei Zhou",
            "Qianqian Xie",
            "Benyou Wang"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:282911236",
          "title": "Using LLMs to support assessment of student work in higher education: a viva voce simulator",
          "authors": [
            "Ian M. Church",
            "Lyndon Drake",
            "Mark Harris"
          ],
          "year": 2025,
          "venue": "",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283569751",
          "title": "SPARK – Smart Plug-and-Play AI Framework for RAG & Knowledge",
          "authors": [
            "Nirmit Dagli",
            "Chetan Jaiswal",
            "Sanjeev Kumar Marimekala"
          ],
          "year": 2025,
          "venue": "Ubiquitous Computing, Electronics & Mobile Communication Conference",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:282107557",
          "title": "Modular Framework Integrating Large Language Models with Drilling Hazard Detection Systems to Provide Operational Context-Informed Interpretations and Recommended Actions",
          "authors": [
            "S. Suhail",
            "T. S. Robinson",
            "O. Revheim",
            "P. Bekkeheien"
          ],
          "year": 2025,
          "venue": "SPE Annual Technical Conference and Exhibition",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:281787333",
          "title": "Defying Data Scarcity: High-Performance Indonesian Short Answer Grading via Reasoning-Guided Language Model Fine-Tuning",
          "authors": [
            "Muhammad Naufal Faza",
            "P. D. Purnamasari",
            "A. A. P. Ratna"
          ],
          "year": 2025,
          "venue": "International Journal of Electrical, Computer, and Biomedical Engineering",
          "citation_count": 0
        }
      ],
      "citations_fetched_at": "2025-12-16T19:03:39.467259",
      "references": [
        {
          "external_id": "CorpusId:266054920",
          "title": "Large Language Models in Law: A Survey",
          "authors": [
            "Jinqi Lai",
            "Wensheng Gan",
            "Jiayang Wu",
            "Zhenlian Qi",
            "Philip S. Yu"
          ],
          "year": 2023,
          "venue": "AI Open",
          "citation_count": 150
        },
        {
          "external_id": "CorpusId:265351653",
          "title": "Multimodal Large Language Models: A Survey",
          "authors": [
            "Jiayang Wu",
            "Wensheng Gan",
            "Zefeng Chen",
            "Shicheng Wan",
            "Philip S. Yu"
          ],
          "year": 2023,
          "venue": "BigData Congress [Services Society]",
          "citation_count": 282
        },
        {
          "external_id": "CorpusId:265352038",
          "title": "Large Language Models in Education: Vision and Opportunities",
          "authors": [
            "Wensheng Gan",
            "Zhenlian Qi",
            "Jiayang Wu",
            "Chun-Wei Lin"
          ],
          "year": 2023,
          "venue": "BigData Congress [Services Society]",
          "citation_count": 128
        },
        {
          "external_id": "CorpusId:265149884",
          "title": "Large Language Models for Robotics: A Survey",
          "authors": [
            "Fanlong Zeng",
            "Wensheng Gan",
            "Yongheng Wang",
            "Ning Liu",
            "Philip S. Yu"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 189
        },
        {
          "external_id": "CorpusId:265128707",
          "title": "Model-as-a-Service (MaaS): A Survey",
          "authors": [
            "Wensheng Gan",
            "Shicheng Wan",
            "Philip S. Yu"
          ],
          "year": 2023,
          "venue": "BigData Congress [Services Society]",
          "citation_count": 35
        },
        {
          "external_id": "CorpusId:263608784",
          "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
          "authors": [
            "Weixin Liang",
            "Yuhui Zhang",
            "Hancheng Cao",
            "Binglu Wang",
            "Daisy Ding",
            "Xinyu Yang",
            "Kailas Vodrahalli",
            "Siyu He",
            "D. Smith",
            "Yian Yin",
            "Daniel A. McFarland",
            "James Zou"
          ],
          "year": 2023,
          "venue": "NEJM AI",
          "citation_count": 217
        },
        {
          "external_id": "CorpusId:263334045",
          "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment",
          "authors": [
            "Tianhao Wu",
            "Banghua Zhu",
            "Ruoyu Zhang",
            "Zhaojin Wen",
            "K. Ramchandran",
            "Jiantao Jiao"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 70
        },
        {
          "external_id": "CorpusId:263310363",
          "title": "A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM",
          "authors": [
            "Jongyoon Lim",
            "Inkyu Sa",
            "Bruce A. MacDonald",
            "Ho Seok Ahn"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 9
        },
        {
          "external_id": "CorpusId:261582620",
          "title": "ImageBind-LLM: Multi-modality Instruction Tuning",
          "authors": [
            "Jiaming Han",
            "Renrui Zhang",
            "Wenqi Shao",
            "Peng Gao",
            "Peng Xu",
            "Han Xiao",
            "Kaipeng Zhang",
            "Chris Liu",
            "Song Wen",
            "Ziyu Guo",
            "Xudong Lu",
            "Shuai Ren",
            "Yafei Wen",
            "Xiaoxin Chen",
            "Xiangyu Yue",
            "Hongsheng Li",
            "Y. Qiao"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 148
        },
        {
          "external_id": "CorpusId:261582366",
          "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function",
          "authors": [
            "Haotian Xu"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 15
        }
      ],
      "references_fetched_at": "2025-12-16T19:03:40.082147"
    }
  },
  "659fea70-f22c-4b54-9382-aa768ec096e8": {
    "id": "659fea70-f22c-4b54-9382-aa768ec096e8",
    "filename": "ssrn-5095149.pdf",
    "file_path": "data/uploads/47e5d413-0cfd-43be-ba5a-dd4b0c5160c5/659fea70-f22c-4b54-9382-aa768ec096e8_ssrn-5095149.pdf",
    "status": "completed",
    "created_at": "2025-12-16 21:36:27.474020",
    "updated_at": "2025-12-16 13:37:57.338826",
    "user_id": "47e5d413-0cfd-43be-ba5a-dd4b0c5160c5",
    "title": "Effects of LLM use and note-taking on reading comprehension and memory: A randomised experiment in secondary schools",
    "markdown_content": "# Effects of LLM use and note-taking on reading comprehension and memory: A randomised experiment in secondary schools\n\nAuthors:\n\nPia Kreijkes<sup>1</sup>, Viktor Kewenig<sup>2*</sup>, Martina Kuvalja<sup>1*</sup>, Mina Lee<sup>2</sup>, Sylvia Vitello<sup>1</sup>, Jake M. Hofman<sup>2</sup>, Abigail Sellen<sup>2</sup>, Sean Rintel<sup>2</sup>, Daniel G. Goldstein<sup>2</sup>, David Rothschild<sup>2</sup>, Lev Tankelevitch<sup>2</sup>, Tim Oates<sup>1</sup>\n\n*Joint second authors\n\n# Affiliations:\n\n$^{1}$ Cambridge University Press and Assessment  \n2Microsoft Research\n\n# Abstract\n\nThe rapid uptake of Generative AI, particularly large language models (LLMs), by students raises urgent questions about their effects on learning. We compared the impact of LLM use to that of traditional note-taking, or a combination of both, on secondary school students' reading comprehension and retention. We conducted a pre-registered, randomised controlled experiment with within- and between-participant design elements in schools. 405 students aged 14-15 studied two text passages and completed comprehension and retention tests three days later. Quantitative results demonstrated that both note-taking alone and combined with the LLM had significant positive effects on retention and comprehension compared to the LLM alone. Yet, most students preferred using the LLM over note-taking, and perceived it as more helpful. Qualitative results revealed that many students valued LLMs for making complex material more accessible and reducing cognitive load, while they appreciated note-taking for promoting deeper engagement and aiding memory. Additionally, we identified \"archetypes\" of prompting behaviour, offering insights into the different ways students interacted with the LLM. Overall, our findings suggest that, while note-taking promotes cognitive engagement and long-term comprehension and retention, LLMs may facilitate initial understanding and student interest. The study reveals the continued importance of traditional learning approaches, the benefits of combining AI use with traditional learning over using AI alone, and the AI skills that students need to maximise those benefits.\n\n# Main\n\nLearners' rapid and widespread adoption of Generative Artificial Intelligence (GenAI) tools, particularly Large Language Models (LLMs), has unsettled the global educational landscape by offering\n\nnew ways for students to engage with learning materials $^{1;2;3;4;5;6}$  while also creating new challenges $^{7;8;9;10;11;12}$ . Large national surveys in the UK and US have found that a sizeable proportion of school students use GenAI tools such as OpenAI's ChatGPT $^{13;14}$ . This development raises fundamental questions about teaching and learning models. And yet, the vast majority of existing research on learning with LLMs has focused on the higher education context, leaving substantial knowledge gaps regarding effects on younger learners $^{15}$ . In addition, previous research has concentrated on second language education, mostly writing performance, as well as computing, health, and physics $^{15}$ . While such studies overall reveal positive effects of LLM use on academic performance, researchers call for caution as these might reflect the quality of LLM-produced work rather than genuine improvements in students' learning $^{15}$ . The effect of LLM use on two foundational aspects of learning – understanding and retaining information – remains critically underexplored. Knowledge stored in long-term memory is a fundamental element of cognition, forming the basis of nearly all human activity $^{16}$ . Thus, understanding the effects of LLMs on these foundations is urgently required to guide how such tools are integrated into schools, as policymakers and educators on the front-line are grappling with many unknowns. This study presents one of the first large-scale quantitative investigation into how reading comprehension and retention are affected by the use of LLMs.\n\nReading comprehension is the process of making sense of written materials resulting in a mental representation of the material<sup>17</sup>. Models of reading comprehension, such as the Construction-Integration (CI) model<sup>18</sup>, highlight that readers need to understand a text at several levels: the surface structure (words and their syntactic relations), the textbase (propositions, which generally represent one full idea), and the situation model (inferences about the text)<sup>17</sup>. This multi-level structure is supported by neuroimaging studies<sup>19;20;21;22;16</sup>. The ability to make inferences is a key aspect of comprehension. Usually, two types of inferences are distinguished: text-based bridging inferences involve connecting information from different text locations (e.g., the current sentence with a previous sentence) and knowledge-based inferences involve connecting information in the text with prior knowledge<sup>17</sup>. A reader's ultimate comprehension of a text depends on complex interactions between various elements, including factors related to the reader's characteristics (e.g., decoding skills, vocabulary and linguistic knowledge, prior domain knowledge, working memory capacity, inference-making ability, knowledge of reading strategies, motivation, and goals)<sup>23;24;25;26;27</sup>, the text itself (e.g., genre, length, word and sentence complexity, cohesion)<sup>28;29</sup>, and the reading context (e.g., reading for leisure or academic purposes)<sup>30;31</sup>.\n\nReading retention is the process of storing the comprehended content from a text in long-term memory. For learning it is necessary to not just comprehend the text at the time of reading, but also being able to remember what one has read and understood later. Retention is, in part, determined by the level and quality of information processing during encoding (i.e., the initial information acquisition while reading). According to the Levels of Processing framework  $^{32;33}$ , information that is processed deeply and elaborately —through semantic analysis involving meaning, inferences, and implications— can be recalled more readily. Deep processing facilitates the formation of rich, interconnected semantic networks, which provide multiple retrieval cues, and thus enhance the retrieval potential, as well as the construction of a robust schematic framework wherein specific details are meaningfully organised and related  $^{32;34}$ .\n\nThere are several reading strategies and learning activities that can enhance comprehension and retention as outlined by McNamara $^{35}$  and Chi $^{36}$ . Throughout the reading process, monitoring comprehension is particularly crucial, and includes strategies such as generating questions to gauge one's understanding $^{35}$ . Text-focused strategies involve interpreting the meaning of words, sentences and ideas (e.g., paraphrasing, breaking up long and complex sentence into manageable chunks, making bridging inferences to link different concepts) $^{35}$ . Strategies such as paraphrasing, selecting, and repeating are also considered active learning strategies, and these can activate prior knowledge and support the encoding, storing and assimilation of new knowledge $^{36}$ . There\n\nare also several effective reading strategies that go beyond the text (e.g., generating questions, using self-explanations, and using external information sources) $^{35}$ . Such strategies are considered to be constructive as learners generate new ideas and integrate information more deeply through explaining, elaborating, and connecting. This involves cognitive processes such as inferring new knowledge, integrating and organising new and existing knowledge, and repairing faulty knowledge $^{36}$ . Lastly, interactive learning activities involve meaningful dialogue with a partner, including with peers or systems like intelligent tutoring agents $^{36;28}$ . Such interactions can enhance learning by providing scaffoldings, corrective feedback, as well as additional information and new perspectives. Importantly, a dialogue is only considered to be interactive if both partners make substantive contributions $^{36}$ .\n\nThe integration of LLM tools into education raises the crucial question of whether their use could facilitate or undermine such learning strategies while reading. These models offer unprecedented flexibility in generating explanations, providing diverse perspectives, responding to complex questions in real-time, and adapting to individual learners' needs<sup>37;38</sup>. By serving as an external knowledge resource that extends beyond learners' personal knowledge and skills, LLMs can potentially enhance students' understanding and engagement with educational materials<sup>39;40;10;41</sup>. Furthermore, LLMs' ability to provide immediate clarifications and simplify complex concepts may help reduce cognitive load<sup>42;43</sup>. Thus, LLMs may be particularly useful in helping learners build understanding at multiple levels: from surface-level text comprehension and identification of key ideas, to deeper text-base representation of meanings, and ultimately to a comprehensive mental representation at the situation-model level of comprehension.\n\nHowever, over-use of LLMs could lead to shallow processing, where learners passively receive information without actively engaging in deep cognitive processing or critical thinking $^{44;36;45;46;47}$ . This superficial engagement could hinder the development of comprehensive mental models, negatively affecting comprehension and long-term retention $^{33;48}$ . When learners depend excessively on LLMs for answers and explanations, they may be less inclined to employ self-explanation and elaboration strategies that are essential for comprehension and meaningful learning $^{35;49;42}$ . While LLMs can make information readily accessible, this accessibility needs to be leveraged in ways that promote, rather than substitute for, the deep cognitive processing necessary for knowledge consolidation and learning $^{50;51}$ .\n\nIn order to assess the effectiveness of using LLMs as a learning tool for reading comprehension and retention, we compared it to a widely used learning activity that can facilitate many active and constructive strategies – note-taking. It is one of the most common and widely used learning activities and has been found to be an effective aid to learning while reading $^{52;53}$ . Note-taking can stimulate active processing of information and encourage the integration of new material with prior knowledge, thereby aiding comprehension as well as creating retrieval cues that aid later recall $^{52;54}$ . The impact of note-taking appears to vary depending on the depth of cognitive processing involved. It could focus readers on shallower processing, because readers might pay more attention to the surface structure and textbase but it could also enhance the situation-model by encouraging elaboration and better mental organisation $^{55;56;57}$ . Kobayashi's $^{52}$  meta-analysis supports the former as it found relatively small effects for higher-order performance tests, suggesting that the generative value of note-taking may be limited and highly dependent on the quality of the notes taken (whether they are verbatim or generative). We also compared the effectiveness of using an LLM on its own with using an LLM in conjunction with note-taking, given that it might be useful to combine the activities of querying LLMs and taking notes to facilitate learning. The two activities could potentially have complementary effects on reading comprehension and retention by drawing on their respective strengths. However, there might also be a risk of dividing attention in a way that renders both activities less effective.\n\nTo examine whether LLMs can be used as a tool to support the fundamental learning processes of reading comprehension and retention, we conducted a large-scale, pre-registered, randomised\n\ncontrolled experiment with within- and between-participant design elements. The study involved 405 secondary school students, aged 14-15 years, and took place in seven schools in England (UK). The experiment consisted of a learning session and a test session, which were three days apart. In the learning session, each student was tasked with understanding and learning two text passages on a different history topic (Apartheid in South Africa and the Cuban Missile Crisis), each by using a different learning activity (learning condition) drawing on evidence-based strategies. Students were not informed that they would be tested on the passages. They were randomly assigned to one of two groups. Group 1 was exposed to conditions referred to as \"LLM\" (i.e., using an LLM to understand and learn a text) and \"Notes\" (i.e., taking notes to understand and learn a text) and Group 2 was exposed to conditions referred to as \"LLM\" and \"LLM+Notes\" (i.e., using an LLM alongside note-taking to understand and learn a text). Both learning condition and text order were randomised. The LLM functionality in the learning session was provided by a private Azure-hosted instance of OpenAI's GPT-3.5 turbo model. After each learning task, students responded to a survey about their learning experience, with both quantitative and qualitative questions.\n\nIn the test session, students completed a range of questions assessing different levels of comprehension and retention. Specifically, we assessed their literal retention, comprehension, and free recall. For each passage, literal retention (i.e., lower-level retention) was measured through eight short response (cued recall) and ten multiple choice (recognition) questions assessing literal information which did not require any knowledge-based inferences, and no or only minimal text-based (bridging) inferences. Comprehension (i.e., higher-level retention) was measured through three open response questions requiring bridging inferences to connect information from several different text locations as well as knowledge-based inferences. Free recall was assessed through one open response question for each text, asking students to write down everything they remembered, and thus measuring how much students retained and understood without any cueing.\n\nOur primary aim was to quantify the impact of using an LLM on students' reading comprehension and retention. We made the choice not to have a \"reading-only\" control condition both because it would limit participant fatigue in responding to conditions, and on the basis that any engagement with the text beyond passive reading is likely going to lead to improved learning outcomes $^{35;36}$ , setting the bar for LLM use comparatively low. Instead, we decided to compare it against the common, evidence-based learning activity of note-taking. We also explored students' learning experiences when engaging in the different learning activities, including which activity they preferred and why, as well as different \"archetypes\" of prompting behaviour that shed light on the learning outcomes. The results offer valuable insights for stakeholders and policy makers of the global education landscape.\n\n# Results\n\nOur study investigated the effects of using an LLM on student learning outcomes compared to traditional note-taking in a sample of 344 students (after applying pre-registered exclusion criteria, see Methods for more information). Group 1 (LLM vs Notes conditions) had a final sample of 184 students and Group 2 (LLM vs LLM+Notes conditions) of 160 students. Among the students there were slightly more males than females, most were English native speakers, a small number of students  $(5.2\\%)$  received free school meals indicating socioeconomic disadvantage, and about half were taking History GCSEs (see Supplementary Table 3 for all student characteristics). Both groups showed similar prior familiarity with the three learning conditions (LLM, Notes, LLM+Notes). About half of the students regularly took notes and most reported limited prior use of LLM for learning (see Supplementary Table 4 for detailed frequencies).\n\n# Learning outcomes\n\nWe compared the impact of LLM (reference condition, used by all students) to the impact of Notes (used by students in Group 1) and LLM+Notes (used by students in Group 2) on students' literal retention, comprehension, and free recall. Traditional note-taking led to the best performance across all measures, followed by LLM+Notes, while using LLM alone resulted in the lowest scores (see Supplementary Table 5 for descriptive statistics).\n\nLinear mixed-effects models confirmed significant differences across the conditions (see Figure 1, see Supplementary Table 6 for all model coefficients, confidence intervals and effect sizes).\n\nFor literal retention, we found significant main effects for both Notes ( $\\beta = 1.92$ ,  $p < 0.001$ , 95% CI [1.42, 2.42]) and LLM+Notes ( $\\beta = 0.57$ ,  $p = 0.040$ , 95% CI [0.03, 1.11]), indicating that students performed better with Notes compared to LLM and better with LLM+Notes compared to LLM.\n\nFor comprehension, we again found significant main effects for both Notes ( $\\beta = 0.95$ ,  $p < 0.001$ ,  $95\\%$  CI [0.62, 1.28]) and LLM+Notes ( $\\beta = 0.35$ ,  $p = 0.049$ ,  $95\\%$  CI [0.00, 0.70]), where students had better performance with Notes compared to LLM and with LLM+Notes compared to LLM.\n\nFor free recall, we found a significant main effect for Notes ( $\\beta = 1.02$ ,  $p = 0.018$ , 95% CI [0.18, 1.86]) but not for LLM+Notes ( $\\beta = -0.08$ ,  $p = 0.855$ , 95% CI [-0.98, 0.81]). Thus, students showed better performance with Notes compared to LLM but there was no significant difference between LLM+Notes compared to LLM. Given the non-normal distribution of free recall scores, we also conducted non-parametric versions of these tests as a robustness check, detailed in the Methods section, which corroborated these findings.\n\nThese results suggest that both note-taking conditions (either alone or with LLM) showed improved learning compared to using LLM on its own. However, the benefit of note-taking was seen across all different measures of learning, whereas the benefit of LLM+Notes was seen for literal retention and comprehension but not for free recall.\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/f9c6b97ec629fd3a5afd56314cf1273a7a23652bdf7aa8dcc448b1d899f826ce.jpg)  \nFigure 1: Distribution of test performance by condition and group for Comprehension (left, max 12 points; Notes:  $M = 4.89$ ,  $SD = 2.52$ ; LLM+Notes:  $M = 4.11$ ,  $SD = 2.65$ ; LLM Group 1:  $M = 4.00$ ,  $SD = 2.44$ ; LLM Group 2:  $M = 3.80$ ,  $SD = 2.47$ ), *Literal retention (middle, max 20 points; Notes:  $M = 10.8$ ,  $SD = 4.29$ ; LLM+Notes:  $M = 9.68$ ,  $SD = 4.83$ ; LLM Group 1:  $M = 8.83$ ,  $SD = 3.96$ ; LLM Group 2:  $M = 8.95$ ,  $SD = 4.29$ ) and *Free recall (right, max 50 points; Notes:  $M = 5.36$ ,  $SD = 5.49$ ; LLM Group 1:  $M = 4.32$ ,  $SD = 4.15$ ; LLM Group 2:  $M = 4.32$ ,  $SD = 4.63$ ; LLM+Notes:  $M = 4.20$ ,  $SD = 5.07$ ). Mean values are indicated by the two large circles within each facet, whereas the smaller points show individual students scores. Error bars indicate one standard error above and below the mean. Group 1 is shown on the left facet of each subfigure, comparing LLM (red) and Notes (blue). Group 2 is on the right facet of each plot, comparing LLM (red) and LLM+Notes (green).\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/41488ca1a6c3943e2825383542041eb80af29edf193795e1cd6d1ef164a3df0a.jpg)\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/cfcb380db33b073aea66229200e4a4b9ce36c4e9d8d6f6b463a22debcaf33262.jpg)\n\n# Behavioural engagement\n\nBehavioural engagement with the LLM and note-taking was quantified by the average number of queries made to the LLM, the average number of words written in students' notes as well as time spent on task. Access to notes alongside the LLM reduced students' query frequency compared to LLM-only conditions (from 9.21 to 6.02 queries in Group 2). While students wrote a similar number of words in their notepad in both Notes and LLM+Notes conditions (around 100 words), a concerning proportion  $(25.63\\%)$  heavily copied from LLM outputs into their notes, with some  $(16.25\\%)$  showing nearly complete copying (more than  $90\\%$  overlap of trigrams between LLM output and notes). Additionally, students spent significantly less time on task when using only the LLM compared to conditions involving note-taking (differences of 0.80 and 1.54 minutes for Groups 1 and 2, respectively), suggesting deeper engagement when note-taking was involved. See Supplementary Table 7 for a full description of behavioural measures.\n\n# Prompting behaviour\n\nIn order to understand how students engaged with the LLM, we performed a qualitative analysis of all prompts  $(n = 4,929)$  using a hierarchical coding scheme where specific prompts were nested within overarching prompt types. Each prompt could be assigned to multiple codes. We identified four behavioural archetypes of how students worked with the LLM in relation to the task as well as two additional overarching prompt types that were not directly related to the task (see Figure 2 for the distribution of prompt types across each LLM session). For exact frequency counts of overarching prompt-types, see Supplementary Table 21 and for specific prompt types see Supplementary Table 22.\n\nThe most frequent archetype was seeking additional information and deeper understanding (2,265 prompts, as shown in the purple bars in Figure 2). The vast majority of students  $(90\\%)$\n\nused such a prompt type at least once, about  $40\\%$  used this as their first prompt, and  $60\\%$  as their most common prompt type (see Figure 3). These prompts primarily comprised requests for elaboration (1,479 instances) and general background information (514 instances). Examples include \"how are people today affected by the apatheid\" and \"why did it take so long to free nelson mandela\".\n\nInformation condensation (749 prompts, as shown in the teal bars in Figure 2) emerged as the second most common archetype, with  $27\\%$  of students using it as their first prompt, typically requesting summaries or key ideas, such as \"What are five key points from the entire text?\" or \"create a timeline of all the events\". The third archetype, basic understanding of the text (615 prompts, green bars in Figure 2), was used by  $70\\%$  of students at least once, mainly for definitions and content simplifications such as \"What is a sanction?\" and \"explain communist\". A fourth archetype, requesting direct study and memory help, was used infrequently (39 instances, red bars in Figure 2) despite students receiving no explicit instructions for such use. These ranged from asking the LLM to generate a quiz (\"ask me 4 questions about the text and tell me if i get them right after my next reply\") to pneumonic devices (\"create me a mnemonic device on the cuban missile crisis\").\n\nBeyond these archetypes, 760 prompts focused on interacting with the LLM rather than (or in addition to) text content (blue bars in Figure 2), primarily requesting specific formats or response improvements. Examples include \"can you put this into bullet points?\" and \"shorten the aftermath into 1 sentence\". Notably, only six prompts questioned the LLM's reliability. Finally, about  $10\\%$  of all interactions (501 prompts, brown bars in Figure 2) were off-topic or irrelevant (e.g., \"what is the meaning to life\" and \"Tell me about Harry Potter\"), showing that a small but potentially relevant prompt proportion was not task-focused, potentially due to low task motivation or boredom.\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/d626ae4afddf164784c2957f218467f2fcf897ba4e897712255c0f3e6a5a4074.jpg)  \nFigure 2: Distribution of prompt types across LLM sessions for different conditions and students. Each panel represents a specific combination of condition (LLM-only or LLM+Notes) and text passage (Apartheid in South Africa or Cuban Missile Crisis). Each bar shows the number of prompts within each type for an individual LLM session, with sessions sorted in descending order by the total number of prompts and ties broken by the number of prompts within each type.\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/b9a2f4d9cc9579f597bbeeb013a133f3f56b5f7e78028c7f54b3caea7c03b5ee.jpg)  \nFigure 3: Distribution of student prompts across different types, showing the percentage of students who used the prompt type at least once (blue), as their most common prompt (magenta), and as their first prompt (green). Prompt types are arranged by overall frequency.\n\n# Learning experiences and perceptions\n\nIn addition to analysing students' behavioural engagement, we asked them about their learning experiences and perceptions of the different conditions. The quantitative results are summarised in Figure 4, with details of statistical tests in Supplementary Table 15. We used an adjusted p-value threshold of  $0.05 / 18 = 0.002$  to gauge statistical significance based on the Bonferroni correction to account for multiple comparisons  $(n = 18)$ .\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/c4c266d6421d905ef8a8bd42b99b86f7e33f41d2190d0d2c236b0c94e604e5c3.jpg)\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/23e6863e1c87df8e23a0c590c8e6744c9f75059bb10033cad565cccdca9a1e8e.jpg)\n\nFigure 4: Differences in learning experiences and perceptions by group and condition. The top panel displays perceived test performance on a 0-100 scale, while the middle and bottom panels show ratings for measures with positive and negative valences, respectively, on a 1-5 scale. Each point represents the mean rating for a condition, with error bars indicating one standard error above and below the mean.  \n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/2f7b3c6eb55edba33c7498db63ee23202e70938030ee28f26ed778c685bd2de3.jpg)  \nCondition  $\\rightarrow$  LLM only  $\\rightarrow$  LLM+Notes  $\\rightarrow$  Notes only\n\nContrary to actual learning outcomes, Group 1 students found the LLM more helpful, easier to use, and more enjoyable than note-taking, while reporting less effort investment. Group 2 showed similar experiences between conditions, except perceiving the LLM-only condition as less difficult than LLM+Notes. Students perceived task performance similar across conditions during learning. Following the test, students in both groups accurately reported their perceived test performance to be lower in the LLM-only conditions than in the Notes and LLM+Notes conditions.\n\nThese findings suggest that while the LLM-only condition was less effective for learning, it provided motivational benefits - particularly evident in Group 1's preferences. Importantly, these motivational benefits were maintained when combining LLM use with note-taking in Group 2.\n\n# Activity preferences\n\nStudents were asked to indicate their preferred learning activities and explain their preferences through an open response (see Table 1). In Group 1, most students preferred the LLM activity over traditional note-taking. Those students cited enhanced understanding, the LLM's ability to answer questions, and ease of the activity as their main reasons. Students favouring traditional notetaking emphasised benefits for understanding, the importance of self-generated work, and improved\n\nmemory retention. In Group 2, a substantial majority preferred the combined activity over using the LLM alone. Students preferring the combined activity noted the complementary benefits of both approaches, enhanced memory retention, and improved organisation. Those favouring the LLM-only activity emphasised its efficiency, particularly appreciating that the LLM did the work for them. This reveals an underlying tension between efficiency and depth of processing - while the LLM-only activity was perceived as more efficient, conditions involving note-taking demonstrated superior learning outcomes through deeper engagement and better retention.\n\nTable 1: Learning activity preferences and reasons by group  \n\n<table><tr><td>Activity preference and reasons</td><td>Count</td><td>Percentage</td></tr><tr><td colspan=\"3\">Group 1: LLM vs Notes</td></tr><tr><td>LLM over Notes</td><td>89</td><td>42.0</td></tr><tr><td>Notes over LLM</td><td>57</td><td>26.9</td></tr><tr><td>No preference</td><td>48</td><td>22.6</td></tr><tr><td>Not sure</td><td>18</td><td>8.5</td></tr><tr><td colspan=\"3\">Group 2: LLM vs LLM+Notes</td></tr><tr><td>LLM over LLM+Notes</td><td>32</td><td>16.2</td></tr><tr><td>LLM+Notes over LLM</td><td>100</td><td>50.5</td></tr><tr><td>No preference</td><td>48</td><td>24.2</td></tr><tr><td>Not sure</td><td>18</td><td>9.1</td></tr><tr><td colspan=\"3\">Reasons for LLM over Notes preference</td></tr><tr><td>Helps understanding</td><td>34</td><td>21.9</td></tr><tr><td>Answers questions</td><td>23</td><td>14.8</td></tr><tr><td>Easy to use</td><td>22</td><td>14.2</td></tr><tr><td>Quick to use</td><td>18</td><td>11.6</td></tr><tr><td>Provides background</td><td>18</td><td>11.6</td></tr><tr><td>Summarises and simplifies</td><td>17</td><td>11.0</td></tr><tr><td>Engaging</td><td>10</td><td>6.5</td></tr><tr><td>Interactive</td><td>8</td><td>5.2</td></tr><tr><td>Helps remember</td><td>4</td><td>2.6</td></tr><tr><td colspan=\"3\">Reasons for Notes over LLM preference</td></tr><tr><td>Helps understanding</td><td>22</td><td>21.4</td></tr><tr><td>Own work</td><td>21</td><td>20.4</td></tr><tr><td>Aids memory</td><td>18</td><td>17.5</td></tr><tr><td>Helps processing</td><td>8</td><td>7.8</td></tr><tr><td>Unclear usage of LLM</td><td>7</td><td>6.8</td></tr><tr><td>Active learning</td><td>6</td><td>5.8</td></tr><tr><td>LLM distracts</td><td>6</td><td>5.8</td></tr><tr><td>Revisitable</td><td>5</td><td>4.9</td></tr><tr><td>Easier</td><td>4</td><td>3.9</td></tr><tr><td>Helps organisation</td><td>4</td><td>3.9</td></tr><tr><td colspan=\"3\">Reasons for LLM over LLM+Notes preference</td></tr><tr><td>Does the work for you</td><td>15</td><td>50.0</td></tr><tr><td>Notes not necessary</td><td>5</td><td>16.7</td></tr><tr><td>Quicker</td><td>4</td><td>13.3</td></tr><tr><td>More time for questions</td><td>4</td><td>13.3</td></tr><tr><td colspan=\"3\">Reasons for LLM+Notes over LLM preference</td></tr><tr><td>Best of both worlds</td><td>35</td><td>23.2</td></tr><tr><td>Helps remember</td><td>27</td><td>17.9</td></tr><tr><td>Helps organisation</td><td>24</td><td>15.9</td></tr><tr><td>Own work</td><td>21</td><td>13.9</td></tr><tr><td>Helps understanding</td><td>16</td><td>10.6</td></tr><tr><td>More helpful and easier</td><td>12</td><td>7.9</td></tr><tr><td>Helps process LLM output</td><td>6</td><td>4.0</td></tr><tr><td>More fun</td><td>4</td><td>2.6</td></tr><tr><td>LLM errors</td><td>3</td><td>2.0</td></tr></table>\n\nNote: This table only includes reasons that have been mentioned by at least three students.\n\n# Future use\n\nAt the end of the learning session, students reported their intentions for future use of each activity. In Group 1, the majority of students  $(64.4\\%)$  indicated they would use LLMs in the future, with only  $7.3\\%$  negating and  $28.2\\%$  being unsure. A smaller majority of students  $(55.3\\%)$  planned to take notes in the future, and  $10.6\\%$  did not think they would do so, while  $34.1\\%$  were uncertain. In Group 2, the majority of students  $(59.5\\%)$  intended to use LLMs in the future,  $10.4\\%$  did not and  $30.1\\%$  were unsure. A similar majority  $(58.5\\%)$  planned to use the combined LLM+Notes activity in the future, while  $14.6\\%$  did not and  $26.8\\%$  were unsure.\n\n# Discussion\n\nThis study provides new insights into how the use of LLMs compares to and interacts with traditional evidence-based practices (specifically note-taking) to support students' reading comprehension, retention, and engagement. It offers important perspectives on the cognitive and motivational dynamics underlying human-AI interactions in learning, and how these interactions influence educational outcomes and perceptions. In particular, it suggests that LLM use and more traditional note-taking have complementary roles in the learning process.\n\nIn this study, we found that note-taking—whether done alone or alongside LLM usage—produced higher comprehension and retention scores compared to using an LLM alone, underscoring the importance and effectiveness of traditional active learning strategies. At the same time, students generally used LLMs constructively and perceived them as more \"helpful\" and preferable to notetaking. How can we reconcile these seemingly conflicting results?\n\nOne part of the answer may be that students simply have a limited metacognitive understanding of what is in fact helpful for their own learning $^{58;59;60}$ , specifically in the context of GenAI $^{61}$ . In particular, they may underweight the importance of the \"desirable difficulties\" induced by activities such as note-taking $^{48}$ . Note-taking requires active processing of information, such as identifying important information, paraphrasing and summarising $^{52}$ . While these tasks demand cognitive effort and may not be inherently enjoyable, past research shows that the learning potential increases with the level of required cognitive engagement $^{62}$ . Having an LLM do some of the work of summarising a passage or explaining a concept may feel more enjoyable and efficient, but can reduce the cognitive engagement necessary for deep comprehension and long-term retention. Similar effects on LLM use on learners' affective-motivational state and mental effort were found in Deng et al.'s meta-analysis $^{15}$ . Additionally, LLMs may sometimes provide learners with distractions that are interesting, but that compete with the primary task at hand.\n\nAt the same time, our exploratory analysis of student prompts suggests that another part of the answer lies in the unique benefits LLMs provide, which may have been genuinely helpful beyond what our primary analyses captured. The vast majority of LLM use was constructive rather than distracting or reductive, with students seeking additional information and deeper understanding. Students demonstrated remarkable curiosity, asking sophisticated questions that extended beyond the immediate text. For example, in a passage about apartheid in South Africa that briefly mentions Nelson Mandela's journey from prisoner to president, one student asked, \"What was Mandela's life story?\" Similarly, in a passage on the Cuban Missile Crisis that assumes some background knowledge of the Cold War, another student asked, \"Why was America afraid of communism?\" These explorations represent a different kind of active learning opportunity that may not result from note-taking alone, underscoring the LLM's potential to expand intellectual horizons. That said, these deeper inquiries may have involved tradeoffs: they could have competed with processing the core information in the passage, reducing performance on tested items, but they likely also enhanced learning in ways not captured by our tests, which focused only on the explicit and implied content within the texts.\n\nTaken together, our findings demonstrate the value of combining LLM use and note-taking, which was not only more effective than LLM use alone but also students' preferred activity. This raises the opportunity and challenge of how to combine traditional evidence-based strategies like note-taking with the unique benefits offered by LLMs. Rather than viewing these as competing alternatives, we should think of them as complements that when thoughtfully integrated can enhance learning outcomes in ways that neither can achieve alone. A key to doing so is leveraging input from educators and researchers in the design and use of new LLM-based tools for learning, as has been key for past hybridisation of traditional and digital approaches $^{63;64}$ .\n\nOur work suggests several such directions. First and most easily would be to separate LLM use from note-taking. Under this model, students would first independently read a text, and then interact with an LLM to further clarify and explore its content. Following this they would take notes independently, without the ability to simply copy and paste output from the LLM. This would prevent students from taking shortcuts we have observed in this study, instead encouraging them to synthesise and internalise information themselves. This is a small but likely meaningful design choice that was not obvious to us a priori, but that emerged through our work and could be tested in future research.\n\nSecond, educators could actively train and guide students to use LLMs in ways that align with active learning strategies, such as asking targeted questions to clarify specific misunderstandings, engage in critical thinking, and integrate information, without overloading them with excessive information or reducing cognitive processing $^{36;35}$ . Likewise, educators could discourage the passive consumption of automatic summaries and explanations. This aligns with the conceptualisation of AI tools as \"thought partners\" that support existing human cognitive processes rather than disrupting them $^{9}$ . Going beyond learning activities, by guiding students to use LLMs more effectively, educators will help students develop their metacognitive skills more generally, which will make them better prepared to use these technologies in the long-term. Furthermore, software could be configured to support these goals by limiting distracting behaviour and encouraging productive use (plausibly by capturing data and using the LLM to provide feedback or nudges to the student based on their LLM interactions).\n\nAnd third, educators could leverage insights from students' interactions with the LLM to better understand what concepts they are struggling with or what they are curious about. This could be done at an individual level but could also be conducted collectively for an entire class, possibly through the use of automated tools that collect and analyse student interactions and then provide data back to the educational instructors in a privacy-protecting way to surface insights. The results could be used to tailor future lessons, activities and group discussions. For example, through analysing the prompts in our experiments, it becomes clear that students were curious about the tenets of communism and why they provoked such fear and opposition in the U.S.\n\nThis research makes several contributions to the growing field of research examining the impact of LLMs in education. While much prior work has focused on the impact of LLMs on task performance and efficiency, the present study investigated aspects that are more fundamental to learning and cognition. In addition, it examined the effects of LLMs within a large sample of secondary school students coming from different school types, rather than amongst students in higher education, who have received much more research attention thus far<sup>15</sup> Such populations can be difficult to reach, especially when several study sessions are involved. In designing the study, we aimed to be authentic to students' experiences in school, ensuring the findings hold practical significance. In particular, we used texts that reflect the topics and difficulty that such students might come across in the classroom, and we compared the effects of LLM use with a learning activity that is, at least until now, commonly used.\n\nOne limitation of the present study is that students received no in-depth training for the different learning activities. While we provided instructions and a demonstration video for how to interact with the LLM and take notes, students did not have an opportunity to practice. This might have\n\nbeen a particular disadvantage for the LLM conditions because students were less familiar with using LLMs than note-taking and might thus not have leveraged the activity as effectively. In addition, the study might have benefited from a baseline or passive reading condition to ascertain whether using the LLM to understand and learn a text provides benefits above passive reading (that is, to gauge its effectiveness per se). Another limitation is that we were practically constrained to a small set of retention and comprehension questions relative to the vast number of potential questions that could have been asked, although we sampled a wide range of content. Thus, we could have underestimated students' learning overall, with the exception of the free recall questions. Furthermore, the study was limited to a single, isolated activity outside of the context of normal use throughout an entire course of study. It is possible that repeated use or use in other settings (e.g., in everyday classrooms or independently for homework, unsupervised) could yield different results. Lastly, while we consider it a strength that we used texts that were appropriate to the student sample, it is possible that LLM usage might be more beneficial for texts that students struggle with, as indicated by a few students who stated they did not know what to ask the LLM. Hence, exploring the effects of LLM use for texts that go beyond students' current capabilities could further expand our understanding of potential applications.\n\nIt is crucial for future research to explore which ways of interacting with LLMs most effectively enhance learning outcomes. Future research must also explore the long-term consequences of LLM integration in learning contexts, particularly its impact on reading skills, independent problem-solving, and metacognition. Additionally, it will become vital to understand how these tools influence societal perceptions of effort, expertise, and achievement. The evolving role of LLMs and generative AI technology may shift the definition of essential expertise and change the landscape of necessary competencies across various fields<sup>8</sup>. Moving forward, it is vital for educators and society to identify which core skills remain indispensable in this new environment and to develop pedagogical strategies that ensure their preservation and growth<sup>9</sup>. This research marks only the beginning of understanding how to effectively use LLMs to complement existing activities and tools while maintaining students' cognitive engagement.\n\nIn summary, this study provides one of the first large-scale quantitative evidence on the effects of LLMs on reading comprehension and retention. Our findings reaffirm the importance of traditional strategies like note-taking, which foster deep cognitive engagement and strong learning outcomes. At the same time, LLMs introduce new possibilities for learning—offering opportunities to clarify, explore, and contextualise material—but these tools must be used with proper guidance aimed at enhancing, rather than bypassing, active learning. Rather than viewing these tools as a disruption to be resisted, educators and researchers have an opportunity to proactively shape their use to maximise learning potential. By doing so, we can prepare students to thrive in an AI-integrated world while preserving the focus, depth, and curiosity that define meaningful education.\n\n# Materials and Methods\n\nThis study comprised two stages: a piloting stage and a main study. The purpose of the piloting stage was to test the tasks and proposed procedures in the school context and amend them as appropriate. The methods and findings reported here are a part of the main study, which took place between March and July 2024.\n\n# Participants\n\nParticipants were 405 Year 10 students (aged 14-15 years) from seven secondary schools in England. Based on our exclusion criteria (see Supplementary Section 1.1), we retained 344 students for analysis. We made efforts to recruit 600 students but were unable to do so as we could not find enough schools before the start of the summer holidays. Recruitment methods included emailing\n\nschool headteachers in several counties and asking participating schools to contact other schools. The final school sample included three non-selective state schools, two grammar schools (one all girls, one all boys) and two independent schools, located in three different counties.\n\nOnce a school agreed to participate, all Year 10 students were invited to take part through the school's project lead. Information sheets were shared with students and their parents/guardians, after which both were asked to provide their informed written consent using an online Microsoft form. This study was conducted in line with the British Educational Research Association's  $^{65}$  ethical guidelines. Ethical approval was provided by the research ethics committees of the researchers' institutions.\n\n# Experimental design and procedure\n\nThe study was a pre-registered randomised controlled experiment with within- and between-participant design elements, as illustrated in Figure 5. Conducted over two sessions spaced three days apart, the experiment consisted of a learning session followed by a test session.\n\nLearning Session: In the learning session, students were tasked with understanding and learning two text passages on different history topics (Passage A and Passage B). Each passage was studied using a specific active learning activity (condition). The three conditions were:\n\n- LLM: Students were asked to use an LLM chatbot we created to help them understand and learn the passage.  \n- Notes: Students were asked to take notes to help them understand and learn the passage.  \n- LLM+Notes: Students were asked to use our LLM chatbot as well as take notes to help them understand and learn the passage.\n\nStudents were randomly assigned to one of two groups:\n\n- Group 1: Exposed to the LLM and Notes conditions.  \n- Group 2: Exposed to the LLM and LLM+Notes conditions.\n\nRandomisation assigned 184 students to Group 1 (53.5%) and 160 to Group 2 (46.5%). The order of conditions and passages was randomised. During this session, students also completed survey questions about their learning experiences.\n\nTest Session: In the test session, students answered comprehension and retention questions about the two passages (with passage order randomised) and completed survey questions regarding their general characteristics.\n\nTiming: Students spent a mean of approximately 35 minutes on the learning session and 30 minutes on the test session.\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/b21bdd2e3d49ceb66072818fc8bb684298786b88b09834ba3fb45c8e408c61ce.jpg)  \nRandomised order of group, condition and passage\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/b9b81a2d9ef90ec106dc670f00146ef1702cc9c8dc0607a32f8ae05c0131d727.jpg)  \nRandomised order of passage  \nFigure 5: Study design illustrating the activities and their order during Session 1 and 2.\n\n# Setup and system\n\nBoth sessions took place in schools during regular school hours. Groups of students participated simultaneously in classrooms, with each student completing the sessions on an individual laptop or computer. At the start of each session, the experimenter or teacher read out a script with introductory instructions. They also monitored students during the entire session and answered their questions.\n\nThe experiment was a web app hosted on github.com that students accessed via the browser. For the LLM functionality in Session 1, the app made backend calls to private Azure Functions that accessed an Azure-hosted instance of OpenAI's GPT-3.5 turbo model. The LLM interactions were limited to Azure and did not go back to OpenAI. Participants could issue a maximum of 20 prompts. The LLM was customised with a meta-prompt that was not visible to students (\"You are an AI chat bot that helps students read and comprehend the following passage: <text> Students can use this tool to define unfamiliar words, explain concepts, or summarise key points of the passage.\"). Figure 6 illustrates the task screen for the LLM+Notes condition. For the Notes and\n\n# Apartheid in South Africa\n\nIn 1910, four British colonies joined to create the \"Union of South Africa.\" The Union was part of the British Empire, and later became the Republic of South Africa that we know today. After World War II, many countries that were controlled by Western nations, including South Africa, wanted independence. The South African government wanted to break free from the British Empire. However, for Black South Africans, the main struggle was against the discrimination by White South Africans who were of British and Dutch descent.\n\nIn 1948, the National Party came to power. This new government formalised the discrimination and racial separation in a system called 'apartheid'. It lasted for over 40 years, during which many unfair laws were passed. For example, every citizen had to be classified by their skin colour, people of different skin colours were not allowed to marry each other, and people were forced to live in specific areas based on their skin colour. More than 3.5 million people of colour were forced to leave their homes, and many were pushed into poverty.\n\nAnti-apartheid groups like the African National Congress (ANC) at first only used peaceful protest. This changed after the Sharpeville Massacre in 1960 when police killed black people that were peacefully protesting outside the police station. Activists now also turned to violence, such as sabotage and attacks on police and military. In response, the government banned anti-apartheid groups. In the decades that followed, anti-apartheid activists faced arrests, prison, and even execution. For example, Nelson Mandela, the leader of the ANC, was in prison for 27 years.\n\nMore and more countries criticised apartheid and used sanctions and boycotts against South Africa. Horrific events at the Soweeto Youth Uprising in 1976 also gained global attention. Black students peacefully protested a new law that forced them to study in Afrikaans, the language of the Dutch colonisers. The police killed more than 100 teenagers. Growing pushback from outside and within South Africa put pressure on the government. Finally, Nelson Mandela was freed from prison, which started negotiations to end apartheid. The elections in 1994 granted all South African citizens, including Black citizens, voting rights. As a result, Mandela became the first democratically elected president. This marked the end of apartheid. However, even today, many Black South Africans still feel the negative effects of apartheid.\n\n# AI Chatbot ②\n\nYou can ask 20 more questions.\n\n# Notepad\n\n![](/uploads/images/659fea70-f22c-4b54-9382-aa768ec096e8/34bb33463af6cbdc665c50ca9aa10ad1e76195cb893c9f0d2effdf2c955d4149.jpg)  \nFigure 6: Example task screen for the LLM+Notes condition.\n\nWhen you are finished with the task,\n\nclick continue.\n\nCONTINUE (12:29)\n\n#\n\nthe LLM conditions, only the notepad or chatbot was displayed, respectively.\n\n# Learning task and materials (Session 1)\n\nIn the learning session, students read two passages on a history topic, each with a different learning activity. They were asked to understand and learn the content of the texts as best as they could. Notably, students had not been told that they would be tested on the materials. For each task, they first received instructions (see Supplementary Section 2.6 about the value of active reading, what it involves, and how the given reading activity might support active reading). They then received more detailed task instructions describing specific strategies, which were followed by a video demonstration of the task and interface. The suggested strategies were based on the active reading and comprehension literature[29;35;36;66]. The content and wording of the instructions for the three conditions were kept as similar as possible. Once the task started, students needed to remain on the task page for 10 (minimum) to 15 (maximum) minutes.\n\nEach student read two expository text passages. Each passage covered a single topic which was included in at least one of the UK exam boards' GCSE History specifications: Apartheid in South Africa (Passage A) and The Cuban Missile Crisis (Passage B). The passages were adapted from two OpenStax textbooks (World History, Volume 2: from 1400; U.S. History). Substantial adaptations were made to ensure that the content and language difficulty as well as text features were comparable and appropriate for Year 10 students. Passages A and B had four paragraphs each and were nearly equal length (386 and 385 words), average word length (5.3 and 4.8 characters), word complexity (i.e., the average position of the words in the 10,000 most frequent English words list, 1986 and 1927), number of sentences (both 26) and CEFR level (both C1 – upper intermediate).\n\nTable 2: Question types and scoring for literal retention, comprehension, and free recall  \n\n<table><tr><td>Outcome</td><td>Question Type (N Questions per Text)</td><td>Scoring</td><td>Maximum score</td></tr><tr><td rowspan=\"2\">Literal retention</td><td>Short response - Cued recall (8)</td><td>For each literal piece of information:0 - missing, incorrect or irrelevant0.5 - incomplete or partially correct1 - correct</td><td>10</td></tr><tr><td>Multiple choice with four response options - Recognition (10)</td><td>0 - missing or incorrect1 - correct</td><td>10</td></tr><tr><td>Comprehension</td><td>Short response - Cued recall (3)</td><td>For each idea:0 - missing, incorrect or irrelevant0.5 - incomplete or partially correct1 - correct</td><td>12</td></tr><tr><td>Free recall</td><td>Open response (1)</td><td>For each literal piece of information/idea:0 - incorrect or irrelevant0.5 - incomplete or partially correct1 - correct</td><td>50</td></tr></table>\n\nNote: Two of the eight \"Short response - Cued recall\" questions for literal retention are worth two points each.\n\nWe divided each passage into 50 main ideas to ensure comparability and to aid scoring.\n\n# Test task and materials (Session 2)\n\nIn the test session, students were told that they would answer some questions about the passages they read in Session 1 as well as some general questions about the task and themselves. For each passage, there were 22 test questions assessing literal retention, comprehension and free recall. Table2 provides an overview of how the different constructs were assessed. As pre-registered, we used a single literal retention score, which was the sum of the short response and multiple-choice scores. The question order for both passages was free response, comprehension, literal retention (cued recall) and, finally, literal retention (recognition). Students had to spend at least three minutes and a maximum of five minutes on the free-recall questions. Questions were carefully sequenced and separated by screens where needed to avoid that previous questions would provide cues for later questions. Example questions can be found in Supplementary Table 11.\n\nLiteral retention questions required literal recall or recognition of information from the passage to provide a correct response. In order to succeed, students did not need background knowledge beyond understanding the vocabulary used in the passage. They did not need to make any knowledge-based inferences (elaborations), and no or only minimal text-based (bridging) inferences, such as connecting two consecutive sentences. Accordingly, literal retention questions targeted the surface and textbase level of representation.\n\nIn contrast, comprehension questions probed for deeper comprehension as they required students to make bridging inferences to connect information from several different locations in the text. Participants needed to make knowledge-based inferences to earn more points, inferring information that was implied but not explicitly stated. Accordingly, comprehension questions targeted the situation-model level of representation.\n\nThe short response and open response questions were scored by three independent raters who were PhD students in Education and/or Psychology who were blind to condition. They were trained to use a scoring scheme that provided general instructions, rules, and detailed explanations and examples for each question. As part of the training, and to demonstrate consistent and accurate use of the scheme, raters scored responses from 25 students and received feedback. Each rater then independently scored the full set of responses, including the questions for both passages, from approximately 140 students.\n\nTo assess inter-rater reliability, the full set of responses from 35 students (approximately  $10\\%$  of the sample) was scored by all three raters. Reliability was evaluated using the intraclass-correlation coefficient (ICC) with a two-way model<sup>67</sup>. We measured absolute agreement and applied the single\n\nmeasure approach as we ultimately used scores from a single rater for all but the 35 students in the reliability sample. For those students, we used the median of the three ratings in subsequent analyses. The inter-rater reliabilities for the combined cued-recall retention scores (one for Passage A and one for Passage B), the combined comprehension scores, and the free recall scores ranged between .97 and .99, indicating excellent reliability $^{67}$ . The lower bounds of the  $95\\%$  confidence intervals were all above the .90 threshold for excellent reliability (see Supplementary Table 12).\n\n# Survey questions\n\nAll questions and response scales can be found in Supplementary Section 2.9. After each task in Session 1, students were asked to self-report on: the difficulty of the text and their familiarity with, and interest in, the topic; enjoyment, difficulty, and helpfulness of the learning activity, and likelihood of its future use; and the overall interest in the task, effort expenditure, and perceived task performance. Students were also asked to indicate whether they preferred any of the learning activities and why, whether they had ever used AI chatbots and if so, with what frequency, and, lastly, how often they had used these learning activities when reading a text for school.\n\nAfter each test in Session 2, students were asked to rate their perceived test performance. At the end of the session, they were asked to indicate whether they had engaged in any learning related to the two texts in between sessions. Students were also asked to report their gender, their English language status, and whether they were taking GCSE History.\n\nIn addition, Free School Meals (FSM) eligibility data was obtained from schools as a measure of student socioeconomic disadvantage $^{68}$ . This is because eligibility for FSM is typically based on family income and other socioeconomic factors.\n\n# Analytic strategies\n\nWe did not deviate from our pre-registered analyses other than described here. First, we extended analyses to conduct qualitative analyses exploring why students preferred one learning activity over another. Second, while we initially planned to explore interaction effects between learning conditions and Gender, EAL, FSM, History GCSE, and School type, we did not do so given our smaller than planned sample size.\n\nQuantitative analyses were run with Python 3.11 and R 4.4.2. We used a significance level of 0.05 (two-tailed) for all analyses. Effect sizes were estimated using Cohen's d, calculated as the mean difference divided by the standard deviation of paired differences for each variable.\n\n# Estimation of condition effects on text comprehension and retention\n\nMissing data handling There were no missing data on the dependent variables because participants were excluded if they did not complete both tests (see exclusion criteria) and because any missing responses on individual questions were scored as 0 points. Missingness in covariates was minimal and only occurred for the variables Gender, EAL and History GCSE  $(5.23\\%, 1.16\\%$  and  $1.16\\%$ , respectively). Missing data were handled using multiple imputation by chained equations (MICE) using the 'mice' package. Models were fitted on five imputed datasets and the results were pooled for combined estimates.\n\nMixed-effects regression We ran three linear mixed-effects regression models using the 'lme4' package, one for each outcome (i.e., literal retention, comprehension, free recall), where students were modelled as a random effect. Note that we pre-registered the regression for free recall as a secondary analysis but we are reporting it alongside the other outcomes for simplicity. The regression specification was as follows:\n\n$$\n\\begin{array}{l} Y _ {i j} = \\beta_ {0} + \\beta_ {1} \\text {C o n d i t i o n} _ {i j} + \\beta_ {2} \\text {G r o u p} _ {i j} + \\beta_ {3} \\text {S c h o o l} _ {i j} + \\beta_ {4} \\text {T e x t} _ {i j} + \\beta_ {5} \\text {T a k} _ {-} \\text {O r d e r} _ {i j} \\\\ + \\beta_ {6} \\text {T e s t} _ {-} \\text {O r d e r} _ {i j} + \\beta_ {7} \\text {G e n d e r} _ {i j} + \\beta_ {8} \\text {F S M} _ {i j} + \\beta_ {9} \\text {E A L} _ {i j} + \\beta_ {1 0} \\text {H i s t o r y} _ {i j} + u _ {i j} + \\epsilon_ {i j} \\\\ \\end{array}\n$$\n\nWhere:\n\n-  $Y_{ij}$  represents the outcome for student  $i$  in condition  $j$ .  \n-  $\\beta_0$  represents the intercept of the model.  \n-  $\\beta_{1}$  to  $\\beta_{10}$  represent the coefficients for the fixed effects:\n\n- Condition: A categorical variable with three levels (0 = LLM, 1 = Notes, 2 = LLM+Notes).  \n- Group: A binary variable indicating group membership.  \n- School: A categorical variable with seven levels indicating school membership.  \n- Text: A binary variable indicating which text student  $i$  studied in condition  $j$ .  \n- Task order: A binary variable indicating whether student  $i$  did condition  $j$  first or second.  \n- Test order: A binary variable indicating whether the text was tested first or second.  \n- Gender: A categorical variable with four levels (0 = female, 1 = male, 2 = other, 3 = prefer not to say).  \n- FSM: A binary variable indicating whether the student received free school meals or not.  \n- EAL: A categorical variable indicating students' English language status (0 = first language, 1 = bilingual, 2 = other)  \n- History: A binary variable indicating whether or not students take History GCSEs.\n\n-  $u_{ij}$  represents the random intercept for each student.  \n-  $\\epsilon_{ij}$  represents the error term for student  $i$  in condition  $j$ .\n\nAs depicted in Figure 1, free recall scores were non-normally distributed, so we ran additional non-parametric permutation tests. Specifically, we used the 'infer' package in R to conduct paired permutation tests at the student level. These tests compared free recall scores between the LLM and Notes conditions in Group 1, and between the LLM and LLM+Notes conditions in Group 2. For each student, we calculated the difference between their two scores and averaged these differences across students. This test statistic was compared to a null distribution, generated by repeatedly randomising the signs of within-student differences and computing means. The process was repeated across all instances of imputed data, and the results were summarised by taking the median p-value across instances to yield a pooled p-value. Doing so gives similar findings to the mixed effects model: in Group 1 we find a significant difference for free recall between the Notes and LLM conditions  $(p = 0.02)$ , but do not find evidence for a significant difference in free recall for Group 2 between the LLM+Notes vs. LLM conditions  $(p = 0.80)$ .\n\n# Qualitative exploration of student prompts\n\nTo provide potential explanations for the effects of the LLM condition on reading comprehension and retention, we sought to understand what kind of prompts students made when using the LLM in planned exploratory analyses. The LLM prompts were analysed using a hierarchical coding scheme through GPT-4 in an automated Python script accessing the Azure OpenAI's API (deployment dated 2024-06-01). Temperature was set to 0 for deterministic outputs with a narrow sampling range (top-p=0.1) to ensure consistent classifications. The model was provided with detailed instructions and examples for each category, along with both texts that students were studying. Each prompt could receive multiple sub-codes.\n\nThe hierarchical coding scheme was developed through several iterations. The initial version was deductively and inductively developed by a researcher using active reading literature, students' task instructions, and piloting work. This scheme was expanded based on the API's suggestions and the API was then asked to code the data using the coding scheme. The researchers then iteratively refined the coding scheme based on checking portions of the API output. They merged, deleted, and added codes as needed and adapted code descriptions and examples to improve the quality of the API output. Finally, one of the researchers manually checked the API output for 500 prompts (approximately  $10\\%$  of the data) and found an error rate of  $5.6\\%$ . This was deemed to be an acceptable level. The assigned codes for these 500 prompts were adjusted where necessary, and the rest of the API output was left as it was. The final coding schemes for student prompts can be found in Supplementary Table 20.\n\n# Quantitative exploration of students' learning experience\n\nAs planned we explored a range of variables capturing students' learning experiences. More specifically, we compared students' learning experiences when using LLM vs. Notes and LLM vs. LLM+Notes using paired  $t$ -tests. We applied Bonferroni corrections to adjust for multiple comparisons. The  $t$ -tests were conducted using the 'tidyverse' package.\n\n# Qualitative exploration of students' activity preferences\n\nWe explored students' open response explanations for preferring one learning activity over another. The explanations were analysed by two of the authors with help from the API described above. Four preference groups were separately analysed:\n\n1. LLM over Notes,  \n2. Notes over LLM,  \n3. LLM over  $\\mathrm{LLM} + \\mathrm{Notes}$ , and  \n4. LLM+Notes over LLM.\n\nEach preference group had its own coding scheme which only included explanations for preferring the favoured activity over the non-favoured activity (i.e., benefits of note-taking were not coded if the student preferred the LLM over Notes). The initial schemes were developed by manually and deductively coding approximately  $30\\%$  of responses of each preference group. Several codes could be applied to each response. The initial coding schemes, including the category label, description and examples were provided to the API alongside the data and general coding instructions. The API did not suggest any further helpful codes. The researchers then iteratively refined the coding schemes by manually checking portions of the API output. They merged, deleted, and added codes as well as refined code descriptions and examples before the API analysis was rerun. This process was repeated until both researchers were satisfied with the coding schemes. Due to the\n\nsmall number of responses that had to be coded ( $n = 278$ ), one researcher checked the entire API output and made adjustments where necessary. The final coding schemes for activity preferences can be found in Supplementary Section 2.11.\n\n# Data availability\n\nAll quantitative data will be made available upon publication. We will not provide the following qualitative data as that would risk sharing identifiable information: Students' LLM interactions (only the applied codes will be shared), students' notes, students' activity preferences (only applied codes will be shared).\n\n# Code availability\n\nThe corresponding code will be shared upon publication.\n\n# Ethics declarations\n\n# Competing interests\n\nSome of the authors conduct research at a company that invests in generative AI and develops technology using generative AI models as a core component. The other authors are part of a publishing, assessment and learning organisation which increasingly uses AI in developing and operating assessment and learning products and services. However, this work is not connected to any specific product or monetisation efforts for either organisation.\n\n# Acknowledgements\n\nWe thank Dr Tom Benton and Dr Matthew Carroll for their valuable advice on the analyses conducted in this study.\n\n# Supplementary Material\n\n# Table of Contents\n\n# Supplementary Information\n\n- Participant Exclusion Criteria\n\n# Supplementary Tables\n\n- Student Characteristics  \nFamiliarity with Learning Activities  \n- Descriptive Statistics  \n- Mixed Effects Regression Results  \nBehavioural Engagement  \n- Introduction to Active Reading  \n- Introduction to Learning Activity\n\n- Specific instructions by Condition  \nTest Questions  \n- Inter-rater Reliability Results  \nSurvey Questions and Response Scales  \nSurvey Questions and Response Scales (session 2)  \n- Learning Experiences and Perceptions  \nCoding Scheme Activity Preferences  \nCoding scheme: LLM over Notes preferences  \nCoding scheme: Notes over LLM preferences  \nCoding scheme: LLM+Notes over LLM preferences  \nCoding Scheme Prompt Interactions  \n- Frequencies of Prompt Types\n\n# References\n\n[1] Cecilia Ka Yuk Chan. A comprehensive AI policy education framework for university teaching and learning. International Journal of Educational Technology in Higher Education, 20(1):38, July 2023. ISSN 2365-9440. doi: 10.1186/s41239-023-00408-3. URL https://doi.org/10.1186/s41239-023-00408-3.  \n[2] Abdulhadi Shoufan. Exploring Students' Perceptions of ChatGPT: Thematic Analysis and Follow-Up Survey. IEEE Access, 11:38805-38818, 2023. ISSN 2169-3536. doi: 10.1109/ACCESS.2023.3268224. URL https://ieeexplore.ieee.org/document/10105236/?arnumber=10105236. Conference Name: IEEE Access.  \n[3] K. Aleksić-Maslac, F. Borović, and Z. Biočina. PERCEPTION AND USAGE OFchat GPT IN THE EDUCATION SYSTEM. INTED2024 Proceedings, pages 1842-1848, 2024. ISSN 2340-1079. doi: 10.21125/inted.2024.0511. URL https://library.iated.org/view/ ALEKSICMASLAC2024PER. Conference Name: 18th International Technology, Education and Development Conference ISBN: 9788409592159 Meeting Name: 18th International Technology, Education and Development Conference Place: Valencia, Spain Publisher: IATED.  \n[4] Nikhil Singh, Guillermo Bernal, Daria Savchenko, and Elena L. Glassman. Where to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal Machine Intelligence. ACM Transactions on Computer-Human Interaction, February 2022. ISSN 1073-0516. doi: 10.1145/3511599. URL https://dl.acm.org/doi/10.1145/3511599. Just Accepted.  \n[5] Heather Johnston, Rebecca F. Wells, Elizabeth M. Shanks, Timothy Boey, and Bryony N. Parsons. Student perspectives on the use of generative artificial intelligence technologies in higher education. International Journal for Educational Integrity, 20(1):2, February 2024. ISSN 1833-2595. doi: 10.1007/s40979-024-00149-4. URL https://doi.org/10.1007/s40979-024-00149-4.\n\n[6] Duong Hoai Lan and Tran Minh Tung. Analyzing the Impact of Chat-GPT Usage by University Students in Vietnam. Migration Letters, 20(S10):259-268, November 2023. ISSN 1741-8992. doi: 10.59670/ml.v20iS10.5134. URL https://migrationletters.com/index.php/ml/article/view/5134. Number: S10.  \n[7] Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnmann, Eyke Hüllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci. ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 2023.  \n[8] Stefan E. Huber, Kristian Kiili, Steve Nebel, Richard M. Ryan, Michael Sailer, and Manuel Ninaus. Leveraging the Potential of Large Language Models in Education Through Playful and Game-Based Learning. Educational Psychology Review, 36(1):25, February 2024. ISSN 1573-336X. doi: 10.1007/s10648-024-09868-z. URL https://doi.org/10.1007/s10648-024-09868-z.  \n[9] Yogesh K. Dwivedi, Nir Kshetri, Laurie Hughes, Emma Louise Slade, Anand Jeyaraj, Arpan Kumar Kar, Abdullah M. Baabdullah, Alex Koohang, Vishnupriya Raghavan, Manju Ahuja, Hanaa Albanna, Mousa Ahmad Albashrawi, Adil S. Al-Busaidi, Janarthanan Balakrishnan, Yves Barlette, Sriparna Basu, Indranil Bose, Laurence Brooks, Dimitrios Buhalis, Lemuria Carter, Soumyadeb Chowdhury, Tom Crick, Scott W. Cunningham, Gareth H. Davies, Robert M. Davison, Rahul De, Denis Dennehy, Yanqing Duan, Rameshwar Dubey, Rohita Dwivedi, John S. Edwards, Carlos Flavian, Robin Gauld, Varun Grover, Mei-Chih Hu, Marijn Janssen, Paul Jones, Iris Junglas, Sangeeta Khorana, Sascha Kraus, Kai R. Larsen, Paul Latreille, Sven Laumer, F. Tegwen Malik, Abbas Mardani, Marcello Mariani, Sunil Mithas, Emmanuel Mogaji, Jeretta Horn Nord, Siobhan O'Connor, Fevzi Okumus, Margherita Pagani, Neeraj Pandey, Savvas Papagiannidis, Ilias O. Pappas, Nishith Pathak, Jan Pries-Heje, Ramakrishnan Raman, Nripendra P. Rana, Sven-Volker Rehm, Samuel Ribeiro-Navarrete, Alexander Richter, Frantz Rowe, Suprateek Sarker, Bernd Carsten Stahl, Manoj Kumar Tiwari, Wil van der Aalst, Viswanath Venkatesh, Giampaoloiglia, Michael Wade, Paul Walton, Jochen Wirtz, and Ryan Wright. Opinion Paper: \"So what if ChatGPT wrote it?\" Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71:102642, August 2023. ISSN 0268-4012. doi: 10. 1016/j.ijinfomgt.2023.102642. URL https://www.sciencedirect.com/science/article/ pii/S0268401223000233.  \n[10] Jun-Jie Zhu, Jinyue Jiang, Meiqi Yang, and Zhiyong Jason Ren. ChatGPT and Environmental Research. *Environmental Science & Technology*, 57(46):17667-17670, November 2023. ISSN 0013-936X. doi: 10.1021/acs.est.3c01818. URL https://doi.org/10.1021/acs.est.3c01818. Publisher: American Chemical Society.  \n[11] Alex Barrett and Austin Pack. Not quite eye to A.I.: student and teacher perspectives on the use of generative artificial intelligence in the writing process. International Journal of Educational Technology in Higher Education, 20(1):59, November 2023. ISSN 2365-9440. doi: 10.1186/s41239-023-00427-0. URL https://doi.org/10.1186/s41239-023-00427-0.  \n[12] Aiste Steponenaite and Basel Barakat. Plagiarism in AI Empowered World. In Margherita Antona and Constantine Stephanidis, editors, Universal Access in Human-Computer Interaction, pages 434–442, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-35897-5. doi: 10.1007/978-3-031-35897-5_31.\n\n[13] Ofcom. Online nation 2024 report. Technical report, Ofcom, November 2024. URL https://www.ofcom.org.uk/media-use-and-attitudes/online-habits/online-nation/.  \n[14] Walton Family Foundation. Teachers and Students Embrace ChatGPT for Education. Technical report, Walton Family Foundation, March 2023. URL https://www.waltonfamilyfoundation.org/learning/teachers-and-students-embrace-chatgpt-for-education. Section: Learning.  \n[15] Ruiqi Deng, Maoli Jiang, Xinlu Yu, Yuyan Lu, and Shasha Liu. Does chatgpt enhance student learning? a systematic review and meta-analysis of experimental studies. Computers Education, 227:105224, 2025. ISSN 0360-1315. doi: https://doi.org/10.1016/j.compedu.2024.105224. URL https://www.sciencedirect.com/science/article/pii/S0360131524002380.  \n[16] Jeffrey R. Binder and Rutvik H. Desai. The neurobiology of semantic memory. Trends in Cognitive Sciences, 15(11):527-536, November 2011. ISSN 1879-307X. doi: 10.1016/j.tics.2011.10.001.  \n[17] Danielle S. McNamara and Joe Magliano. Toward a comprehensive model of comprehension. In The psychology of learning and motivation, Vol. 51, The psychology of learning and motivation, pages 297-384. Elsevier Academic Press, San Diego, CA, US, 2009. ISBN 978-0-12-374489-0. doi: 10.1016/S0079-7421(09)51009-2.  \n[18] Walter Kintsch. The role of knowledge in discourse comprehension: A construction-integration model. *Psychological Review*, 95(2):163–182, 1988. ISSN 1939-1471. doi: 10.1037/0033-295X.95.2.163. Place: US Publisher: American Psychological Association.  \n[19] Gregory Hickok and David Poeppel. The cortical organization of speech processing. Nature Reviews Neuroscience, 8(5):393-402, May 2007. ISSN 1471-0048. doi: 10.1038/nrn2113. URL https://www.nature.com/articles/nrn2113. Publisher: Nature Publishing Group.  \n[20] Evelina Fedorenko, Anna A. Ivanova, and Tamar I. Regev. The language network as a natural kind within the broader landscape of the human brain. Nature Reviews Neuroscience, 25 (5):289-312, May 2024. ISSN 1471-0048. doi: 10.1038/s41583-024-00802-4. URL https://www.nature.com/articles/s41583-024-00802-4. Publisher: Nature Publishing Group.  \n[21] Rolf A. Zwaan and Gabriel A. Radvansky. Situation models in language comprehension and memory. *Psychological Bulletin*, 123(2):162–185, 1998. ISSN 1939-1455. doi: 10.1037/0033-2909.123.2.162. Place: US Publisher: American Psychological Association.  \n[22] Junhua Ding, Keliang Chen, Haoming Liu, Lin Huang, Yan Chen, Yingru Lv, Qing Yang, Qihao Guo, Zaizhu Han, and Matthew A. Lambon Ralph. A unified neurocognitive model of semantics language social behaviour and face recognition in semantic dementia. Nature Communications, 11(1):2595, May 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-16089-9. URL https://www.nature.com/articles/s41467-020-16089-9. Publisher: Nature Publishing Group.  \n[23] Kate Cain and Jane Oakhill. Reading Comprehension Difficulties: Correlates, Causes, and Consequences. In Children's comprehension problems in oral and written language: A cognitive perspective, Challenges in language and literacy, pages 41-75. The Guilford Press, New York, NY, US, 2007. ISBN 978-1-59385-443-0.  \n[24] Meredithyth Daneman and Patricia A. Carpenter. Individual differences in working memory and reading. Journal of Verbal Learning & Verbal Behavior, 19(4):450-466, 1980. ISSN 0022-5371. doi: 10.1016/S0022-5371(80)90312-6. Place: Netherlands Publisher: Elsevier Science.\n\n[25] Charles A. Perfetti, Nicole Landi, and Jane Oakhill. The Acquisition of Reading Comprehension Skill. In *The science of reading: A handbook*, Blackwell handbooks of developmental psychology, pages 227-247. Blackwell Publishing, Malden, 2005. ISBN 978-1-4051-1488-2. doi: 10.1002/9780470757642.ch13.  \n[26] Jane V. Oakhill, Molly S. Berenhaus, and Kate Cain. Children's reading comprehension and comprehension difficulties. In *The Oxford handbook of reading*, Oxford library of psychology, pages 344-360. Oxford University Press, New York, NY, US, 2015. ISBN 978-0-19-932457-6. doi: 10.1093/oxfordhb/9780199324576.001.0001.  \n[27] Keith E. Stanovich. Matthew effects in reading: Some consequences of individual differences in the acquisition of literacy. Reading Research Quarterly, 21(4):360-407, 1986. ISSN 1936-2722. doi: 10.1598/RRQ.21.4.1. Place: US Publisher: International Reading Association.  \n[28] A. C. Graesser, M. Singer, and T. Trabasso. Constructing inferences during narrative text comprehension. *Psychological Review*, 101(3):371–395, July 1994. ISSN 0033-295X. doi: 10.1037/0033-295x.101.3.371.  \n[29] Danielle S. McNamara, Irwin B. Levinstein, and Chutima Boonthum. iSTART: Interactive strategy training for active reading and thinking. Behavior Research Methods, Instruments, 3 Computers, 36(2):222-233, May 2004. ISSN 1532-5970. doi: 10.3758/BF03195567. URL https://doi.org/10.3758/BF03195567.  \n[30] John T. Guthrie and Allan Wigfield. Engagement and motivation in reading. In Handbook of reading research, Vol. III, pages 403-422. Lawrence Erlbaum Associates Publishers, Mahwah, NJ, US, 2000. ISBN 978-0-8058-2398-1 978-0-8058-2399-8.  \n[31] Tracy Linderholm, Sandra Virtue, Yuhtsuen Tzeng, and Paul van den Broek. Fluctuations in the Availability of Information During Reading: Capturing Cognitive Processes Using the Landscape Model. pages 165-186. December 2018. ISBN 978-1-315-04610-5. doi: 10.4324/9781315046105-5.  \n[32] Fergus I. M. Craik. Levels of processing: Past, present . . . and future? Memory, 10(5-6): 305-318, 2002. ISSN 1464-0686. doi: 10.1080/09658210244000135. Place: United Kingdom Publisher: Taylor & Francis.  \n[33] Fergus I. M. Craik and Endel Tulving. Depth of processing and the retention of words in episodic memory. Journal of Experimental Psychology: General, 104(3):268-294, 1975. ISSN 1939-2222. doi: 10.1037/0096-3445.104.3.268. Place: US Publisher: American Psychological Association.  \n[34] John R. Anderson. A spreading activation theory of memory. Journal of Verbal Learning and Verbal Behavior, 22(3):261-295, June 1983. ISSN 0022-5371. doi: 10.1016/S0022-5371(83)90201-3. URL https://www.sciencedirect.com/science/article/pii/S0022537183902013.  \n[35] Danielle S. McNamara, editor. Reading comprehension strategies: Theories, interventions, and technologies. Lawrence Erlbaum Associates Publishers, Mahwah, NJ, 2007.  \n[36] Michelene T. H. Chi. Active-Constructive-Interactive: A Conceptual Framework for Differentiating Learning Activities. Topics in Cognitive Science, 1(1):73-105, 2009. ISSN 1756-8765. doi: 10.1111/j.1756-8765.2008.01005.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-8765.2008.01005.x. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1756-8765.2008.01005.x.\n\n[37] Rose Luckin, Wayne Holmes, and Laurie B Forcier. Intelligence Unleashed: An argument for AI in Education. Technical report, Open Ideas at Pearson / UCL, 2016. URL https://www.pearson.com/content/dam/corporate/global/pearson-dot-com/files/innovation/Intelligence-Unleashed-Publication.pdf.  \n[38] Wayne Holmes, Maya Bialik, and Charles Fadel. Artificial Intelligence in Education. Promise and Implications for Teaching and Learning. March 2019. ISBN 978-1-79429-370-0.  \n[39] Margherita Bernabei, Silvia Colabianchi, Andrea Falegnami, and Francesco Costantino. Students' use of large language models in engineering education: A case study on technology acceptance, perceptions, efficacy, and detection chances. Computers and Education: Artificial Intelligence, 5:100172, October 2023. doi: 10.1016/j.caeai.2023.100172.  \n[40] Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models. In Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 1, pages 27-43, Lugano and Virtual Event Switzerland, August 2022. ACM. ISBN 978-1-4503-9194-8. doi: 10.1145/3501385.3543957. URL https://dl.acm.org/doi/10.1145/3501385.3543957.  \n[41] Harsh Kumar, David M Rothschild, Daniel G Goldstein, and Jake M Hofman. Math Education With Large Language Models: Peril or Promise? 2023.  \n[42] John Sweller, Jeroen J. G. van Merrienboer, and Fred Paas. Cognitive architecture and instructional design: 20 years later. Educational Psychology Review, 31(2):261-292, 2019. ISSN 1573-336X. doi: 10.1007/s10648-019-09465-5. Place: Germany Publisher: Springer.  \n[43] Richard E. Mayer. Should There Be a Three-Strikes Rule Against Pure Discovery Learning? American Psychologist, 59(1):14-19, 2004. ISSN 1935-990X. doi: 10.1037/0003-066X.59.1.14. Place: US Publisher: American Psychological Association.  \n[44] Fergus I. M. Craik and Robert S. Lockhart. Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal Behavior, 11(6):671-684, December 1972. ISSN 0022-5371. doi: 10.1016/S0022-5371(72)80001-X. URL https://www.sciencedirect.com/science/article/pii/S002253717280001X.  \n[45] Xiaoming Zhai, Matthew Nyaaba, and Wenchao Ma. Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?, January 2024. URL http://arxiv.org/abs/2401.15081. arXiv:2401.15081.  \n[46] Faycal Farhi, Riadh Jeljeli, Ibtehal Aburezeq, Fawzi Fayez Dweikat, Samer Ali Al-shami, and Radouane Slamene. Analyzing the students' views, concerns, and perceived ethics about chat GPT usage. Computers and Education: Artificial Intelligence, 5:100180, January 2023. ISSN 2666-920X. doi: 10.1016/j.caeai.2023.100180. URL https://www.sciencedirect.com/science/article/pii/S2666920X23000590.  \n[47] Hao Yu and Yunyun Guo. Generative artificial intelligence empowers educational reform: current status, issues, and prospects. Frontiers in Education, 8:1183162, June 2023. ISSN 2504-284X. doi: 10.3389/feduc.2023.1183162. URL https://www.frontiersin.org/articles/10.3389/feduc.2023.1183162/full.  \n[48] Elizabeth Ligon Bjork and Robert A. Bjork. Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning. In *Psychology and the real world: Essays illustrating fundamental contributions to society*, pages 56-64. Worth Publishers, New York, NY, US, 2011. ISBN 978-1-4292-3043-8.\n\n[49] Michelene Chi, Stephanie Siler, Heisawn Jeong, Takashi Yamauchi, and Robert Hausmann. Learning from human tutoring. Cognitive Science, 25:471-533, July 2001. doi: 10.1016/S0364-0213(01)00044-1.  \n[50] Alvaro Pascual-Leone, Amir Amedi, Felipe Fregni, and Lotfi B. Merabet. The plastic human brain cortex. Annual Review of Neuroscience, 28:377-401, 2005. ISSN 0147-006X. doi: 10.1146/annurev.neuro.27.070203.144216.  \n[51] S. Dehaene and L. Naccache. Towards a cognitive neuroscience of consciousness: basic evidence and a workspace framework. Cognition, 79(1-2):1-37, April 2001. ISSN 0010-0277. doi: 10.1016/s0010-0277(00)00123-2.  \n[52] Keiichi Kobayashi. What limits the encoding eVect of note-taking? A meta-analytic examination. Contemporary Educational Psychology, 2005.  \n[53] Kenneth A. Kiewra. A review of note-taking: The encoding storage paradigm and beyond. Educational Psychology Review, 1(2):147-172, 1989. ISSN 1573-336X. doi: 10.1007/BF01326640. Place: Germany Publisher: Springer.  \n[54] Kenneth A. Kiewra. Investigating notetaking and review: A depth of processing alternative. Educational Psychologist, 20(1):23-32, 1985. ISSN 1532-6985. doi: 10.1207/s15326985ep2001_4. Place: US Publisher: Lawrence Erlbaum.  \n[55] Mark Bohay, Daniel P. Blakely, Andrea K. Tamplin, and Gabriel A. Radvansky. Note taking, review, memory, and comprehension. The American Journal of Psychology, 124(1):63-73, 2011. ISSN 0002-9556. doi: 10.5406/amerjpsyc.124.1.0063.  \n[56] Dung C. Bui and Joel Myerson. The role of working memory abilities in lecture note-taking. Learning and Individual Differences, 33:12-22, 2014. ISSN 1873-3425. doi: 10.1016/j.lindif.2014.05.002. Place: Netherlands Publisher: Elsevier Science.  \n[57] Ralf Rummer, Judith Schweppe, Kathleen Gerst, and Simon Wagner. Is testing a more effective learning strategy than note-taking? Journal of Experimental Psychology. Applied, 23(3):293-300, September 2017. ISSN 1939-2192. doi: 10.1037/xap0000134.  \n[58] Lisa Geraci, Nikhil Kurpad, Rachel Tirso, Kathryn N. Gray, and Yuxiang Wang. Metacognitive errors in the classroom: The role of variability of past performance on exam prediction accuracy. *Metacognition and Learning*, 2022. doi: 10.1007/s11409-022-09326-7. URL https://doi.org/10.1007/s11409-022-09326-7. Advance online publication.  \n[59] Robert A. Bjork, John Dunlosky, and Nate Kornell. Self-Regulated Learning: Beliefs, Techniques, and Illusions. Annual Review of Psychology, 64(1):417-444, January 2013. ISSN 0066-4308, 1545-2085. doi: 10.1146/annurev-psych-113011-143823. URL https://www.annualreviews.org/doi/10.1146/annurev-psych-113011-143823.  \n[60] Justin Kruger and David Dunning. Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments. Journal of Personality and Social Psychology, 77(6):1121-1134, Dec 1999. doi: 10.1037//0022-3514.77.6.1121.  \n[61] Lev Tankelevitch, Viktor Kewenig, Auste Simkute, Ava Elizabeth Scott, Advait Sarkar, Abigail Sellen, and Sean Rintel. The metacognitive demands and opportunities of generative ai. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703300. doi: 10.1145/3613904.3642902. URL https://doi.org/10.1145/3613904.3642902.\n\n[62] Axel Grund, Stefan Fries, Matthias Nückles, Alexander Renkl, and Julian Roelle. When is Learning \"Effortful\"? Scrutinizing the Concept of Mental Effort in Cognitively Oriented Research from a Motivational Perspective. Educational Psychology Review, 36(1):11, March 2024. ISSN 1040-726X, 1573-336X. doi: 10.1007/s10648-024-09852-7. URL https://link.springer.com/10.1007/s10648-024-09852-7.  \n[63] Louise Starkey. A review of research exploring teacher preparation for the digital age. Cambridge Journal of Education, 50(1):37-56, 2020. doi: 10.1080/0305764X.2019.1625867.  \n[64] Honghong Wang and Weiping Shi. Practical approaches to integrated values education for foreign language majors. Foreign Language World, (6):38-45, 2021.  \n[65] British Educational Research Association. Ethical Guidelines for Educational Research, fourth edition, 2018. URL https://www.bera.ac.uk/publication/ethical-guidelines-for-educational-research-2018.  \n[66] P. David Pearson, Laura R. Roehler, Janice A. Dole, and Gerald G. Duffy. Developing expertise in reading comprehension: What should be taught? How should it be taught? Technical Report 512, University of Illinois Urbana-Champaign Center for the Study of Reading, 1990. URL https://hdl.handle.net/2142/17648. Publisher: Champaign, Ill.: University of Illinois at Urbana-Champaign, Center for the Study of Reading.  \n[67] Terry K Koo and Mae Y Li. A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research. 2016.  \n[68] Chris Taylor. The reliability of free school meal eligibility as a measure of socio-economic disadvantage: Evidence from the millennium cohort study in wales. *British Journal of Educational Studies*, 66(1):29-51, 2018. doi: 10.1080/00071005.2017.1330464.\n\n# 1 Supplementary Information\n\n# 1.1 Participant Exclusion Criteria\n\nParticipants  $(n = 61)$  were excluded for the following reasons:\n\n1. Did not take part in Session 2 (n=36)  \n2. Did not complete both tasks in Session 1 (and/or withdrew intentionally)  $(n = 2)$  \n3. Stopped Session 2 before attempting all comprehension and retention questions  $(n = 8)$  \n4. Completed Session 2 in 10 minutes or less  $(n = 1)$  \n5. Reported substantially different prior knowledge of the two topics (3-point difference on a 5-point Likert-scale item)  $(n = 13)$  \n6. Cheated during a session (as observed by researcher, including opening a different browser to look up answers, copying answers from others, continuing conversation with neighbours). Responses of suspicious students were scanned and compared with that of other students in the same group. If suspicion confirmed based on responses (e.g., high overlap with a student), these were excluded  $(n = 1)$\n\n# 2 Supplementary Tables\n\n# 2.1 Student Characteristics\n\nTable 3: Student characteristics by group and overall totals (after exclusion,  $\\mathrm{N} = {344}$  )  \n\n<table><tr><td>Characteristic</td><td>Group 1\nN students (%)</td><td>Group 2\nN students (%)</td><td>Total\nN students (%)</td></tr><tr><td>Male</td><td>102 (29.7%)</td><td>78 (22.7%)</td><td>180 (52.3%)</td></tr><tr><td>Female</td><td>57 (16.6%)</td><td>63 (18.3%)</td><td>120 (34.9%)</td></tr><tr><td>Other</td><td>1 (0.3%)</td><td>1 (0.3%)</td><td>2 (0.6%)</td></tr><tr><td>Prefer not to say</td><td>2 (0.6%)</td><td>0 (0.0%)</td><td>2 (0.6%)</td></tr><tr><td>FSM_Yes</td><td>9 (2.6%)</td><td>10 (2.9%)</td><td>19 (5.5%)</td></tr><tr><td>FSM_No</td><td>160 (46.5%)</td><td>163 (47.4%)</td><td>323 (93.9%)</td></tr><tr><td>EAL_Yes</td><td>130 (37.8%)</td><td>117 (34.0%)</td><td>247 (71.8%)</td></tr><tr><td>EAL_Other Language</td><td>2 (0.6%)</td><td>3 (0.9%)</td><td>5 (1.5%)</td></tr><tr><td>EAL_Bilingual</td><td>35 (10.2%)</td><td>29 (8.4%)</td><td>64 (18.6%)</td></tr><tr><td>History_Yes</td><td>99 (28.8%)</td><td>80 (23.3%)</td><td>179 (52.0%)</td></tr><tr><td>History_No</td><td>81 (23.5%)</td><td>58 (16.9%)</td><td>139 (40.4%)</td></tr></table>\n\n# 2.2 Familiarity with Learning Activities\n\nTable 4: Frequencies of prior learning activity use  \n\n<table><tr><td>Activity and frequency</td><td>Group 1\nN students (%)</td><td>Group 2\nN students (%)</td></tr><tr><td colspan=\"3\">Note-taking for learning</td></tr><tr><td>Never</td><td>7 (3.8%)</td><td>6 (3.8%)</td></tr><tr><td>Rarely</td><td>34 (18.5%)</td><td>25 (15.6%)</td></tr><tr><td>Sometimes</td><td>47 (25.5%)</td><td>44 (27.5%)</td></tr><tr><td>Often</td><td>69 (37.5%)</td><td>70 (43.8%)</td></tr><tr><td>Always</td><td>22 (12.0%)</td><td>17 (10.6%)</td></tr><tr><td colspan=\"3\">LLM use for learning</td></tr><tr><td>Never</td><td>32 (25.6%)</td><td>19 (18.1%)</td></tr><tr><td>Rarely</td><td>45 (36.0%)</td><td>44 (41.9%)</td></tr><tr><td>Sometimes</td><td>29 (23.2%)</td><td>26 (24.8%)</td></tr><tr><td>Often</td><td>15 (12.0%)</td><td>15 (14.3%)</td></tr><tr><td>Always</td><td>4 (3.2%)</td><td>1 (1.0%)</td></tr><tr><td colspan=\"3\">LLM + Notes for learning</td></tr><tr><td>Never</td><td>-</td><td>1 (1.6%)</td></tr><tr><td>Rarely</td><td>-</td><td>31 (48.4%)</td></tr><tr><td>Sometimes</td><td>-</td><td>23 (35.9%)</td></tr><tr><td>Often</td><td>-</td><td>8 (12.5%)</td></tr><tr><td>Always</td><td>-</td><td>1 (1.6%)</td></tr><tr><td colspan=\"3\">Prior LLM use</td></tr><tr><td>Yes</td><td>125 (70.2%)</td><td>105 (64.0%)</td></tr><tr><td>No</td><td>53 (29.8%)</td><td>59 (36.0%)</td></tr><tr><td colspan=\"3\">Frequency of LLM use amongst users</td></tr><tr><td>Less than once a week</td><td>74 (59.2%)</td><td>68 (64.8%)</td></tr><tr><td>One or two days a week</td><td>28 (22.4%)</td><td>33 (31.4%)</td></tr><tr><td>Three to five days a week</td><td>11 (8.8%)</td><td>5 (4.8%)</td></tr><tr><td>Most days of the week</td><td>12 (9.6%)</td><td>1 (1.0%)</td></tr></table>\n\n# 2.3 Descriptive Statistics\n\nTable 5: Descriptive statistics for comprehension, literal retention, and free recall across conditions.  \n\n<table><tr><td>Measure</td><td>Condition</td><td>Mean (M)</td><td>Standard Deviation (SD)</td></tr><tr><td rowspan=\"4\">Comprehension (max 12 points)</td><td>Notes</td><td>4.89</td><td>2.52</td></tr><tr><td>LLM + Notes</td><td>4.11</td><td>2.65</td></tr><tr><td>LLM only (Group 1)</td><td>4.00</td><td>2.44</td></tr><tr><td>LLM only (Group 2)</td><td>3.80</td><td>2.47</td></tr><tr><td rowspan=\"4\">Literal retention (max 20 points)</td><td>Notes</td><td>10.8</td><td>4.29</td></tr><tr><td>LLM + Notes</td><td>9.68</td><td>4.83</td></tr><tr><td>LLM only (Group 1)</td><td>8.83</td><td>3.96</td></tr><tr><td>LLM only (Group 2)</td><td>8.95</td><td>4.29</td></tr><tr><td rowspan=\"4\">Free recall (max 50 points)</td><td>Notes</td><td>5.36</td><td>5.49</td></tr><tr><td>LLM Group 1</td><td>4.32</td><td>4.15</td></tr><tr><td>LLM Group 2</td><td>4.32</td><td>4.63</td></tr><tr><td>LLM + Notes</td><td>4.20</td><td>5.07</td></tr></table>\n\n# 2.4 Mixed Effects Regression Results\n\nTable 6: Model coefficients for literal retention, comprehension, and free recall  \n\n<table><tr><td>Term</td><td>Estimate</td><td>Std. Error</td><td>95% CI</td><td>Statistic</td><td>df</td><td>p-value</td><td>d</td></tr><tr><td colspan=\"8\">Literal retention</td></tr><tr><td>Intercept</td><td>8.2429</td><td>0.7966</td><td>[6.68, 9.81]</td><td>10.3476</td><td>489.3004</td><td>7.95 × 10-23</td><td>-</td></tr><tr><td>Condition LLM_notes</td><td>0.5668</td><td>0.2752</td><td>[0.03, 1.11]</td><td>2.0597</td><td>660.4521</td><td>0.0398</td><td>0.132</td></tr><tr><td>Condition notes</td><td>1.9188</td><td>0.2559</td><td>[1.42, 2.42]</td><td>7.4974</td><td>663.2789</td><td>2.09 × 10-13</td><td>0.443</td></tr><tr><td>Group 1</td><td>-0.6147</td><td>0.4155</td><td>[-1.43, 0.20]</td><td>-1.4793</td><td>661.9230</td><td>0.1395</td><td>-0.143</td></tr><tr><td>school_id S03</td><td>-0.8645</td><td>0.5993</td><td>[-2.04, 0.31]</td><td>-1.4424</td><td>638.7162</td><td>0.1497</td><td>-0.198</td></tr><tr><td>school_id S01</td><td>-1.9789</td><td>0.8005</td><td>[-3.55, -0.41]</td><td>-2.4720</td><td>657.4886</td><td>0.0137</td><td>-0.465</td></tr><tr><td>school_id S05</td><td>-0.3908</td><td>0.8562</td><td>[-2.07, 1.29]</td><td>-0.4564</td><td>612.9203</td><td>0.6483</td><td>-0.094</td></tr><tr><td>school_id S02</td><td>1.2932</td><td>0.5514</td><td>[0.21, 2.37]</td><td>2.3452</td><td>643.8234</td><td>0.0193</td><td>0.299</td></tr><tr><td>school_id S07</td><td>2.7561</td><td>1.1408</td><td>[0.52, 4.99]</td><td>2.4160</td><td>663.8251</td><td>0.0160</td><td>0.623</td></tr><tr><td>school_id S04</td><td>-4.7045</td><td>0.8102</td><td>[-6.29, -3.12]</td><td>-5.8067</td><td>641.0030</td><td>1.00 × 10-8</td><td>-1.075</td></tr><tr><td>Text Cuba</td><td>1.5218</td><td>0.1880</td><td>[1.15, 1.89]</td><td>8.0952</td><td>663.5151</td><td>2.74 × 10-15</td><td>0.351</td></tr><tr><td>Task_order 0</td><td>0.2310</td><td>0.1880</td><td>[-0.14, 0.60]</td><td>1.2283</td><td>659.9704</td><td>0.2198</td><td>0.052</td></tr><tr><td>Test_order 0</td><td>0.5186</td><td>0.1875</td><td>[0.15, 0.89]</td><td>2.7656</td><td>663.7540</td><td>0.0058</td><td>0.119</td></tr><tr><td>Gender (Male)</td><td>0.8396</td><td>0.4609</td><td>[-0.06, 1.74]</td><td>1.8217</td><td>335.9448</td><td>0.0694</td><td>0.193</td></tr><tr><td>Gender (Other)</td><td>1.1737</td><td>1.5839</td><td>[-1.93, 4.28]</td><td>0.7410</td><td>187.9029</td><td>0.4596</td><td>0.228</td></tr><tr><td>Gender (Prefer not to say)</td><td>1.7770</td><td>1.4362</td><td>[-1.04, 4.59]</td><td>1.2373</td><td>474.9248</td><td>0.2166</td><td>0.226</td></tr><tr><td>FSM (Yes)</td><td>-0.9135</td><td>0.8574</td><td>[-2.59, 0.77]</td><td>-1.0654</td><td>653.1653</td><td>0.2871</td><td>-0.207</td></tr><tr><td>EAL (Bilingual)</td><td>0.4650</td><td>0.4780</td><td>[-0.47, 1.40]</td><td>0.9728</td><td>645.1354</td><td>0.3310</td><td>0.116</td></tr><tr><td>EAL (Other)</td><td>-0.3369</td><td>1.6161</td><td>[-3.50, 2.83]</td><td>-0.2085</td><td>660.9281</td><td>0.8349</td><td>-0.027</td></tr><tr><td>History (No)</td><td>-1.5365</td><td>0.3832</td><td>[-2.29, -0.79]</td><td>-4.0095</td><td>641.2946</td><td>6.80 × 10-5</td><td>-0.351</td></tr><tr><td colspan=\"8\">Comprehension</td></tr><tr><td>Intercept</td><td>4.0264</td><td>0.4409</td><td>[3.16, 4.89]</td><td>9.1318</td><td>638.9518</td><td>8.77 × 10-19</td><td>-</td></tr><tr><td>Condition LLM_notes</td><td>0.3533</td><td>0.1785</td><td>[0.00, 0.70]</td><td>1.9792</td><td>655.5471</td><td>0.0482</td><td>0.142</td></tr><tr><td>Condition notes</td><td>0.9500</td><td>0.1658</td><td>[0.62, 1.28]</td><td>5.7306</td><td>662.6375</td><td>1.52 × 10-8</td><td>0.382</td></tr><tr><td>Group 1</td><td>-0.0735</td><td>0.2395</td><td>[-0.54, 0.40]</td><td>-0.3068</td><td>657.2449</td><td>0.7591</td><td>-0.033</td></tr><tr><td>school_id S03</td><td>-0.9749</td><td>0.3320</td><td>[-1.63, -0.32]</td><td>-2.9365</td><td>655.1779</td><td>0.0034</td><td>-0.399</td></tr><tr><td>school_id S01</td><td>-1.9371</td><td>0.4438</td><td>[-2.81, -1.07]</td><td>-4.3645</td><td>662.1221</td><td>1.48 × 10-5</td><td>-0.783</td></tr><tr><td>school_id S05</td><td>-0.3167</td><td>0.4735</td><td>[-1.24, 0.61]</td><td>-0.6688</td><td>648.4704</td><td>0.5039</td><td>-0.142</td></tr><tr><td>school_id S02</td><td>0.5254</td><td>0.3052</td><td>[-0.07, 1.12]</td><td>1.7215</td><td>659.5381</td><td>0.0856</td><td>0.201</td></tr><tr><td>school_id S07</td><td>0.9683</td><td>0.6335</td><td>[-0.27, 2.21]</td><td>1.5284</td><td>663.5186</td><td>0.1269</td><td>0.377</td></tr><tr><td>school_id S04</td><td>-2.9725</td><td>0.4493</td><td>[-3.85, -2.09]</td><td>-6.6154</td><td>651.4740</td><td>7.74 × 10-11</td><td>-1.192</td></tr><tr><td>Text Cuba</td><td>-0.6057</td><td>0.1218</td><td>[-0.84, -0.37]</td><td>-4.9727</td><td>662.4076</td><td>8.42 × 10-7</td><td>-0.245</td></tr><tr><td>Task_order 0</td><td>0.0428</td><td>0.1219</td><td>[-0.20, 0.28]</td><td>0.3508</td><td>657.5431</td><td>0.7258</td><td>0.015</td></tr><tr><td>Test_order 0</td><td>0.6679</td><td>0.1215</td><td>[0.43, 0.91]</td><td>5.4958</td><td>662.7896</td><td>5.55 × 10-8</td><td>0.266</td></tr><tr><td>Gender (Male)</td><td>0.2287</td><td>0.2517</td><td>[-0.26, 0.72]</td><td>0.9086</td><td>542.3928</td><td>0.3640</td><td>0.078</td></tr><tr><td>Gender (Other)</td><td>0.0375</td><td>0.9339</td><td>[-1.79, 1.87]</td><td>0.0401</td><td>102.4863</td><td>0.9681</td><td>0.574</td></tr><tr><td>Gender (Prefer not to say)</td><td>1.5360</td><td>0.9257</td><td>[-0.28, 3.35]</td><td>1.6593</td><td>68.4482</td><td>0.1016</td><td>0.006</td></tr><tr><td>FSM (Yes)</td><td>-0.6056</td><td>0.4786</td><td>[-1.54, 0.33]</td><td>-1.2655</td><td>626.0565</td><td>0.2062</td><td>-0.236</td></tr><tr><td>EAL (Bilingual)</td><td>0.5813</td><td>0.2649</td><td>[0.06, 1.10]</td><td>2.1943</td><td>655.2427</td><td>0.0286</td><td>0.228</td></tr><tr><td>EAL (Other)</td><td>-0.2195</td><td>0.9140</td><td>[-2.01, 1.57]</td><td>-0.2402</td><td>556.3704</td><td>0.8103</td><td>-0.103</td></tr><tr><td>History (No)</td><td>-0.6719</td><td>0.2138</td><td>[-1.09, -0.25]</td><td>-3.1423</td><td>613.1612</td><td>0.0018</td><td>-0.262</td></tr><tr><td colspan=\"8\">Free recall</td></tr><tr><td>Intercept</td><td>4.4052</td><td>0.8507</td><td>[2.74, 6.08]</td><td>5.1786</td><td>662.4966</td><td>2.97 × 10-7</td><td>-</td></tr><tr><td>Condition LLM_notes</td><td>-0.0847</td><td>0.4590</td><td>[-0.98, 0.81]</td><td>-0.1846</td><td>661.9195</td><td>0.8536</td><td>-0.015</td></tr><tr><td>Condition notes</td><td>1.0185</td><td>0.4269</td><td>[0.18, 1.86]</td><td>2.3856</td><td>663.2739</td><td>0.0173</td><td>0.211</td></tr><tr><td>Group 1</td><td>-0.2703</td><td>0.4958</td><td>[-1.24, 0.70]</td><td>-0.5452</td><td>662.0547</td><td>0.5858</td><td>-0.058</td></tr><tr><td>school_id S03</td><td>-0.4702</td><td>0.6185</td><td>[-1.68, 0.74]</td><td>-0.7603</td><td>663.5556</td><td>0.4474</td><td>-0.086</td></tr><tr><td>school_id S01</td><td>-0.9612</td><td>0.8290</td><td>[-2.59, 0.66]</td><td>-1.1595</td><td>660.3122</td><td>0.2467</td><td>-0.189</td></tr><tr><td>school_id S05</td><td>2.1564</td><td>0.8819</td><td>[0.43, 3.89]</td><td>2.4452</td><td>662.7977</td><td>0.0147</td><td>0.459</td></tr><tr><td>school_id S02</td><td>2.7874</td><td>0.5687</td><td>[1.67, 3.90]</td><td>4.9012</td><td>663.9081</td><td>1.20 × 10-6</td><td>0.578</td></tr><tr><td>school_id S07</td><td>2.2260</td><td>1.1824</td><td>[-0.09, 4.54]</td><td>1.8827</td><td>663.2415</td><td>0.0602</td><td>0.459</td></tr><tr><td>school_id S04</td><td>-2.3075</td><td>0.8366</td><td>[-3.95, -0.67]</td><td>-2.7583</td><td>663.2134</td><td>0.0060</td><td>-0.468</td></tr><tr><td>Text Cuba</td><td>-0.1187</td><td>0.3137</td><td>[-0.73, 0.50]</td><td>-0.3783</td><td>662.8799</td><td>0.7053</td><td>-0.027</td></tr><tr><td>Task_order 0</td><td>-0.1370</td><td>0.3134</td><td>[-0.75, 0.48]</td><td>-0.4372</td><td>662.9483</td><td>0.6621</td><td>-0.029</td></tr><tr><td>Test_order 0</td><td>-0.3089</td><td>0.3130</td><td>[-0.92, 0.31]</td><td>-0.9870</td><td>663.8172</td><td>0.3240</td><td>-0.062</td></tr><tr><td>Gender (Male)</td><td>0.7972</td><td>0.4653</td><td>[-0.11, 1.71]</td><td>1.7133</td><td>662.1998</td><td>0.0871</td><td>0.178</td></tr><tr><td>Gender (Other)</td><td>1.5025</td><td>1.6550</td><td>[-1.74, 4.75]</td><td>0.9079</td><td>586.1239</td><td>0.3643</td><td>0.336</td></tr><tr><td>Gender (Prefer not to say)</td><td>-0.7067</td><td>1.7223</td><td>[-4.08, 2.67]</td><td>-0.4103</td><td>284.0426</td><td>0.6819</td><td>-0.249</td></tr><tr><td>FSM (Yes)</td><td>-0.0013</td><td>0.8884</td><td>[-1.74, 1.74]</td><td>-0.0014</td><td>660.6054</td><td>0.9886</td><td>0.016</td></tr><tr><td>EAL (Bilingual)</td><td>-0.4993</td><td>0.4958</td><td>[-1.47, 0.47]</td><td>-1.0070</td><td>644.7815</td><td>0.3143</td><td>-0.104</td></tr><tr><td>EAL (Other)</td><td>-0.7021</td><td>1.6974</td><td>[-4.03, 2.62]</td><td>-0.4137</td><td>647.6784</td><td>0.6793</td><td>-0.157</td></tr><tr><td>History (No)</td><td>-1.0261</td><td>0.3967</td><td>[-1.80, -0.25]</td><td>-2.5868</td><td>658.8462</td><td>0.0099</td><td>-0.210</td></tr></table>\n\n# 2.5 Behavioural Engagement\n\nTable 7: Behavioural engagement with the LLM and note-taking, including queries made, words in notes, and time on task. Significant differences in time spent on tasks are highlighted for comparison between conditions.  \n\n<table><tr><td>Measure</td><td>Condition</td><td>Mean (M)</td><td>Standard Deviation (SD)</td></tr><tr><td rowspan=\"3\">Number of queries</td><td>Group 1 (LLM + Notes)</td><td>10.98</td><td>6.46</td></tr><tr><td>Group 2 (LLM only)</td><td>9.21</td><td>5.72</td></tr><tr><td>Group 2 (LLM + Notes)</td><td>6.02</td><td>4.64</td></tr><tr><td rowspan=\"2\">Words in notes</td><td>Group 1 (Notes)</td><td>100.74</td><td>115.63</td></tr><tr><td>Group 2 (LLM + Notes)</td><td>103.83</td><td>158.24</td></tr><tr><td>Trigram overlap (%)</td><td colspan=\"2\">Substantial overlap (≥ 70%)</td><td>25.63%</td></tr><tr><td>Trigram overlap (%)</td><td colspan=\"2\">High overlap (≥ 90%)</td><td>16.25%</td></tr><tr><td rowspan=\"4\">Time on task (minutes)</td><td>Group 1 (LLM)</td><td>-0.80</td><td>95% CI [-1.15, -0.46], d = -0.34</td></tr><tr><td>Group 1 (Notes)</td><td>10-15 range</td><td>-</td></tr><tr><td>Group 2 (LLM only)</td><td>-1.54</td><td>95% CI [-1.91, -1.17], d = -0.66</td></tr><tr><td>Group 2 (LLM + Notes)</td><td>10-15 range</td><td>-</td></tr></table>\n\n# 2.6 Student Task Instructions\n\nTable 8: Introduction to active reading (common across all conditions)  \n\n<table><tr><td>When you are trying to learn and understand a text, active reading can be a useful strategy.\nIt can help you to process the information more deeply and thus to learn better. Active reading\ninvolves:\n· figuring out what the main ideas and concepts in the text are,\n· what they mean,\n· how they relate to each other, and\n· asking questions about the information and then trying to answer them.</td></tr></table>\n\nTable 9: Learning activity introduction by condition  \n\n<table><tr><td>Condition</td><td>Activity introduction</td></tr><tr><td>Notes</td><td>Your task is to try to understand and learn a history text. To do so, please ac- \ntively read the text and take notes to help you. Taking notes is an important \npart of active reading. It is not about copying a lot of information from the text. \nInstead, find the key information in a section, think about what it means, and \nnote it down in your own words.</td></tr><tr><td>LLM</td><td>Your task is to try to understand and learn a history text. To do so, please \nactively read the text and use an AI chatbot to help you. Having a con-\nversation with the AI chatbot might help you to read more actively. You can \nask different questions about the text to help you understand what happened. \nIt may also help you to identify and understand key information.</td></tr><tr><td>LLM+Notes</td><td>Your task is to try to understand and learn a history text. To do so, please \nactively read the text, use an AI chatbot, and take notes to help you. \nHaving a conversation with the AI chatbot might help you to read more actively. \nYou can ask different questions about the text to help you understand what \nhappened. It may also help you to identify and understand key information. \nTaking notes is also important for active reading. It is not about copying a lot \nof information from the text. Instead, find the key information in a section, \nthink about what it means, and note it down in your own words.</td></tr></table>\n\nTable 10: Specific instructions by condition  \n\n<table><tr><td>Condition</td><td>Specific instructions</td></tr><tr><td>Notes</td><td>Actively read the text and take notes as you go along. Even if you think you understand everything, try doing so as best as you can. Think about the following things and note them down to help you:\n· The meaning of important words and concepts\n· The meaning of complex sentences\n· The key points or ideas, such as the dates, places, people and events\n· The connections between places, people and events\n· What happened, and why and how it happened\n· Similarities and differences between ideas and concepts\n· Your understanding of the text</td></tr><tr><td>LLM</td><td>Actively read the text and use the AI chatbot as you go along. Even if you think you understand everything, try doing so as best as you can. Think about the following things and use the AI chatbot to help you. For example, you can use it to:\n· Explain the meaning of important words and concepts\n· Rephrase or simplify complex sentences and explain them\n· Summarise the text and identify the key points or ideas, such as the dates, places, people and events\n· Clarify information you don’t understand\n· Explain the connections between places, people and events\n· Explain what happened, and why and how it happened\n· Identify similarities and differences between ideas and concepts\n· Check your understanding of the text\nYou can also:\n· Ask the AI chatbot for more explanation if you do not understand its response or think that something might not be quite right\n· Ask follow-up questions\n· Ask it to use bullet points, make its answer shorter, or use simpler language</td></tr><tr><td>LLM+Notes</td><td>Actively read the text, use the AI chatbot and take notes as you go along. Even if you think you understand everything, try doing so as best as you can. Think about the following things, and use the AI chatbot and take notes to help you. For example, you can use the AI chatbot to:\n· Explain the meaning of important words and concepts\n· Rephrase or simplify complex sentences and explain them\n· Summarise the text and identify the key points or ideas, such as the dates, places, people and events\n· Clarify information you don’t understand\n· Explain the connections between places, people and events\n· Explain what happened, and why and how it happened\n· Identify similarities and differences between ideas and concepts\n· Check your understanding of the text\n You can also:\n· Ask the AI chatbot for more explanation if you do not understand its response or think that something might not be quite right\n· Ask follow-up questions\n· Ask it to use bullet points, make its answer shorter, or use simpler language</td></tr></table>\n\n# 2.7 Test Questions\n\nTable 11: Example questions for literal retention, comprehension, and free recall  \n\n<table><tr><td>Construct\nItem type</td><td>Example question</td></tr><tr><td colspan=\"2\">Literal retention</td></tr><tr><td>Short response</td><td>What horrific event happened at the Soweto Youth Uprising in 1976? (Passage A)\nWhy did US President Kennedy avoid the term &quot;blockade&quot; when announcing the naval action around Cuba? (Passage B)</td></tr><tr><td>Multiple choice</td><td>What led to violent anti-apartheid protests? (Passage A)\n1) Police forcefully segregating people.\n2) Police arresting Nelson Mandela.\n3) Police killing Black civilians.\n4) Police implementing strict curfews.\nHow did the US government discover the presence of Soviet missiles in Cuba? (Passage B)\n1) A Cuban informant told them about the missiles.\n2) The Cuban government made threats to employ the missiles.\n3) The US Navy intercepted a Soviet ship carrying the missiles.\n4) A US plane captured photos of the missiles.</td></tr><tr><td colspan=\"2\">Comprehension</td></tr><tr><td>Short response</td><td>Explain the role that Nelson Mandela played during apartheid and its eventual end.\nYou only need to write a short paragraph. (Passage A)\nExplain the role of the Soviet Union in the Cuban Missile Crisis.\nYou only need to write a short paragraph. (Passage B)</td></tr><tr><td colspan=\"2\">Free recall</td></tr><tr><td>Open response</td><td>Write down everything you remember from the text &quot;[title]&quot;. Try to include as many details as possible.\nFor example, think about what happened, why and how, when, where, and who was involved.\nYou can write in full sentences or bullet points.</td></tr></table>\n\n# 2.8 Inter-rater Reliability Results\n\nTable 12: Inter-coder reliability  \n\n<table><tr><td>Item</td><td>ICC (A,1)</td><td>p-value</td><td>95% CI</td><td>Item</td><td>ICC (A,1)</td><td>p-value</td><td>95% CI</td></tr><tr><td>1</td><td>0.867</td><td>3.08 × 10-24</td><td>[0.781, 0.925]</td><td>15</td><td>0.923</td><td>2.17 × 10-32</td><td>[0.871, 0.958]</td></tr><tr><td>2</td><td>0.918</td><td>5.77 × 10-32</td><td>[0.863, 0.955]</td><td>16</td><td>0.989</td><td>1.29 × 10-61</td><td>[0.980, 0.994]</td></tr><tr><td>3</td><td>0.967</td><td>1.30 × 10-45</td><td>[0.943, 0.982]</td><td>17</td><td>0.962</td><td>8.52 × 10-43</td><td>[0.935, 0.979]</td></tr><tr><td>4</td><td>0.911</td><td>1.38 × 10-30</td><td>[0.851, 0.951]</td><td>18</td><td>0.961</td><td>4.95 × 10-42</td><td>[0.933, 0.979]</td></tr><tr><td>5</td><td>0.891</td><td>1.92 × 10-27</td><td>[0.819, 0.939]</td><td>19</td><td>0.938</td><td>7.34 × 10-36</td><td>[0.895, 0.966]</td></tr><tr><td>6</td><td>1.000</td><td>NaN</td><td>[NaN, NaN]</td><td>20</td><td>0.963</td><td>8.25 × 10-44</td><td>[0.936, 0.980]</td></tr><tr><td>7</td><td>0.951</td><td>2.65 × 10-39</td><td>[0.916, 0.973]</td><td>21</td><td>0.859</td><td>3.92 × 10-24</td><td>[0.770, 0.921]</td></tr><tr><td>8</td><td>0.936</td><td>2.38 × 10-33</td><td>[0.891, 0.965]</td><td>22</td><td>0.893</td><td>3.34 × 10-27</td><td>[0.822, 0.940]</td></tr><tr><td>9</td><td>0.930</td><td>9.00 × 10-31</td><td>[0.880, 0.962]</td><td>23</td><td>0.953</td><td>2.93 × 10-25</td><td>[0.912, 0.976]</td></tr><tr><td>10</td><td>0.954</td><td>1.88 × 10-39</td><td>[0.921, 0.975]</td><td>24</td><td>0.971</td><td>9.27 × 10-33</td><td>[0.947, 0.985]</td></tr><tr><td>11</td><td>0.920</td><td>1.89 × 10-30</td><td>[0.864, 0.956]</td><td>25</td><td>0.959</td><td>3.71 × 10-39</td><td>[0.928, 0.978]</td></tr><tr><td>12</td><td>0.969</td><td>5.35 × 10-40</td><td>[0.946, 0.984]</td><td>26</td><td>0.988</td><td>1.02 × 10-60</td><td>[0.980, 0.994]</td></tr><tr><td>13</td><td>0.959</td><td>6.30 × 10-42</td><td>[0.930, 0.978]</td><td>27</td><td>0.968</td><td>4.23 × 10-38</td><td>[0.943, 0.983]</td></tr><tr><td>14</td><td>0.927</td><td>2.80 × 10-33</td><td>[0.877, 0.960]</td><td>28</td><td>0.983</td><td>7.93 × 10-56</td><td>[0.971, 0.991]</td></tr></table>\n\n# 2.9 Survey Questions and Response Scales\n\nTable 13: Survey questions and response scales - Session 1  \n\n<table><tr><td>Variable</td><td>Question and response scale</td></tr><tr><td>Text difficulty</td><td>How difficult to understand did you find the text on [Passage title]? \n(Not at all difficult, Not very difficult, Somewhat difficult, Quite difficult, Very difficult)</td></tr><tr><td>Topic familiarity</td><td>How much did you already know about [Passage title] before starting the task? \n(Nothing at all, Not very much, A moderate amount, Quite a bit, Very much)</td></tr><tr><td>Topic interest</td><td>How interesting was the text on [Passage title]? \n(Not at all interesting, Not very interesting, Somewhat interesting, Quite interesting, Very interesting)</td></tr><tr><td>Activity enjoyment</td><td>How enjoyable was learning the text with the help of [activity]? \n(Not at all enjoyable, Not very enjoyable, Somewhat enjoyable, Quite enjoyable, Very enjoyable)</td></tr><tr><td>Activity difficulty</td><td>Overall, how difficult did you find the [activity]? \n(Not at all difficult, Not very difficult, Somewhat difficult, Quite difficult, Very difficult)</td></tr><tr><td>Activity helpfulness</td><td>How helpful was [activity] for understanding and learning the text? \n(Not at all helpful, Not very helpful, Somewhat helpful, Quite helpful, Very helpful)</td></tr><tr><td>Activity future use</td><td>Would you use a similar approach ([activity]) to understand and learn a text in the future? \n(Yes, No, I am not sure)</td></tr><tr><td>Task interest</td><td>How interesting was this task overall? \n(Not at all interesting, Not very interesting, Somewhat interesting, Quite interesting, Very interesting)</td></tr><tr><td>Task effort</td><td>How much effort did you put into understanding and learning the text on [Passage title]? \n(No effort at all, Only a little bit of effort, Some effort, Quite a bit of effort, A lot of effort)</td></tr><tr><td>Perceived task performance</td><td>How well do you think you did on the task? \n(Not at all well, Not very well, Somewhat well, Quite well, Very well)</td></tr><tr><td>Activity preference</td><td>Group 1: Which of the two learning approaches of this study did you prefer (note-taking or AI chatbot)? \n(I preferred learning by note-taking, I preferred learning with the help of the AI chatbot, I had no preference, I am not sure) \nGroup 2: Which of the two learning approaches of this study did you prefer (AI chatbot only or AI chatbot with note-taking)? \n(I preferred learning only with the help of the AI chatbot, I preferred learning with the help of the AI chatbot and by taking notes simultaneously, I had no preference, I am not sure)</td></tr><tr><td>Reason for preference</td><td>Can you tell us why you preferred this approach? [Open response]</td></tr><tr><td>Prior LLM use</td><td>Have you ever used an AI chatbot (such as ChatGPT, Microsoft Bing, and Google Bard AI) before this study? \n(Yes, No)</td></tr><tr><td>LLM use frequency</td><td>How often do you use an AI chatbot (approximately)? \n(Less than once a week, One or two days a week, Three to five days a week, Most days of the week)</td></tr><tr><td>Notes for learning frequency</td><td>How often do you take notes when reading a text for schoolwork, such as to prepare for a lesson or a test? \n(Never, Rarely, Sometimes, Often, Always)</td></tr><tr><td>LLM for learning frequency</td><td>How often do you use an AI chatbot when reading a text for schoolwork, such as to prepare for a lesson or a test? \n(Never, Rarely, Sometimes, Often, Always)</td></tr><tr><td>LLM+Notes for learning frequency</td><td>Group 2 only: How often do you use the two approaches (using an AI chatbot and taking notes) at the same time when reading a text for schoolwork? \n(Never, Rarely, Sometimes, Often, Always)</td></tr></table>\n\nTable 14: Survey questions and response scales - Session 2  \n\n<table><tr><td>Variable</td><td>Item and response categories</td></tr><tr><td>Perceived test performance</td><td>If all the questions on [Passage title] combined were worth a maximum of 100 points, how many points do you think you would have (approximately) scored? [Open response]</td></tr><tr><td>Learning in between sessions</td><td>Have you done anything between the first session and today&#x27;s session to further explore or understand the topics of the two texts? That could include looking up information online, taking notes after the session or discussing the topic with others. If so, please provide as much detail as you can about what you have done. [Open response]</td></tr><tr><td>Gender</td><td>What is your gender? [Open response]</td></tr><tr><td>EAL</td><td>Which language do you feel most comfortable speaking and communicating in?\n(English, A language other than English, Equally English and another language)</td></tr><tr><td>History</td><td>Are you taking GCSE History? (Yes, No)</td></tr></table>\n\n# 2.10 Learning Experiences and Perceptions\n\nTable 15: Differences in learning experiences and perceptions between conditions (for Group 1 and Group 2)  \n\n<table><tr><td rowspan=\"2\">Variable</td><td colspan=\"5\">Group 1: LLM vs Notes</td><td colspan=\"5\">Group 2: LLM vs LLM+Notes</td></tr><tr><td>Diff.</td><td>t(df)</td><td>p</td><td>95% CI</td><td>d</td><td>Diff.</td><td>t(df)</td><td>p</td><td>95% CI</td><td>d</td></tr><tr><td>Activity helpfulness</td><td>0.41</td><td>4.38(181)</td><td>&lt;0.001</td><td>[0.22, 0.59]</td><td>0.33</td><td>-0.03</td><td>-0.35(157)</td><td>0.724</td><td>[-0.21, 0.15]</td><td>-0.03</td></tr><tr><td>Activity difficulty</td><td>-0.51</td><td>-7.00(181)</td><td>&lt;0.001</td><td>[-0.66, -0.37]</td><td>-0.52</td><td>-0.41</td><td>-4.99(159)</td><td>&lt;0.001</td><td>[-0.57, -0.25]</td><td>-0.40</td></tr><tr><td>Task effort</td><td>-0.25</td><td>-3.53(182)</td><td>0.001</td><td>[-0.38, -0.11]</td><td>-0.26</td><td>-0.08</td><td>-1.03(159)</td><td>0.305</td><td>[-0.22, 0.07]</td><td>-0.08</td></tr><tr><td>Activity enjoyment</td><td>0.68</td><td>6.50(181)</td><td>&lt;0.001</td><td>[0.47, 0.89]</td><td>0.48</td><td>0.00</td><td>0.00(158)</td><td>1.000</td><td>[-0.16, 0.16]</td><td>0.00</td></tr><tr><td>Text interest</td><td>-0.11</td><td>-1.38(183)</td><td>0.170</td><td>[-0.26, 0.05]</td><td>-0.10</td><td>0.06</td><td>0.79(159)</td><td>0.428</td><td>[-0.09, 0.22]</td><td>0.06</td></tr><tr><td>Text difficulty</td><td>0.03</td><td>0.50(183)</td><td>0.621</td><td>[-0.10, 0.16]</td><td>0.04</td><td>0.03</td><td>0.41(159)</td><td>0.684</td><td>[-0.10, 0.15]</td><td>0.03</td></tr><tr><td>Task interest</td><td>0.09</td><td>1.01(183)</td><td>0.315</td><td>[-0.09, 0.27]</td><td>0.07</td><td>-0.06</td><td>-0.79(159)</td><td>0.430</td><td>[-0.20, 0.08]</td><td>-0.06</td></tr><tr><td>Perceived task performance</td><td>0.00</td><td>0.00(182)</td><td>1.000</td><td>[-0.14, 0.14]</td><td>0.00</td><td>-0.11</td><td>-1.45(158)</td><td>0.150</td><td>[-0.25, 0.04]</td><td>-0.12</td></tr><tr><td>Perceived test performance</td><td>-9.66</td><td>-5.53(177)</td><td>&lt;0.001</td><td>[-13.11, -6.22]</td><td>-0.42</td><td>-6.80</td><td>-3.55(143)</td><td>0.001</td><td>[-10.59, -3.02]</td><td>-0.30</td></tr></table>\n\n# 2.11 Coding Scheme Activity Preferences\n\nTable 16: Coding scheme: LLM over LLM+Notes preferences  \n\n<table><tr><td>Code</td><td>Description</td><td>Examples</td></tr><tr><td>LLM alone is quicker</td><td>Using the LLM alone is quicker than also taking notes, which takes time.</td><td>“It took less time to use the LLM”, “Notes take too much time.”</td></tr><tr><td>Both together not necessary</td><td>Notes are not necessary when the LLM already explains the text.</td><td>“The note-taking seemedunnec-\nsessary as the bot already helped explain”, “Using one sort of meant I didn’t need the other.”</td></tr><tr><td>LLM does the work for you</td><td>If you use the LLM alone, you don’t have to do the work your-\nself. The task becomes easier if you don’t have to take notes.</td><td>“Didn’t have to do any work”, “Clarify any information I didn’t know immediately without hav-\ning to scour the text”, “It was difficult to take notes at the same time as using the chatbot.”</td></tr><tr><td>Note-taking reduces question time</td><td>Note-taking takes away time from asking the LLM questions or understanding the text.</td><td>“I didn’t have enough time to ask as many questions when taking notes”, “I had more time to un-\nderstand the text.”</td></tr><tr><td>LLM does not support note-taking</td><td>LLM does not make note-taking easier.</td><td>&quot;Not as useful for making note-\ntaking easier.”</td></tr></table>\n\nTable 17: Coding scheme: LLM over Notes preferences  \n\n<table><tr><td>Code</td><td>Description</td><td>Examples</td></tr><tr><td>LLM is quick</td><td>LLM is quick and saves time.</td><td>“Less time-consuming”, “Much quicker.”</td></tr><tr><td>LLM is easy</td><td>LLM is easy and requires little effort compared to note-taking, which takes more effort and is more difficult.</td><td>“More simple”, “It was easier.”</td></tr><tr><td>LLM is (inter)active</td><td>LLM is an interactive or active learning activity.</td><td>“Actively engaging with the bot”, “Felt more interactive.”</td></tr><tr><td>LLM is emotionally engaging</td><td>LLM is more fun, enjoyable, and interesting.</td><td>“Enjoyed reading its responses”, “More fun to use.”</td></tr><tr><td>LLM helps you focus</td><td>LLM helps you focus on the text.</td><td>“Allowed me to focus more on the text.”</td></tr><tr><td>LLM helps you understand</td><td>LLM helps understanding and helps you check your understanding.</td><td>“It gives you a better understanding”, “I could confirm anything I was unsure of to ensure I understood it.”</td></tr><tr><td>LLM helps you learn</td><td>LLM supports learning.</td><td>“The AI helped me to learn more efficiently”, “I was able to understand and learn the text a lot easier and quicker at a higher level.”</td></tr><tr><td>LLM answers questions</td><td>LLM is helpful for understanding because it can answer questions and explain what you don’t understand.</td><td>“Ask any relevant questions”, “If I had a question, it could answer it.”</td></tr><tr><td>LLM can provide background and additional information</td><td>LLM is helpful for understanding because it provides background information and can elaborate on what happens.</td><td>“I was given more background”, “It gives me the full context.”</td></tr><tr><td>LLM can summarise and simplify information</td><td>LLM is helpful for understanding because it can simplify and rephrase information as well as summarise.</td><td>“It puts it in a simpler way and form”, “I can ask the AI chatbot to rephrase key points”, “It can summarise key points.”</td></tr><tr><td>LLM helps you remember</td><td>LLM helps you to remember the information in the text.</td><td>“It has stuck in my head more”, “Giving me prompt questions, mnemonics, etc., which helped me remember”, “Took less time to memorise than note-taking.”</td></tr></table>\n\nTable 18: Coding scheme: Notes over LLM preferences  \n\n<table><tr><td>Code</td><td>Description</td><td>Examples</td></tr><tr><td>Notes help you remember better</td><td>Note-taking helps you to remember information because you are physically writing it down. LLM does not help you remember as well.</td><td>“I can remember things better when I write them down”, “More helpful for developing recall”, “I learned more with note-taking”, “Just gave more background, rather than consolidating the knowledge.”</td></tr><tr><td>Notes help you understand</td><td>Note-taking helps you to understand better and check your understanding.</td><td>“It was easier for me to understand what I was reading”, “I was understanding it more”, “Test what you have learned by paraphrasing.”</td></tr><tr><td>Note-taking is active</td><td>Note-taking is more active.</td><td>“Better active reading”, “Allows me to actively engage.”</td></tr><tr><td>Notes are your own work</td><td>Note-taking means that you do the work yourself. You do the thinking and can use your own words and capture your own views.</td><td>“You have to personally analyse it”, “I could condense the information into my own words”, “Made me think for myself”, “It is your view on the matter you are looking at”, “Alows me to feel proud of my work in the future.”</td></tr><tr><td>Notes help you process information</td><td>Note-taking helps you process the information.</td><td>“I was able to break down and process the text”, “Summarising the second text myself helped me to process the information.”</td></tr><tr><td>Notes help you learn</td><td>Notes help you to learn, capture what you have learned, or test what you have learned.</td><td>“I am able to write down my own knowledge of what I had learned”, “I could actually learn the information rather than being told it.”</td></tr><tr><td>Notes can be revisited</td><td>Notes can be more easily revisited than the LLM output. You can easily access what you have learned or thought so far.</td><td>“I can come back to these notes at a later date if I am doing revision”, “Note-taking gives you something better to look back on in future.”</td></tr><tr><td>Notes are easier</td><td>Note-taking is easier than using the LLM.</td><td>“Easier to summarise”, “IDK, easier.”</td></tr><tr><td>Notes help with organisation</td><td>Notes help you to organise the information and thoughts and break it down into smaller parts to aid clarity.</td><td>“It is easy to organise my notes”, “It is easier to keep track of your train of thoughts”, “Helped me to break down the text into smaller chunks.”</td></tr><tr><td>LLM is distracting and provides too much information</td><td>LLM is distracting as you may ask questions that are not relevant or focus on things that are not important. LLM provides too much information, which can be overwhelming or confusing.</td><td>“I found myself easily distracted by the AI and was more tempted to ask random questions”, “It’s not clear as it gives too much information.”</td></tr><tr><td>LLM is repetitive and boring</td><td>LLM is boring and repetitive as it restates the information many times.</td><td>“It felt that it was just repeating it-self.”</td></tr><tr><td>Not sure what to ask the bot</td><td>The LLM is not needed because everything is understood, or one does not know how to use it and what kind of questions to ask.</td><td>“I struggled to think of questions to ask the AI”, “The text was very easy therefore didn’t feel the need to ask many questions.”</td></tr></table>\n\nTable 19: Coding scheme: LLM+Notes over LLM preferences  \n\n<table><tr><td>Code</td><td>Description</td><td>Examples</td></tr><tr><td>Both together are more enjoyable</td><td>Using LLM and notes together is more fun and enjoyable, whereas LLM alone can be boring.</td><td>“I enjoy using both at the same time”, “If I had to use the chatbot and ask it 20 questions, I would be very bored.”</td></tr><tr><td>Both together combine the best of both worlds</td><td>LLM and notes can be used in complementary ways to get the best of both, such as doing the work yourself and then using the LLM when you are unsure or stuck.</td><td>“It was easier to have my key notes summarised as well as text with more detail”, “It allowed me to note down the crucial parts of the event in a way that I can understand it and also get help from the AI chatbot on anything that isn’t clear.”</td></tr><tr><td>Both together are more helpful and easier</td><td>General statements about the strategy being more helpful, better, or easier for understanding and learning.</td><td>“Most helpful and easy to learn”, “Because I find it easier to remember and learn this way.”</td></tr><tr><td>Notes help you process and understand the information from the LLM</td><td>Notes help you process and understand the information given by the LLM.</td><td>“In order for me to process this, I find note-taking at the same time very helpful.”</td></tr><tr><td>Notes help with organisation</td><td>LLM provides information, but notes are needed to organise and structure ideas. The notes are also more focused and accessible.</td><td>“If I am only using the chatbot, then I have to scroll up to find what I am looking for”, “It was easier to keep track of things and go back over them.”</td></tr><tr><td>Notes are your own work</td><td>Taking notes means you do actual work and can capture your own thoughts rather than just reading output.</td><td>“It meant I was doing actual work.”</td></tr><tr><td>Notes help you remember</td><td>Notes help to remember the information.</td><td>“I like to write out information as I think it helps me remember it better.”</td></tr><tr><td>Notes help you understand</td><td>Note-taking helps you to understand better and to check your understanding.</td><td>“Simplifying it on paper made it easier to understand and remember.”</td></tr><tr><td>Notes help you learn</td><td>Notes help you to learn, capture what you have learned, or test what you have learned.</td><td>“You learn more”, “You can simplify what you have learnt in the notes.”</td></tr><tr><td>LLM can provide bad answers</td><td>LLM does not always answer questions well and sometimes not at all. LLM can be harmful.</td><td>“Some of the questions I had for the bot were not answered explicitly.”</td></tr><tr><td>LLM not always available</td><td>One needs to know how to take notes as LLMs might not always be available.</td><td>“You will not get an AI chatbot at all times.”</td></tr><tr><td>Not sure what to ask the bot</td><td>The LLM is not needed because everything is understood, or one does not know how to use it or what kind of questions to ask.</td><td>“I wasn’t sure what I was supposed to say to the bot. It was just kinda irritating.”</td></tr></table>\n\n# 2.12 Coding Scheme Prompt Interactions\n\nFor the full prompt coding scheme, please refer to tabular file 'PromptCoding.xlsx'\n\nTable 20: Prompt Coding Scheme  \n\n<table><tr><td>Overarching Code</td><td>Sub-code</td><td>Description and Examples</td></tr><tr><td>Information condensation</td><td>Summarise</td><td>The student asks the bot to summarise the entire text or a specific text selection.\nExamples: “Help me to summarise this paragraph”, “Summarise the text”, “Give me a summary of the first paragraph”, “Tell me what this text is about.”</td></tr><tr><td></td><td>Take notes</td><td>The student asks the bot to take notes about the text as a whole or a specific paragraph.\nExamples: “Make notes for the first paragraph.”</td></tr><tr><td></td><td>Identify key ideas</td><td>The student asks the bot to identify the key ideas or takeaway messages from the text, including key dates, places, people, and events.\nExamples: “What are the main points?”, “Give me all the important dates”, “What’s the takeaway message?”</td></tr><tr><td></td><td>Create timeline</td><td>The student asks the bot to create a timeline of events described in the text.\nExamples: “Put the important dates into chronological order”, “Give me a timeline of the events.”</td></tr><tr><td>Understanding the text</td><td>Define a word or concept</td><td>The student asks the bot to define or explain a specific word or concept from the text. They request help to understand terminology but do not ask for factual information beyond that.\nExamples: “What does apartheid mean?”, “What is a colony?”, “What is a missile?”, “I don’t know what a blockade is.”</td></tr><tr><td></td><td>Simplify or explain difficult sentences</td><td>The student asks the bot to simplify or explain the provided passage or a specific selection of the passage.\nExamples: “Explain this in simple words”, “Make the text simpler”, “What does this sentence mean?”, “Simplify this text.”</td></tr><tr><td></td><td>Checking understanding</td><td>The student explains their understanding and seeks confirmation from the bot.\nExamples: “The US did not like Cuba because they thought that Castro was a communist, right?”, “So it was one officer that prevented the whole war?”</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>General background</td><td>The student asks for background information on a place, time, or person mentioned in the text to provide context—information that is not too central for understanding the text but could be relevant.\nExamples: “Who was Kennedy?”, “What was Mandela famous for?”, “Tell me more about Cuba”, “How many British colonies were there in Africa?”, “Where were the Turkish missiles located?”</td></tr></table>\n\nContinued on next page\n\n<table><tr><td>Overarching Code</td><td>Sub-code</td><td>Description and Examples</td></tr><tr><td></td><td>Elaboration and deeper understanding</td><td>The student asks for more details about an event, such as why it happened, who was involved, and the outcome.\nExamples: “Why did the US not like Castro?”, “Why did the exiles invade Cuba?”, “How did black people feel during apartheid?”</td></tr><tr><td></td><td>Ask for examples or analogies</td><td>The student requests examples or analogies to better understand a concept or event.\nExamples: “What are examples of how apartheid affected daily life?”, “Is there an analogy that explains the Cold War tensions?”, “What unfair laws were passed?”, “What were some of the boycotts?”</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Ask for contrasts or comparisons</td><td>The student asks the bot to compare or contrast concepts, events, or figures.\nExamples: “How is apartheid different from segregation in the US?”, “Compare Kennedy and Khrushchev&#x27;s leadership styles.”</td></tr><tr><td></td><td>Critical analysis or evaluation</td><td>The student requests the bot to critically analyze or evaluate an action, situation, decision, or statement.\nExamples: “What are the strengths and weaknesses of Kennedy&#x27;s decision?”, “Evaluate the effectiveness of the blockade.”</td></tr><tr><td></td><td>Implications and significance</td><td>The student inquires about the broader implications, relevance, or consequences of information in the text.\nExamples: “What were the long-term effects of the crisis?”, “What is the situation like now?”, “Why should I care or learn about this?”</td></tr><tr><td>Study and memory help</td><td>Study and memory help</td><td>The student asks for assistance to learn and remember the text, including requests to be quizzed on the content.\nExamples: “Make a mnemonic”, “Write four questions about the text”, “How can I remember this better?”</td></tr><tr><td>Interacting with the Bot</td><td>Request specific format or length</td><td>The student requests that the bot provides its response in a specific format or length.\nExamples: “Summarize the main points in bullet points”, “Can you create a chart of the different policies?”, “Use only a few words”, “Make it short.”</td></tr><tr><td></td><td>Request improvement</td><td>The student asks the bot to improve its response or restate it in a simpler or shorter way rather than asking for simplifications of the provided passage.\nExamples: “I don’t understand what you said”, “Explain that again but shorter”, “What do you mean?”,\n“Simpler please”, “Can you write that in simpler terms?”, “Make the summary shorter.”</td></tr><tr><td></td><td>Relational language</td><td>The student engages in casual, polite conversation that is unrelated to the text.\nExamples: “How are you?”, “Thank you”, “Hello.”</td></tr></table>\n\nContinued on next page\n\n<table><tr><td>Overarching Code</td><td>Sub-code</td><td>Description and Examples</td></tr><tr><td></td><td>Checking source and trustworthiness</td><td>The student inquires about the sources or questions the accuracy of information.\nExamples: “What are your sources?”, “Why should I believe you?”, “I think your answer is wrong.”</td></tr><tr><td></td><td>Pasting text without specific request</td><td>The student pastes text directly from the provided passages without framing it as a specific question or request.\nExamples: “Nelson Mandela”, “In 1910, four British colonies joined to create the Union of South Africa”, “Missile.”</td></tr><tr><td>Irrelevant, Off-topic, miscellaneous</td><td>Irrelevant to text</td><td>The student asks a question unrelated to the text or its background.\nExamples: “Who is Che Guevara?”, “What is the song Abraxas?”</td></tr><tr><td></td><td>Miscellaneous</td><td>Use this code for segments that don’t fit any other codes. Use this as a last resort.</td></tr><tr><td>Irrelevant, Off-topic, Miscellaneous</td><td>Nonsensical input</td><td>The student types nonsensical characters, symbols, or text that does not form coherent words or sentences.\nExamples: “asdfgh”, “.”, “123”, “???”</td></tr></table>\n\n# 2.13 Frequency of Prompt Types\n\nTable 21: Frequencies of overarching prompt types  \n\n<table><tr><td>Overarching prompt type</td><td>Frequency</td></tr><tr><td>Archetype</td><td></td></tr><tr><td>Seeking additional information and deeper understanding</td><td>2265</td></tr><tr><td>Information condensation</td><td>749</td></tr><tr><td>Understanding the text</td><td>615</td></tr><tr><td>Study and memory help</td><td>39</td></tr><tr><td>Other</td><td></td></tr><tr><td>Interacting with the bot</td><td>760</td></tr><tr><td>Irrelevant, off-topic, miscellaneous</td><td>501</td></tr></table>\n\nTable 22: Frequencies of specific prompt types  \n\n<table><tr><td>Overarching prompt type</td><td>Specific prompt type</td><td>Frequency</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Elaboration and deeper understanding</td><td>1479</td></tr><tr><td>Information condensation</td><td>Summarise</td><td>588</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>General background</td><td>514</td></tr><tr><td>Understanding the text</td><td>Define a word or concept</td><td>463</td></tr><tr><td>Interacting with the Bot</td><td>Request specific format or length</td><td>430</td></tr><tr><td>Irrelevant, Off-topic, Miscellaneous</td><td>Irrelevant to text</td><td>296</td></tr><tr><td>Understanding the text</td><td>Simplify or explain difficult sentences</td><td>126</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Implications and significance</td><td>119</td></tr><tr><td>Information condensation</td><td>Identify key ideas</td><td>114</td></tr><tr><td>Interacting with the bot</td><td>Request improvement</td><td>113</td></tr><tr><td>Interacting with the bot</td><td>Pasting text without specific request</td><td>106</td></tr><tr><td>Interacting with the bot</td><td>Relational language</td><td>105</td></tr><tr><td>Irrelevant, off-topic, miscellaneous</td><td>Nonsensical input</td><td>109</td></tr><tr><td>Irrelevant, off-topic, miscellaneous</td><td>Miscellaneous</td><td>96</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Ask for examples or analogies</td><td>66</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Critical analysis or evaluation</td><td>54</td></tr><tr><td>Study and memory help</td><td>Study and memory help</td><td>39</td></tr><tr><td>Seeking additional information and deeper understanding</td><td>Ask for contrasts or comparisons</td><td>31</td></tr><tr><td>Understanding the text</td><td>Checking understanding</td><td>26</td></tr><tr><td>Information condensation</td><td>Take notes</td><td>26</td></tr><tr><td>Information condensation</td><td>Create timeline</td><td>21</td></tr><tr><td>Interacting with the bot</td><td>Checking source and trustworthiness</td><td>6</td></tr></table>\n\nNote: This table only includes prompt types that have been used at least three times by students.",
    "arxiv_id": "2401.15081",
    "error_message": null,
    "embedding": [
      -1.625,
      -1.7578125,
      0.1728515625,
      -4.40625,
      -1.7265625,
      0.091796875,
      -0.55859375,
      -1.7578125,
      0.64453125,
      2.515625,
      2.109375,
      1.1484375,
      2.0625,
      2.984375,
      -0.890625,
      4,
      -0.0172119140625,
      -0.5390625,
      0.33984375,
      -7.1875,
      0.50390625,
      1.640625,
      1.1953125,
      -5.5,
      3.75,
      -4.28125,
      -1.125,
      1.0546875,
      0.5703125,
      0.302734375,
      7.09375,
      -7.0625,
      0.65234375,
      2.53125,
      0.89453125,
      1.1875,
      -2.15625,
      -2.03125,
      7.15625,
      3.375,
      -4.9375,
      0.302734375,
      1.7890625,
      1.5,
      -0.0123291015625,
      2.890625,
      1.421875,
      -1.90625,
      -4.4375,
      -0.66796875,
      -3.203125,
      -5,
      7.96875,
      -3.34375,
      3.046875,
      -3.171875,
      -5.0625,
      5.4375,
      -3.296875,
      -0.431640625,
      0.287109375,
      -0.92578125,
      1.265625,
      0.23046875,
      2.890625,
      1.4765625,
      1.484375,
      0.96484375,
      -3.390625,
      1.71875,
      2.265625,
      1.625,
      5.84375,
      -4.34375,
      9.0625,
      7.75,
      3.21875,
      2.09375,
      -0.296875,
      2.5625,
      -5.9375,
      5.0625,
      4.1875,
      -2.125,
      6.53125,
      3.03125,
      -1.09375,
      -2.15625,
      -2.78125,
      2.53125,
      -1.09375,
      -0.34765625,
      -6.3125,
      -3.046875,
      0.2392578125,
      3.0625,
      -0.8203125,
      -5.25,
      -4.78125,
      -0.470703125,
      -1.8515625,
      -3.140625,
      1.0390625,
      -6.71875,
      -4.71875,
      -4.53125,
      -3,
      -7.25,
      -0.76171875,
      -1.5625,
      -0.458984375,
      2.53125,
      0.51171875,
      -1.3125,
      0.7109375,
      -0.470703125,
      3.296875,
      -5.40625,
      -3.1875,
      -0.4375,
      0.578125,
      2.625,
      -1.0390625,
      0.515625,
      1.2265625,
      2.90625,
      -5.28125,
      2.984375,
      5.53125,
      -0.90234375,
      3.296875,
      -1.1640625,
      5.46875,
      -0.6328125,
      -6.25,
      -3.375,
      -4.25,
      2.046875,
      4.84375,
      7.125,
      -4.4375,
      -0.1943359375,
      -1.8046875,
      -9,
      2.859375,
      0.953125,
      -6.84375,
      0.3046875,
      3.84375,
      -4.59375,
      -1.15625,
      0.82421875,
      4.09375,
      4.78125,
      -1.90625,
      -5,
      3.125,
      2.5625,
      0.1484375,
      -2.171875,
      2.390625,
      1.109375,
      -1.2421875,
      1.1953125,
      -0.546875,
      -3.234375,
      -7.03125,
      1.7578125,
      -3.078125,
      -1.0078125,
      2.90625,
      13.875,
      2.453125,
      -1.5546875,
      0.055419921875,
      0.65234375,
      -2.71875,
      6.21875,
      2.09375,
      -0.765625,
      2.65625,
      1.7109375,
      -3.78125,
      1.609375,
      -0.1689453125,
      0.439453125,
      3.453125,
      -3.375,
      2.203125,
      -1.2109375,
      2.8125,
      4.90625,
      1.9375,
      0.072265625,
      -4.65625,
      0.296875,
      3.8125,
      1.390625,
      -0.66015625,
      2.046875,
      1.1796875,
      -8.4375,
      1.25,
      -1.484375,
      -3.765625,
      -2.25,
      4.125,
      -1.0859375,
      2.984375,
      -1.4453125,
      1.671875,
      -1.703125,
      0.76171875,
      1.6953125,
      7.4375,
      1.8125,
      1.3203125,
      -0.18359375,
      3.953125,
      3.28125,
      4.0625,
      3.0625,
      2.46875,
      1.1875,
      -0.76953125,
      3.109375,
      2.609375,
      3.71875,
      -0.0908203125,
      5.40625,
      -0.2373046875,
      4.0625,
      5.53125,
      -1.9921875,
      -2.046875,
      0.287109375,
      -3.390625,
      -0.73828125,
      -1.2734375,
      0.1591796875,
      -3.984375,
      -2.265625,
      0.671875,
      2.75,
      1.71875,
      -1.4453125,
      -0.5546875,
      -1.078125,
      -0.69140625,
      -6.875,
      0.671875,
      1.9296875,
      -9.875,
      -3.609375,
      5.625,
      8.8125,
      -0.392578125,
      -0.220703125,
      -2.890625,
      -0.59375,
      3.390625,
      -1.5625,
      -3.640625,
      -0.2041015625,
      1.90625,
      -4.15625,
      5.78125,
      -1.46875,
      3.796875,
      1.2890625,
      0.57421875,
      -1.78125,
      -1.984375,
      -0.0498046875,
      0.1845703125,
      5.4375,
      3.78125,
      -3.34375,
      0.51953125,
      -2.234375,
      -6.28125,
      -9.25,
      5.625,
      -3.25,
      5.5,
      -0.89453125,
      0.578125,
      4.84375,
      -1.640625,
      12.6875,
      2.09375,
      1.6875,
      0.6484375,
      -1.0859375,
      -4.53125,
      1.359375,
      -4.0625,
      2.65625,
      -4.8125,
      -0.365234375,
      4.75,
      4.15625,
      -1.609375,
      -4.21875,
      1.140625,
      4.3125,
      1.625,
      -3.578125,
      0.58984375,
      2.84375,
      -3.9375,
      1.984375,
      7,
      -3.53125,
      3.640625,
      -7.0625,
      -2.375,
      6.25,
      1.3125,
      -0.89453125,
      -5.1875,
      -3.15625,
      -1.0546875,
      -2.65625,
      -2.90625,
      -2.53125,
      0.33984375,
      -0.2890625,
      2.546875,
      -0.55859375,
      1.0859375,
      0.7109375,
      -5.40625,
      -9.625,
      7.5625,
      -3.578125,
      5.3125,
      0.91796875,
      -1.3671875,
      3.96875,
      -2.28125,
      -3.5625,
      3.46875,
      -1.9921875,
      -0.125,
      -1.7421875,
      4.5625,
      -2.5,
      1.4765625,
      -8.1875,
      2.96875,
      0.8125,
      1.25,
      5.0625,
      6.8125,
      -3.8125,
      1.1875,
      -2.375,
      -2.578125,
      -0.404296875,
      1,
      -1.4140625,
      6.65625,
      -0.72265625,
      -3.953125,
      -2.734375,
      -0.515625,
      3.8125,
      -0.80078125,
      -0.6015625,
      0.162109375,
      -4.25,
      0.44140625,
      0.85546875,
      1.1015625,
      3.515625,
      2.640625,
      -2.421875,
      -1.7109375,
      3.671875,
      -3.671875,
      0.81640625,
      -0.0703125,
      -1.015625,
      3.046875,
      1.6796875,
      0.9609375,
      3.015625,
      2.75,
      0.054931640625,
      -2.265625,
      1.0390625,
      -2.5625,
      3.15625,
      1.6171875,
      3.34375,
      -0.9921875,
      1.984375,
      -0.88671875,
      -2.453125,
      5.6875,
      0.1982421875,
      -0.3359375,
      -2.84375,
      -3.5625,
      1.1875,
      1.859375,
      -7.1875,
      0.326171875,
      2.078125,
      -1.3671875,
      3,
      -2.203125,
      0.053955078125,
      -0.228515625,
      3.359375,
      -1,
      2.78125,
      -5.71875,
      -3.140625,
      -3.359375,
      0.142578125,
      1.5390625,
      -0.73046875,
      -0.330078125,
      0.6015625,
      4.34375,
      6.03125,
      1.109375,
      -1.6328125,
      1.3125,
      0.06494140625,
      -2.734375,
      0.306640625,
      -0.0174560546875,
      0.06982421875,
      1.234375,
      -4.6875,
      -4.5,
      -0.2578125,
      2.921875,
      0.60546875,
      5.1875,
      5.28125,
      -2.5625,
      -2.34375,
      2,
      3.140625,
      -4.0625,
      -5.09375,
      -0.9453125,
      0.515625,
      -1.28125,
      1.8046875,
      1.875,
      0.81640625,
      -0.031494140625,
      4.59375,
      2.0625,
      1.6015625,
      2.53125,
      -0.291015625,
      -1.5546875,
      -0.287109375,
      -0.015625,
      0.1484375,
      -3.671875,
      4.125,
      6.3125,
      -11.4375,
      -10.875,
      3.171875,
      0.86328125,
      -0.54296875,
      -0.2373046875,
      4.84375,
      2,
      1.6953125,
      -6.21875,
      -4.40625,
      -0.3046875,
      -0.734375,
      3.5,
      2.140625,
      -0.99609375,
      -2.921875,
      -3.890625,
      4.46875,
      -0.51953125,
      4.53125,
      -0.2431640625,
      0.0751953125,
      1.8203125,
      -5.375,
      2.234375,
      1.5703125,
      6.9375,
      -6.03125,
      0.12255859375,
      0.8125,
      -8.375,
      -1.3828125,
      -3.71875,
      -0.12060546875,
      -0.047119140625,
      -0.953125,
      5.3125,
      -3.28125,
      -0.48828125,
      -1.109375,
      2.4375,
      -4.09375,
      -3.34375,
      3.4375,
      -3.1875,
      -3.171875,
      0.333984375,
      0.70703125,
      1.078125,
      -1.375,
      -1.265625,
      -0.33203125,
      3.84375,
      2.171875,
      0.140625,
      -1.546875,
      -0.94140625,
      -2.640625,
      0.326171875,
      4.09375,
      3.1875,
      3.03125,
      1.265625,
      -5.125,
      -2.0625,
      -0.6171875,
      0.5859375,
      1.5390625,
      -0.1962890625,
      0.11181640625,
      0.388671875,
      3.453125,
      -4.125,
      1.5546875,
      0.5078125,
      0.470703125,
      -3.40625,
      0.0478515625,
      4.21875,
      3.34375,
      2.984375,
      3.109375,
      -0.5859375,
      1.15625,
      0.83984375,
      -4.1875,
      -3.40625,
      -1.546875,
      2.078125,
      1.09375,
      -3.34375,
      -0.1591796875,
      -1.6171875,
      -0.9765625,
      0.46875,
      -0.8984375,
      3.375,
      6.90625,
      -2.34375,
      1.1015625,
      -0.318359375,
      2.25,
      -2.125,
      1.96875,
      1.984375,
      2.859375,
      -5.9375,
      -6.09375,
      -3.03125,
      -0.0947265625,
      6.75,
      0.045166015625,
      4.40625,
      -2.78125,
      5.34375,
      1.3125,
      -0.06396484375,
      -15.1875,
      2.4375,
      -1.03125,
      -3.4375,
      -0.0260009765625,
      -4.21875,
      2.90625,
      -1.8046875,
      4.0625,
      0.3671875,
      -1.5859375,
      -2.703125,
      5.25,
      1.0390625,
      -0.54296875,
      4.8125,
      2.28125,
      -5.8125,
      1.28125,
      -0.41796875,
      -1.0859375,
      0.9140625,
      -1.2890625,
      1.6875,
      4.5625,
      4.59375,
      -1.7890625,
      -0.7421875,
      1.5625,
      -4.3125,
      -0.171875,
      1.59375,
      2.546875,
      1.5390625,
      2.609375,
      -0.81640625,
      2.90625,
      -3.75,
      2.015625,
      -0.1591796875,
      -1.78125,
      -0.5859375,
      6.34375,
      0.6484375,
      -3.078125,
      -4.03125,
      -2.328125,
      4.625,
      3.3125,
      -5.84375,
      0.50390625,
      0.8359375,
      -2.15625,
      3.765625,
      -0.421875,
      -0.34765625,
      -2.671875,
      0.41796875,
      1.328125,
      -3.328125,
      6.46875,
      2.90625,
      -5.78125,
      0.3984375,
      0.38671875,
      1.3125,
      0.3046875,
      1.1640625,
      0.1572265625,
      -2.6875,
      -1.1171875,
      3.71875,
      -0.6328125,
      -3.90625,
      -0.9296875,
      1.4609375,
      -4.6875,
      4.65625,
      -3.6875,
      -0.4375,
      -2.65625,
      -1.21875,
      3.53125,
      2.25,
      2.171875,
      0.054931640625,
      2.859375,
      2.890625,
      -3.125,
      5.90625,
      -5.25,
      -1.7109375,
      0.765625,
      -4.96875,
      -3.609375,
      0.71484375,
      -0.33203125,
      2.359375,
      -0.1875,
      -1.6953125,
      -1.5546875,
      -6.375,
      -1.625,
      1.1171875,
      0.009033203125,
      2.109375,
      -0.35546875,
      7.65625,
      -2.34375,
      -1.25,
      1.9140625,
      -1.4140625,
      -2.6875,
      -1.7890625,
      -1.6171875,
      -6.875,
      0.30859375,
      5.53125,
      4.28125,
      -1.4453125,
      7.03125,
      -0.3046875,
      -1.6875,
      -1,
      6.3125,
      -1.140625,
      -0.5234375,
      1.1796875,
      -0.62109375,
      -1.3203125,
      -4.8125,
      -0.78125,
      -2.203125,
      3.625,
      -1.640625,
      -0.1220703125,
      1.3359375,
      -4.375,
      2.1875,
      -0.9609375,
      -3.015625,
      -1.1875,
      3.875,
      -0.5234375,
      -1.890625,
      4.59375,
      1.3671875,
      2.390625,
      2.328125,
      -1.6953125,
      5,
      0.6328125,
      2.34375,
      -3.734375,
      -4.65625,
      -1.0859375,
      -0.412109375,
      -1.140625,
      2.359375,
      -2.203125,
      0.80859375,
      5.34375,
      1.3125,
      -4.90625,
      -1.5390625,
      4.125,
      -0.875,
      0.03564453125,
      -0.4453125,
      4.15625,
      1.3828125,
      1.6796875,
      -0.8671875,
      -4.90625,
      -1.9609375,
      4.59375,
      3.28125,
      -1.609375,
      0.9609375,
      4.3125,
      0.52734375,
      3.625,
      -4.6875,
      -5.09375,
      1.2734375,
      -0.1015625,
      -1,
      -1.609375,
      8.3125,
      -4.625,
      0.19921875,
      -0.203125,
      0.5546875,
      0.0206298828125,
      1.609375,
      -0.9765625,
      0.359375,
      2.3125,
      -1.109375,
      1.421875,
      3.96875,
      -0.8046875,
      -0.1884765625,
      0.431640625,
      -0.376953125,
      -1.8671875,
      2.109375,
      1.5703125,
      -1.265625,
      -1.484375,
      -0.80078125,
      -1.4296875,
      4.65625,
      5.90625,
      0.77734375,
      -0.58203125,
      2.796875,
      5.875,
      -0.0341796875,
      6.78125,
      1.625,
      9.0625,
      -2.5,
      -2.296875,
      4.8125,
      -4.15625,
      0.150390625,
      2.28125,
      -6.40625,
      -0.44140625,
      -0.68359375,
      -1.171875,
      -3.5,
      0.859375,
      -3.484375,
      -2.234375,
      6.25,
      2.90625,
      0.267578125,
      3.921875,
      5.59375,
      0.96875,
      -0.9765625,
      -4.8125,
      -2.796875,
      1.09375,
      0.8046875,
      2.46875,
      -1.9921875,
      0.65625,
      1.8828125,
      2.328125,
      1.1171875,
      0.0128173828125,
      -2.46875,
      5.6875,
      0.234375,
      0.35546875,
      5.34375,
      -2.359375,
      2.71875,
      0.380859375,
      3.515625,
      -3.890625,
      -1.1875,
      8.1875,
      2.484375,
      -2.0625,
      0.84765625,
      0.244140625,
      0.150390625,
      -2.421875,
      -1.609375,
      -0.6328125,
      0.251953125,
      -3.78125,
      -1.625,
      -0.361328125,
      1.953125,
      -3.0625,
      1.7109375,
      2.046875,
      -4.25,
      1.3046875,
      -0.255859375,
      -0.6171875,
      3.5,
      0.8046875,
      4.5,
      0.99609375,
      -2.953125,
      -5.125,
      -4.1875,
      -2.109375,
      0.486328125,
      1.8515625,
      1.9140625,
      0.3515625,
      -3.65625,
      0.75,
      -5.84375,
      -0.287109375,
      -6.28125,
      4.625,
      -0.00518798828125,
      -6.375,
      -1.0859375,
      -2.796875,
      -7.59375,
      -2.484375,
      -1.3671875,
      -2.546875,
      -1.015625,
      -4.4375,
      -0.08349609375,
      -1.1953125,
      -4.125,
      -0.0712890625,
      -2.421875,
      -1.1953125,
      3.203125,
      -0.59375,
      2.21875,
      -5,
      1.015625,
      0.921875,
      -0.66796875,
      2.296875,
      1.4609375,
      4.59375,
      4.90625,
      -0.9765625,
      2.15625,
      -2.34375,
      3.875,
      -2.859375,
      8.75,
      4.21875,
      0.3359375,
      -6.15625,
      2.390625,
      -0.6953125,
      0.66015625,
      -4.71875,
      -1.78125,
      -0.953125,
      -2.75,
      -0.88671875,
      0.32421875,
      2.359375,
      -4.96875,
      -2.96875,
      0.546875,
      -4.46875,
      1.90625,
      2.609375,
      -0.67578125,
      4.875,
      -0.042236328125,
      3.109375,
      1.234375,
      2.15625,
      2.390625,
      -1.6484375,
      4.0625,
      -7,
      2.59375,
      0.271484375,
      -1.03125,
      -0.6171875,
      -2.953125,
      1.546875,
      -1.0078125,
      0.72265625,
      2.390625,
      2.59375,
      1.1484375,
      -0.5234375,
      5.125,
      -1.140625,
      -1,
      -0.55078125,
      -1.734375,
      3.78125,
      1.6640625,
      1.65625,
      -3,
      1.3359375,
      -2.671875,
      -0.09716796875,
      2.234375,
      -2.78125,
      -1.4375,
      -1.40625,
      -4.4375,
      5.84375,
      -1.8515625,
      -0.26953125,
      2.1875,
      3.203125,
      3.171875,
      3.296875,
      1.6875,
      1.703125,
      1.2578125,
      -5.21875,
      5,
      -1.421875,
      -3.09375,
      4.90625,
      3.28125,
      6.625,
      -2.1875,
      -2.21875,
      -2.109375,
      -3.59375,
      7.25,
      -3.15625,
      -0.6015625,
      7.5,
      -0.83203125,
      0.06005859375,
      -1.8984375,
      0.2333984375,
      1.1484375,
      -0.76171875,
      -1.8515625,
      8.875,
      6,
      -2.453125,
      -1.1015625,
      -4.09375,
      -2.859375,
      -1.6015625,
      -1.6875,
      0.42578125,
      -0.78515625,
      2.046875,
      0.76953125,
      -0.5703125,
      -1.734375,
      0.6484375,
      -1.75,
      2.578125,
      3.359375,
      0.359375,
      1.4921875,
      -1.3828125,
      1.734375,
      -2.125,
      1.96875,
      3.828125,
      -5.03125,
      1.3359375,
      3.078125,
      -0.78125,
      3.34375,
      2.25,
      -0.5234375,
      -1.3125,
      -0.6171875,
      0.9375,
      1.0703125,
      -1.1171875,
      0.87890625,
      -4.3125,
      1.2734375,
      4.8125,
      2.78125,
      -3.578125,
      5.9375,
      0.51953125,
      -2.796875,
      1.71875,
      -1.6171875,
      -4.15625,
      1.5234375,
      -3.234375,
      0.9140625,
      -0.130859375,
      -0.9765625,
      0.3125,
      1.4375,
      -6.8125,
      5,
      -3.328125,
      6.125,
      1.5625,
      -0.484375,
      0.52734375,
      -0.3671875,
      -0.65625,
      -1.109375,
      2.25,
      2.140625,
      -2.15625,
      1.03125,
      -3.703125,
      -0.126953125,
      -3.15625,
      -2.4375,
      4.625,
      -2.046875,
      2.46875,
      -2.109375,
      2.1875,
      -7.15625,
      -2.53125,
      -0.44140625,
      -0.291015625,
      -0.1318359375,
      2.8125,
      1.7109375,
      -1.2890625,
      1.609375,
      -2.96875,
      0.921875,
      -3.921875,
      -1.1171875,
      0.1455078125,
      -1.25,
      1.609375,
      0.76953125,
      -1.953125,
      1.65625,
      -0.875,
      -3.546875,
      -1.3203125,
      -1.7734375,
      2.0625,
      -1.0703125,
      -1.875,
      0.47265625,
      1.3828125,
      1.3671875,
      -0.65234375,
      1.0859375,
      0.69921875,
      3.0625,
      -0.7734375,
      2.296875,
      -0.91015625,
      -2.875,
      -3.359375,
      -3.625,
      -5.15625,
      1.65625,
      3.265625,
      -3.90625,
      1.5546875,
      0.036376953125,
      2.59375,
      1.796875,
      1.140625,
      -0.79296875,
      1.765625,
      1.7890625,
      0.890625,
      -0.134765625,
      2.078125,
      -0.859375,
      3.40625,
      -1.3359375,
      -3.171875,
      -2.453125,
      -1.5703125,
      -0.5234375,
      1.5,
      -0.71484375,
      -2.34375,
      -1.953125,
      -2.6875,
      -0.6171875,
      -3.453125,
      6.0625,
      2.140625,
      5.71875,
      3.0625,
      -0.70703125,
      0.474609375,
      -0.251953125,
      0.27734375,
      3.125,
      0.3359375,
      0.56640625,
      -0.1279296875,
      -0.6328125,
      2.59375,
      -3.875,
      -2.09375,
      -4.1875,
      0.5234375,
      4.53125,
      -0.1416015625,
      0.0155029296875,
      1.3984375,
      -0.9765625,
      0.9140625,
      0.263671875,
      3.9375,
      -1.2890625,
      -1.0625,
      -1.0234375,
      2.421875,
      1.2890625,
      -3.375,
      1.109375,
      0.77734375,
      -0.93359375,
      0.35546875,
      2.453125,
      6.125,
      -0.65234375,
      3.984375,
      -1.7578125,
      -4,
      -4.03125,
      3,
      -0.5625,
      0.201171875,
      -1.578125,
      -6.25,
      -0.5390625,
      -2.390625,
      -1.609375,
      6.21875,
      0.78125,
      3.59375,
      -0.609375,
      3.875,
      0.154296875,
      -5.65625,
      -3.3125,
      3.25,
      1.34375,
      -5.40625,
      -0.162109375,
      -4.34375,
      -0.4609375,
      -1.125,
      -8.25,
      0.80859375,
      1.4140625,
      3.703125,
      2.203125,
      0.154296875,
      2.125,
      -0.8359375,
      -4.28125,
      -2.984375,
      -1.0703125,
      2.921875,
      2.21875,
      0.890625,
      -2.984375,
      1.2578125,
      -3.625,
      1.1796875,
      -3.578125,
      4.59375,
      -0.5625,
      5.46875,
      1.4296875,
      4.84375,
      -8.5625,
      -2.296875,
      -2.75,
      -2.75,
      -3.40625,
      -0.3359375,
      0.8671875,
      1.578125,
      1.8359375,
      0.318359375,
      -3.125,
      -1.046875,
      -2.390625,
      3.53125,
      1.4375,
      0.34765625,
      -0.8984375,
      -0.1181640625,
      -5.21875,
      -1.4921875,
      2.28125,
      -2.140625,
      -3.3125,
      3.671875,
      -0.96875,
      4.9375,
      2.34375,
      0.26171875,
      -2.21875,
      4.375,
      -0.6328125,
      -4.34375,
      -5.8125,
      -0.60546875,
      -0.279296875,
      -5.59375,
      1.0546875,
      -3.34375,
      1.4765625,
      -4.5,
      0.03466796875,
      -0.41015625,
      -1.515625,
      0.70703125,
      5.8125,
      2.375,
      -2.484375,
      3.125,
      -3.75,
      6.4375,
      3.6875,
      0.5,
      2.265625,
      0.55078125,
      1.1171875,
      -4.46875,
      4.59375,
      -0.65625,
      -2.984375,
      3.546875,
      5.125,
      -3.90625,
      -0.0267333984375,
      1,
      1.125,
      -1.5078125,
      -0.054931640625,
      -3.046875,
      4.75,
      -1.46875,
      -0.89453125,
      -5.4375,
      1.1328125,
      2.03125,
      -1.3359375,
      2.375,
      -1.578125,
      3.03125,
      -3.9375,
      -4.71875,
      3.1875,
      2.15625,
      4.25,
      -3.421875,
      1.25,
      2.765625,
      1.359375,
      2.9375,
      -1.125,
      -4.09375,
      -1.34375,
      0.6171875,
      -5.09375,
      0.337890625,
      -4.8125,
      -0.271484375,
      -4.09375,
      0.69921875,
      -4.40625,
      0.9765625,
      0.17578125,
      -1.359375,
      -2.546875,
      -3.53125,
      -5.53125,
      -3.28125,
      -1.1640625,
      -1.40625,
      -6.53125,
      -2.421875,
      1.65625,
      3.8125,
      4.21875,
      5.90625,
      3.59375,
      -1.53125,
      0.71484375,
      -0.859375,
      0.267578125,
      0.296875,
      0.248046875,
      -1.765625,
      -2.203125,
      0.921875,
      -0.58984375,
      5.25,
      -3.46875,
      -2.4375,
      -3.3125,
      0.0927734375,
      -1.4765625,
      1.1796875,
      0.625,
      -0.80859375,
      4.65625,
      -2.859375,
      2.875,
      3.03125,
      -1.6328125,
      -5,
      4.0625,
      -1.015625,
      1.9140625,
      2.9375,
      0.333984375,
      -5,
      3.15625,
      3.859375,
      -4.25,
      3.625,
      1.1640625,
      -1.53125,
      -3.453125,
      0.28125,
      1.1640625,
      5.75,
      -1.7734375,
      3.5,
      4.125,
      -1.0859375,
      3.515625,
      5.5,
      -0.35546875,
      -2.0625,
      2.15625,
      0.44921875,
      -2,
      -2.484375,
      1.7265625,
      -2.765625,
      -4.71875,
      3.609375,
      2.515625,
      -0.9921875,
      -0.72265625,
      -1.5078125,
      4.40625,
      3.125,
      -3.859375,
      0.404296875,
      -4.15625,
      1.6796875,
      -2.9375,
      -1.1640625,
      -4.75,
      -0.490234375,
      -0.64453125,
      6.15625,
      -2.5625,
      -4.09375,
      5.0625,
      5.15625,
      -1.0546875,
      -1.1953125,
      5.0625,
      1.2109375,
      -0.9921875,
      -4.5,
      -2.484375,
      5.65625,
      1.3203125,
      4.96875,
      -0.98046875,
      1.3515625,
      0.96484375,
      -2.546875,
      -5.53125,
      1.171875,
      -2.421875,
      -4.8125,
      -3.171875,
      -2.40625,
      2.78125,
      -2.640625,
      4.25,
      3.96875,
      0.58203125,
      2.1875,
      0.3359375,
      2.6875,
      3.015625,
      -3.46875,
      1.4296875,
      1.328125,
      -2.9375,
      0.3984375,
      1.1328125,
      -2.140625,
      0.345703125,
      -2.546875,
      1.8359375,
      3.5,
      -0.84765625,
      -1.9765625,
      -7,
      2.234375,
      3.71875,
      5.125,
      -0.1708984375,
      -4.53125,
      1.5078125,
      1.3359375,
      -2.6875,
      -1.25,
      2.53125,
      0.73828125,
      1.5546875,
      -1.6328125,
      -1.0859375,
      4.78125,
      0.8125,
      1.7265625,
      -1.421875,
      -4.09375,
      -2.65625,
      -0.98046875,
      -3.375,
      -0.1552734375,
      -2.09375,
      5.9375,
      -1.859375,
      -1.0625,
      0.1416015625,
      5.96875,
      1.6015625,
      2.328125,
      0.1416015625,
      0.404296875,
      -0.310546875,
      3,
      2.671875,
      -3.5625,
      1.9296875,
      -0.150390625,
      -1.1484375,
      -2.59375,
      -1.6796875,
      -1.484375,
      1.34375,
      -3.890625,
      -0.40625,
      -0.1259765625,
      2.140625,
      -1.8515625,
      -9.625,
      0.75,
      1.15625,
      -3.25,
      -2.25,
      -2.390625,
      1.40625,
      6.5625,
      1,
      4.8125,
      0.193359375,
      -2.90625,
      3.40625,
      13.875,
      -3.125,
      -3.75,
      0.3984375,
      -1.296875,
      6.28125,
      6.46875,
      0.546875,
      2.640625,
      1.0546875,
      -0.396484375,
      3.625,
      0.734375,
      4.375,
      1.0078125,
      1.515625,
      3.15625,
      0.99609375,
      -3.265625,
      -5.875,
      -1.03125,
      -1.0703125,
      -1.484375,
      2.15625,
      -1.5,
      0.5234375,
      0.7421875,
      -3.109375,
      -1.8046875,
      -2.46875,
      -3.703125,
      -0.83984375,
      -0.1962890625,
      -1.984375,
      -0.703125,
      -3.890625,
      -0.9765625,
      0.55078125,
      -0.177734375,
      -3.84375,
      -5.65625,
      -6.15625,
      2.171875,
      -0.99609375,
      -4.96875,
      2.109375,
      -3.15625,
      -3.328125,
      0.00933837890625,
      -1,
      -1.1171875,
      0.2197265625,
      -6.6875,
      0.87890625,
      0.265625,
      -0.95703125,
      -1.3203125,
      -3.5625,
      -4.375,
      -4.4375,
      -1.3515625,
      -0.53515625,
      -0.45703125,
      -3.40625,
      -3.875,
      -3.21875,
      -0.83203125,
      1.9375,
      1.3125,
      1.1328125,
      -2.984375,
      -1.1953125,
      2.78125,
      2.875,
      -0.765625,
      -3.46875,
      2.171875,
      0.890625,
      0.97265625,
      2.5625,
      -3.265625,
      2.625,
      -2.0625,
      -0.9296875,
      -0.53125,
      1.4765625,
      -4.6875,
      0.373046875,
      1.9375,
      -2.421875,
      5.9375,
      -5.78125,
      3.4375,
      -0.1474609375,
      -6.4375,
      -0.4140625,
      1.125,
      1.765625,
      -2.28125,
      -0.99609375,
      2.078125,
      -1.296875,
      1.6875,
      0.427734375,
      -0.130859375,
      1.46875,
      0.734375,
      -5.21875,
      0.0693359375,
      1.234375,
      0.80859375,
      4.3125,
      2.375,
      -0.4609375,
      -0.875,
      4.40625,
      -1.28125,
      -1.5546875,
      -3.0625,
      -2.234375,
      6,
      1.1796875,
      -0.93359375,
      0.294921875,
      -7.0625,
      -3.65625,
      -1.484375,
      -5.25,
      4,
      -1.2265625,
      0.396484375,
      -3.46875,
      -4.125,
      -0.0390625,
      3.25,
      -4.5625,
      3.65625,
      2.3125,
      3.96875,
      -0.07421875,
      -3.515625,
      1.328125,
      0.8671875,
      -2.078125,
      -3.609375,
      3.421875,
      -3.0625,
      2.59375,
      5.03125,
      2.375,
      -4.8125,
      4.1875,
      -1.4140625,
      -4.6875,
      -0.11767578125,
      2.859375,
      -1.71875,
      -1.1328125,
      -4.6875,
      0.052734375,
      -0.5234375,
      -2.046875,
      -0.8203125,
      -5.46875,
      -2.484375,
      -0.9921875,
      3.296875,
      1.3671875,
      -4.875,
      2.59375,
      2.671875,
      -6.0625,
      -0.283203125,
      -4.8125,
      4.53125,
      -1.6171875,
      2.921875,
      4.53125,
      -0.73828125,
      -1.0625,
      1.015625,
      1.9375,
      -1.8671875,
      -4.3125,
      3.796875,
      2.09375,
      -3.90625,
      3.140625,
      3.296875,
      -1.9921875,
      -0.310546875,
      -4.375,
      -1.3359375,
      5.15625,
      3.53125,
      -3.46875,
      -2.125,
      -0.8828125,
      1.328125,
      -4.5625,
      -5.40625,
      4.0625,
      1.359375,
      2.046875,
      1.9296875,
      1.3125,
      -3.046875,
      -1.75,
      -0.40625,
      -2.21875,
      -3.40625,
      2.515625,
      -2.296875,
      3.59375,
      -3.3125,
      6.09375,
      2.8125,
      1.2109375,
      -1.625,
      0.1728515625,
      2.59375,
      1.1328125,
      -3.9375,
      0.453125,
      0.5390625,
      3.828125,
      -6.53125,
      4.1875,
      -2.859375,
      2.9375,
      -4.8125,
      3.921875,
      -0.6875,
      -0.5,
      -5.0625,
      -4.1875,
      -3.265625,
      3.296875,
      0.99609375,
      0.84765625,
      3.328125,
      -0.40234375,
      0.291015625,
      -0.279296875,
      -3.25,
      1.578125,
      -2.34375,
      -0.057861328125,
      1.15625,
      -0.609375,
      0.7421875,
      0.3359375,
      7.25,
      -2.78125,
      -1.0703125,
      -2.359375,
      -2.328125,
      -0.59765625,
      -4.75,
      2.53125,
      -1.8125,
      -3.5,
      3.40625,
      1.8515625,
      -1.90625,
      3.125,
      1.0625,
      -2.78125,
      4.78125,
      3.25,
      -5.4375,
      5.0625,
      -1.078125,
      -3.984375,
      0.478515625,
      -0.7890625,
      -0.97265625,
      -5.8125,
      -0.30859375,
      4.09375,
      -3.484375,
      0.1376953125,
      2.359375,
      0.7734375,
      0.32421875,
      -2.859375,
      5.15625,
      1.765625,
      1.6484375,
      2.78125,
      4.09375,
      3.125,
      -4.78125,
      0.51953125,
      -1.8828125,
      -0.470703125,
      -2.390625,
      -4.65625,
      2.375,
      -1.1875,
      3.125,
      4.53125,
      -1.640625,
      -9.625,
      -3.71875,
      2.171875,
      -2.140625,
      2.8125,
      1.9765625,
      0.9765625,
      -2,
      0.69140625,
      0.84765625,
      4.1875,
      -0.9921875,
      -0.84765625,
      -5.5625,
      0.09619140625,
      -0.78515625,
      -5.6875,
      0.73828125,
      0.91796875,
      -0.330078125,
      -2.25,
      0.5,
      -0.96484375,
      0.99609375,
      3.53125,
      -4.3125,
      2.25,
      -1,
      -0.4375,
      5.90625,
      0.4921875,
      5.15625,
      3.078125,
      -2.875,
      -5.28125,
      0.1865234375,
      -0.2421875,
      1.265625,
      1.8359375,
      1.171875,
      5.21875,
      0.373046875,
      2.40625,
      -6.8125,
      5.03125,
      0.09765625,
      -11.1875,
      2.375,
      -3.015625,
      0.8203125,
      -0.7578125,
      -8.3125,
      -3.046875,
      4.21875,
      -0.796875,
      -0.8046875,
      5.78125,
      0.47265625,
      0.185546875,
      1.5390625,
      -0.52734375,
      -4.9375,
      2.25,
      3.953125,
      -2.3125,
      0.28125,
      3.796875,
      -1.953125,
      0.78125,
      -0.197265625,
      3.015625,
      0.318359375,
      -3.375,
      3.421875,
      2.234375,
      -2.421875,
      -3.234375,
      -4.15625,
      -0.1328125,
      1.0859375,
      -3.765625,
      1.3046875,
      -2.15625,
      2.328125,
      -1.8125,
      -0.038818359375,
      -0.365234375,
      -3.984375,
      -1.671875,
      -2.75,
      -2.125,
      0.84765625,
      3,
      3.03125,
      -2.34375,
      -1.9765625,
      4.25,
      0.400390625,
      -0.640625,
      -2.65625,
      4.0625,
      -3.09375,
      -3.125,
      -2.515625,
      4.1875,
      -2.75,
      -3.1875,
      1.9140625,
      -0.5625,
      -2.203125,
      -3.453125,
      -2.78125,
      2.421875,
      -1.921875,
      -1.5234375,
      -2.296875,
      -0.9375,
      3.859375,
      4.6875,
      -4.84375,
      -3.421875,
      -2.796875,
      -0.72265625,
      -5.53125,
      -3.609375,
      -0.6796875,
      -2.9375,
      0.8671875,
      5.1875,
      -0.150390625,
      0.9921875,
      0.349609375,
      -6.71875,
      -2.421875,
      3.859375,
      1.453125,
      -3.734375,
      1.375,
      -1.0546875,
      -4.15625,
      4.84375,
      9.5625,
      2.53125,
      -3.1875,
      1.0625,
      -1.1328125,
      -2.515625,
      -1.734375,
      2.4375,
      -0.48046875,
      -5.125,
      5.8125,
      -2.8125,
      4.1875,
      2.125,
      -1.265625,
      5.75,
      -4.9375,
      3,
      -1.2578125,
      0.62890625,
      3.234375,
      -4.5625,
      -1.828125,
      -3.484375,
      -2.734375,
      0.99609375,
      -0.5546875,
      1.1875,
      -1.234375,
      2.625,
      2.28125,
      0.74609375,
      -0.609375,
      -0.2373046875,
      5.3125,
      -4.46875,
      -0.443359375,
      -2.296875,
      -1.734375,
      -0.322265625,
      -2.59375,
      1.5859375,
      -5.0625,
      2.375,
      0.7578125,
      1.609375,
      1.1640625,
      1.7109375,
      -3.390625,
      -2.3125,
      -5.84375,
      1.1796875,
      1.140625,
      4.90625,
      1.34375,
      1.9140625,
      3.078125,
      0.279296875,
      3.4375,
      4.125,
      6.25,
      -3.640625,
      0.69921875,
      -0.8125,
      0.43359375,
      -0.087890625,
      -1.3203125,
      -0.75,
      1.1328125,
      4.625,
      0.31640625,
      -2.15625,
      -3.46875,
      -1.625,
      -2.53125,
      -1.1171875,
      1.6328125,
      2.515625,
      0.90234375,
      0.86328125,
      -1.25,
      0.77734375,
      0.796875,
      5.125,
      0.80859375,
      0.55078125,
      -2.671875,
      1.8203125,
      -2.734375,
      -2.125,
      2.859375,
      -2.0625,
      2.375,
      -1.0546875,
      -1.2890625,
      -0.44140625,
      2.078125,
      -3.859375,
      3.203125,
      -1.6640625,
      2.546875,
      1.2421875,
      1.515625,
      -2.703125,
      1.4921875,
      -0.01287841796875,
      2.515625,
      -4.0625,
      -0.890625,
      2.828125,
      2.6875,
      0.01470947265625,
      0.36328125,
      0.01336669921875,
      1.0859375,
      0.71484375,
      -1.515625,
      -1.0625,
      -0.85546875,
      3.171875,
      -0.486328125,
      0.6484375,
      1.8984375,
      0.458984375,
      -3.75,
      2.203125,
      -0.96875,
      -0.51171875,
      -3.390625,
      -0.193359375,
      2.984375,
      1.0703125,
      -0.10107421875,
      -0.90234375,
      -3.546875,
      -2.21875,
      -2.703125,
      -3.890625,
      -0.287109375,
      -2.3125,
      -4.59375,
      1.78125,
      2.71875,
      0.40625,
      0.484375,
      0.2734375,
      1.3984375,
      0.263671875,
      -2.296875,
      -0.267578125,
      1.34375,
      0.29296875,
      -2.46875,
      2.4375,
      1.265625,
      2.21875,
      2.15625,
      -0.71875,
      0.474609375,
      -0.90625,
      0.6796875,
      0.458984375,
      -2.171875,
      0.365234375,
      0.53515625,
      -1.6328125,
      -0.6953125,
      -0.86328125,
      -1.1328125,
      0.1220703125,
      -1.3828125,
      -0.83984375,
      1.4140625,
      0.022216796875,
      -2.15625,
      2.296875,
      4.28125,
      -3.25,
      -1.6875,
      -3.140625,
      -0.62109375,
      -0.484375,
      1.4140625,
      -4.53125,
      0.609375,
      -3.296875,
      -1.203125,
      -0.671875,
      1.1015625,
      1.578125,
      -3.90625,
      -0.8984375,
      -1.765625,
      0.451171875,
      0.048095703125,
      2.609375,
      1.203125,
      -5.09375,
      -3.265625,
      0.70703125,
      -0.58203125,
      -0.43359375,
      1.2421875,
      -2.046875,
      -1.546875,
      2.71875,
      3.25,
      -0.83984375,
      -2,
      1.1484375,
      0.69921875,
      0.306640625,
      1.9375,
      -0.14453125,
      1.8828125,
      0.16015625,
      -1.484375,
      0.4609375,
      -1.09375,
      -0.51171875,
      -0.486328125,
      -0.55859375,
      0.28515625,
      -1.9609375,
      -1.5703125,
      -2.140625,
      0.091796875,
      -2.6875,
      1.8125,
      1.1953125,
      -1.6484375,
      1.375,
      1.5234375,
      -0.29296875,
      2.453125,
      1.2421875,
      2.625,
      -0.62109375,
      0.345703125,
      0.72265625,
      1.953125,
      1.765625,
      1.2890625,
      -2.25,
      -1.5234375,
      0.2470703125,
      1.015625,
      0.90234375,
      -3.453125,
      3.375,
      1.0625,
      0.65234375,
      1.7265625,
      1.046875,
      0.86328125,
      1.1875,
      -4.03125,
      2.28125,
      2.703125,
      -0.69140625,
      -0.462890625,
      0.89453125,
      0.36328125,
      -0.74609375,
      -1.2578125,
      -0.33203125,
      -0.99609375,
      3.5625,
      -0.00469970703125,
      1.1171875,
      2.03125,
      -2.296875,
      -0.09912109375,
      -1.453125,
      1.3046875,
      -2.03125,
      5.09375,
      -2.453125,
      1.390625,
      2.8125,
      0.337890625,
      -2.09375,
      -0.6171875,
      -7.90625,
      -2.046875,
      -0.6953125,
      0.7421875,
      -0.54296875,
      -1.0390625,
      -1.515625,
      -2.296875,
      -0.189453125,
      0.59765625,
      -2.109375,
      2.359375,
      2.109375,
      1.7890625,
      2.296875,
      0.486328125,
      -3.609375,
      -0.72265625,
      3.109375,
      -1.4921875,
      3.375,
      -0.1328125,
      -2.078125,
      -1.1953125,
      -2.03125,
      -5.25,
      -0.7421875,
      -1.7109375,
      1.1015625,
      0.90234375,
      6.1875,
      1.265625,
      -2.359375,
      -0.5703125,
      0.72265625,
      2.5625,
      -1.203125,
      -2.5,
      3.3125,
      -0.7890625,
      1.1640625,
      -2.328125,
      -0.609375,
      -2.234375,
      0.5703125,
      1.4609375,
      -0.45703125,
      3.4375,
      -0.609375,
      -1.3203125,
      -0.04833984375,
      -0.09814453125,
      0.48046875,
      1.296875,
      0.12109375,
      4.6875,
      0.310546875,
      0.734375,
      2.125,
      3.6875,
      0.1494140625,
      -0.84765625,
      3.140625,
      0.0947265625,
      3.1875,
      4,
      -2.921875,
      -2.921875,
      2.109375,
      0.7578125,
      0.37109375,
      0.392578125,
      0.2099609375,
      3.5,
      -0.8125,
      0.703125,
      1.375,
      -0.408203125,
      -2.65625,
      -1.640625,
      2.78125,
      -4.8125,
      -0.3359375,
      0.69921875,
      -2,
      -0.2294921875,
      0.953125,
      4.40625,
      1.2578125,
      0.294921875,
      1.46875,
      1.0859375,
      -0.1396484375,
      0.42578125,
      -1.390625,
      1.9296875,
      3.625,
      -0.171875,
      -2.375,
      1.8984375,
      3.03125,
      4.0625,
      -0.017822265625,
      -2.390625,
      2.96875,
      0.7265625,
      1.0078125,
      0.890625,
      -1.4765625,
      2.03125,
      2.03125,
      -0.408203125,
      -0.56640625,
      -1.453125,
      0.5234375,
      2.765625,
      0.55859375,
      -2.640625,
      3.625,
      1.015625,
      -1.90625,
      -1.1328125,
      -0.54296875,
      -1.3203125,
      -1.5390625,
      -2.625,
      -1.015625,
      -2.03125,
      4.78125,
      -3.125,
      0.322265625,
      2.234375,
      2.296875,
      2.734375,
      2.859375,
      1.1171875,
      2.265625,
      -4.9375,
      -0.80078125,
      1.703125,
      -1.4765625,
      -4.4375,
      1.640625,
      -2.21875,
      -5.03125,
      0.75390625,
      -1.1015625,
      -1.53125,
      -1.6953125,
      -0.31640625,
      -2,
      -2.5,
      0.74609375,
      -1.5625,
      1.5546875,
      -4.375,
      -1.0078125,
      -0.5703125,
      3.703125,
      1.0078125,
      -1.765625,
      2.09375,
      -0.205078125,
      0.89453125,
      0.359375,
      -0.59765625,
      -0.77734375,
      -1.9296875,
      1.2109375,
      1.71875,
      -1.0390625,
      -0.96484375,
      -1.34375,
      3.625,
      -1.8359375,
      -0.92578125,
      -1.4296875,
      -1.21875,
      -0.06884765625,
      6.90625,
      2.546875,
      0.9453125,
      -2.34375,
      -2.171875,
      -0.58203125,
      3.703125,
      -0.78125,
      -1.515625,
      -2.1875,
      3.171875,
      -1.734375,
      -1.3125,
      -1.0703125,
      0.68359375,
      -1.4453125,
      -3.265625,
      0.423828125,
      -4.09375,
      0.244140625,
      4.8125,
      1.15625,
      -0.0615234375,
      -1.765625,
      2.609375,
      -3.296875,
      3.421875,
      -1.1328125,
      0.078125,
      5.125,
      0.6796875,
      0.2060546875,
      0.9765625,
      0.546875,
      0.2578125,
      2.390625,
      1.140625,
      2.453125,
      2.5,
      -0.404296875,
      -0.921875,
      0.234375,
      2.265625,
      -0.67578125,
      -3.140625,
      -1.1328125,
      -1.328125,
      -4.25,
      0.25390625,
      3.828125,
      1.28125,
      0.1396484375,
      1.15625,
      2.171875,
      -0.01318359375,
      2.6875,
      2.859375,
      0.08056640625,
      0.1748046875,
      -1.0390625,
      -2.703125,
      -2.65625,
      1.28125,
      -0.62890625,
      1.171875,
      1.2265625,
      -1.3515625,
      1.359375,
      -1.71875,
      4.90625,
      -2.375,
      0.54296875,
      -0.06689453125,
      -1.4609375,
      -2.8125,
      -2.453125,
      -2.53125,
      2.84375,
      -1.4296875,
      -0.05615234375,
      2.140625,
      0.072265625,
      -1.6484375,
      -1.3046875,
      -0.15625,
      3.03125,
      -0.4609375,
      -3.578125,
      -0.43359375,
      3.625,
      -1.7265625,
      0.828125,
      4.5,
      1.9765625,
      0.072265625,
      0.09326171875,
      1.8671875,
      -3.90625,
      0.1787109375,
      0.02099609375,
      1.1796875,
      3.109375,
      -2.484375,
      -4.09375,
      2.59375,
      1.921875
    ],
    "summary": "通过预注册的随机对照实验，在真实学校环境中比较单独使用LLM、传统笔记以及两者结合对中学生阅读理解与记忆的影响，并采用定量与定性相结合的混合方法进行分析。",
    "structure": {
      "sections": [
        {
          "title": "Effects of LLM use and note-taking on reading comprehension and memory: A randomised experiment in secondary schools",
          "level": 1,
          "start_line": 1
        },
        {
          "title": "Affiliations:",
          "level": 1,
          "start_line": 9
        },
        {
          "title": "Abstract",
          "level": 1,
          "start_line": 14
        },
        {
          "title": "Main",
          "level": 1,
          "start_line": 18
        },
        {
          "title": "Results",
          "level": 1,
          "start_line": 46
        },
        {
          "title": "Learning outcomes",
          "level": 1,
          "start_line": 50
        },
        {
          "title": "Behavioural engagement",
          "level": 1,
          "start_line": 71
        },
        {
          "title": "Prompting behaviour",
          "level": 1,
          "start_line": 75
        },
        {
          "title": "Learning experiences and perceptions",
          "level": 1,
          "start_line": 93
        },
        {
          "title": "Activity preferences",
          "level": 1,
          "start_line": 109
        },
        {
          "title": "Future use",
          "level": 1,
          "start_line": 121
        },
        {
          "title": "Discussion",
          "level": 1,
          "start_line": 125
        },
        {
          "title": "Materials and Methods",
          "level": 1,
          "start_line": 153
        },
        {
          "title": "Participants",
          "level": 1,
          "start_line": 157
        },
        {
          "title": "Experimental design and procedure",
          "level": 1,
          "start_line": 165
        },
        {
          "title": "Setup and system",
          "level": 1,
          "start_line": 193
        },
        {
          "title": "Apartheid in South Africa",
          "level": 1,
          "start_line": 199
        },
        {
          "title": "AI Chatbot ②",
          "level": 1,
          "start_line": 209
        },
        {
          "title": "Notepad",
          "level": 1,
          "start_line": 213
        },
        {
          "title": "Learning task and materials (Session 1)",
          "level": 1,
          "start_line": 228
        },
        {
          "title": "Test task and materials (Session 2)",
          "level": 1,
          "start_line": 242
        },
        {
          "title": "Survey questions",
          "level": 1,
          "start_line": 256
        },
        {
          "title": "Analytic strategies",
          "level": 1,
          "start_line": 264
        },
        {
          "title": "Estimation of condition effects on text comprehension and retention",
          "level": 1,
          "start_line": 270
        },
        {
          "title": "Qualitative exploration of student prompts",
          "level": 1,
          "start_line": 302
        },
        {
          "title": "Quantitative exploration of students' learning experience",
          "level": 1,
          "start_line": 308
        },
        {
          "title": "Qualitative exploration of students' activity preferences",
          "level": 1,
          "start_line": 312
        },
        {
          "title": "Data availability",
          "level": 1,
          "start_line": 325
        },
        {
          "title": "Code availability",
          "level": 1,
          "start_line": 329
        },
        {
          "title": "Ethics declarations",
          "level": 1,
          "start_line": 333
        },
        {
          "title": "Competing interests",
          "level": 1,
          "start_line": 335
        },
        {
          "title": "Acknowledgements",
          "level": 1,
          "start_line": 339
        },
        {
          "title": "Supplementary Material",
          "level": 1,
          "start_line": 343
        },
        {
          "title": "Table of Contents",
          "level": 1,
          "start_line": 345
        },
        {
          "title": "Supplementary Information",
          "level": 1,
          "start_line": 347
        },
        {
          "title": "Supplementary Tables",
          "level": 1,
          "start_line": 351
        },
        {
          "title": "References",
          "level": 1,
          "start_line": 374
        },
        {
          "title": "1 Supplementary Information",
          "level": 1,
          "start_line": 451
        },
        {
          "title": "1.1 Participant Exclusion Criteria",
          "level": 1,
          "start_line": 453
        },
        {
          "title": "2 Supplementary Tables",
          "level": 1,
          "start_line": 464
        },
        {
          "title": "2.1 Student Characteristics",
          "level": 1,
          "start_line": 466
        },
        {
          "title": "2.2 Familiarity with Learning Activities",
          "level": 1,
          "start_line": 475
        },
        {
          "title": "2.3 Descriptive Statistics",
          "level": 1,
          "start_line": 483
        },
        {
          "title": "2.4 Mixed Effects Regression Results",
          "level": 1,
          "start_line": 489
        },
        {
          "title": "2.5 Behavioural Engagement",
          "level": 1,
          "start_line": 495
        },
        {
          "title": "2.6 Student Task Instructions",
          "level": 1,
          "start_line": 501
        },
        {
          "title": "2.7 Test Questions",
          "level": 1,
          "start_line": 567
        },
        {
          "title": "2.8 Inter-rater Reliability Results",
          "level": 1,
          "start_line": 589
        },
        {
          "title": "2.9 Survey Questions and Response Scales",
          "level": 1,
          "start_line": 595
        },
        {
          "title": "2.10 Learning Experiences and Perceptions",
          "level": 1,
          "start_line": 624
        },
        {
          "title": "2.11 Coding Scheme Activity Preferences",
          "level": 1,
          "start_line": 630
        },
        {
          "title": "2.12 Coding Scheme Prompt Interactions",
          "level": 1,
          "start_line": 653
        },
        {
          "title": "2.13 Frequency of Prompt Types",
          "level": 1,
          "start_line": 691
        }
      ]
    },
    "suggested_tags": [
      "教育技术",
      "LLM应用",
      "学习科学",
      "人机交互"
    ],
    "tag_suggestions": [
      {
        "name": "教育技术",
        "confidence": 0.98,
        "reason": "论文核心研究领域是评估大型语言模型（LLM）在中学教育场景中对学生学习效果（阅读理解与记忆）的影响，属于教育技术与学习科学的交叉研究。"
      },
      {
        "name": "LLM应用",
        "confidence": 0.95,
        "reason": "研究聚焦于生成式AI（特别是LLM，如ChatGPT）在教育中的实际使用效果、学生交互模式（提示行为原型）及其对学生认知过程的影响。"
      },
      {
        "name": "学习科学",
        "confidence": 0.9,
        "reason": "研究基于认知科学理论（如建构-整合模型、加工水平理论），通过随机对照实验定量比较传统笔记与AI辅助学习对长期记忆和理解的影响，探讨认知参与机制。"
      },
      {
        "name": "人机交互",
        "confidence": 0.85,
        "reason": "论文采用混合方法（定量实验与定性分析），重点分析了学生与LLM的交互行为、感知有用性及认知负荷，属于教育场景下的人机交互研究。"
      }
    ],
    "s2_graph": {
      "citations": [
        {
          "external_id": "CorpusId:283549586",
          "title": "ChatGPT ile tasarlanan matematiksel oyunların 4. sınıf öğrencilerinin matematik motivasyon ve tutumlarına etkisi",
          "authors": [
            "Ramazan Divrik",
            "Ömer Çelik",
            "Zeynep Elmas"
          ],
          "year": 2025,
          "venue": "Kocaeli Üniversitesi Eğitim Dergisi",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:282974790",
          "title": "Artificial intelligence and its effects on critical thinking and problem-solving abilities in higher education",
          "authors": [
            "Ide Aprianto",
            "Sofyan Sofyan",
            "Sophia Rahmawati",
            "Susanti Sufyadi"
          ],
          "year": 2025,
          "venue": "Indonesian Journal of Educational Development",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283138035",
          "title": "L'Intelligence Artificielle dans la Formation des Enseignants des SVT : Entre Adoption Prometteuse et Défis Persistants",
          "authors": [
            "Zerrouqi Zahra",
            "Laghmari Mustapha",
            "Bouzidi Chaymae",
            "E. Ahlam",
            "Mazza Fatima Zahra"
          ],
          "year": 2025,
          "venue": "International Journal For Multidisciplinary Research",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:282755329",
          "title": "An AI for an AI: AI-generated interactive animated questions as a defense against AI-based cheating",
          "authors": [
            "Saleem Hamady"
          ],
          "year": 2025,
          "venue": "The Physical Educator",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:282955449",
          "title": "STEM Education: Understanding Secondary Students’ Epistemic Cognition in the Design Process with the Support of a Personalized Multi-Agent System",
          "authors": [
            "Lei Gao",
            "Morris Siu-Yung Jong",
            "Ching-sing Chai",
            "Keru Li"
          ],
          "year": 2025,
          "venue": "Computers &amp; Education",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:283326712",
          "title": "Effects of LLM Use and Note-Taking on Reading Comprehension and Memory: A Randomised Experiment in Secondary Schools",
          "authors": [
            "Pia Kreijkes",
            "Viktor Kewenig",
            "Martina Kuvalja",
            "Mina Lee",
            "Jake M. Hofman",
            "Sylvia Vitello",
            "Abigail Sellen",
            "Sean Rintel",
            "Daniel G. Goldstein",
            "David Rothschild",
            "Lev Tankelevitch",
            "Tim Oates"
          ],
          "year": 2025,
          "venue": "Computers &amp; Education",
          "citation_count": 6
        },
        {
          "external_id": "CorpusId:283351055",
          "title": "Will AI Write the Next \"Chapter\" in Literature Reviews?",
          "authors": [
            "Felix Blanc-Durand",
            "M. Koopman",
            "S. P. Patel",
            "M. Aldea",
            "J. Kather"
          ],
          "year": 2025,
          "venue": "Annals of Oncology",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:282580958",
          "title": "Do generative artificial intelligence (GenAI) and science education mix? A systematic review of the literature",
          "authors": [
            "Kason Ka Ching Cheung",
            "Amina Zerouali",
            "Jenna Koenen",
            "S. Erduran"
          ],
          "year": 2025,
          "venue": "Studies in science education",
          "citation_count": 1
        },
        {
          "external_id": "CorpusId:282432222",
          "title": "Use of ChatGPT in nursing education: A mixed method research on student perceptions and experiential practice recommendations.",
          "authors": [
            "Suna Uysal Yalçın",
            "Y. Dikmen"
          ],
          "year": 2025,
          "venue": "Nurse Education in Practice",
          "citation_count": 0
        },
        {
          "external_id": "CorpusId:281931116",
          "title": "Exploring the Impact of AI Tools on Cognitive Skills: A Comparative Analysis",
          "authors": [
            "Nurlan Musazade",
            "J. Mezei",
            "Xiaolu Wang"
          ],
          "year": 2025,
          "venue": "Algorithms",
          "citation_count": 1
        }
      ],
      "citations_fetched_at": "2025-12-16T21:43:30.232272",
      "references": [
        {
          "external_id": "CorpusId:267242459",
          "title": "Generative AI Professional Development Needs for Teacher Educators",
          "authors": [
            "Matthew Nyaaba",
            "Xiaoming Zhai"
          ],
          "year": 2024,
          "venue": "Journal of AI",
          "citation_count": 42
        },
        {
          "external_id": "CorpusId:265352181",
          "title": "NERIF: GPT-4V for Automatic Scoring of Drawn Models",
          "authors": [
            "Gyeong-Geon Lee",
            "Xiaoming Zhai"
          ],
          "year": 2023,
          "venue": "Journal of Science Education and Technology",
          "citation_count": 13
        },
        {
          "external_id": "CorpusId:264144600",
          "title": "Efficacy and limitations of ChatGPT as a biostatistical problem-solving tool in medical education in Serbia: a descriptive study",
          "authors": [
            "Aleksandra Ignjatović",
            "Lazar Stevanović"
          ],
          "year": 2023,
          "venue": "Journal of Educational Evaluation for Health Professions",
          "citation_count": 31
        },
        {
          "external_id": "CorpusId:261049075",
          "title": "Elucidating STEM Concepts through Generative AI: A Multi-modal Exploration of Analogical Reasoning",
          "authors": [
            "Chen Cao",
            "Zijian Ding",
            "Gyeong-Geon Lee",
            "Jiajun Jiao",
            "Jionghao Lin",
            "Xiaoming Zhai"
          ],
          "year": 2023,
          "venue": "arXiv.org",
          "citation_count": 13
        },
        {
          "external_id": "CorpusId:259968382",
          "title": "Artificial Intelligence Generative Tools and Conceptual Knowledge in Problem Solving in Chemistry",
          "authors": [
            "Wajeeh M. Daher",
            "Hussam Diab",
            "A. Rayan"
          ],
          "year": 2023,
          "venue": "Inf.",
          "citation_count": 46
        },
        {
          "external_id": "CorpusId:257943792",
          "title": "Revolutionizing education with AI: Exploring the transformative potential of ChatGPT",
          "authors": [
            "Tufan Adiguzel",
            "M. H. Kaya",
            "Fatih Kursat Cansu"
          ],
          "year": 2023,
          "venue": "Contemporary Educational Technology",
          "citation_count": 596
        },
        {
          "external_id": "CorpusId:259196547",
          "title": "A Testing Load: Investigating Test Mode Effects on Test Score, Cognitive Load and Scratch Paper Use with Secondary School Students",
          "authors": [
            "James Pengelley",
            "P. Whipp",
            "N. Rovis-Hermann"
          ],
          "year": 2023,
          "venue": "Educational Psychology Review",
          "citation_count": 5
        },
        {
          "external_id": "CorpusId:258846153",
          "title": "Human-like problem-solving abilities in large language models using ChatGPT",
          "authors": [
            "G. Orrú",
            "Andrea Piarulli",
            "C. Conversano",
            "A. Gemignani"
          ],
          "year": 2023,
          "venue": "Frontiers in Artificial Intelligence",
          "citation_count": 137
        },
        {
          "external_id": "CorpusId:258570040",
          "title": "Revolutionizing Medical Education: Can ChatGPT Boost Subjective Learning and Expression?",
          "authors": [
            "R. Seetharaman"
          ],
          "year": 2023,
          "venue": "Journal of medical systems",
          "citation_count": 69
        },
        {
          "external_id": "CorpusId:267376872",
          "title": "ChatGPT in Education",
          "authors": [
            "Sayım Aktay",
            "Seçkin Gök",
            "Dilşat Uzunoğlu"
          ],
          "year": 2023,
          "venue": "Türk Akademik Yayınlar Dergisi",
          "citation_count": 44
        }
      ],
      "references_fetched_at": "2025-12-16T21:43:30.878453"
    }
  }
}